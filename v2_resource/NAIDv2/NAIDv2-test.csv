id,title,abstract,pub_year,score_mean,score_weighted,score_median,score_mode,cluster_cat,accept,scores,confs,gpt_kwd,RTS
540805,ChemThinker: Thinking Like a Chemist with Multi-Agent LLMs for Deep Molecular Insights,"Molecular property prediction is vital in drug discovery and cheminformatics, yet many current models lack interpretability, making it difficult for experts to understand the rationale behind predictions. To address this, we introduce ChemThinker, a novel large language models (LLMs) multi-agent framework designed to effectively control the internal representations of concepts and functions within LLMs. ChemThinker emulates the way chemists approach molecular analysis by integrating insights from three perspectives: general molecular properties, data-driven analysis, and task-specific factors. Each perspective uses an agentic approach to stimulate the LLM's internal representations, enabling more targeted and interpretable outputs based on the problem at hand, akin to how stimuli trigger the brain's cognitive processes. By feeding representations from these three perspectives into a simple multi-layer perceptron (MLP), ChemThinker achieves superior performance, significantly outperforming existing baselines across multiple benchmarks. Furthermore, our framework provides interpretable insights into the molecular mechanisms driving the predictions, making it a practical tool for drug discovery and other cheminformatics applications.",2025,0.2727271735537551,0.2722771738065002,0.2666666666666666,0.25,f32606af-dee6-49ce-a688-4f29ddd42f88,0,"[0.0, 0.25, 0.25, 0.5]","[0.95, 1.0, 0.95, 0.95]",ChemThinker,0.25
540884,Decomposition Ascribed Synergistic Learning for Unified Image Restoration,"Learning to restore multiple image degradations within a single model is quite beneficial for real-world applications. Nevertheless, existing works typically concentrate on regarding each degradation independently, while their relationship has been less comprehended to ensure the synergistic learning. To this end, we revisit the diverse degradations through the lens of singular value decomposition, with the observation that the decomposed singular vectors and singular values naturally undertake the different types of degradation information, dividing various restoration tasks into two groups, \ie, singular vector dominated and singular value dominated. The above analysis renders a more unified perspective to ascribe diverse degradation connections, compared to previous task-level independent learning. The dedicated optimization of degraded singular vectors and singular values inherently utilizes the potential partnership among diverse restoration tasks, attributing to the Decomposition Ascribed Synergistic Learning (DASL). Specifically, DASL comprises two effective operators, namely, Singular VEctor Operator (SVEO) and Singular VAlue Operator (SVAO), to favor the decomposed optimization, which can be lightly integrated into existing image restoration backbone. Moreover, the congruous decomposition loss has been devised for auxiliary. Extensive experiments on five image restoration tasks demonstrate the effectiveness of our method.",2025,0.5113634504132908,0.5074255430840509,0.5333333333333333,0.5,e9edf7f1-d157-4f98-b2a9-c3edc32b78e3,0,"[0.25, 0.5, 0.5, 0.625]","[1.0, 0.95, 0.95, 0.95]",Decomposition Ascribed Synergistic Learning,0.4513530927835051
540885,Towards Realistic Long-tailed Semi-supervised Learning in an Open World,"Open-world long-tailed semi-supervised learning (OLSSL) has increasingly attracted attention. However, existing OLSSL algorithms generally assume that the distributions between known and novel categories are nearly identical. Against this backdrop, we construct a more Realistic Open-world Long-tailed Semi-supervised Learning (ROLSSL) setting where there is no premise on the distribution relationships between known and novel categories. Furthermore, even within the known categories, the number of labeled samples is significantly smaller than that of the unlabeled samples, as acquiring valid annotations is often prohibitively costly in the real world. Under the proposed ROLSSL setting, we propose a simple yet potentially effective solution called dual-stage post-hoc logit adjustments. The proposed approach revisits the logit adjustment strategy by considering the relationships among the frequency of samples, the total number of categories, and the overall size of data. Then, it estimates the distribution of unlabeled data for both known and novel categories to dynamically readjust the corresponding predictive probabilities, effectively mitigating category bias during the learning of known and novel classes with more selective utilization of imbalanced unlabeled data. Extensive experiments on datasets such as CIFAR100 and ImageNet100 have demonstrated performance improvements of up to 50.1%, validating the superiority of our proposed method and establishing a strong baseline for this task. For further researches, the experimental code will be open soon.",2025,0.3818180429752571,0.3811880433291003,0.2666666666666666,0.25,68631335-102b-4ac5-8b4b-b6fb736bcf3b,0,"[0.25, 0.25, 0.25, 0.5, 0.5]","[0.9, 1.0, 0.95, 0.95, 0.95]",Realistic Open-world Long-tailed Semi-supervised Learning,0.3476969290185967
540894,Proof Search Augmented Language Models,"Transformer language models (TLMs) exhibit an impressively general range of capabilities. A growing body of work aims to harness these models for complex reasoning problems expressed in natural language. However, recent theoretical and empirical results have revealed limits to the algorithmic generalization of TLM reasoning. Transformers trained to solve deduction problems from one distribution fail to solve instances of the same problem type drawn from other distributions. We propose to improve the systematic reasoning capabilities of TLMs via a differentiable proof search module, yielding proof-search augmented language models (PSALMs).
In a PSALM, a Transformer is responsible for predicting rule and fact representations for a neural theorem prover (NTP). The NTP performs a backward-chaining search over proofs, scoring them based on a soft unification operation. Our results show that PSALMs successfully generalize in deduction tasks where vanilla transformers do not learn systematic behavior, can be adapted to more natural text with only label supervision, and robustly handle large examples where proprietary LLMs make mistakes.",2025,0.5795452438017296,0.5790431526647223,0.5333333333333333,0.5,df3b381e-093e-44e6-a4ab-097d6f183dd9,0,"[0.5, 0.5, 0.5, 0.625]","[0.9, 0.95, 0.95, 0.95]",Proof-search augmented language models,0.5331365628042843
540896,Train once and generalize: Zero-shot quantum state preparation with RL,"Quantum state preparation forms an essential cornerstone of quantum information science and quantum algorithms. Designing efficient and scalable methods for approximate state preparation on near-term quantum devices remains a significant challenge, with worst-case hardness results compounding this difficulty. In this work, we propose a deep reinforcement learning framework for quantum state preparation, capable of immediate inference of arbitrary stabilizer states at a fixed system size post a training phase. Our approach scales substantially beyond previous works by leveraging a novel reward function. In our experiments on stabilizer states up to nine qubits, our trained agent successfully prepares nearly all previously unseen states, despite being trained on less than $10^{-3}$\% of the state space -- demonstrating significant generalization to novel states. Benchmarking shows our model produces stabilizer circuits with size $60$\% that of existing algorithms, setting a new state of the art in circuit efficiency. Furthermore, we show that this performance advantage is consistent across states with varying entanglement content. We also analyze the rate of increase of entanglement entropy across the prepared circuit, obtaining insight into the quantum entanglement dynamics generated by our trained agent. Finally, we prove our agent generalizes to (almost) the entire space of stabilizer states.",2025,0.4545456528924899,0.4510325839539438,0.4,0.25,566c6e23-ff9d-49b7-abe5-ab9d9806ee1a,0,"[0.25, 0.25, 0.25, 0.5, 0.625, 0.625]","[1.0, 0.95, 0.95, 1.0, 0.9, 0.95]",Reinforcement Learning,0.4048705013959363
540913,Group Ligands Docking to Protein Pockets,"Molecular docking is a key task in computational biology that has attracted increasing interest from the machine learning community. While existing methods have achieved success, they generally treat each protein-ligand pair in isolation. Inspired by the biochemical observation that ligands binding to the same target protein tend to adopt similar poses, we propose \textsc{GroupBind}, a novel molecular docking framework that simultaneously considers multiple ligands docking to a protein. This is achieved by introducing an interaction layer for the group of ligands and a triangle attention module for embedding protein-ligand and group-ligand pairs. By integrating our approach with diffusion based docking model, we set a new state-of-the-art performance on the PDBBind blind docking benchmark, demonstrating the effectiveness of our paradigm in enhancing molecular docking accuracy.",2025,0.7840906239670459,0.7761663809571523,0.8,0.875,ef27f568-615b-42f3-ba41-5af8e6a1141c,1,"[0.5, 0.625, 0.875, 0.875]","[1.0, 1.0, 0.9, 0.95]",GroupBind,0.6865814934949512
540918,Foundation of Scalable Constraint Learning from Human Feedback,"Constraint learning from human feedback (CLHF) has garnered significant interest in the domain of safe reinforcement learning (RL) due to the challenges associated with designing constraints that elicit desired behaviors. However, a comprehensive theoretical analysis of CLHF is still missing. This paper addresses this gap by establishing a theoretical foundation. Concretely, trajectory-wise feedback, which is the most natural form of feedback, is shown to be helpful only for learning chance constraints. Building on this insight, we propose and theoretically analyze algorithms for CLHF and for solving chance constrained RL problems. Our algorithm is empirically shown to outperform an existing algorithm.",2025,0.4090907603306326,0.4084157607097504,0.4,0.25,d9c8cf54-964a-4b34-97e4-7f8f08009d2e,0,"[0.25, 0.25, 0.5, 0.5]","[0.9, 0.95, 0.95, 0.9]",Constraint Learning from Human Feedback,0.375
540921,TimeAutoDiff: Generation of Heterogeneous Time Series Data via Latent Diffusion Model,"In this paper, we leverage the power of latent diffusion models to generate synthetic time series tabular data.
Along with the temporal and feature correlations, the heterogeneous nature of the feature in the table has been one of the main obstacles in time series tabular data modeling. 
We tackle this problem by combining the ideas of the variational auto-encoder (VAE) and the denoising diffusion probabilistic model (DDPM).
Our model named as \texttt{TimeAutoDiff} has several key advantages including 
(1) \textit{\textbf{Generality}}: the ability to handle the broad spectrum of time series tabular data with heterogeneous, continuous only, or categorical only features; 
(2) \textit{\textbf{Fast sampling speed}}: entire time series data generation as opposed to the sequential data sampling schemes implemented in the existing diffusion-based models, eventually leading to significant improvements in sampling speed, 
(3) \textit{\textbf{Time varying metadata conditional generation}}: the implementation of time series tabular data generation of heterogeneous outputs conditioned on heterogenous, time varying features, enabling scenario exploration across multiple scientific and engineering domains.
(4) \textit{\textbf{Good fidelity and utility guarantees}}: numerical experiments on eight publicly available datasets demonstrating significant improvements over state-of-the-art models in generating time series tabular data, across four metrics measuring fidelity and utility; 
Codes for model implementations are available at the supplementary materials.",2025,0.2727271735537551,0.2885321210827483,0.2666666666666666,0.25,f583bfe8-00af-4647-a643-b3a26010379a,0,"[0.0, 0.25, 0.25, 0.5]","[0.7, 0.8, 0.95, 0.9]",TimeAutoDiff,0.2902233848546857
540926,Deep Kernel Relative Test for Machine-generated Text Detection,"Recent studies demonstrate that two-sample test can effectively detect machine-generated texts (MGTs) with excellent adaptation ability to texts generated by newer LLMs.  However, two-sample test-based detection relies on the assumption that human-written texts (HWTs) must follow the distribution of seen HWTs. As a result, it tends to make mistakes in identifying HWTs that deviate from the seen HWT distribution, limiting their use in sensitive areas like academic integrity verification. To address this issue, we propose to employ non-parametric kernel relative test to detect MGTs by testing whether it is statistically significant that the distribution of a text to be tested is closer to the distribution of HWTs than to the MGTs' distribution.  We further develop a kernel optimisation algorithm in relative test to select the best kernel that can enhance the testing capability for MGT detection. As relative test does not assume that a text to be tested must belong exclusively to either MGTs or HWTs, relative test can largely reduce the false positive error compared to two-sample test, offering significant advantages in practice. Extensive experiments demonstrate the superior performance of our method, compared to state-of-the-art non-parametric and parametric detectors. The code and demo are available: https://github.com/xLearn-AU/R-Detect.",2025,0.6818179338843877,0.6806929345162506,0.6666666666666666,0.625,813bdd3b-cfb7-4842-ba64-30940ef9fc34,1,"[0.625, 0.625, 0.625]","[0.9, 1.0, 0.9]",Kernel Relative Test,0.6250000000000001
540939,DAViD: Domain Adaptive Visually-Rich Document Understanding with Synthetic Insights,"Visually-Rich Documents (VRDs), encompassing elements like charts, tables, and references, convey complex information across various fields. However, extracting information from these rich documents is labor-intensive, especially given their inconsistent formats and domain-specific requirements. While pretrained models for VRD Understanding have progressed, their reliance on large, annotated datasets limits scalability. This paper introduces the Domain Adaptive Visually-rich Document Understanding (DAViD) framework, which utilises machine-generated synthetic data for domain adaptation. DAViD integrates fine-grained and coarse-grained document representation learning and employs synthetic annotations to reduce the need for costly manual labelling. By leveraging pretrained models and synthetic data, DAViD achieves competitive performance with minimal annotated datasets. Extensive experiments validate DAViD’s effectiveness, demonstrating its ability to efficiently adapt to domain-specific VRDU tasks.",2025,0.4090907603306326,0.410231304904692,0.4,0.25,ff572331-ffa6-4111-baa5-97f422bdecb8,0,"[0.25, 0.25, 0.5, 0.5]","[0.95, 0.9, 0.95, 0.95]",DAViD,0.3825462512171373
540946,Four eyes see more than two: Dataset Distillation with Mixture-of-Experts,"The ever-growing size of datasets in deep learning presents a significant challenge in terms of training efficiency and computational cost. Dataset distillation (DD) has emerged as a promising approach to address this challenge by generating compact synthetic datasets that retain the essential information of the original data. However, existing DD methods often suffer from performance degradation when transferring distilled datasets across different network architectures (i.e. the model utilizing distilled dataset for further training is different from the one used in dataset distillation). To overcome this limitation, we propose a novel mixture-of-experts framework for dataset distillation. Our goal focuses on promoting diversity within the distilled dataset by distributing the distillation tasks to multiple expert models. Each expert specializes in distilling a distinct subset of the dataset, encouraging them to capture different aspects of the original data distribution. To further enhance diversity, we introduce a distance correlation minimization strategy to encourage the experts to learn distinct representations. Moreover, during the testing stage (where the distilled dataset is used for training a new model), the mixup-based fusion strategy is applied to better leverage the complementary information captured by each expert. Through extensive experiments, we demonstrate that our framework effectively mitigates the issue of cross-architecture performance degradation in dataset distillation, particularly in low-data regimes, leading to more efficient and versatile deep learning models while being trained upon the distilled dataset.",2025,0.5454543471075102,0.5445543476130005,0.5333333333333333,0.5,9c7007f0-a940-415f-ac9e-850cfc7fb2ec,0,"[0.5, 0.5, 0.5, 0.5]","[0.95, 0.95, 1.0, 0.95]",Mixture-of-Experts,0.5
540947,SpaceSet: A Large-scale Realistic Space-based Image Dataset for Space Situational Awareness,"Space situational awareness (SSA) plays an imperative role in maintaining safe space operations, especially given the increasingly congested space traffic around Earth. Space-based SSA offers a flexible and lightweight solution compared to traditional ground-based SSA. With advanced machine learning approaches, space-based SSA can extract features from high-resolution images in space to detect and track resident space objects (RSOs). However, existing spacecraft image datasets, such as SPARK, fall short of providing realistic camera observations, rendering the derived algorithms unsuitable for real SSA systems. In this research, we introduce SpaceSet, a large-scale realistic space-based image dataset for SSA. We consider accurate space orbit dynamics and a physical camera model with various noise distributions, generating images at the photon level. To extend the available observation window, four overlapping cameras are simulated with a fixed rotation angle. SpaceSet includes images of RSOs observed from $19 km$ to $63,000 km$, captured by a tracker operating in LEO, MEO, and GEO orbits over a period of $5,000$ seconds. Each image has a resolution of $4418 \times 4418$ pixels, providing detailed features for developing advanced SSA approaches. We split the dataset into three subsets: SpaceSet-100, SpaceSet-5000, and SpaceSet-full, catering to various image processing applications. The SpaceSet-full corpus includes a comprehensive data-loader with $781.5GB$ of images and $25.9MB$ of ground truth labels. We also benchmark detection and tracking algorithms on the SpaceSet-100 dataset using a specified splitting method to accelerate the training process.",2025,0.443181657024852,0.4632774330143697,0.5333333333333333,0.5,cb1b5387-06a0-4aa2-b7b5-544a1793d8ac,0,"[0.0, 0.5, 0.5, 0.625]","[0.7, 0.95, 0.9, 0.8]",SpaceSet,0.4570700694954433
540960,Adaptive Vision Encoders: Balancing Efficiency and Robustness in Vision-Language Models,"Vision-language models (VLMs) demonstrate impressive capabilities in visual question answering and image captioning, acting as a crucial link between visual and language modalities. However, existing open-source VLMs rely heavily on pretrained vision encoders, such as CLIP. Despite CLIP’s robustness across diverse domains, it still exhibits significant image understanding errors. These errors propagate to the VLM responses, resulting in sub-optimal performance. In our work, we propose an efficient and robust method for updating vision encoders within VLMs. Our approach selectively and locally updates the model parameters, leading to substantial performance improvements on data where previous mistakes occurred, while maintaining overall robustness. We demonstrate the effectiveness of our method during offline and continual few-shot updates, simulating a model editing regime for VLMs. While our method also scales efficiently and effectively to adapting the language model (LLM) component of the VLM, we show that separately updating the vision encoder can be a very efficient alternative. This approach improves VLM performance with less than 10x the compute resources required for updating the LLM. Our method is also supported by theoretical justifications on the parameter selection strategy.",2025,0.2045453801653163,0.2060473849411118,0.2666666666666666,0.25,0d7a6b0d-5fd8-417e-a294-356750d2f438,0,"[0.0, 0.25, 0.25, 0.25]","[0.9, 0.95, 0.95, 0.9]",Adaptive Vision Encoders,0.1955310881037645
540968,Distilling an End-to-End Voice Assistant Without Instruction Training Data,"Voice assistants, such as Siri and Google Assistant, typically model audio and text separately, resulting in lost speech information and increased complexity. Recent efforts to address this with end-to-end Speech Large Language Models (LLMs) trained with supervised finetuning (SFT) 
 have led to models ``forgetting"" capabilities from text-only LLMs. Our work proposes an alternative paradigm for training Speech LLMs without instruction data, using the response of a text-only LLM to transcripts as self-supervision. Importantly, this process can be performed without annotated responses. We show that our Distilled Voice Assistant (DiVA) generalizes to Spoken Question Answering, Classification, and Translation. Furthermore, we show that DiVA better meets user preferences, achieving a 72\% win rate compared with state-of-the-art models like Qwen 2 Audio, despite using $>$100x less training compute.",2025,0.5113634504132908,0.5105197008871879,0.5333333333333333,0.5,ae1b490a-6011-4978-a008-0052c0402d4c,0,"[0.25, 0.5, 0.5, 0.625]","[0.95, 0.95, 0.95, 0.95]",Distilled Voice Assistant,0.4687499999999999
540985,Collaborative Hybrid Propagator for Temporal Misalignment in Audio-Visual Segmentation,"Audio-visual video segmentation (AVVS) aims to generate pixel-level maps of sound-producing objects that accurately align with the corresponding audio. However, existing methods often face temporal misalignment, where audio cues and segmentation results are not temporally coordinated. Audio provides two critical pieces of information: i) target object-level details and ii) the timing of when objects start and stop producing sounds. Current methods focus more on object-level information but neglect the boundaries of audio semantic changes, leading to temporal misalignment. To address this issue, we propose a Collaborative Hybrid Propagator Framework~(Co-Prop). This framework includes two main steps: Preliminary Audio Boundary Anchoring and Frame-by-Frame Audio-Insert Propagation. To Anchor the audio boundary, we employ retrieval-assist prompts with Qwen large language models to identify control points of audio semantic changes. These control points split the audio into semantically consistent audio portions. After obtaining the control point lists, we propose the Audio Insertion Propagator to process each audio portion using a frame-by-frame audio insertion propagation and matching approach. We curated a compact dataset comprising diverse source conversion cases and devised a metric to assess alignment rates. Compared to traditional simultaneous processing methods, our approach reduces memory requirements and facilitates frame alignment. Experimental results demonstrate the effectiveness of our approach across three datasets and two backbones. Furthermore, our method can be integrated with existing AVVS approaches, offering plug-and-play functionality to enhance their performance.",2025,0.5727270644628857,0.5760807770137074,0.5333333333333333,0.5,27041fe2-c5b8-4f28-a5c6-dedc1d6fe3cf,0,"[0.25, 0.5, 0.5, 0.5, 0.875]","[0.95, 0.95, 0.95, 0.9, 1.0]",Collaborative Hybrid Propagator,0.5497472762562694
540993,Mechanism design with multi-armed bandit,"A popular approach of automated mechanism design is to formulate a linear program (LP) whose solution gives a mechanism with desired properties.  We analytically derive a class of optimal solutions for such an LP that gives mechanisms achieving standard properties of efficiency, incentive compatibility, strong budget balance (SBB), and individual rationality (IR), where SBB and IR are satisfied in expectation.  Notably, our solutions are represented by an exponentially smaller number of essential variables than the original variables of LP.  Our solutions, however, involve a term whose exact evaluation requires solving a certain optimization problem exponentially many times as the number of players grows.  We thus evaluate this term by modeling it as the problem of estimating the mean reward of the best arm in multi-armed bandit (MAB), propose a Probably and Approximately Correct estimator, and prove its asymptotic optimality by establishing a lower bound on its sample complexity.  This MAB approach reduces the number of times the optimization problem is solved from exponential to linear.  Numerical experiments show that the proposed approach finds mechanisms that are guaranteed to achieve desired properties with high probability for environments with up to 128 players, which substantially improves upon the prior work.",2025,0.4999994545456529,0.4900989128517004,0.5333333333333333,0.25,954bb084-17e0-4668-b3b3-b3a4cbeddd34,0,"[0.25, 0.5, 0.625]","[0.9, 0.8, 0.8]",Multi-armed bandit,0.4263734128012437
541005,Enhancing Certified Robustness via Block Reflector Orthogonal Layers,"Lipschitz neural networks are well-known for providing certified robustness in deep learning. In this paper, we present a novel efficient Block Reflector Orthogonal layer that enables the construction of simple yet effective Lipschitz neural networks. 
In addition, by theoretically analyzing the nature of Lipschitz neural networks, we introduce a new loss function that employs an annealing mechanism to improve margin for most data points.
This enables Lipschitz models to provide better certified robustness.
By employing our BRO layer and loss function, we design BRONet, which provides state-of-the-art certified robustness.	
Extensive experiments and empirical analysis on CIFAR-10, CIFAR-100, and Tiny-ImageNet validate that our method outperforms existing baselines.",2025,0.5181816297521347,0.5082510874750322,0.6666666666666666,0.625,db84dfa7-4414-4c2c-920f-cf37d20e7ed9,0,"[0.25, 0.25, 0.625, 0.625, 0.625]","[0.95, 0.95, 0.8, 0.9, 0.9]",Block Reflector Orthogonal,0.4395782860146433
541012,Adversarial Perturbations Cannot Reliably Protect Artists From Generative AI,"Artists are increasingly concerned about advancements in image generation models that can closely replicate their unique artistic styles.
In response, several protection tools against style mimicry have been developed that incorporate small adversarial perturbations into artworks published online. In this work, we evaluate the effectiveness of popular protections---with millions of downloads---and show they only provide a false sense of security. We find that low-effort and ""off-the-shelf"" techniques, such as image upscaling, are sufficient to create robust mimicry methods that significantly degrade existing protections. Through a user study, we demonstrate that **all existing protections can be easily bypassed**, leaving artists vulnerable to style mimicry.  We caution that tools based on adversarial perturbations cannot reliably protect artists from the misuse of generative AI, and urge the development of alternative protective solutions.",2025,0.886363314049704,0.8849008148711258,0.9333333333333332,0.875,98a9b60d-88e1-4926-a751-5b7de97e40c0,1,"[0.625, 0.875, 0.875, 0.875]","[0.95, 0.95, 0.95, 0.95]",Adversarial Perturbations,0.8124999999908489
541021,Towards Human-like Virtual Beings: Simulating Human Behavior in 3D Scenes,"Building autonomous agents that can replicate human behavior in the realistic 3D world is a key step toward artificial general intelligence. This requires agents to be holistic goal achievers and to naturally adapt to environmental dynamics. In this work, we introduce ACTOR, an agent capable of performing high-level, long-horizon, abstract goals in 3D households, guided by its internal value similar to those of humans. ACTOR operates in a perceive-plan-act cycle, extending the ungrounded, scene-agnostic LLM controller with deliberate goal decomposition and decision-making through actively searching the behavior space, generating activity choices based on a hierarchical prior, and evaluating these choices using customizable value functions to determine the subsequent steps. Furthermore, we introduce BehaviorHub, a large-scale human behavior simulation dataset in scene-aware, complicated tasks. Considering the unaffordable acquisition of human-authored 3D human behavior data, we construct BehaviorHub by exploring the commonsense knowledge of LLMs learned from large corpora, and automatically aligning motion resources with 3D scene for knowledgeable generation. Extensive experiments on our established benchmark demonstrate that the proposed architecture leads to effective behavior planning and simulation. BehaviorHub also proves beneficial for downstream task development. Our code and dataset will be publicly released.",2025,0.4772725537190714,0.4783245587476121,0.5333333333333333,0.5,3a87156a-cf58-4153-88ca-5b6ac3c8e7d8,0,"[0.25, 0.5, 0.5, 0.5]","[0.9, 0.95, 0.95, 0.9]",ACTOR,0.4455310880829015
541049,SEAL: Scaling to Emphasize Attention for Long-Context Retrieval,"In this work, we introduce a novel approach called Scaling to Emphasize Attention for Long-context retrieval (SEAL), which enhances the retrieval performance of large language models (LLMs) over extended contexts. Previous studies have shown that each attention head in LLMs has a unique functionality and collectively contributes to the overall behavior of the model. Similarly, we observe that specific heads are closely tied to long-context retrieval, showing positive or negative correlation with retrieval scores. Built on this insight, we propose a learning-based mechanism using zero-shot generated data to emphasize these heads, improving the model's performance in long-context retrieval tasks. 
By applying SEAL, we can achieve significant improvements in in-domain retrieval performance, including document QA tasks from LongBench, and considerable improvements in out-of-domain cases.
Additionally, when combined with existing training-free context extension techniques, SEAL extends the context limits of LLMs while maintaining highly reliable outputs, opening new avenues for research in this field.",2025,0.5795452438017296,0.5785889943388131,0.5333333333333333,0.5,07254413-5ded-40ef-950a-5b47a32c6ca1,0,"[0.5, 0.5, 0.5, 0.625]","[0.95, 0.95, 0.95, 0.95]",SEAL,0.53125
541052,Pacmann: Efficient Private Approximate Nearest Neighbor Search,"We propose a new private Approximate Nearest Neighbor (ANN) search scheme
named Pacmann
that allows a client to perform ANN search
in a vector database 
without revealing the query vector to the server.
Unlike prior constructions that run encrypted search on the server side,
Pacmann carefully offloads limited computation and storage to the client,
no longer requiring computationally-intensive cryptographic techniques.
Specifically, clients run a graph-based ANN search, where in each hop on the graph, the client privately retrieves local graph information from the server. 
To make this efficient, we combine two ideas: 
(1) we adapt a leading graph-based ANN search algorithm to be compatible with private information retrieval (PIR) for subgraph retrieval;
(2) we use a recent class of PIR schemes that trade offline preprocessing for online computational efficiency. 
Pacmann achieves significantly better search quality than
the state-of-the-art private ANN search schemes,
showing up to 2.5$\times$ better search accuracy on 
real-world datasets than prior work and
reaching 90\% quality of a state-of-the-art 
non-private ANN algorithm.
Moreover on large datasets with up to 100 million vectors,
Pacmann shows better scalability 
than prior private ANN schemes
with up to 62\% reduction in computation time
and 22\% reduction in overall latency.",2025,0.7499997272728265,0.7556682662043038,0.6666666666666666,0.625,b919e0e9-77bc-42ec-97da-04f686e8d398,1,"[0.625, 0.625, 0.625, 0.875]","[0.8, 0.9, 0.8, 0.95]",Pacmann,0.7151595203966565
541058,ReCogLab: a framework testing relational reasoning & cognitive hypotheses on LLMs,"A fundamental part of human cognition is the ability to not only recall previous memories, but also reason across them to draw conclusions. In cognitive science and psychology, this is termed relational reasoning and a number of effects and biases have been observed in human cognition. Designing experiments to measure these reasoning effects is effortful and does not transfer easily to analyzing language model reasoning patterns. To make exploring language models on relational reasoning easier, we introduce ReCogLab – a generative framework for constructing reasoning examples. Unlike static datasets, our framework has a number of benefits that help us in our goal of flexible evaluation of LLMs. First, our framework allows us to control the difficulty and context-length of the problem, allowing us to scale with model capability and evaluate LLMs at a variety of scales. Second, the ability to change the configuration of a dataset dynamically allows us to probe models on different aspects and capabilities. Finally, the flexibility of our approach enables the recreation of classic cognitive science experiments and the systematic study of relational reasoning biases in language models. We demonstrate several such experiments and present our findings on a wide variety of open and closed-source language models. We release all data and code at https://github.com/google-deepmind/recoglab.",2025,0.5454543471075102,0.5481843468941888,0.6,0.625,732aa2ca-2716-4a0f-82af-57b8e9eeac45,1,"[0.25, 0.5, 0.625, 0.625]","[0.9, 0.95, 0.95, 0.95]",ReCogLab,0.5150925024342745
541070,Bayesian Learning of Adaptive Koopman Operator with Application to Robust Motion Planning for Autonomous Trucks,"Koopman theory has recently been shown to enable an efficient data-driven approach for modeling physical systems, offering a linear framework despite underlying nonlinear dynamics. It is, however, not clear how to account for uncertainty or temporal distributional shifts within this framework, both commonly encountered in real-world autonomous driving with changing weather conditions and time-varying vehicle dynamics. In this work, we introduce BLAK, Bayesian Learning of Adaptive Koopman operator to address these limitations. Specifically, we propose a Bayesian Koopman operator that incorporates uncertainty quantification, enabling more robust predictions. To tackle distributional shifts, we propose an online adaptation mechanism, ensuring the operator remains responsive to changes in system dynamics. Additionally, we apply the architecture to motion planning and show that it gives fast and precise predictions. By leveraging uncertainty awareness and real-time updates, our planner generates dynamically accurate trajectories and makes more informed decisions. We evaluate our method on real-world truck dynamics data under varying weather conditions—such as wet roads, snow, and ice—where uncertainty and dynamic shifts are prominent, as well as in other simulated environments. The results demonstrate our method’s ability to deliver accurate, uncertainty-aware open-loop predictions for dynamic systems.",2025,0.6590910330578063,0.670608877107153,0.6,0.5,1d2fbcca-997b-4158-9317-fba3c8ed0534,0,"[0.25, 0.5, 0.5, 0.625, 0.875, 0.875]","[0.8, 0.9, 0.9, 0.9, 0.95, 0.95]",BLAK,0.649770376254645
541098,RDT-1B: a Diffusion Foundation Model for Bimanual Manipulation,"Bimanual manipulation is essential in robotics, yet developing foundation models is extremely challenging due to the inherent complexity of coordinating two robot arms (leading to multi-modal action distributions) and the scarcity of training data. In this paper, we present the Robotics Diffusion Transformer (RDT), a pioneering diffusion foundation model for bimanual manipulation. RDT builds on diffusion models to effectively represent multi-modality, with innovative designs of a scalable Transformer to deal with the heterogeneity of multi-modal inputs and to capture the nonlinearity and high frequency of robotic data. To address data scarcity, we further introduce a Physically Interpretable Unified Action Space, which can unify the action representations of various robots while preserving the physical meanings of original actions, facilitating learning transferrable physical knowledge. With these designs, we managed to pre-train RDT on the largest collection of multi-robot datasets to date and scaled it up to $1.2$B parameters, which is the largest diffusion-based foundation model for robotic manipulation. We finally fine-tuned RDT on a self-created multi-task bimanual dataset with over $6$K+ episodes to refine its manipulation capabilities. Experiments on real robots demonstrate that RDT significantly outperforms existing methods. It exhibits zero-shot generalization to unseen objects and scenes, understands and follows language instructions, learns new skills with just 1$\sim$5 demonstrations, and effectively handles complex, dexterous tasks. We refer to https://rdt-robotics.github.io/rdt-robotics/ for the code and videos.",2025,0.8181815206612653,0.8168315214195008,0.8,0.625,65beaf2a-41cb-4209-af90-e5401b0a0e10,1,"[0.625, 0.625, 0.875, 0.875]","[0.95, 0.9, 0.95, 0.9]",Robotics Diffusion Transformer,0.7499999999999999
541113,Towards Efficient LLM Grounding for Embodied Multi-Agent Collaboration,"Grounding the reasoning ability of large language models (LLMs) for embodied tasks is challenging due to the complexity of the physical world. Especially, LLM planning for multi-agent collaboration requires communication of agents or credit assignment as the feedback to re-adjust the proposed plans and achieve effective coordination. However, existing methods that overly rely on physical verification or self-reflection suffer from excessive and inefficient querying of LLMs. In this paper, we propose a novel framework for multi-agent collaboration that introduces Reinforced Advantage feedback (ReAd) for efficient self-refinement of plans. Specifically, we perform critic regression to learn a sequential advantage function from LLM-planned data, and then treat the LLM planner as an optimizer to generate actions that maximize the advantage function. It endows the LLM with the foresight to discern whether the action contributes to accomplishing the final task. We provide theoretical analysis by extending advantage-weighted regression in reinforcement learning to multi-agent systems. Experiments on Overcooked-AI and a difficult variant of RoCoBench show that ReAd surpasses baselines in success rate, and also significantly decreases the interaction steps of agents and query rounds of LLMs, demonstrating its high efficiency for grounding LLMs. More results are given at \url{https://read-llm.github.io/}.",2025,0.5454543471075102,0.5427388034180588,0.6,0.625,bf71c2e0-053e-4fbb-a655-2dd8103acb59,0,"[0.25, 0.5, 0.625, 0.625]","[0.95, 0.95, 0.9, 0.95]",Reinforced Advantage Feedback,0.4924537487828628
541116,PIG: Physics-Informed Gaussians as Adaptive Parametric Mesh Representations,"The numerical approximation of partial differential equations (PDEs) using neural networks has seen significant advancements through Physics-Informed Neural Networks (PINNs). Despite their straightforward optimization framework and flexibility in implementing various PDEs, PINNs often suffer from limited accuracy due to the spectral bias of Multi-Layer Perceptrons (MLPs), which struggle to effectively learn high-frequency and nonlinear components. Recently, parametric mesh representations in combination with neural networks have been investigated as a promising approach to eliminate the inductive bias of MLPs. However, they usually require high-resolution grids and a large number of collocation points to achieve high accuracy while avoiding overfitting. In addition, the fixed positions of the mesh parameters restrict their flexibility, making accurate approximation of complex PDEs challenging. To overcome these limitations, we propose Physics-Informed Gaussians (PIGs), which combine feature embeddings using Gaussian functions with a lightweight neural network. Our approach uses trainable parameters for the mean and variance of each Gaussian, allowing for dynamic adjustment of their positions and shapes during training. This adaptability enables our model to optimally approximate PDE solutions, unlike models with fixed parameter positions. Furthermore, the proposed approach maintains the same optimization framework used in PINNs, allowing us to benefit from their excellent properties. Experimental results show the competitive performance of our model across various PDEs, demonstrating its potential as a robust tool for solving complex PDEs. Our project page is available at https://namgyukang.github.io/Physics-Informed-Gaussians/",2025,0.7499997272728265,0.7515590590972161,0.6666666666666666,0.625,a68d1900-0862-44cf-a903-0b7f25fcbfb2,1,"[0.625, 0.625, 0.625, 0.875]","[0.95, 0.95, 0.8, 0.95]",Physics-Informed Gaussians,0.696422501699524
541121,Judging the Judges: A Systematic Investigation of Position Bias in Pairwise Comparative Assessments by LLMs,"LLM-as-a-Judge presents a promising alternative to human evaluators across various tasks, but inherent biases, especially position bias — a tendency to favor solutions based on their position in the prompt — have compromised its effectiveness. Our study introduces a systematic framework to examine position bias in pairwise comparisons, focusing on repetition stability, position consistency, and preference fairness. This research significantly contributes to the field by introducing new concepts for understanding position bias and providing a multi-dimensional framework for evaluations. We conducted experiments with 12 LLM judges across MTBench and DevBench, covering 22 tasks and approximately 40 solution-generating models — candidates, resulting in over 100,000 evaluation instances. Our findings confirm that position bias in capable LLM judges is not due to random chances, along with notable variations observed across judges and tasks. Moreover, position bias is weakly influenced by the length of prompt components but significantly impacted by the quality gap between solutions. These insights can help optimize judge model selections, improve benchmark design, and inform future research on debiasing strategies, ultimately enhancing the reliability of LLM judges.",2025,0.4090907603306326,0.4028210093423744,0.4,0.25,5739ee26-dd5b-449f-84a1-7c62cffc2f5d,0,"[0.25, 0.25, 0.5, 0.5]","[0.95, 0.95, 0.95, 0.8]",Position Bias,0.3571549966009518
541126,EmbodiedCity: A Benchmark Platform for Embodied Agent in Real-world City Environment,"Embodied artificial intelligence (EmbodiedAI) emphasizes the role of an agent's body in generating human-like behaviors. The recent efforts on  EmbodiedAI pay a lot of attention to building up machine learning models to possess perceiving, planning, and acting abilities, thereby enabling real-time interaction with the world. However, most works focus on bounded indoor environments, such as navigation in a room or manipulating a device, with limited exploration of embodying the agents in open-world scenarios. That is, embodied intelligence in the open and outdoor environment is less explored, for which one potential reason is the lack of high-quality simulators, benchmarks, and datasets. To address it, in this paper, we construct a benchmark platform for embodied intelligence evaluation in real-world city environments. Specifically, we first construct a highly realistic 3D simulation environment based on the real buildings, roads, and other elements in a real city. In this environment, we combine historically collected data and simulation algorithms to conduct simulations of pedestrian and vehicle flows with high fidelity. Further, we designed a set of evaluation tasks covering different EmbodiedAI abilities. Moreover, we provide a complete set of input and output interfaces for access, enabling embodied agents to easily take task requirements and current environmental observations as input and then make decisions and obtain performance evaluations. On the one hand, it expands the capability of existing embodied intelligence to higher levels. On the other hand, it has a higher practical value in the real world and can support more potential applications for artificial general intelligence. Based on this platform, we evaluate some popular large language models for embodied intelligence capabilities of different dimensions and difficulties. The executable program of this platform is available for download, and we have also released an easy-to-use Python library and detailed tutorial documents. All of the software, Python library, codes, datasets, tutorials, and real-time online service are available on this anonymous website: https://embodied-ai.city.",2025,0.3409089669421938,0.336764388759527,0.2666666666666666,0.25,947555c5-ca9d-42c0-82b2-2cad6cb09c71,0,"[0.25, 0.25, 0.25, 0.5]","[0.95, 1.0, 0.95, 0.9]",EmbodiedAI,0.2968851909784909
541134,Learning Dynamic 3D Gaussians from Monocular Videos without Camera Poses,"Dynamic scene reconstruction aims to recover the time-varying geometry and appearance of a dynamic scene. Existing methods, however, heavily rely on the existence of multiple-view captures or the accurate camera poses estimated by Structure from Motion (SfM) algorithms. To relax this constraint, we introduce a method capable of reconstructing generic dynamic scenes, from casually captured monocular videos without known camera poses. Unlike recent works that treat static and dynamic content separately, we propose a unified Hexplane-based Gaussian field to capture the complex effects of scene deformation and camera motion. The Hexplane decomposition enables feasible disentanglement for effective optimization. Combined with an efficient camera pose initialization strategy, our approach significantly improves view synthesis quality and camera pose estimation accuracy over previous methods, while enhancing computational efficiency.",2025,0.5795452438017296,0.5781468162085512,0.5333333333333333,0.5,d78f4575-f346-4540-863c-a75ca22d4bcf,0,"[0.25, 0.5, 0.5, 0.875]","[0.95, 1.0, 0.95, 0.95]",Hexplane-based Gaussian field,0.5287647275405007
541145,Imit-Diff: Semantics Guided Diffusion Transformer with Dual Resolution Fusion for Imitation Learning,"Diffusion-based methods have become one of the most important paradigms in the field of imitation learning. However, even in state-of-the-art diffusion-based policies, there has been insufficient focus on semantics and fine-grained feature extraction, resulting in weaker generalization and a reliance on controlled environments. To address this issue, we propose Imit-Diff, which consists of three key components: 1) Dual Resolution Fusion for extracting fine-grained features with a manageable number of tokens by integrating high-resolution features into low-resolution visual embedding through an attention mechanism; 2) Semantics Injection to explicitly incorporate semantic information by using prior masks obtained from open vocabulary models, achieving a world-level understanding of imitation learning tasks; and 3) Consistency Policy on Diffusion Transformer to reduce the inference time of diffusion models by training a student model to implement few-step denoising on the Probability Flow ODE trajectory. Experimental results show that our method significantly outperforms state-of-the-art methods, especially in cluttered scenes, and is highly robust to task interruptions. The code will be publicly available.",2025,0.4090907603306326,0.4029702172336203,0.4,0.25,6fd82841-cfcd-4cc3-b134-70d1ddc7b4cd,0,"[0.25, 0.25, 0.5, 0.5]","[1.0, 0.95, 0.9, 0.9]",Imit-Diff,0.34925977934256
541148,Unlocking the Power of Gradient Guidance for Structure-Based Molecule Optimization,"Structure-based molecule optimization (SBMO) aims to optimize molecules with both continuous coordinates and discrete types against protein targets.
A promising direction is to exert gradient guidance on generative models given its remarkable success in images, but it is challenging to guide discrete data and risks inconsistencies between modalities.
To this end, we leverage a continuous and differentiable space derived through Bayesian inference, presenting Molecule Joint Optimization (MolJO), the first gradient-based SBMO framework that facilitates joint guidance signals across different modalities while preserving SE(3)-equivariance.
We introduce a novel backward correction strategy that optimizes within a sliding window of the past histories, allowing for a seamless trade-off between explore-and-exploit during optimization.
Our proposed MolJO achieves state-of-the-art performance on CrossDocked2020 benchmark (Success Rate 51.3% , Vina Dock -9.05 and SA 0.78), more than 4x improvement in Success Rate compared to the gradient-based counterpart, and 2x ""Me-Better"" Ratio as much as 3D baselines.
Furthermore, we extend MolJO to a wide range of optimization settings, including multi-objective optimization and challenging tasks in drug design such as R-group optimization and scaffold hopping, further underscoring its versatility and potential.",2025,0.5113634504132908,0.5017676234123518,0.5333333333333333,0.5,95daec0c-a6a2-4bed-9b7f-78fc53e88d86,0,"[0.25, 0.5, 0.5, 0.625]","[0.95, 0.8, 0.95, 0.8]",Molecule Joint Optimization,0.4375247819191118
541151,JudgeLM: Fine-tuned Large Language Models are Scalable Judges,"Evaluating Large Language Models (LLMs) in open-ended scenarios is challenging because existing benchmarks and metrics can not measure them comprehensively. To address this problem, we propose to fine-tune LLMs as scalable judges (JudgeLM) to evaluate LLMs efficiently and effectively in open-ended benchmarks. We first propose a comprehensive, large-scale, high-quality dataset containing task seeds, LLMs-generated answers, and GPT-4-generated judgments for fine-tuning high-performance judges, as well as a new benchmark for evaluating the judges. We train JudgeLM at different scales from 7B, 13B, to 33B parameters, and conduct a systematic analysis of its capabilities and behaviors. We then analyze the key biases in fine-tuning LLM as a judge and consider them as position bias, knowledge bias, and format bias. To address these issues, JudgeLM introduces a bag of techniques including swap augmentation, reference support, and reference drop, which clearly enhance the judge's performance. JudgeLM obtains the state-of-the-art judge performance on both the existing PandaLM benchmark and our proposed new benchmark. Our JudgeLM is efficient and the JudgeLM-7B only needs 3 minutes to judge 5K samples with 8 A100 GPUs. JudgeLM obtains high agreement with the teacher judge, achieving an agreement exceeding 90% that even surpasses human-to-human agreement. JudgeLM also demonstrates extended capabilities in being judges of the single answer, multimodal models, multiple answers, multi-turn chat, etc.",2025,0.886363314049704,0.8876235866091907,0.9333333333333332,0.875,54fc6269-da97-44a5-a257-4edc1b9c3093,1,"[0.625, 0.875, 0.875, 0.875]","[0.9, 0.95, 0.95, 0.95]",JudgeLM,0.823819376512218
541170,Elucidating the Design Space of Text-to-Audio Models,"Recent years have seen significant progress in Text-To-Audio (TTA) synthesis, enabling users to enrich their creative workflows with synthetic audio generated from natural language prompts. Despite this progress, the effects of data, model architecture, training objective functions, and sampling strategies on target benchmarks are not well understood. With the purpose of providing a holistic understanding of the design space of TTA models, we setup a large-scale empirical experiment focused on diffusion and flow matching models. Our contributions include: 1) AF-Synthetic, a large dataset of high quality synthetic captions obtained from an audio understanding model; 2) a systematic comparison of different architectural, training, and inference design choices for TTA models; 3) an analysis of sampling methods and their Pareto curves with respect to generation quality and inference speed. We leverage the knowledge obtained from this extensive analysis to propose our best model dubbed Elucidated Text-To-Audio (ETTA). When evaluated on AudioCaps and MusicCaps, ETTA provides improvements over the baselines trained on publicly available data, while being competitive with models trained on proprietary data. Finally, we show ETTA's improved ability to generate creative audio following complex and imaginative captions — a task that is more challenging than current benchmarks.",2025,0.6818179338843877,0.6841835278844499,0.6,0.5,dce80af3-eaa4-4e8c-8359-1470c609ba8d,0,"[0.5, 0.5, 0.625, 0.875]","[0.95, 0.95, 1.0, 1.0]",Elucidated Text-To-Audio,0.6434174624829467
541179,SafeWatch: An Efficient Safety-Policy Following Video Guardrail Model with Transparent Explanations,"With the rise of generative AI and rapid growth of high-quality video generation, video guardrails have become more crucial than ever to ensure safety and security across platforms. Current video guardrails, however, are either overly simplistic, relying on pure classification models trained on simple policies with limited unsafe categories, which lack detailed explanations, or prompting multimodal large language models (MLLMs) with long safety guidelines, which are inefficient and impractical for guardrailing real-world content. To bridge this gap, we propose SafeWatch, an efficient MLLM-based video guardrail model designed to follow customized safety policies and provide multi-label video guardrail outputs with content-specific explanations in a zero-shot manner. In particular, unlike traditional MLLM-based guardrails that encode all safety policies autoregressively, causing inefficiency and bias, SafeWatch uniquely encodes each policy chunk in parallel and eliminates their position bias such that all policies are attended simultaneously with equal importance. In addition, to improve efficiency and accuracy, SafeWatch incorporates a policy-aware visual token pruning algorithm that adaptively selects the most relevant video tokens for each policy, discarding noisy or irrelevant information. This allows for more focused, policy-compliant guardrail with significantly reduced computational overhead. Considering the limitations of existing video guardrail benchmarks, we propose SafeWatch-Bench, a large-scale video guardrail benchmark comprising over 2M videos spanning six safety categories which covers over 30 tasks to ensure a comprehensive coverage of all potential safety scenarios. We have conducted extensive experiments, showing that SafeWatch outperforms all SOTA video guardrails on SafeWatch-Bench by 28.2%, and achieves a 13.6% improvement on existing benchmarks, all while reducing inference costs by an average of 10%. SafeWatch also demonstrates strong policy-following abilities and outperforms previous SOTAs by 5.6% and 15.6% in zero-shot generalizability to new policies and new prompting tasks. Additionally, both LLM-as-a-judge and human evaluators confirm the high quality of the explanations provided by SafeWatch. Our project is open-sourced at https://safewatch-aiguard.github.io.",2025,0.8181815206612653,0.8168315214195008,0.8,0.625,ef48d936-4a36-4ff0-9fb1-0eab8737f6ec,1,"[0.625, 0.625, 0.875, 0.875]","[0.9, 0.9, 0.9, 0.9]",SafeWatch,0.7499999999999957
541180,Revisiting In-context Learning Inference Circuit in Large Language Models,"In-context Learning (ICL) is an emerging few-shot learning paradigm on Language Models (LMs) with inner mechanisms un-explored. There are already existing works describing the inner processing of ICL, while they struggle to capture all the inference phenomena in large language models. Therefore, this paper proposes a comprehensive circuit to model the inference dynamics and try to explain the observed phenomena of ICL. In detail, we divide ICL inference into 3 major operations: (1) Input Text Encode: LMs encode every input text (in the demonstrations and queries) into linear representation in the hidden states with sufficient information to solve ICL tasks. (2) Semantics Merge: LMs merge the encoded representations of demonstrations with their corresponding label tokens to produce joint representations of labels and demonstrations. (3) Feature Retrieval and Copy: LMs search the joint representations of demonstrations similar to the query representation on a task subspace, and copy the searched representations into the query. Then, language model heads capture these copied label representations to a certain extent and decode them into predicted labels. Through careful measurements, the proposed inference circuit successfully captures and unifies many fragmented phenomena observed during the ICL process, making it a comprehensive and practical explanation of the ICL inference process. Moreover, ablation analysis by disabling the proposed steps seriously damages the ICL performance, suggesting the proposed inference circuit is a dominating mechanism. Additionally, we confirm and list some bypass mechanisms that solve ICL tasks in parallel with the proposed circuit.",2025,0.7499997272728265,0.7487622279678756,0.6666666666666666,0.625,3e327d92-e546-4efb-bcbe-37c916e9e8d7,1,"[0.625, 0.625, 0.625, 0.875]","[0.9, 0.9, 0.9, 0.9]",Inference Circuit,0.6875
541238,Discrete Distribution Networks,"We introduce a novel generative model, the Discrete Distribution Networks (DDN), that approximates data distribution using hierarchical discrete distributions. We posit that since the features within a network inherently capture distributional information, enabling the network to generate multiple samples simultaneously, rather than a single output, may offer an effective way to represent distributions. Therefore, DDN fits the target distribution, including continuous ones, by generating multiple discrete sample points. To capture finer details of the target data, DDN selects the output that is closest to the Ground Truth (GT) from the coarse results generated in the first layer. This selected output is then fed back into the network as a condition for the second layer, thereby generating new outputs more similar to the GT. As the number of DDN layers increases, the representational space of the outputs expands exponentially, and the generated samples become increasingly similar to the GT. This hierarchical output pattern of discrete distributions endows DDN with unique properties: more general zero-shot conditional generation and 1D latent representation. We demonstrate the efficacy of DDN and its intriguing properties through experiments on CIFAR-10 and FFHQ. The code is available at https://discrete-distribution-networks.github.io/",2025,0.8181815206612653,0.8168315214195008,0.9333333333333332,0.875,8123db7a-a088-4d6a-8745-9a624b3717b2,1,"[0.5, 0.875, 0.875]","[0.95, 0.95, 0.95]",Discrete Distribution Networks,0.7499999999999901
541240,On the Cost-Effectiveness of Partially-Annotating Methods for Multi-Label Learning,"Precisely annotating instances with multiple labels is costly and has emerged as a significant bottleneck in the real-world multi-label learning tasks. To deal with this problem, the most straightforward strategy is partially-annotating, which aims to reduce the cost by annotating only a subset of labels. Existing works mainly includes label-level partially-annotating (LPA), where each instance is assigned a subset of positive labels, and instance-level partially-annotating (IPA), where all positive labels are assigned to an instance, but only a subset of instances are annotated. However, these methods tend to focus on improving model performance under each type of partial annotation, often neglecting a fundamental question: \textit{which method is the most cost-effective?} In this paper, we empirically evaluate which partially-annotating method achieves better model performance at the same annotation cost. To make a fair comparison, we manually annotated images in the MS-COCO dataset using two partially-annotating methods and recorded their averaging annotation time per image. This allows us to train models on two types of partial annotations with the same annotation cost and to compare their performance. Empirical results show that even when the number of examples annotated with IPA is only one-fifth that of LPA, models trained on IPA annotations significantly outperform those trained on LPA annotations, yielding that IPA is significantly more cost-effective than LPA. To explain the superiority of IPA, our causal reasoning framework shows that compared to LPA, IPA preserves complete co-occurrence relationships, enabling the model to capture correlative patterns, which is useful for improving model performance.",2025,0.613636140495949,0.6126236410646255,0.6,0.25,6e7ecaf4-5ed8-40d3-b0f3-bfd685042db0,0,"[0.25, 0.5, 0.625, 0.875]","[1.0, 1.0, 1.0, 1.0]",Instance-level partially-annotating (IPA),0.5625
541242,Enhancing PPB Affinity Prediction through Data Integration and Feature Alignment: Approaching Structural Model Performance with Sequences,"One key step of protein drug development is the screening of protein-protein binding (PPB) affinity. The current mainstream screening method of PPB affinity is laboratory experiments, which are costly and time-consuming, making it difficult to quickly perform high-throughput screening. Various deep learning methods have been proposed to predict PPB affinity, but they are often limited by the availability of high-quality data and the compatibility of the algorithms with that data. In this work, we developed two AI models, PPBind-3D and PPBind-1D, to predict PPB affinity. PPBind-3D leverages structural information near the protein-protein binding interface to make its predictions. By employing monotonic neural network constrained multi-task learning, we effectively utilized heterogeneous affinity data from diverse wet lab experiments to expand the development dataset to over 23,000 samples, thereby enhancing the model's generalization capabilities. Additionally, PPBind-1D was developed using sequence data to address the lack of structural data in practical applications. During the training of PPBind-1D, we aligned it with PPBind-3D by incorporating an additional 42,108 no-affinity-label samples through an alignment approach. Finally, we demonstrated three application cases of our AI models in the virtual screening of protein drugs, illustrating that our models can significantly facilitate high-throughput screening.",2025,0.4909089123967591,0.489802675286599,0.5333333333333333,0.25,0e4f2e09-8d85-4a1f-8538-5f2307740cf7,0,"[0.25, 0.25, 0.5, 0.625, 0.625]","[0.9, 0.95, 0.9, 0.95, 0.9]",PPBind,0.4486811316741119
541269,Stagewise Development in Transformers and the Geometry of the Loss Landscape,"Deep learning involves navigating a high-dimensional parameter space guided by the loss landscape. In the process, complex computational structures form and re-form inside the neural network, leading to shifts in input--output behavior. It is a priority for the science of deep learning to uncover principles governing the development of neural network structure and behavior. Drawing from the framework of singular learning theory, we propose that model development is governed by the local geometry of the loss landscape. We investigate this link by monitoring the geometry of the loss landscape throughout training for transformers trained as language models or for a synthetic in-context regression task. We divide training into ``developmental stages'' marking discrete shifts in loss landscape geometry. We then confirm that these stages coincide with significant changes in the internal computational structure and the input--output behavior of our models. Our findings provide new insights into transformer development and underscore the potential of a geometric perspective for understanding modern deep learning.",2025,0.613636140495949,0.607691067783947,0.6,0.5,328c798e-4dd2-4255-8949-9914ebd97b33,0,"[0.5, 0.5, 0.625, 0.625]","[0.9, 0.95, 0.8, 0.8]",Loss Landscape Geometry,0.545106577426287
541290,TWO STAGES DOMAIN INVARIANT REPRESENTATION LEARNERS SOLVE THE LARGE CO-VARIATE SHIFT IN UNSUPERVISED DOMAIN ADAPTATION WITH TWO DIMENSIONAL DATA DOMAINS,"Recent developments in the unsupervised domain adaptation (UDA) enable the unsupervised machine learning (ML) prediction for target data, thus this will accelerate real world applications with ML models such as image recognition tasks in self-driving. Researchers have reported the UDA techniques are not working well under large co-variate shift problems where e.g. supervised source data consists of handwritten digits data in monotone color and unsupervised target data colored digits data from the street view. Thus there is a need for a method to resolve co-variate shift and transfer source labelling rules under this dynamics. We perform two stages domain invariant representation learning to bridge the gap between source and target with semantic intermediate data (unsupervised). The proposed method can learn domain invariant features simultaneously between source and intermediate also intermediate and target. Finally this achieves good domain invariant representation between source and target plus task discriminability owing to source labels. This induction for the gradient descent search greatly eases learning convergence in terms of classification performance for target data even when large co-variate shift. We also derive a theorem for measuring the gap between trained models and unsupervised target labelling rules, which is necessary for the free parameters optimization. Finally we demonstrate that proposing method is superiority to previous UDA methods using 4 representative ML classification datasets including 38 UDA tasks. Our experiment will be a basis for challenging UDA problems with large co-variate shift.",2025,0.2454544561983795,0.2404152989276635,0.2666666666666666,0.0,dd39825f-cd7b-48cd-b874-62f88bfe6bdf,0,"[0.0, 0.0, 0.25, 0.25, 0.625]","[1.0, 0.9, 0.95, 0.95, 0.9]",Domain Invariant Representation,0.2009489164147166
541298,Large Language Models based Graph Convolution for Text-Attributed Networks,"Text-attributed graph (TAG) tasks involve analyzing both structural information and textual attributes. Existing methods employ text embeddings as node features, and leverage structural information by employing Graph Neural Networks (GNNs) to aggregate features from neighbors. These approaches demand substantial computational resources and rely on two cascaded stages, limiting scalability in large-scale scenarios and making them vulnerable to the influence of irrelevant neighboring nodes. The advancement of language models (LMs) presents new avenues for tackling this task without GNNs, leveraging their ability to process text attributes of both the target node and its important neighbors. Instead of using graph convolution modules, LMs can assign weights to these tokens based on relevance, enabling token-level weighted summarization. However, it is nontrivial to directly employ LMs for TAG tasks because assessing the importance of neighbor nodes involves both semantic and structural considerations. Additionally, the large search space presents efficiency issues for computing importance scores in a scalable manner.
To this end, we propose a novel semantic knowledge and Structural Enrichment framework, namely SKETCH, to adapt LMs for TAG tasks by retrieving both structural and text-related content. Specifically, we propose a retrieval model that identifies neighboring nodes exhibiting similarity to the target node across two dimensions: structural similarity and text similarity. To enable efficient retrieval, we introduce a hash-based common neighbor estimation algorithm for structural similarity and a nearest-neighbor recalling algorithm for embedding similarity. These two similarity measures are then aggregated using a weighted rank aggregation mechanism. The text attributes of both the retrieved nodes and the target node provide effective descriptions of the target node and are used as input for the LM predictor. Extensive experiments demonstrate that SKETCH can outperform other baselines on three datasets with fewer resources.",2025,0.5113634504132908,0.5079014835838647,0.5333333333333333,0.5,740373ec-2e02-481c-b94e-f520792058c4,0,"[0.25, 0.5, 0.5, 0.625]","[1.0, 1.0, 0.95, 0.95]",SKETCH,0.4549369031377899
541328,Sketch-to-Skill: Bootstrapping Robot Learning with Human Drawn Trajectory Sketches,"Training robotic manipulation policies traditionally requires numerous demonstrations and/or environmental rollouts. While recent Imitation Learning (IL) and Reinforcement Learning (RL) methods have reduced the number of required demonstrations, they still rely on expert knowledge to collect high-quality data, limiting scalability and accessibility. We propose Sketch-to-Skill, a novel framework that leverages human-drawn 2D sketch trajectories to bootstrap and guide RL for robotic manipulation. Our approach extends beyond previous sketch-based methods, which were primarily focused on imitation learning or policy conditioning, limited to specific trained tasks. Sketch-to-Skill employs a Sketch-to-3D Trajectory Generator that translates 2D sketches into 3D trajectories, which are then used to autonomously collect initial demonstrations. We utilize these sketch-generated demonstrations in two ways: to pre-train an initial policy through behavior cloning and to refine this policy through RL with guided exploration. Experimental results demonstrate that Sketch-to-Skill achieves $\sim$96\% of the performance of the baseline model that leverages teleoperated demonstration data, while exceeding the performance of a pure reinforcement learning policy by $\sim$170\%, only from sketch inputs. This makes robotic manipulation learning more accessible and potentially broadens its applications across various domains.",2025,0.6545452165290122,0.6546240287873211,0.5333333333333333,0.5,d7012029-4973-48c5-ac70-650b290341c2,0,"[0.25, 0.5, 0.5, 0.875, 0.875]","[0.95, 0.9, 0.95, 0.95, 0.95]",Sketch-to-Skill,0.604771983836829
541392,Context-Aware Kernel Search for Bayesian Optimization with Large Language Models,"The efficiency of Bayesian optimization (BO) relies on careful selection of the surrogate model to balance exploration and exploitation under limited budget. Traditional BO methods often struggle with sub-optimal kernel choices when using Gaussian processes (GPs) as the surrogate model. When the kernel is inadequately chosen, BO may converge slowly or even get stuck at an undesired local minimum. To address such drawback, we propose the novel Context-Aware Kernel Search (CAKES) to automate optimal kernel design in BO with large language models (LLMs). Concretely, CAKES exploits LLMs as crossover and mutation operators to adaptively generate and refine GP kernels based on the observed data. CAKES works entirely in-context and can be easily integrated into existing systems without requiring any fine-tuning. We further present a theoretical analysis demonstrating that our method achieves sub-linear regret relative to the budget for any input dimension. Experimental results demonstrate that CAKES outperforms various salient baseline methods in numerous synthetic and real-world optimization tasks. Notably, CAKES improves the overall performance on benchmark functions by roughly 36\%. In hyperparameter tuning tasks, CAKES can effectively leverage fewer data samples to quickly identify high-performing configurations and consistently ranks first across various datasets. As an encouraging real application, we successfully applied CAKES to design photonic chips,  achieving significant improvements in key performance indicators while speeding up the design cycle by a factor of ten compared to the baselines. Our code is accessible at https://github.com/cakes4bo/cakes.",2025,0.6818179338843877,0.6825324391024874,0.6,0.5,9a55e414-5862-4abe-99e1-8949fa6aad8c,0,"[0.5, 0.5, 0.625, 0.875]","[0.95, 0.9, 0.9, 0.95]",CAKES,0.6330310880829013
541393,Inspection and Control of Self-Generated-Text Recognition Ability in Llama3-8b-Instruct,"It has been reported that LLMs can recognize their own writing. As this has potential implications for AI safety, yet is relatively understudied, we investigate the phenomenon, seeking to establish: whether it robustly occurs at the behavioral level, how the observed behavior is achieved, and whether it can be controlled. First, we find that the Llama3-8b–Instruct chat model - but not the base Llama3-8b model - can reliably distinguish its own outputs from those of humans, and present evidence that the chat model is likely using its experience with its own outputs, acquired during post-training, to succeed at the writing recognition task. Second, we identify a vector in the residual stream of the model that is differentially activated when the model makes a correct self-written-text recognition judgment, show that the vector activates in response to information relevant to self-authorship, present evidence that the vector is related to the concept of ``self'' in the model, and demonstrate that the vector is causally related to the model’s ability to perceive and assert self-authorship. Finally, we show that the vector can be used to control both the model’s behavior and its perception, steering the model to claim or disclaim authorship by applying the vector to the model’s output as it generates it, and steering the model to believe or disbelieve it wrote arbitrary texts by applying the vector to them as the model reads them.",2025,0.6818179338843877,0.682583627211163,0.7333333333333333,0.875,c448aadf-bf94-49fd-8d55-aee9bdd73a0f,1,"[0.25, 0.5, 0.875, 0.875]","[0.95, 0.8, 0.9, 0.95]",Self-Authorship Recognition,0.6266897610130754
541394,PowerSoftmax: Towards secure LLM Inference Over Encrypted Data,"Modern cryptographic methods for implementing privacy-preserving LLMs such as Homomorphic Encryption require the LLMs to have a polynomial form. Forming such a representation is challenging because Transformers include non-polynomial components, such as Softmax and layer normalization. Previous approaches have either directly approximated pre-trained models with large-degree polynomials, which are less efficient over HE, or replaced non-polynomial components with easier-to-approximate primitives before training, e.g., Softmax with pointwise attention. The latter approach might introduce scalability challenges. 

We present a new HE-friendly variant of self-attention that offers a stable form for training and is easy to approximate with polynomials for secure inference. Our work introduces the first polynomial LLMs with 32 layers and over a billion parameters, exceeding the size of previous models by more than tenfold. The resulting models demonstrate reasoning and in-context learning (ICL) capabilities comparable to standard transformers of the same size, representing a breakthrough in the field. Finally, we provide a detailed latency breakdown for each computation over encrypted data, paving the way for further optimization, and explore the differences in inductive bias between transformers relying on our HE-friendly variant and standard transformers. Our code is attached as a supplement.",2025,0.3749998636364132,0.3757424998529703,0.2666666666666666,0.25,7b577d0a-2ac8-4449-86d4-25bc48f535ad,0,"[0.25, 0.25, 0.25, 0.625]","[0.95, 0.95, 0.9, 0.95]",PowerSoftmax,0.3494096884128529
541397,Efficient Protein Optimization via Structure-aware Hamiltonian Dynamics,"The ability to engineer optimized protein variants has transformative potential for biotechnology and medicine. Prior sequence-based optimization methods struggle with the high-dimensional complexities due to the epistasis effect and the disregard for structural constraints. To address this, we propose HADES, a Bayesian optimization method utilizing Hamiltonian dynamics to efficiently sample from a structure-aware approximated posterior. Leveraging momentum and uncertainty in the simulated physical movements, HADES enables rapid transition of proposals toward promising areas. A position discretization procedure is introduced to propose discrete protein sequences from such continuous state system. The posterior surrogate is powered by a two-stage encoder-decoder framework to determine the structure and function relationships between mutant neighbors, consequently learning a smoothed landscape to sample from. Extensive experiments demonstrate that our method outperforms state-of-the-art baselines in in-silico evaluations across most metrics. Remarkably, our approach offers a unique advantage by leveraging the mutual constraints between protein structure and sequence, facilitating the design of protein sequences with similar structures and optimized properties.",2025,0.4090907603306326,0.404925167341551,0.4,0.25,d9d0920a-f673-4076-8a29-036025ba944f,0,"[0.25, 0.25, 0.5, 0.5]","[1.0, 1.0, 0.95, 0.95]",HADES,0.3565825375170531
541417,SEAL: SEmantic-Augmented Imitation Learning via Language Model,"Hierarchical Imitation Learning (HIL) is a promising approach for tackling long-horizon decision-making tasks. While it is a challenging task due to the lack of detailed supervisory labels for sub-goal learning, and reliance on hundreds to thousands of expert demonstrations. In this work, we introduce SEAL, a novel framework that leverages Large Language Models (LLMs)'s powerful semantic and world knowledge for both specifying sub-goal space and pre-labeling states to semantically meaningful sub-goal representations without prior knowledge of task hierarchies. SEAL employs a dual-encoder structure, combining supervised LLM-guided sub-goal learning with unsupervised Vector Quantization (VQ) for more robust sub-goal representations. Additionally, SEAL incorporates a transition-augmented low-level planner for improved adaptation to sub-goal transitions. Our experiments demonstrate that SEAL outperforms state-of-the-art HIL methods and LLM-based planning approaches, particularly in settings with small expert datasets and complex long-horizon tasks.",2025,0.443181657024852,0.4437769418263481,0.4,0.25,4579dfa5-4628-458b-ba1b-3486698c0cf3,0,"[0.25, 0.25, 0.5, 0.625]","[0.95, 0.95, 1.0, 0.95]",SEAL,0.4137058173784978
541425,Diffusion On Syntax Trees For Program Synthesis,"Large language models generate code one token at a time. Their autoregressive generation process lacks the feedback of observing the program's output. Training LLMs to suggest edits directly can be challenging due to the scarcity of rich edit data. To address these problems, we propose neural diffusion models that operate on syntax trees of any context-free grammar. Similar to image diffusion models, our method also inverts ""noise"" applied to syntax trees. Rather than generating code sequentially, we iteratively edit it while preserving syntactic validity, which makes it easy to combine this neural model with search. We apply our approach to inverse graphics tasks, where our model learns to convert images into programs that produce those images. Combined with search, our model is able to write graphics programs, see the execution result, and debug them to meet the required specifications. We additionally show how our system can write graphics programs for hand-drawn sketches. Video results can be found at https://tree-diffusion.github.io.",2025,0.8454542380166408,0.8475727034509498,0.9333333333333332,0.875,a375e7bd-b29e-4793-9225-a63427c49d7f,1,"[0.625, 0.625, 0.875, 0.875, 0.875]","[0.9, 0.9, 0.95, 0.9, 1.0]",Syntax Tree Diffusion,0.7921813437475665
541429,VideoEval: Comprehensive Benchmark Suite for Low-Cost Evaluation of Video Foundation Model,"With the accumulation of high-quality data and advancements in visual pretraining paradigms, recent Video Foundation Models (VFMs) have made significant progress, demonstrating remarkable performance on popular video understanding benchmarks. However, conventional benchmarks (e.g. Kinetics) and evaluation protocols are limited by their relatively poor diversity, high evaluation costs, and saturated performance metrics. In this work, we introduce a comprehensive benchmark suite to address these issues, namely **VideoEval**. We establish the **Vid**eo **T**ask **A**daption **B**enchmark (VidTAB) and the **Vid**eo **E**mbedding **B**enchmark (VidEB) from two perspectives: evaluating the task adaptability of VFMs under few-shot conditions and assessing their feature embedding's direct applicability to downstream tasks. With VideoEval, we conduct a large-scale study of 20 popular open-source vision foundation models. Our study reveals some insightful findings, 1) overall, current VFMs exhibit weak generalization across diverse tasks, 2) increasing video data, whether labeled or in video-text pairs, does not necessarily improve task performance, 3) the effectiveness of some pre-training paradigms may not be fully validated in previous benchmarks, and 4) combining different pre-training paradigms can help develop models with better generalization capabilities. We believe this study serves as a important complement to the current evaluation methods for VFMs and offers valuable insights for future research directions.",2025,0.4090907603306326,0.4066481372973985,0.4,0.25,dcd1b540-fc25-4131-82c9-4cfb8066b54e,0,"[0.25, 0.25, 0.5, 0.5]","[0.95, 1.0, 0.95, 0.95]",VideoEval,0.3650589101620029
541431,Do Vision-Language Models Really Understand Visual Language?,"Visual language is a system of communication that conveys information through symbols, shapes, and spatial arrangements. Diagrams are a typical example of a visual language depicting complex concepts and their relationships in the form of an image. The symbolic nature of diagrams presents significant challenges for building models capable of understanding them. Yet, recent studies seem to suggest that Large Vision-Language Models (LVLMs) can even tackle complex reasoning tasks involving diagrams. In this paper, we investigate this phenomenon by developing a comprehensive test suite to evaluate the diagram comprehension capability of LVLMs. Our test suite uses a variety of questions focused on concept entities and their relationships over a set of synthetic as well as real diagrams across several domains to evaluate the recognition and reasoning abilities of models. Our evaluation of three LVLMs (GPT-4V, GPT-4o, and Gemini) shows that while these models can accurately identify and reason about entities, their ability to understand relationships is notably limited. Further testing reveals that the decent performance on diagram understanding largely stems from leveraging their background knowledge as shortcuts to identify and reason about the relational information. Thus, we conclude that LVLMs have a limited capability for genuine diagram understanding, and their impressive performance in diagram reasoning is an illusion emanating from other confounding factors, such as the background knowledge in the models.",2025,0.6818179338843877,0.6826663994720001,0.6,0.5,32a1238a-03c7-4a6a-87b2-4e1001c2bdfe,0,"[0.5, 0.5, 0.625, 0.875]","[0.95, 0.8, 0.8, 0.9]",Diagram Comprehension,0.6269942758392981
541433,LancBiO: Dynamic Lanczos-aided Bilevel Optimization via Krylov Subspace,"Bilevel optimization, with broad applications in machine learning, has an intricate hierarchical structure. Gradient-based methods have emerged as a common approach to large-scale bilevel problems. However, the computation of the hyper-gradient, which involves a Hessian inverse vector product, confines the efficiency and is regarded as a bottleneck. To circumvent the inverse, we construct a sequence of low-dimensional approximate Krylov subspaces with the aid of the Lanczos process. As a result, the constructed subspace is able to dynamically and incrementally approximate the Hessian inverse vector product with less effort and thus leads to a favorable estimate of the hyper-gradient. Moreover, we propose a provable subspace-based framework for bilevel problems where one central step is to solve a small-size tridiagonal linear system. To the best of our knowledge, this is the first time that subspace techniques are incorporated into bilevel optimization. This successful trial not only enjoys $\mathcal{O}(\epsilon^{-1})$ convergence rate but also demonstrates efficiency in a synthetic problem and two deep learning tasks.",2025,0.5795452438017296,0.5781228558172563,0.5333333333333333,0.5,a9b31a41-7cd4-4458-b247-d53556cdb432,1,"[0.5, 0.5, 0.5, 0.625]","[0.95, 0.9, 0.9, 0.9]",Lanczos-aided Bilevel Optimization,0.5291043743078626
541438,EXAGREE: Towards Explanation Agreement in Explainable Machine Learning,"Explanations in machine learning are critical for trust, transparency, and fairness. Yet, complex disagreements among these explanations limit the reliability and applicability of machine learning models, especially in high-stakes environments. We formalize four fundamental ranking-based explanation disagreement problems and introduce a novel framework, EXplanation AGREEment (EXAGREE), to bridge diverse interpretations in explainable machine learning, particularly from stakeholder-centric perspectives. Our approach leverages a Rashomon set for attribution predictions and then optimizes within this set to identify Stakeholder-Aligned Explanation Models (SAEMs) that minimize disagreement with diverse stakeholder needs while maintaining predictive performance.  Rigorous empirical analysis on synthetic and real-world datasets demonstrates that EXAGREE reduces explanation disagreement and improves fairness across subgroups in various domains. EXAGREE not only provides researchers with a new direction for studying explanation disagreement problems but also offers data scientists a tool for making better-informed decisions in practical applications.",2025,0.1363635867768775,0.1343230427083084,0.1333333333333333,0.0,28ecc43d-29eb-4166-b25a-6596b8f25cba,0,"[0.0, 0.0, 0.25, 0.25]","[0.95, 0.95, 0.9, 0.95]",EXAGREE,0.1174587892956949
541449,Learning Fine-Grained Representations through Textual Token Disentanglement in Composed Video Retrieval,"With the explosive growth of video data, finding videos that meet detailed requirements in large datasets has become a challenge. To address this, the composed video retrieval task has been introduced, enabling users to retrieve videos using complex queries that involve both visual and textual information. However, the inherent heterogeneity between the modalities poses significant challenges. Textual data are highly abstract, while video content contains substantial redundancy. The modality gap in information representation makes existing methods struggle with the modality fusion and alignment required for fine-grained composed retrieval. To overcome these challenges, we first introduce FineCVR-1M, a fine-grained composed video retrieval dataset containing 1,010,071 video-text triplets with detailed textual descriptions. This dataset is constructed through an automated process that identifies key concept changes between video pairs to generate textual descriptions for both static and action concepts. For fine-grained retrieval methods, the key challenge lies in understanding the detailed requirements. Text description serves as clear expressions of intent, but it requires models to distinguish subtle differences in the description of video semantics. Therefore, we propose a textual Feature Disentanglement and Cross-modal Alignment framework (FDCA) that disentangles features at both the sentence and token levels. At the sequence level, we separate text features into retained and injected features. At the token level, an Auxiliary Token Disentangling mechanism is proposed to disentangle texts into retained, injected, and excluded tokens. The disentanglement at both levels extracts fine-grained features, which are aligned and fused with the reference video to extract global representations for video retrieval. Experiments on FineCVR-1M dataset demonstrate the superior performance of FDCA. Our code and dataset are available at: https://may2333.github.io/FineCVR/.",2025,0.6818179338843877,0.6858629334924884,0.7333333333333333,0.875,b3fe827d-2cd8-4b1d-83b5-04bedb7c5b52,1,"[0.25, 0.5, 0.875, 0.875]","[0.95, 1.0, 1.0, 1.0]",Textual Feature Disentanglement,0.6507306226175351
541461,Intrinsic User-Centric Interpretability through Global Mixture of Experts,"In human-centric settings like education or healthcare, model accuracy and model explainability are key factors for user adoption. Towards these two goals, intrinsically interpretable deep learning models have gained popularity, focusing on accurate predictions alongside faithful explanations. However, there exists a gap in the human-centeredness of these approaches, which often produce nuanced and complex explanations that are not easily actionable for downstream users. We present InterpretCC (interpretable conditional computation), a family of intrinsically interpretable neural networks at a unique point in the design space that optimizes for ease of human understanding and explanation faithfulness, while maintaining comparable performance to state-of-the-art models. InterpretCC achieves this through adaptive sparse activation of features before prediction, allowing the model to use a different, minimal set of features for each instance. We extend this idea into an interpretable, global mixture-of-experts (MoE) model that allows users to specify topics of interest, discretely separates the feature space for each data point into topical subnetworks, and adaptively and sparsely activates these topical subnetworks for prediction. We apply InterpretCC for text, time series and tabular data across several real-world datasets, demonstrating comparable performance with non-interpretable baselines and outperforming intrinsically interpretable baselines. Through a user study involving 56 teachers, InterpretCC explanations are found to have higher actionability and usefulness over other intrinsically interpretable approaches.",2025,0.8181815206612653,0.8186960755057276,0.8,0.625,f1c87e8e-0786-4bac-99c8-6dfb2b2d99f4,1,"[0.625, 0.625, 0.875, 0.875]","[0.9, 0.9, 0.95, 0.9]",InterpretCC,0.7585825027685452
541469,How Does Data Diversity Shape The Weight Landscape of Neural Networks?,"To enhance the generalization of machine learning models to unseen data, techniques such as dropout, weight decay (L2 regularization), and noise augmentation
are commonly employed. While regularization methods (i.e., dropout and weight
decay) are geared toward adjusting model parameters to prevent overfitting, data
augmentation increases the diversity of the input training set, a method purported
to improve accuracy and calibration error. In this paper, we investigate the impact of each of these techniques on the parameter space of neural networks, with
the goal of understanding how they alter the weight landscape in transfer learning
scenarios. To accomplish this, we employ Random Matrix Theory to analyze the
eigenvalue distributions of pre-trained models, fine-tuned using these techniques
but using different levels of data diversity, for the same downstream tasks. We
observe that diverse data influences the weight landscape in a similar fashion as
dropout. Additionally, we compare commonly used data augmentation methods
with synthetic data created by generative models. We conclude that synthetic data
can bring more diversity into real input data, resulting in a better performance on
out-of-distribution test instances.",2025,0.443181657024852,0.4401197148277793,0.4,0.25,2cd30b06-7d5b-41a4-aa8c-df09774c97c3,0,"[0.25, 0.25, 0.5, 0.625]","[0.95, 0.9, 0.9, 0.9]",Weight Landscape,0.3955218715393134
541472,Efficient Low-Bit Quantization with Adaptive Scales for Multi-Task Co-Training,"Co-training can achieve parameter-efficient multi-task models but remains unexplored for quantization-aware training. Our investigation shows that directly introducing co-training into existing quantization-aware training (QAT) methods results in significant performance degradation. Our experimental study identifies that the primary issue with existing QAT methods stems from the inadequate activation quantization scales for the co-training framework. To address this issue, we propose Task-Specific Scales Quantization for Multi-Task Co-Training (TSQ-MTC) to tackle mismatched quantization scales. Specifically, a task-specific learnable multi-scale activation quantizer (TLMAQ) is incorporated to enrich the representational ability of shared features for different tasks. Additionally, we find that in the deeper layers of the Transformer model, the quantized network suffers from information distortion within the attention quantizer. A structure-based layer-by-layer distillation (SLLD) is then introduced to ensure that the quantized features effectively preserve the information from their full-precision counterparts. Our extensive experiments in two co-training scenarios demonstrate the effectiveness and versatility of TSQ-MTC. In particular, we successfully achieve a 4-bit quantized low-level visual foundation model based on IPT, which attains a PSNR comparable to the full-precision model while offering a $7.99\times$ compression ratio in the $\times4$ super-resolution task on the Set5 benchmark.",2025,0.772726628099408,0.7779350934781993,0.6666666666666666,0.625,18a62c07-0859-4ea1-a53d-9d368f73447e,1,"[0.625, 0.625, 0.875]","[0.9, 0.9, 1.0]",Task-Specific Scales Quantization,0.7413958956819109
541477,Counterfactual Concept Bottleneck Models,"Current deep learning models are not designed to simultaneously address three fundamental questions: predict class labels to solve a given classification task (the ""What?""), simulate changes in the situation to evaluate how this impacts class predictions (the ""How?""), and imagine how the scenario should change to result in different class predictions (the ""Why not?""). While current approaches in causal representation learning and concept interpretability are designed to address some of these questions individually (such as Concept Bottleneck Models, which address both ``what'' and ``how'' questions), no current deep learning model is specifically built to answer all of them at the same time. To bridge this gap, we introduce CounterFactual Concept Bottleneck Models (CF-CBMs), a class of models designed to efficiently address the above queries all at once without the need to run post-hoc searches. Our experimental results demonstrate that CF-CBMs: achieve classification accuracy comparable to black-box models and existing CBMs (“What?”), rely on fewer important concepts leading to simpler explanations (“How?”), and produce interpretable, concept-based counterfactuals (“Why not?”). Additionally, we show that training the counterfactual generator jointly with the CBM leads to two key improvements: (i) it alters the model's decision-making process, making the model rely on fewer important concepts (leading to simpler explanations), and (ii) it significantly increases the causal effect of concept interventions on class predictions, making the model more responsive to these changes.",2025,0.7840906239670459,0.7873961807136276,0.8,0.875,d9681b36-91d6-4d84-b90e-110395767c77,1,"[0.5, 0.625, 0.875, 0.875]","[0.9, 0.9, 0.95, 0.95]",CounterFactual Concept Bottleneck Models,0.7388277202072538
541479,Advancing Algorithmic Trading with Large Language Models: A Reinforcement Learning Approach for Stock Market Optimization,"In the fast-evolving landscape of financial markets, effective decision-making tools are essential for managing complexities driven by economic indicators and market dynamics. Algorithmic trading strategies have gained prominence for their ability to execute trades autonomously, with Deep Reinforcement Learning (DRL) emerging as a key approach for optimizing trading actions through continuous market interaction. However, RL-based systems face significant challenges, particularly in adapting to evolving time series data and incorporating unstructured textual information. In response to these limitations, recent advancements in Large Language Models (LLMs) offer new opportunities. LLMs possess the capacity to analyze vast volumes of data, providing enhanced insights that can complement traditional market analysis. This study proposes a novel approach that integrates six distinct LLMs into algorithmic trading frameworks, developing Stock-Evol-Instruct, an innovative instruction generation algorithm. This algorithm enables RL agents to fine-tune their trading strategies by leveraging LLM-driven insights for daily stock trading decisions. Empirical evaluation using real-world stock data from Silver and JPMorgan demonstrates the significant potential of this approach to outperform conventional trading models. By bridging the gap between LLMs and RL in algorithmic trading, this study contributes to a new frontier in financial technology, setting the stage for future advancements in autonomous trading systems.",2025,0.4772725537190714,0.4764850541613754,0.4666666666666667,0.0,5adc6e7a-4f6b-4a41-819c-afd0f1346f0a,0,"[0.0, 0.25, 0.625, 0.875]","[1.0, 0.95, 0.95, 1.0]",Stock-Evol-Instruct,0.4375
541481,To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic reasoning,"Chain-of-thought (CoT) via prompting is the de facto method for eliciting reasoning capabilities from large language models (LLMs). But for what kinds of tasks is this extra ""thinking"" really helpful? To analyze this, we conducted a quantitative meta-analysis covering over 100 papers using CoT and ran our own evaluations of 20 datasets across 14 models. Our results show that CoT gives strong performance benefits primarily on tasks involving math or logic, with much smaller gains on other types of tasks. On MMLU, directly generating the answer without CoT leads to almost identical accuracy as CoT unless the question or model's response contains an equals sign, indicating symbolic operations and reasoning. Following this finding, we analyze the behavior of CoT on these problems by separating planning and execution and comparing against tool-augmented LLMs. Much of CoT's gain comes from improving symbolic execution, but it underperforms relative to using a symbolic solver. Our results indicate that CoT can be applied selectively, maintaining performance while saving inference costs. Furthermore, they suggest a need to move beyond prompt-based CoT to new paradigms that better leverage intermediate computation across the whole range of LLM applications.",2025,0.772726628099408,0.7682104419385264,0.6666666666666666,0.625,e2841036-bb35-40bf-9488-36086fbcf168,1,"[0.625, 0.625, 0.875]","[0.95, 0.95, 0.9]",Symbolic Execution,0.694642265650878
541482,Roll-AE: A Spatiotemporal Invariant Autoencoder for Neuronal Electro-Physiology,"Micro-electrode array (MEA) assays enable high-throughput recording of the electrophysiological activity in biological tissues, both in vivo and in vitro. While various classical and deep learning models have been developed for MEA signal analysis, the majority focus on in vivo experiments or specific downstream applications in vitro. Consequently, extracting relevant features from in vitro MEA recordings has remained largely dependent on particular curated features known as neural metrics. In this work, we introduce Roll-AE, a novel autoencoder designed to extract spatiotemporally invariant features from in vitro MEA recordings. Roll-AE serves as a foundational model that facilitates a wide range of downstream tasks. We demonstrate that 1) Roll-AE's embeddings outperform those from standard autoencoders across various classification tasks, and 2) Roll-AE's embeddings effectively characterize electrophysiological phenotypic traits in induced Pluripotent Stem Cells (iPSC)-derived neuronal cultures.",2025,0.3409089669421938,0.3385069626718885,0.2666666666666666,0.25,84f5f300-422c-4328-a4fe-46c47fcbdba6,0,"[0.25, 0.25, 0.25, 0.5]","[0.9, 0.95, 0.95, 0.9]",Roll-AE,0.3044689119170984
541483,"Field-DiT: Diffusion Transformer on Unified Video, 3D, and Game Field Generation","The probabilistic field models the distribution of continuous functions defined over metric spaces. While these models hold great potential for unifying data generation across various modalities, including images, videos, and 3D geometry, they still struggle with long-context generation beyond simple examples. This limitation can be attributed to their MLP architecture, which lacks sufficient inductive bias to capture global structures through uniform sampling. To address this, we propose a new and simple model that incorporates a view-wise sampling algorithm to focus on local structure learning, along with autoregressive generation to preserve global geometry. It adapts cross-modality conditions, such as text prompts for text-to-video generation, camera poses for 3D view generation, and control actions for game generation. Experimental results across various modalities demonstrate the effectiveness of our model, with its 675M parameter size, and highlight its potential as a foundational framework for scalable, architecture-unified visual content generation for different modalities with different weights. Our project page can be found at https://kfmei.com/Field-DiT/.",2025,0.5454543471075102,0.5445543476130005,0.6666666666666666,0.625,b546778a-2944-416d-8c87-3789427b2e41,1,"[0.25, 0.625, 0.625]","[0.95, 0.95, 0.95]",Field-DiT,0.5
541500,Revisiting the Variational Information Bottleneck,"The Information Bottleneck (IB) framework offers a theoretically optimal approach to data modeling, though it is often intractable. Recent efforts have optimized supervised deep neural networks (DNNs) using a variational upper bound on the IB objective, leading to enhanced robustness to adversarial attacks. In these studies, supervision assumes a dual role: sometimes as a presumably constant and observed random variable, and at other times as its variational approximation. This work proposes an extension to the IB framework, and consequently to the derivation of its variational bound, that resolves this duality. Applying the resulting bound as an objective for supervised DNNs induces significant empirical improvements, and provides an information theoretic motivation for decoder regularization.",2025,0.443181657024852,0.4437769418263481,0.4,0.25,812c7424-efe2-4d70-9e55-29d28d07d38c,0,"[0.25, 0.25, 0.5, 0.625]","[0.9, 1.0, 1.0, 0.95]",Variational Information Bottleneck,0.4093827889746631
541507,DistillHGNN: A Knowledge Distillation Approach for High-Speed Hypergraph Neural Networks,"In this paper, we propose a novel framework to significantly enhance the inference speed and memory efficiency of Hypergraph Neural Networks (HGNNs) while preserving their high accuracy. Our approach utilizes an advanced teacher-student knowledge distillation strategy. The teacher model, consisting of an HGNN and a Multi-Layer Perceptron (MLP), not only produces soft labels but also transfers structural and high-order information to a lightweight Graph Convolutional Network (GCN) known as TinyGCN. This dual transfer mechanism enables the student model to effectively capture complex dependencies while benefiting from the faster inference and lower computational cost of the lightweight GCN. The student model is trained using both labeled data and soft labels provided by the teacher, with contrastive learning further ensuring that the student retains high-order relationships. This makes the proposed method efficient and suitable for real-time applications, achieving performance comparable to traditional HGNNs but with significantly reduced resource requirements.",2025,0.727272826446245,0.7222271837173894,0.6666666666666666,0.5,ea04f5f1-fd92-4643-a016-831b6522f165,1,"[0.5, 0.625, 0.875]","[1.0, 1.0, 0.95]",DistillHGNN,0.6471608149111401
541528,Benchmarking Agentic Workflow Generation,"Large Language Models (LLMs), with their exceptional ability to handle a wide range of tasks, have driven significant advancements in tackling reasoning and planning tasks, wherein decomposing complex problems into executable workflows is a crucial step in this process. Existing workflow evaluation frameworks either focus solely on holistic performance or suffer from limitations such as restricted scenario coverage, simplistic workflow structures, and lax evaluation standards. To this end, we introduce WorfBench, a unified workflow generation benchmark with multi-faceted scenarios and intricate graph workflow structures. Additionally, we present WorfEval, a systemic evaluation protocol utilizing subsequence and subgraph matching algorithms to accurately quantify the LLM agent's workflow generation capabilities. Through comprehensive evaluations across different types of LLMs, we discover distinct gaps between the sequence planning capabilities and graph planning capabilities of LLM agents, with even GPT-4 exhibiting a gap of around 15%. We also train two open-source models and evaluate their generalization abilities on held-out tasks. Furthermore, we observe that the generated workflows can enhance downstream tasks, enabling them to achieve superior performance with less time during inference. Code and dataset are available at https://github.com/zjunlp/WorfBench.",2025,0.7363633685951387,0.7328307459741098,0.6666666666666666,0.625,f09800ed-979f-4083-9593-0d065b3ae506,1,"[0.625, 0.625, 0.625, 0.625, 0.875]","[0.95, 0.95, 0.9, 1.0, 0.9]",WorfBench,0.6644809108059282
541540,FlightBench: Benchmarking Learning-based Methods for Ego-vision-based Quadrotors Navigation,"Ego-vision-based navigation in cluttered environments is crucial for mobile systems, particularly agile quadrotors. While learning-based methods have shown promise recently, head-to-head comparisons with cutting-edge optimization-based approaches are scarce, leaving open the question of where and to what extent they truly excel. In this paper, we introduce FlightBench, the first comprehensive benchmark that implements various learning-based methods for ego-vision-based navigation and evaluates them against mainstream optimization-based baselines using a broad set of performance metrics. Additionally, we develop a suite of criteria to assess scenario difficulty and design test cases that span different levels of difficulty based on these criteria. Our results show that while learning-based methods excel in high-speed flight and faster inference, they struggle with challenging scenarios like sharp corners or view occlusion. Analytical experiments validate the correlation between our difficulty criteria and flight performance. We hope this benchmark and these criteria will drive future advancements in learning-based navigation for ego-vision quadrotors. The source code and documentation is available at https://github.com/Anonymous314159265358/FlightBench.",2025,0.5113634504132908,0.5072556421275956,0.4,0.25,6b86dd76-4d77-45d8-8b60-b5606ae53a36,0,"[0.25, 0.25, 0.5, 0.875]","[0.9, 0.95, 0.9, 0.9]",FlightBench,0.4537306201550388
541543,Param$\Delta$ for Direct Mixing: Post-Train Large Language Model At Zero Cost,"The post-training phase of large language models is essential for enhancing capabilities such as instruction-following, reasoning, and alignment with human preferences. However, it demands extensive high-quality data and poses risks like overfitting, alongside significant computational costs due to repeated post-training and evaluation after each base model update. This paper introduces Param$\Delta$, a novel method that streamlines post-training by transferring knowledge from an existing post-trained model to a newly updated base model with \textbf{zero} additional training. By computing the difference between post-trained model weights ($\Theta_\text{post}$) and base model weights ($\Theta_\text{base}$), and adding this to the updated base model ($\Theta_\text{base}'$), we define Param$\Delta$ Model as: $\Theta_{\text{Param}\Delta} = \Theta_\text{post} - \Theta_\text{base} + \Theta_\text{base}'$. This approach surprisingly equips the new base model with post-trained capabilities, achieving performance comparable to direct post-training. We did analysis on LLama3, Llama3.1, Qwen, and DeepSeek-distilled models. Results indicate Param$\Delta$ Model effectively replicates traditional post-training. For example, the Param$\Delta$ Model obtained from 70B Llama3-inst, Llama3-base, Llama3.1-base models attains approximately 95\% of Llama3.1-inst model's performance on average. Param$\Delta$ brings a new perspective on how to fully leverage models in the open-weight community, where checkpoints for base and instruct models are readily available and frequently updated, by providing a cost-free framework to accelerate the iterative cycle of model development.",2025,0.7499997272728265,0.7469227233816389,0.6666666666666666,0.625,66705006-9d17-40a9-b693-a74387d72f97,1,"[0.625, 0.625, 0.625, 0.875]","[0.95, 0.95, 0.9, 0.9]",Param$\Delta$,0.6794689119170985
541556,RetCompletion:High-Speed Inference Image Completion with Retentive Network,"Time cost is a major challenge in achieving high-quality pluralistic image completion. Recently, the Retentive Network (RetNet) in natural language processing offers a novel approach to this problem with its low-cost inference capabilities. Inspired by this, we apply RetNet to the pluralistic image completion task in computer vision. We present RetCompletion, a two-stage framework. In the first stage, we introduce Bi-RetNet, a bidirectional sequence information fusion model that integrates contextual information from images. During inference, we employ a unidirectional pixel-wise update strategy to restore consistent image structures, achieving both high reconstruction quality and fast inference speed. In the second stage, we use a CNN for low-resolution upsampling to enhance texture details. Experiments on ImageNet and CelebA-HQ demonstrate that our inference speed is 10$\times$ faster than ICT and 15$\times$ faster than RePaint. The proposed RetCompletion significantly improves inference speed and delivers strong performance, especially when masks cover large areas of the image.",2025,0.3545453256198816,0.3548784445785258,0.2666666666666666,0.25,19774e18-72be-452b-bb9e-2f927ca1e038,0,"[0.25, 0.25, 0.25, 0.25, 0.625]","[0.7, 0.95, 1.0, 0.9, 0.9]",RetCompletion,0.3182954611402128
541557,"$F^3Set$: Towards Analyzing Fast, Frequent, and Fine-grained Events from Videos","Analyzing Fast, Frequent, and Fine-grained ($F^3$) events presents a significant challenge in video analytics and multi-modal LLMs. Current methods struggle to identify events that satisfy all the $F^3$ criteria with high accuracy due to challenges such as motion blur and subtle visual discrepancies. To advance research in video understanding, we introduce $F^3Set$, a benchmark that consists of video datasets for precise $F^3$ event detection. Datasets in $F^3Set$ are characterized by their extensive scale and comprehensive detail, usually encompassing over 1,000 event types with precise timestamps and supporting multi-level granularity. Currently, $F^3Set$ contains several sports datasets, and this framework may be extended to other applications as well. We evaluated popular temporal action understanding methods on $F^3Set$, revealing substantial challenges for existing techniques. Additionally, we propose a new method, $F^3ED$, for $F^3$ event detections, achieving superior performance. The dataset, model, and benchmark code are available at https://github.com/F3Set/F3Set.",2025,0.8181815206612653,0.8121374629430766,0.9333333333333332,0.875,0374b5ec-fc0d-4262-97f1-e99792736e55,1,"[0.5, 0.875, 0.875]","[1.0, 0.95, 0.95]",$F^3Set$,0.7241750358680057
541576,Gradient Inversion Transcript: A Generative Model to Reconstruct Training Data by Gradient Leakage,"We propose Gradient Inversion Transcript (GIT), a generic approach for reconstructing training data from gradient leakage in distributed learning using a generative model. Unlike traditional gradient matching techniques, GIT requires only the model architecture information, without access to the model's parameters, making it more applicable to real-world distributed learning settings. Additionally, GIT operates offline, eliminating the need for intensive gradient requests and online optimization.
Compared to existing generative methods, GIT adaptively constructs a generative network, with an architecture specifically tailored to the structure of the distributed learning model. Our extensive experiments demonstrate that GIT significantly improves reconstruction accuracy, especially in the case of deep models.
In summary, we offer a more effective and theoretically grounded strategy for exploiting vulnerabilities of gradient leakage in distributed learning, advancing the understanding of privacy risks in collaborative learning environments.",2025,0.5113634504132908,0.5064355432800904,0.5333333333333333,0.5,530fe2e6-f3ff-4192-b11f-b5ff73245b59,0,"[0.25, 0.5, 0.5, 0.625]","[1.0, 0.9, 0.9, 0.95]",Gradient Inversion Transcript,0.445657430683863
541583,DEPT: Decoupled Embeddings for Pre-training Language Models,"Language Model pre-training uses broad data mixtures to enhance performance across domains and languages. However, training on such heterogeneous text corpora requires extensive and expensive efforts. Since these data sources vary significantly in lexical, syntactic, and semantic aspects, they cause negative interference or the ``curse of multilinguality''. To address these challenges we propose a communication-efficient pre-training framework, DEPT. Our method decouples embeddings from the transformer body while simultaneously training the latter on multiple data sources without requiring a shared vocabulary. DEPT can: (1) train robustly and effectively under significant data heterogeneity, (2) minimize token embedding parameters to only what the data source vocabulary requires, while cutting communication costs in direct proportion to both the communication frequency and the reduction in parameters, (3) enhance transformer body plasticity and generalization, improving both average perplexity (up to 20%) and downstream task performance, and (4) enable training with custom optimized vocabularies per data source. We demonstrate DEPT's potential via the first vocabulary-agnostic federated pre-training of billion-scale models, reducing communication costs by orders of magnitude and embedding memory by 4-5x.",2025,0.9545451074381428,0.9529701083227508,0.9333333333333332,0.875,3a9b0032-1c74-40f1-991b-2d8a4ac9f123,1,"[0.875, 0.875, 0.875]","[0.95, 1.0, 0.95]",DEPT,0.8749948218582205
541592,Approximating Full Conformal Prediction for Neural Network Regression with Gauss-Newton Influence,"Uncertainty quantification is an important prerequisite for the deployment of deep learning models in safety-critical areas. Yet, this hinges on the uncertainty estimates being useful to the extent the prediction intervals are well-calibrated and sharp. In the absence of inherent uncertainty estimates (e.g. pretrained models predicting only point estimates), popular approaches that operate post-hoc include Laplace’s method and split conformal prediction (split-CP). However, Laplace’s method can be miscalibrated when the model is misspecified and split-CP requires sample splitting, and thus comes at the expense of statistical efficiency. In this work, we construct prediction intervals for neural network regressors post-hoc without held-out data. This is achieved by approximating the full conformal prediction method (full-CP). Whilst full-CP nominally requires retraining the model for every test point and candidate label, we propose to train just once and locally perturb model parameters using Gauss-Newton influence to approximate the effect of retraining. Coupled with linearization of the network, we express the absolute residual nonconformity score as a piecewise linear function of the candidate label allowing for an efficient procedure that avoids the exhaustive search over the output space. On standard regression benchmarks and bounding box localization, we show the resulting prediction intervals are locally-adaptive and often tighter than those of split-CP.",2025,0.7159088305786071,0.7142614427205064,0.6666666666666666,0.625,6c2171fa-052d-4f61-9f61-bada5d0fa3f4,1,"[0.5, 0.625, 0.625, 0.875]","[1.0, 0.9, 0.8, 0.95]",Full Conformal Prediction,0.6476081416780827
541601,BiQAP: Neural Bi-level Optimization-based Framework for Solving Quadratic Assignment Problems,"Quadratic Assignment Problem (QAP) has attracted lasting attention for its wide applications and computational challenge. Despite the rich literature in machine learning for QAP, most works often address the problem in the setting of image matching, whereby deep networks could play a vital role in extracting useful features for the subsequent matching. While its power on pure numerical QAP instances is limited in node embedding, often with a vanilla graph neural network. This paper tries to tap the potential of deep nets for QAP, specifically by modifying the input instance which is orthogonal to previous efforts. Specifically, we develop a bi-level  unsupervised framework, where the inner optimization involves trying to solve the modified instance with entropic regularization that can be solved iteratively using the Sinkhorn algorithm without affecting backpropagation by truncating gradients during training. The outer minimization deals with the quadratic objective function of the original QAP. In particular, seeing the intractable scale of the most general form i.e. Lawler's QAP and the practical utility of the more efficient Koopmans-Beckmann QAP (KBQAP) form for solving other graph and combinatorial problems like TSP and graph edit distance, we embody our network on the KBQAP, and show its strong performance on various benchmarks in our experiments. Source code will be made publicly available.",2025,0.6363630413225304,0.6353130425122688,0.6666666666666666,0.625,d3b27156-f9ed-4c5e-bf6f-4e48e471dc1e,0,"[0.5, 0.625, 0.625]","[0.9, 0.9, 0.9]",BiQAP,0.5833333333333333
541602,DynST: Large-Scale Spatial-Temporal Dataset for Transferable Traffic Forecasting with Dynamic Road Networks,"In real-world traffic networks, it is common to encounter a shortage of historical data in the target region. Researchers often address this issue through transfer learning. However, transfer learning tasks in traffic prediction currently lack dedicated datasets and instead rely on datasets designed for non-transfer prediction tasks. The major drawback of these existing datasets is the adoption of a fixed network topology to model the real world's road networks. This does not align with reality and limits the model's transferability. To tackle this issue, we propose DynST, a dataset specifically designed for transfer learning tasks in traffic prediction, with a massive data volume of 20.35 billion, spanning 20 years and 9 regions. The key feature of DynST is evolving dynamic road network topology, which reflects the evolution of real road networks. Moreover, to address the shortcomings of the distance-based adjacency generation algorithm, we introduce a novel tree-based algorithm. Extensive experiments demonstrate that the adoption of DynST as the source dataset can significantly enhance the performance of the target region. The comparative experiment also validates that our adjacency matrix generation algorithm can lead to improved prediction accuracy. We believe that DynST, with rich spatial variation information, will facilitate research in the field of transfer traffic prediction.",2025,0.4909089123967591,0.4900989128517004,0.5333333333333333,0.5,7f93dbd0-d379-451e-bfa3-7441583f5151,0,"[0.25, 0.5, 0.5, 0.5, 0.5]","[0.95, 0.95, 0.95, 0.95, 0.95]",DynST,0.45
541638,Latent Radiance Fields with 3D-aware 2D Representations,"Latent 3D reconstruction has shown great promise in empowering 3D semantic understanding and 3D generation by distilling 2D features into the 3D space. However, existing approaches struggle with the domain gap between 2D feature space and 3D representations, resulting in degraded rendering performance. To address this challenge, we propose a novel framework that integrates 3D awareness into the 2D latent space. The framework consists of three stages: (1) a correspondence-aware autoencoding method that enhances the 3D consistency of 2D latent representations, (2) a latent radiance field (LRF) that lifts these 3D-aware 2D representations into 3D space, and (3) a VAE-Radiance Field (VAE-RF) alignment strategy that improves image decoding from the rendered 2D representations. Extensive experiments demonstrate that our method outperforms the state-of-the-art latent 3D reconstruction approaches in terms of synthesis performance and cross-dataset generalizability across diverse indoor and outdoor scenes. To our knowledge, this is the first work showing the radiance field representations constructed from 2D latent representations can yield photorealistic 3D reconstruction performance.",2025,0.6477270371901683,0.6471004659206998,0.6666666666666666,0.625,a5511f57-93a8-44c3-b20d-9052d69e81cc,1,"[0.5, 0.625, 0.625, 0.625]","[0.95, 0.95, 0.95, 1.0]",Latent Radiance Field,0.5962352724594994
541640,Text as Any-Modality for Zero-shot Classification by Consistent Prompt Tuning,"The integration of prompt tuning with multimodal learning has shown significant generalization abilities for various downstream tasks. Despite advancements, existing methods heavily depend on massive modality-specific labeled data (e.g., video, audio, and image), or are customized for a single modality. In this study, we present Text as Any-Modality by Consistent Prompt Tuning (TaAM-CPT), a scalable approach for constructing a general representation model toward unlimited modalities using solely text data. TaAM-CPT comprises modality prompt pools, text construction, and modality-aligned text encoders from pre-trained models, which allows for extending new modalities by adding prompt pools and modality-aligned text encoders. To harmonize the learning across different modalities, TaAM-CPT designs intra- and inter-modal learning objectives, which can capture category details within modalities while maintaining semantic consistency across different modalities. Benefiting from its scalable architecture and pre-trained models, TaAM-CPT can be seamlessly extended to accommodate unlimited modalities. Remarkably, without any modality-specific labeled data, TaAM-CPT achieves leading results on diverse datasets spanning various modalities, including video classification (Kinetic-400/600/700), image classification (MSCOCO, VOC2007, NUSWIDE, VOC2012, Objects365), and audio classification (ESC50, US8K). The code is available at https://anonymous.4open.science/r/TaAM-CPT-0EA6.",2025,0.5454543471075102,0.5445543476130005,0.5333333333333333,0.5,a0ee0c56-34ed-4bb0-881d-4e066256a075,0,"[0.5, 0.5, 0.5, 0.5]","[0.9, 0.95, 0.9, 0.95]",TaAM-CPT,0.5
541643,Sample what you can't compress,"For learned image representations, basic autoencoders often produce blurry results. Reconstruction quality can be improved by incorporating additional penalties such as adversarial (GAN) and perceptual losses. Arguably, these approaches lack a principled interpretation. Concurrently, in generative settings diffusion has demonstrated a remarkable ability to create crisp, high quality results and has solid theoretical underpinnings (from variational inference to direct study as the Fisher Divergence). Our work combines autoencoder representation learning with diffusion and is, to our knowledge, the first to demonstrate  jointly learning
a continuous encoder and decoder under a diffusion-based loss
and showing that it can lead to higher compression and better generation.. 
We demonstrate that this approach yields better reconstruction quality as compared to GAN-based
autoencoders while being easier to tune. 
We also show that the resulting representation is easier to model
with a latent diffusion model as compared to the representation obtained from a state-of-the-art GAN-based loss.
Since our decoder is stochastic, it can generate details not encoded in the otherwise deterministic latent representation; we therefore name our approach ``Sample what you can't compress'', or SWYCC for short.",2025,0.2999998909091306,0.3000603366217155,0.2666666666666666,0.25,66c9bbc3-99e7-4596-95b7-d47c058f9d1d,0,"[0.0, 0.25, 0.25, 0.25, 0.625]","[1.0, 0.95, 0.95, 1.0, 1.0]",SWYCC,0.2778624436787702
541648,The Labyrinth of Links: Navigating the Associative Maze of Multi-modal LLMs,"Multi-modal Large Language Models (MLLMs) have exhibited impressive capability. However, recently many deficiencies of MLLMs have been found compared to human intelligence, $\textit{e.g.}$, hallucination. To drive the MLLMs study, the community dedicated efforts to building larger benchmarks with complex tasks. In this paper, we propose benchmarking an essential but usually overlooked intelligence: $\textbf{association}$, a human's basic capability to link observation and prior practice memory. To comprehensively investigate MLLM's performance on the association, we formulate the association task and devise a standard benchmark based on adjective and verb semantic concepts. Instead of costly data annotation and curation, we propose a convenient $\textbf{annotation-free}$ construction method transforming the general dataset for our association tasks. Simultaneously, we devise a rigorous data refinement process to eliminate confusion in the raw dataset. Building on this database, we establish three levels of association tasks: single-step, synchronous, and asynchronous associations. Moreover, we conduct a comprehensive investigation into the MLLMs' zero-shot association capabilities, addressing multiple dimensions, including three distinct memory strategies, both open-source and closed-source MLLMs, cutting-edge Mixture-of-Experts (MoE) models, and the involvement of human experts. Our systematic investigation shows that current open-source MLLMs consistently exhibit poor capability in our association tasks, even the currently state-of-the-art GPT-4V(vision) also has a significant gap compared to humans. We believe our benchmark would pave the way for future MLLM studies.  $\textit{Our data and code are available at:} https://mvig-rhos.com/llm_inception.",2025,0.7840906239670459,0.7810052908900414,0.8,0.875,2640cfe2-49e3-430d-b6b6-41dfddcfabfb,1,"[0.5, 0.625, 0.875, 0.875]","[1.0, 0.9, 1.0, 0.9]",Association,0.7102902567094517
541665,Approximating Optima of Nonconvex Functions,"We study the computability of approximating optima of non-convex functions. We give a simple proof to show that the problem of finding the optimal value (and optimal point) or its approximation is not even computable in the oracle setting. We also give a property a function has to satisfy if its global optima can be approximated. Next we give an example of such a global property we call basin of attraction. Then we give a simple algorithm which converges to the global optima when this is known. Finally, we give some numerical results.",2025,0.2045453801653163,0.2051401573979886,0.2666666666666666,0.25,d1d27f52-f1d3-49fd-a1d8-34639b5ff69b,0,"[0.0, 0.25, 0.25, 0.25]","[0.9, 1.0, 0.8, 0.95]",Basin of attraction,0.1966395249801862
541678,Does SGD really happen in tiny subspaces?,"Understanding the training dynamics of deep neural networks is challenging due to their high-dimensional nature and intricate loss landscapes. Recent studies have revealed that, along the training trajectory, the gradient approximately aligns with a low-rank top eigenspace of the training loss Hessian, referred to as the dominant subspace. Given this alignment, this paper explores whether neural networks can be trained within the dominant subspace, which, if feasible, could lead to more efficient training methods. Our primary observation is that when the SGD update is projected onto the dominant subspace, the training loss does not decrease further.
This suggests that the observed alignment between the gradient and the dominant subspace is spurious. Surprisingly, projecting out the dominant subspace proves to be just as effective as the original update, despite removing the majority of the original update component. We observe similar behavior across practical setups, including the large learning rate regime (also known as Edge of Stability), Sharpness-Aware Minimization, momentum, and adaptive optimizers. We discuss the main causes and implications of this spurious alignment, shedding light on the dynamics of neural network training.",2025,0.6477270371901683,0.6417965065749491,0.6666666666666666,0.625,86cd0258-8209-4b0a-93a9-ebb7ec7b04e0,1,"[0.25, 0.625, 0.625, 0.875]","[1.0, 0.95, 0.95, 0.95]",Dominant subspace,0.566412002945508
541715,FlowDec: A flow-based full-band general audio codec with high perceptual quality,"We propose FlowDec, a neural full-band audio codec for general audio sampled at 48 kHz that combines non-adversarial codec training with a stochastic postfilter based on a novel conditional flow matching method. Compared to the prior work ScoreDec which is based on score matching, we generalize from speech to general audio and move from 24 kbit/s to as low as 4 kbit/s, while improving output quality and reducing the required postfilter DNN evaluations from 60 to 6 without any fine-tuning or distillation techniques. We provide theoretical insights and geometric intuitions for our approach in comparison to ScoreDec as well as another recent work that uses flow matching, and conduct ablation studies on our proposed components. We show that FlowDec is a competitive alternative to the recent GAN-dominated stream of neural codecs, achieving FAD scores better than those of the established GAN-based codec DAC and listening test scores that are on par, and producing qualitatively more natural reconstructions for speech and harmonic structures in music.",2025,0.8181815206612653,0.820413599918099,0.8,0.625,46839c1a-2dbe-43e8-8bdc-b9c16825b073,1,"[0.625, 0.625, 0.875, 0.875]","[0.9, 0.95, 0.95, 1.0]",FlowDec,0.7674037828912157
541719,Counterfactual Realizability,"It is commonly believed that, in a real-world environment, samples can only be drawn from observational and interventional distributions, corresponding to Layers 1 and 2 of the *Pearl Causal Hierarchy*. Layer 3, representing counterfactual distributions, is believed to be inaccessible by definition. However, Bareinboim, Forney, and Pearl (2015) introduced a procedure that allows an agent to sample directly from a counterfactual distribution, leaving open the question of what other counterfactual quantities can be estimated directly via physical experimentation. We resolve this by introducing a formal definition of realizability, the ability to draw samples from a distribution, and then developing a complete algorithm to determine whether an arbitrary counterfactual distribution is realizable given fundamental physical constraints, such as the inability to go back in time and subject the same unit to a different experimental condition. We illustrate the implications of this new framework for counterfactual data collection using motivating examples from causal fairness and causal reinforcement learning. While the baseline approach in these motivating settings typically follows an interventional or observational strategy, we show that a counterfactual strategy provably dominates both.",2025,0.5795452438017296,0.5743926585361072,0.6666666666666666,0.625,4921c3c2-9506-41ba-8ca0-3efb4f052e62,1,"[0.0, 0.625, 0.625, 0.875]","[0.95, 0.8, 0.95, 0.95]",Realizability,0.5178662474507137
541731,Measurement information multiple-reuse allows deeper quantum transformer,"The current era has witnessed the success of the transformer in the field of classical deep neural networks (DNNs) and the potential of quantum computing. One naturally expects that quantum computing can offer significant speedup for the transformer. Recent developments of quantum transformer models are faced with challenges including the expensive cost of non-linear operations and the information loss problem caused by measurements. To address this issue, this paper proposes a scheme called measurement information multiple-reuse (MIMR). MIMR enables the repeated utilization of intermediate measurement data from former layers, thus enhancing information-transferring efficiency. This scheme facilitates our quantum vision transformer (QViT) capable of achieving exponential speedup compared to classical counterparts, with the support of many parameters and large depth. Our QViT model is further examined with an instance of 86 million parameters, which halves the requirements for tomography error compared to the one without MIMR. This demonstrates the superior performance of MIMR over existing schemes. Our findings underscore the importance of exploiting the value of information from each measurement, offering a key strategy towards scalable quantum deep neural networks.",2025,0.4090907603306326,0.4084157607097504,0.4,0.25,51fd5af6-48d5-4f1e-96da-bc66461ad8f2,0,"[0.25, 0.25, 0.5, 0.5]","[1.0, 0.9, 0.95, 0.95]",Measurement Information Multiple-Reuse,0.3714220522605865
541735,X-Diffusion: Generating Detailed 3D MRI Volumes From a Single Image Using Cross-Sectional Diffusion Models,"Magnetic Resonance Imaging (MRI) is a crucial diagnostic tool, but high-resolution scans are often slow and expensive due to extensive data acquisition requirements. Traditional MRI reconstruction methods aim to expedite this process by filling in missing frequency components in the K-space, performing _3D-to-3D_ reconstructions that demand full 3D scans. In contrast, we introduce _X-Diffusion_, a novel cross-sectional diffusion model that reconstructs detailed 3D MRI volumes from extremely sparse spatial-domain inputs—achieving _2D-to-3D_ reconstruction from as little as a single 2D MRI slice or few slices.
A key aspect of X-Diffusion is that it models MRI data as holistic 3D volumes during the cross-sectional training and inference, unlike previous learning approaches that treat MRI scans as collections of 2D slices in standard planes (coronal, axial, sagittal).
We evaluated X-Diffusion on brain tumor MRIs from the BRATS dataset and full-body MRIs from the UK Biobank dataset. Our results demonstrate that X-Diffusion not only surpasses state-of-the-art methods in quantitative accuracy (PSNR) on unseen data but also preserves critical anatomical features such as tumor profiles, spine curvature, and brain volume. Remarkably, the model generalizes beyond the training domain, successfully reconstructing knee MRIs despite being trained exclusively on brain data. Medical expert evaluations further confirm the clinical relevance and fidelity of the generated images.
To promote reproducibility and trust in our findings, we will publicly release the accompanying code upon publication. To our knowledge, X-Diffusion is the first method capable of producing detailed 3D MRIs from highly limited 2D input data, potentially accelerating MRI acquisition and reducing associated costs.",2025,0.613636140495949,0.6108320572609788,0.6,0.5,fcb3ffb3-b26b-4971-bd6c-65835c84965c,0,"[0.5, 0.5, 0.625, 0.625]","[0.95, 1.0, 0.9, 0.95]",X-Diffusion,0.5537981085543922
541762,Realizing Video Summarization from the Path of Language-based Semantic Understanding,"The recent development of Video-based Large Language Models (VideoLLMs), has significantly advanced video summarization by aligning video features—and, in some cases, audio features—with Large Language Models (LLMs). Each of these VideoLLMs possesses unique strengths and weaknesses. Many recent methods have required extensive fine-tuning to overcome the limitations of these models, which can be resource-intensive. In this work, we observe that the strengths of one VideoLLM can complement the weaknesses of another. Leveraging this insight, we propose a novel video summarization framework inspired by the Mixture of Experts (MoE) paradigm, which operates as an inference-time algorithm without requiring any form of fine-tuning. Our approach integrates multiple VideoLLMs to generate comprehensive and coherent textual summaries. It effectively combines visual and audio content, provides detailed background descriptions, and excels at identifying keyframes, which enables more semantically meaningful retrieval compared to traditional computer vision approaches that rely solely on visual information, all without the need for additional fine-tuning. Moreover, the resulting summaries enhance performance in downstream tasks such as summary video generation, either through keyframe selection or in combination with text-to-image models. Our language-driven approach offers a semantically rich alternative to conventional methods and provides flexibility to incorporate newer VideoLLMs, enhancing adaptability and performance in video summarization tasks.",2025,0.2045453801653163,0.2024620391164279,0.2666666666666666,0.25,d73bb42f-ba01-4913-91ba-e93027635033,0,"[0.0, 0.25, 0.25, 0.25]","[1.0, 1.0, 0.95, 0.95]",Mixture of Experts,0.1782912687609006
541779,Visually Descriptive Language Model for Vector Graphics Reasoning,"Despite significant advancements, current large multimodal models (LMMs) struggle to bridge the gap between low-level visual perception—focusing on shapes, sizes and layouts—and high-level language reasoning involving semantics, events and logic. This limitation becomes evident in tasks requiring precise visual perception, such as comparing geometric properties or solving visual algorithmic reasoning problems. To study this failure mode, we focus on an important visual domain: vector graphics—images composed purely of 2D objects and shapes, which are prevalent in various LMM-based agent tasks in web, visual design, and OS environments. We identify two key research questions: how can we enable precise visual perception, and how can we facilitate high-level reasoning based on such low-level perceptions? To accurately capture low-level visual details, we utilize Scalable Vector Graphics (SVG) for precise encoding of visual scenes. However, SVGs are not readily interpretable by LLMs or LMMs in a zero-shot manner. To address this challenge, we propose the Visually Descriptive Language Model (VDLM), which introduces an intermediate textual representation called Primal Visual Description (PVD). PVD translates SVGs into a text-based abstraction comprising primitive attributes (e.g., shape, position, measurement) along with their corresponding values. PVD can be learned with task-agnostic synthesized data and represents visual primitives that are universal across various vector graphics. This abstraction is more structured, allowing for direct interpretation by foundation models for zero-shot generalization to different reasoning tasks. Without any human-annotated data, empirical results demonstrate that VDLM leads to significant improvements in state-of-the-art LMMs, such as GPT-4o, across various low-level multimodal perception and reasoning tasks on vector graphics. Additionally, we provide extensive analyses of VDLM’s performance, showing that our framework offers improved interpretability due to its disentangled perception and reasoning processes. Finally, we demonstrate the promise of this representation by showing a positive correlation between the quality of the PVD perception and the end-task performance.",2025,0.4772725537190714,0.4736871339233398,0.5333333333333333,0.5,3af5dfc6-a6ce-457b-96d4-436ee3ce2fab,0,"[0.25, 0.5, 0.5, 0.5]","[0.95, 0.8, 0.95, 0.95]",Primal Visual Description,0.4285774983004759
541793,MULAN: Multimodal Protein Language Model for Sequence and Structure Encoding,"Most protein language models (PLMs), which produce high-quality protein representations, use only protein sequences during training.
However, the known protein structure is crucial in many protein property prediction tasks, so there is a growing interest in incorporating the knowledge about the protein structure into a PLM. Currently, structure-aware PLMs are trained from scratch or introduce a huge parameter overhead for the structure encoder. In this study, we propose MULAN, a MULtimodal PLM for both sequence and ANgle-based structure encoding. MULAN has a pre-trained sequence encoder and an introduced parameter-efficient Structure Adapter, which are then fused and trained together. According to the evaluation on 9 downstream tasks, MULAN models of various sizes show quality improvement compared to both sequence-only ESM2 and structure-aware SaProt as well as comparable performance to Ankh, ESM3, ProstT5, and other PLMs considered in the study. Importantly, unlike other models, MULAN offers a cheap increase in the structural awareness of the protein representations due to finetuning of existing PLMs instead of training from scratch. We perform a detailed analysis of the proposed model and demonstrate its awareness of the protein structure.",2025,0.4772725537190714,0.4773694104218989,0.5333333333333333,0.5,0f15b022-b753-48a1-bd21-83705f56cde1,0,"[0.25, 0.5, 0.5, 0.5]","[0.95, 0.95, 0.95, 1.0]",MULAN,0.4424705449189984
541795,Semantic or Covariate? A Study on the Intractable Case of Out-of-Distribution Detection,"The primary goal of out-of-distribution (OOD) detection tasks is to identify inputs with semantic shifts, i.e., if samples from novel classes are absent in the in-distribution (ID) dataset used for training, we should reject these OOD samples rather than misclassifying them into existing ID classes. However, we find the current definition of ""semantic shift"" is ambiguous, which renders certain OOD testing protocols intractable for the post-hoc OOD detection methods based on a classifier trained on the ID dataset. In this paper, we offer a more precise definition of the Semantic Space and the Covariate Space for the ID distribution, allowing us to theoretically analyze which types of OOD distributions make the detection task intractable. To avoid the flaw in the existing OOD settings, we further define the ""Tractable OOD"" setting which ensures the distinguishability of OOD and ID distributions for the post-hoc OOD detection methods. Finally, we conduct several experiments to demonstrate the necessity of our definitions and validate the correctness of our theorems.",2025,0.4090907603306326,0.4084157607097504,0.4,0.25,83387ab2-693e-441e-ba9c-06f5a2fd958e,0,"[0.25, 0.25, 0.5, 0.5]","[0.95, 0.9, 0.95, 0.9]",Tractable OOD,0.375
541804,Learning Splitting Heuristics in Divide-and-Conquer SAT Solvers with Reinforcement Learning,"We propose RDC-SAT, a novel approach to optimize splitting heuristics in Divide-and-Conquer SAT solvers using deep reinforcement learning.  Our method dynamically extracts features from the current solving state whenever a split is required.  These features, such as learned clauses, variable activity scores, and clause LBD (Literal Block Distance) values, are represented as a graph.  A GNN integrated with an Actor-Critic model processes this graph to determine the optimal split variable.  Unlike traditional linear state transitions characterized by Markov processes, divide-and-conquer challenges involve tree-like state transitions.  To address this, we developed a reinforcement learning environment based on the Painless framework that efficiently handles these transitions.  Additionally, we designed different discounted reward functions for satisfiable and unsatisfiable SAT problems, capable of handling tree-like state transitions.  We trained our model using the Decentralized Proximal Policy Optimization (DPPO) algorithm on phase transition random 3-SAT problems and implemented the RDC-SAT solver, which operates in both GPU-accelerated and non-GPU modes.  Evaluations show that RDC-SAT significantly improves the performance of D\&C solvers on phase transition random 3-SAT datasets and generalizes well to the SAT Competition 2023 dataset, substantially outperforming traditional splitting heuristics.",2025,0.6477270371901683,0.6417965065749491,0.6666666666666666,0.625,3b5f5991-6298-4e20-bcb1-edb37cd2c5bc,1,"[0.25, 0.625, 0.625, 0.875]","[1.0, 0.95, 0.95, 0.95]",RDC-SAT,0.566412002945508
541806,IGL-Bench: Establishing the Comprehensive Benchmark for Imbalanced Graph Learning,"Deep graph learning has gained grand popularity over the past years due to its versatility and success in representing graph data across a wide range of domains. However, the pervasive issue of imbalanced graph data distributions, where certain parts exhibit disproportionally abundant data while others remain sparse, undermines the efficacy of conventional graph learning algorithms, leading to biased outcomes. To address this challenge, Imbalanced Graph Learning (IGL) has garnered substantial attention, enabling more balanced data distributions and better task performance. Despite the proliferation of IGL algorithms, the absence of consistent experimental protocols and fair performance comparisons pose a significant barrier to comprehending advancements in this field. To bridge this gap, we introduce **IGL-Bench**, a foundational comprehensive benchmark for imbalanced graph learning, embarking on **17** diverse graph datasets and **24** distinct IGL algorithms with uniform data processing and splitting strategies. Specifically, IGL-Bench systematically investigates state-of-the-art IGL algorithms in terms of **effectiveness**, **robustness**, and **efficiency** on node-level and graph-level tasks, with the scope of class-imbalance and topology-imbalance. Extensive experiments demonstrate the potential benefits of IGL algorithms on various imbalanced conditions, offering insights and opportunities in the IGL field. Further, we have developed an open-sourced and unified package to facilitate reproducible evaluation and inspire further innovative research, available at: https://github.com/RingBDStack/IGL-Bench.",2025,0.8636364132231226,0.8622114134234825,0.9333333333333332,0.875,f771bf1d-aabc-4c65-9a0b-52dae79e376d,1,"[0.625, 0.875, 0.875]","[0.9, 0.9, 0.9]",IGL-Bench,0.791666661931764
541809,Understand Clean Generalization and Robust Overfitting in Adversarial Training from Two Theoretical Views: Representation Complexity and Training Dynamics,"Similar to surprising performance in the standard deep learning, deep nets trained by adversarial training also generalize well for unseen clean data (natural data). However, despite adversarial training can achieve low robust training error, there exists a significant robust generalization gap. We call this phenomenon the Clean Generalization and Robust Overfitting (CGRO). In this work, we study the CGRO phenomenon in adversarial training from two views: representation complexity and training dynamics. Specifically, we consider a binary classification setting with $N$ separated training data points. First, we prove that, based on the assumption that we assume there is $\operatorname{poly}(D)$-size clean classifier (where $D$ is the data dimension), ReLU net with only $O(N D)$ extra parameters is able to leverages robust memorization to achieve the CGRO, while robust classifier still requires exponential representation complexity in worst case. Next, we focus on a structured-data case to analyze training dynamics, where we train a two-layer convolutional network with $O(N D)$ width against adversarial perturbation. We then show that a three-stage phase transition occurs during learning process and the network provably converges to robust memorization regime, which thereby results in the CGRO. Besides, we also empirically verify our theoretical analysis by experiments in real-image recognition datasets.",2025,0.5999997818182612,0.5974970103966316,0.5333333333333333,0.5,a8e1ffc9-a174-42d9-bab3-594f7f0e7de8,0,"[0.5, 0.5, 0.5, 0.625, 0.625]","[0.95, 0.9, 0.9, 0.95, 0.8]",Clean Generalization and Robust Overfitting,0.5463646205305442
541814,Shake-It-Off: Jailbreaking Black-Box Large Language Models by Shaking Off Objectionable Semantics,"Large language models (LLMs) are vulnerable to jailbreaking attacks (Zou et al., 2023; Liu et al., 2024), in which attackers use adversarially designed prompts to bypass the model’s safeguard and force the model to generate objectionable content. The present paper studies jailbreaking attacks from a red team’s viewpoint and proposes a novel black-box attack method, called Shake-It-Off (SHAKE), that only requires the response generated by the victim model. Given objective query $T_{obj}$, our method iteratively shakes off the objectionable semantics of $T_{obj}$, making it gradually approximates a pre-defied decontaminated query $T_{dec}$. We conduct extensive experiments on multiple baseline methods and victim LLMs. The experimental results show that SHAKE outperforms the baseline methods in attack success rates while requiring much less running time and access to the victim model.",2025,0.5113634504132908,0.5074255430840509,0.5333333333333333,0.5,c63ada1a-6841-45de-aee3-918c1044b614,0,"[0.25, 0.5, 0.5, 0.625]","[1.0, 0.95, 0.95, 0.95]",SHAKE,0.4513530927835051
541829,ConDiff: A Challenging Dataset for Neural Solvers of Partial Differential Equations,"We present ConDiff, a novel dataset for scientific machine learning. ConDiff focuses on the diffusion equation with varying coefficients, a fundamental problem in many applications of parametric partial differential equations (PDEs). The main novelty of the proposed dataset is that we consider discontinuous coefficients with high contrast. These coefficient functions are sampled from a selected set of distributions. This class of problems is not only of great academic interest, but is also the basis for describing various environmental and industrial problems. In this way, ConDiff shortens the gap with real-world problems while remaining fully synthetic and easy to use. ConDiff consists of a diverse set of diffusion equations with coefficients covering a wide range of contrast levels and heterogeneity with a measurable complexity metric for clearer comparison between different coefficient functions. We baseline ConDiff on standard deep learning models in the field of scientific machine learning. By providing a large number of problem instances, each with its own coefficient function and right-hand side, we hope to encourage the development of novel physics-based deep learning approaches, such as neural operators, ultimately driving progress towards more accurate and efficient solutions of complex PDE problems.",2025,0.2727271735537551,0.2722771738065002,0.2666666666666666,0.25,4f968593-b6db-44e1-850c-14d63ab55170,0,"[0.25, 0.25, 0.25, 0.25]","[0.9, 0.9, 0.95, 0.95]",ConDiff,0.25
541845,Do Unlearning Methods Remove Information from Language Model Weights?,"Large Language Models' knowledge of how to perform cyber-security attacks, create bioweapons, and manipulate humans poses risks of misuse. Previous work has proposed methods to unlearn this knowledge. Historically, it has been unclear whether unlearning techniques are removing information from the model weights or just making it harder to access. To disentangle these two objectives, we propose an adversarial evaluation method to test for the removal of information from model weights: we give an attacker access to some facts that were supposed to be removed, and using those, the attacker tries to recover other facts from the same distribution that cannot be guessed from the accessible facts. We show that using fine-tuning on the accessible facts can recover 88% of the pre-unlearning accuracy when applied to current unlearning methods, revealing the limitations of these methods in removing information from the model weights.",2025,0.613636140495949,0.6091330476964262,0.6,0.25,942dfdf7-b0f2-41b0-9910-4e8c9ce1d71d,0,"[0.25, 0.5, 0.625, 0.875]","[1.0, 0.95, 1.0, 0.95]",Adversarial evaluation,0.5440825375170533
541862,Adaptive Rank Allocation: Speeding Up Modern Transformers with RaNA Adapters,"Large Language Models (LLMs) are computationally intensive, particularly during inference. Neuron-adaptive techniques, which selectively activate neurons in Multi-Layer Perceptron (MLP) layers, offer some speedups but suffer from limitations in modern Transformers. These include reliance on sparse activations, incompatibility with attention layers, and the use of costly neuron masking techniques. To address these issues, we propose the Adaptive Rank Allocation framework and introduce the Rank and Neuron Allocator (RaNA) adapter. RaNA adapters leverage rank adapters, which operate on linear layers by applying both low-rank matrix decompositions and adaptive masking to efficiently allocate compute without depending on activation sparsity. This enables RaNA to be generally applied to MLPs and linear components of attention modules, while eliminating the need for expensive maskers found in neuron-adaptive methods. Notably, when compared to neuron adapters, RaNA improves perplexity by up to 7 points and increases accuracy by up to 8 percentage-points when reducing FLOPs by $\sim$44\% in state-of-the-art Transformer architectures. These results position RaNA as a robust solution for improving inference efficiency in  modern Transformer architectures.",2025,0.6818179338843877,0.6806929345162506,0.6666666666666666,0.625,48931eb9-3bb8-4319-8d71-ecbfeb9f7462,1,"[0.625, 0.625, 0.625, 0.625]","[0.95, 0.9, 0.9, 0.9]",RaNA Adapters,0.6250000000000001
541871,Instant Transformer Adaption via HyperLoRA,"While Foundation Models provide a general tool for rapid content creation, they regularly require task-specific adaptation. Traditionally, this exercise involves careful curation of datasets and repeated fine-tuning of the underlying model. Fine-tuning techniques enable practitioners to adapt foundation models for many new applications but require expensive and lengthy training while being notably sensitive to hyper-parameter choices. To overcome these limitations, we introduce HyperLoRA, a model capable of adapting Large Language Models on the fly---solely based on a natural language description of the target task.  HyperLoRA is a hypernetwork trained to construct LoRAs in a single inexpensive forward pass. After training HyperLoRA on a suite of 9 pre-trained LoRA adapters (GSM8K, Arc, etc.), we show that the ad-hoc reconstructed LoRA instances match the performance of task-specific adapters across the corresponding test sets.
Furthermore, HyperLoRA can compress hundreds of LoRA instances and zero-shot generalize to entirely unseen tasks. This approach provides a significant step towards democratizing the specialization of foundation models and enables language-based adaptation with minimal compute requirements. Our code and pre-trained checkpoints will be available through https://github.com/AnonymousAuthor/hyperlora and https://huggingface.co/ upon publication.",2025,0.5795452438017296,0.5781228558172563,0.5333333333333333,0.5,da3152f0-df4a-4ce9-93b8-3b52987d46dd,0,"[0.5, 0.5, 0.5, 0.625]","[0.9, 0.9, 0.95, 0.9]",HyperLoRA,0.5291043743078626
541892,Block Diffusion: Interpolating Between Autoregressive and Diffusion Language Models,"Diffusion language models offer unique benefits over autoregressive models due to their potential for parallelized generation and controllability, yet they lag in likelihood modeling and are limited to fixed-length generation. In this work, we introduce a class of block diffusion language models that interpolate between discrete denoising diffusion and autoregressive models. Block diffusion overcomes key limitations of both approaches by supporting flexible-length generation and improving inference efficiency with KV caching and parallel token sampling. We propose a recipe for building effective block diffusion models that includes an efficient training algorithm, estimators of gradient variance, and data-driven noise schedules to minimize the variance. Block diffusion sets a new state-of-the-art performance among diffusion models on language modeling benchmarks and enables generation of arbitrary-length sequences. We provide the code, along with the model weights and blog post on the project page: https://m-arriola.com/bd3lms/",2025,0.9545451074381428,0.9529701083227508,0.9333333333333332,0.875,e388b0fd-1afa-4e1b-944e-1e1106c2aba6,1,"[0.875, 0.875, 0.875, 0.875]","[0.9, 0.95, 0.9, 0.8]",Block Diffusion,0.874989054726746
541900,DM3D: Parameter-Efficient and Lossless 3D Object Detection via Distortion Minimization,"Recent advancements in 3D deep learning have garnered significant attention, given their superior performance in fields like AR/VR, autonomous driving, and robotics. 
However, as the models and point cloud data continues to scale up, managing computational and memory demands becomes a critical challenge, particularly for real-world applications with strict latency and energy requirements.
Previous methods have primarily focused on reducing computational costs and memory usage by addressing spatial redundancy, \textit{i.e.}, filtering out irrelevant points or voxels. In contrast, this work presents a novel post-training weight pruning technique tailored specifically for 3D object detection.
Our approach stands out in two key ways: (1) it operates independently from existing point cloud sparsification methods, targeting redundant parameters in pre-trained models that minimally affect both spatial accuracy and detection confidence (collectively referred to as ""detection distortion""), and (2) it provides a flexible, plug-and-play framework compatible with other sparsity schemes including spatial sparsity and with any 3D detection model.
Our method reduces detection distortion by employing a second-order Taylor approximation to identify layer-wise sparsity, allowing for a substantial reduction in model complexity without sacrificing detection accuracy. 
To efficiently manage the necessary second-order information, we devised a lightweight algorithm to gather Hessian information, followed by dynamic programming to optimize layer-wise sparsity allocation.
Extensive experiments on the KITTI, nuScenes, and ONCE datasets validate the effectiveness of our approach, where we not only preserve detection performance but also notice enhancement while significantly reducing computational overhead. 
Noticeably, we achieve FLOPs reductions for Centerpoint model of as much as $\mathbf{3.89}\times$ and $\mathbf{3.01}\times$ on ONCE and nuScenes datasets respectively, without noticeable loss in mean Average Precision (mAP), and at most $\mathbf{1.65}\times$ reduction \textbf{losslessly} for PVRCNN model on the ONCE dataset, thus pushing the boundaries of state-of-the-art performance.",2025,0.4090907603306326,0.4084157607097504,0.4,0.25,519a486a-751e-472b-ab13-5f2b2a04fbbe,0,"[0.25, 0.25, 0.5, 0.5]","[0.95, 0.95, 1.0, 0.9]",Distortion Minimization,0.3785779477394135
541911,Interpretable Vision-Language Survival Analysis with Ordinal Inductive Bias for Computational Pathology,"Histopathology Whole-Slide Images (WSIs) provide an important tool to assess cancer prognosis in computational pathology (CPATH). While existing survival analysis (SA) approaches have made exciting progress, they are generally limited to adopting highly-expressive network architectures and only coarse-grained patient-level labels to learn visual prognostic representations from gigapixel WSIs. Such learning paradigm suffers from critical performance bottlenecks, when facing present scarce training data and standard multi-instance learning (MIL) framework in CPATH. To overcome it, this paper, for the first time, proposes a new Vision-Language-based SA (**VLSA**) paradigm. Concretely, (1) VLSA is driven by pathology VL foundation models. It no longer relies on high-capability networks and shows the advantage of *data efficiency*. (2) In vision-end, VLSA encodes textual prognostic prior and then employs it as *auxiliary signals* to guide the aggregating of visual prognostic features at instance level, thereby compensating for the weak supervision in MIL. Moreover, given the characteristics of SA, we propose i) *ordinal survival prompt learning* to transform continuous survival labels into textual prompts; and ii) *ordinal incidence function* as prediction target to make SA compatible with VL-based prediction. Notably, VLSA's predictions can be interpreted intuitively by our Shapley values-based method. The extensive experiments on five datasets confirm the effectiveness of our scheme. Our VLSA could pave a new way for SA in CPATH by offering weakly-supervised MIL an effective means to learn valuable prognostic clues from gigapixel WSIs. Our source code is available at https://github.com/liupei101/VLSA.",2025,0.6363630413225304,0.6377014578808994,0.6666666666666666,0.625,776c33f4-d13a-43e2-ada0-2c42ee59fc4d,1,"[0.5, 0.625, 0.625]","[0.9, 0.95, 1.0]",Vision-Language-based Survival Analysis,0.5940386145980937
541918,Physics-Informed Diffusion Models,"Generative models such as denoising diffusion models are quickly advancing their ability to approximate highly complex data distributions. They are also increasingly leveraged in scientific machine learning, where samples from the implied data distribution are expected to adhere to specific governing equations. We present a framework that unifies generative modeling and partial differential equation fulfillment by introducing a first-principle-based loss term that enforces generated samples to fulfill the underlying physical constraints. Our approach reduces the residual error by up to two orders of magnitudes compared to previous work in a fluid flow case study and outperforms task-specific frameworks in relevant metrics for structural topology optimization. We also present numerical evidence that our extended training objective acts as a natural regularization mechanism against overfitting. Our framework is simple to implement and versatile in its applicability for imposing equality and inequality constraints as well as auxiliary optimization objectives. Code is available at https://github.com/jhbastek/PhysicsInformedDiffusionModels.",2025,0.6477270371901683,0.6480196736594706,0.6666666666666666,0.625,68d2f7f8-c6d1-4a10-b75e-a8e755deefff,1,"[0.5, 0.625, 0.625, 0.625]","[0.9, 0.95, 0.95, 0.95]",Physics-Informed Diffusion,0.5994096884128528
541933,DOTS: Learning to Reason Dynamically in LLMs via Optimal Reasoning Trajectories Search,"Enhancing the capability of large language models (LLMs) in reasoning has gained significant attention in recent years. Previous studies have demonstrated the effectiveness of various prompting strategies in aiding LLMs in reasoning (called ""reasoning actions""), such as step-by-step thinking, reflecting before answering, solving with programs, and their combinations. However, these approaches often applied static, predefined reasoning actions uniformly to all questions, without considering the specific characteristics of each question or the capability of the task-solving LLM. In this paper, we propose DOTS, an approach enabling LLMs to reason Dynamically via Optimal reasoning Trajectories Search, tailored to the specific characteristics of each question and the inherent capability of the task-solving LLM. 
Our approach involves three key steps: i) defining atomic reasoning action modules that can be composed into various reasoning action trajectories; ii) searching for the optimal action trajectory for each training question through iterative exploration and evaluation for the specific task-solving LLM; and iii) using the collected optimal trajectories to train an LLM to plan for the reasoning trajectories of unseen questions. In particular, we propose two learning paradigms, i.e., fine-tuning an external LLM as a planner to guide the task-solving LLM, or directly fine-tuning the task-solving LLM with an internalized capability for reasoning actions planning. Our experiments across eight reasoning tasks show that our method consistently outperforms static reasoning techniques and the vanilla instruction tuning approach. Further analysis reveals that our method enables LLMs to adjust their computation based on problem complexity, allocating deeper thinking and reasoning to harder problems.",2025,0.7159088305786071,0.7169961946542188,0.6666666666666666,0.625,e2841036-bb35-40bf-9488-36086fbcf168,1,"[0.5, 0.625, 0.625, 0.875]","[0.9, 0.95, 0.95, 0.95]",Optimal Reasoning Trajectories,0.6656828140214215
541948,Node-wise Filtering in Graph Neural Networks: A Mixture of Experts Approach,"Graph Neural Networks (GNNs) have proven to be highly effective for node classification tasks across diverse graph structural patterns. Traditionally, GNNs employ a uniform global filter—typically a low-pass filter for homophilic graphs and a high-pass filter for heterophilic graphs. However, real-world graphs often exhibit a complex mix of homophilic and heterophilic patterns, rendering a single filter approach suboptimal. In this work, we theoretically demonstrate that a global filter optimized for one pattern can adversely affect performance on nodes with differing patterns. To address this, we introduce a novel GNN framework Node-MoE that utilizes a mixture of experts to adaptively select the appropriate filters for different nodes. Extensive experiments demonstrate the effectiveness of the proposed Node-MoE on both homophilic and heterophilic graphs.",2025,0.5454543471075102,0.547481871785768,0.5333333333333333,0.5,f0528b9d-e1c5-4075-90f9-75e219293322,0,"[0.25, 0.5, 0.5, 0.625, 0.625]","[0.9, 0.9, 0.95, 0.95, 0.95]",Node-MoE,0.5125277833905839
541961,Scaling Long Context Training Data by Long-Distance Referrals,"Training large language models for long context understanding faces the challenge of data shortage.
Previous data engineering approaches mechanically concatenate short documents, which may create many pseudo long documents but raise concerns about data quality.
In this paper, we study the core attribute of high quality data for long context training, and provide a data pipeline, LongPack, to scale
such data.
We found that long distance referrals, which occur in natural long documents, are crucial for long-context training.
However, simply concatenating short documents does not reliably generate these relations.
We further show that the density of long-distance referrals, which is higher in longer documents, has a key role in training efficiency, making previous upsampling methods suboptimal.
To enrich long documents, we propose LongPack, a data pipeline that constructs long documents by packing shorter ones based on referral relationships.
Specifically, for web pages, which are the primary source for language model training, we found hyper-link a native signal for such a relation.
By packing web pages through their hyper-link connection, we can create longer, high-quality documents.
Our experiments demonstrate that LongPackis highly scalable, generating a corpus of long documents equivalent in size to an entire pretraining dataset using just 0.5% root documents.
Furthermore, the constructed documents have a ‘near-natural’ quality as innate long documents for long context training, reaching a 32.7% higher score than previous state-of-the-art methods.",2025,0.613636140495949,0.6126236410646255,0.6,0.5,b12c50fb-3330-4104-a421-ec2343d783d4,1,"[0.5, 0.5, 0.625, 0.625]","[0.9, 0.95, 0.9, 0.95]",LongPack,0.5625
541968,Asymmetric Embedding Models for Hierarchical Retrieval: Provable Constructions and a Pretrain-Finetune Recipe,"Dual encoder (DE) models, where a pair of matching query and document are embedded into similar vector representations, are widely used in information retrieval due to their efficiency and scalability. However, DEs are known to have a limited expressive power due to the Euclidean geometry of the embedding space, which may compromise their quality. This paper investigate such limitations in the context of \emph{hierarchical retrieval}, the task where the document set has a hierarchical structure and the matching keywords for a query are all of its ancestor nodes. We first prove the feasibility of representing hierarchical structures within the Euclidean embedding space by providing a constructive algorithm for generating effective embeddings from a given hierarchy. Then we delve into the learning of DEs when the hierarchy is unknown, which is a practical assumption since usually only samples of matching query and document pairs are available during training. Our experiments reveal a ""lost in the long distance"" phenomenon, where retrieval accuracy degrades for documents further away in the hierarchy. To address this, we introduce a pretrain-finetune approach that significantly improves long-distance retrieval without sacrificing performance on closer documents. Finally, we validate our findings on a realistic hierarchy from WordNet, demonstrating the effectiveness of our approach in retrieving documents at various levels of abstraction.",2025,0.3409089669421938,0.3412536948012485,0.4,0.5,7b5a3844-1646-4c86-ba16-f841063482b5,0,"[0.0, 0.25, 0.5, 0.5]","[0.95, 0.9, 0.95, 0.95]",Asymmetric Embedding,0.3162731256085687
541970,"IDEA: Enhancing the Rule Learning Ability of Large Language Model Agent through Induction, Deduction, and Abduction","While large language models (LLMs) have been thoroughly evaluated for deductive and inductive reasoning, their proficiency in abductive reasoning and holistic rule learning in interactive environments remains less explored. We introduce RULEARN, a novel benchmark specifically designed to assess the rule-learning abilities of LLM agents in interactive settings. In RULEARN, agents strategically interact with simulated environments to gather observations, discern patterns, and solve complex problems. To enhance the rule-learning capabilities for LLM agents, we propose IDEA, a novel reasoning framework that integrates the process of Induction, Deduction, and Abduction. The IDEA agent generates initial hypotheses from limited observations through abduction, devises plans to validate these hypotheses or leverages them to solve problems via deduction, and refines previous hypotheses using patterns identified from new observations through induction, dynamically establishing and applying rules that mimic human rule-learning behaviors. Our evaluation of the IDEA framework, which involves five representative LLMs, demonstrates significant improvements over the baseline. Furthermore, within this framework, our comparison with 50 human participants reveals notable discrepancies in rule-learning behaviors. LLM agents tend to generate plausible initial hypotheses but struggle to refine them through interaction. Conversely, humans, despite sometimes overlooking initial details, excel at incorporating feedback and continuously improving their hypotheses. We believe our benchmark, RULEARN, will serve as a valuable and challenging resource, and that the IDEA framework will provide crucial insights for the development of LLM agents capable of human-like rule learning in real-world scenarios. We will release our code and data upon acceptance of the paper.",2025,0.4909089123967591,0.4906663384819131,0.5333333333333333,0.25,e72ff702-c320-4afb-95d7-a5e862ecfa31,0,"[0.25, 0.25, 0.5, 0.625, 0.625]","[0.95, 0.95, 1.0, 0.95, 0.95]",RULEARN,0.4532325651002694
541971,Flow Matching with General Discrete Paths: A Kinetic-Optimal Perspective,"The design space of discrete-space diffusion or flow generative models are significantly less well-understood than their continuous-space counterparts, with many works focusing only on a simple masked construction.
In this work, we aim to take a holistic approach to the construction of discrete generative models based on continuous-time Markov chains, and for the first time, allow the use of arbitrary discrete probability paths, or colloquially, corruption processes. 
Through the lens of optimizing the symmetric kinetic energy, we propose velocity formulas that can be applied to any given probability path, completely decoupling the probability and velocity, and giving the user the freedom to specify any desirable probability path based on expert knowledge specific to the data domain. 
Furthermore, we find that a special construction of mixture probability paths optimizes the symmetric kinetic energy for the discrete case.
We empirically validate the usefulness of this new design space across multiple modalities: text generation, inorganic material generation, and image generation. We find that we can outperform the mask construction even in text with kinetic-optimal mixture paths, while we can make use of domain-specific constructions of the probability path over the visual domain.",2025,0.6545452165290122,0.6547623455916147,0.6666666666666666,0.625,7bdf79fe-228a-4bda-adc4-486a17e03f84,1,"[0.0, 0.625, 0.625, 0.875, 0.875]","[0.8, 0.9, 0.9, 0.8, 0.8]",Kinetic-Optimal Mixture Paths,0.6044652285412582
541977,Cybench: A Framework for Evaluating Cybersecurity Capabilities and Risks of Language Models,"Language Model (LM) agents for cybersecurity that are capable of autonomously identifying vulnerabilities and executing exploits have potential to cause real-world impact. Policymakers, model providers, and researchers in the AI and cybersecurity communities are interested in quantifying the capabilities of such agents to help mitigate cyberrisk and investigate opportunities for penetration testing. Toward that end, we introduce Cybench, a framework for specifying cybersecurity tasks and evaluating agents on those tasks. We include 40 professional-level Capture the Flag (CTF) tasks from 4 distinct CTF competitions, chosen to be recent, meaningful, and spanning a wide range of difficulties. Each task includes its own description, starter files, and is initialized in an environment where an agent can execute commands and observe outputs. Since many tasks are beyond the capabilities of existing LM agents, we introduce subtasks for each task, which break down a task into intermediary steps for a more detailed evaluation. To evaluate agent capabilities, we construct a cybersecurity agent and evaluate 8 models: GPT-4o, OpenAI o1-preview, Claude 3 Opus, Claude 3.5 Sonnet, Mixtral 8x22b Instruct, Gemini 1.5 Pro, Llama 3 70B Chat, and Llama 3.1 405B Instruct. For the top performing models (GPT-4o and Claude 3.5 Sonnet), we further investigate performance across 4 agent scaffolds (structured bash, action-only, pseudoterminal, and web search). Without subtask guidance, agents leveraging Claude 3.5 Sonnet, GPT-4o, OpenAI o1-preview, and Claude 3 Opus successfully solved complete tasks that took human teams up to 11 minutes to solve. In comparison, the most difficult task took human teams 24 hours and 54 minutes to solve. Anonymized code and data are available at https://drive.google.com/file/d/1kp3H0pw1WMAH-Qyyn9WA0ZKmEa7Cr4D4 and https://drive.google.com/file/d/1BcTQ02BBR0m5LYTiK-tQmIK17_TxijIy.",2025,0.6363630413225304,0.6296409644275318,0.9333333333333332,0.875,653019ee-c218-4c21-b999-673a871e6c56,1,"[0.0, 0.875, 0.875]","[0.95, 0.95, 0.9]",Cybench,0.5593739648890363
541981,Measuring Language Model Uncertainty With Internal Concepts,"We study the problem of evaluating the predictive uncertainty of large language models (LLMs). 
We assign an uncertainty measure to the correctness of outputs from an LLM conditioned on a query using a form of entropy that applies to semantic objects (concepts).
Unlike prior works, the notion of meaning used to define concepts is derived from the LLM, rather than from
an external model. 
Our method measures an uncertainty over concept structures by drawing from ideas in Formal Concept Analysis (FCA) and lattice/order theory, and can be used to estimate correctness in closed- and open-ended scenarios.
Our method has a relative improvement of up to 4.8% on average across five standard benchmarks as well as improves over comparable baselines on datasets consisting of both closed- and open-ended questions.",2025,0.6272724991736367,0.6227948271693411,0.6666666666666666,0.625,0a44d0c9-5058-418e-a6d5-d48f828313e3,0,"[0.5, 0.5, 0.625, 0.625, 0.625]","[0.95, 0.9, 0.9, 0.8, 0.8]",Conceptual Entropy,0.5625104876937459
541993,E2LLM: Encoder Elongated Large Language Models for Long-Context Understanding and Reasoning,"In the realm of Large Language Models (LLMs), the ability to process long contexts is increasingly crucial for tasks such as multi-round dialogues, code generation, and document summarization. This paper addresses the challenges of achieving high long-context performance, low computational complexity, and compatibility with pretrained models -- collectively termed the ``impossible triangle''. We introduce E2LLM (Encoder Elongated Large Language Models), a novel approach that effectively navigates this paradox. The method involves splitting long contexts into chunks, compressing each into soft prompts via a pretrained text encoder, and utilizing an adapter to align these representations with a decoder-only LLM. To further enhance the LLM's understanding and reasoning capabilities regarding the soft prompts, we implement two training objectives: one focused on reconstructing the encoder output and the other on long-context instruction fine-tuning. Extensive experiments including Needle in a Haystack and LongBench reveal that E2LLM not only outperforms seven existing state-of-the-art (SOTA) methods across various long-context tasks, but also achieves the lowest inference time and memory usage. Code will be available upon publication.",2025,0.6272724991736367,0.6259412621898491,0.6666666666666666,0.625,07254413-5ded-40ef-950a-5b47a32c6ca1,0,"[0.5, 0.5, 0.625, 0.625, 0.625]","[0.95, 0.9, 0.9, 0.95, 0.9]",E2LLM,0.573681131674112
542021,SteBen: Steiner Tree Problem Benchmark for Neural Combinatorial Optimization on Graphs,"The Steiner Tree Problem (STP) is an NP-hard combinatorial optimization problem with applications in areas like network design and facility location. Despite its importance, learning-based solvers for STP have been hindered by the lack of large-scale, diverse datasets necessary to train and evaluate advanced neural models. To address this limitation, we introduce a standardized dataset comprising over a million high-quality instances with optimal solutions, spanning various problem sizes and graph structures. Our dataset enables benchmarking of neural combinatorial optimization methods across both supervised and reinforcement learning paradigms, encompassing autoregressive and non-autoregressive inference approaches. Our experiments show that supervised learning excels in in-distribution settings, while reinforcement learning generalizes better to unseen problem sizes, highlighting a trade-off between solution quality and generalization. We compare NCO methods across different STP scales and graph types, and demonstrate that solvers trained on our datasets generalize well to real-world instances without fine-tuning, proving its practical utility. We hope this benchmark promotes further STP research and advances NCO techniques for broader combinatorial optimization challenges.",2025,0.3409089669421938,0.3412536948012485,0.2666666666666666,0.25,e9b0a66c-b042-4eb5-a2e6-e6cea5ce497f,0,"[0.25, 0.25, 0.25, 0.5]","[0.95, 0.95, 0.9, 0.95]",Steiner Tree Problem Benchmark,0.3162731256085687
542065,Mitigate Position Bias in Large Language Models via Scaling a Single Dimension,"Large Language Models (LLMs) are increasingly applied in various real-world scenarios due to their excellent generalization capabilities and robust generative abilities. However, they exhibit position bias, also known as ""lost in the middle"", a phenomenon that is especially pronounced in long-context scenarios, which indicates the placement of the key information in different positions of a prompt can significantly affect accuracy. This paper first explores the micro-level manifestations of position bias, concluding that attention weights are a micro-level expression of position bias. It further identifies that, in addition to position embeddings, causal attention mask also contributes to position bias by creating position-specific hidden states. Based on these insights, we propose a method to mitigate position bias by scaling this positional hidden states. Experiments on the NaturalQuestions Multi-document QA, KV retrieval, LongBench and timeline reorder tasks, using various models including RoPE models, context window-extended models, and Alibi models, demonstrate the effectiveness and generalizability of our approach. Our method can improve performance by up to 15.2% by modifying just one dimension of hidden states.",2025,0.5113634504132908,0.5132795023208906,0.5333333333333333,0.5,60eec8a8-255d-4eca-bca4-9ce9b2329d23,0,"[0.25, 0.5, 0.5, 0.625]","[0.9, 0.95, 0.9, 0.95]",Position Bias Mitigation,0.4807966321243523
542074,Understanding the Interplay between Parametric and Contextual Knowledge for Large Language Models,"Large language models (LLMs) encode vast amounts of knowledge during pre-training (parametric knowledge or PK) and can further be enhanced by incorporating contextual knowledge (CK). Can LLMs effectively integrate their internal PK with external CK to solve complex problems? In this paper, we investigate the dynamic interaction between PK and CK, categorizing their relationships into Supportive, Complementary, Conflicting, and Irrelevant types. To support this investigation, we introduce EchoQA, a benchmark spanning scientific, factual, and commonsense knowledge. Our results show that LLMs tend to suppress their PK when contextual information is available, even when it is complementary or irrelevant. While tailored instructions can encourage LLMs to rely more on their PK, they still struggle to fully leverage it. These findings reveal a key vulnerability in LLMs, raising concerns about their reliability in knowledge-intensive tasks.",2025,0.5454543471075102,0.541718308570632,0.5333333333333333,0.5,e63eb487-517b-4216-9c54-bd9cc4399d44,0,"[0.25, 0.5, 0.5, 0.625, 0.625]","[1.0, 0.95, 0.95, 0.95, 0.95]",Interplay,0.4838371744986531
542081,RaCNN: Region-aware Convolutional Neural Network with Global Receptive Field,"Recent Convolutional Neural Networks (CNNs) utilize large-kernel convolutions (e.g., 101 kernel convolutions) to simulate a large receptive field of Vision Transformers (ViTs). 
    However, these models introduce specialized techniques like re-parameterization, sparsity, and weight decomposition, increasing the complexity of the training and inference stages. 
    To address this challenge, we propose Region-aware CNN (RaCNN), which achieves a global receptive field without requiring extra complexity, yet surpasses state-of-the-art models. 
    Specifically, we design two novel modules to capture global visual dependencies. 
    The first is the Region-aware Feed Forward Network (RaFFN). 
    It uses a novel Region Point-Wise Convolution (RPWConv) to capture global visual cues in a region-aware manner. 
    In contrast, traditional PWConv shares the same weights for all spatial pixels and cannot capture spatial information. 
    The second is the Region-aware Gated Linear Unit (RaGLU). 
    This channel mixer captures long-range visual dependencies in a sparse global manner and can become a better substitute for the original FFN. 
    Under only 84\% computational complexity, RaCNN significantly outperforms the state-of-the-art CNN model MogaNet (83.9\% vs. 83.4\%). 
    It also demonstrates good scalability and surpasses existing state-of-the-art lightweight models. 
    Furthermore, our RaCNN shows comparability with state-of-the-art ViTs, MLPs, and Mambas in object detection, instance segmentation, and semantic segmentation.  
    All codes and logs are released in the supplementary materials.",2025,0.4909089123967591,0.4872323787658655,0.5333333333333333,0.5,b6639ffd-4847-4840-88d9-ab5bcb40eec7,0,"[0.25, 0.5, 0.5, 0.5, 0.5]","[1.0, 1.0, 0.9, 0.9, 0.95]",RaCNN,0.4357492466128798
542083,Cheating Automatic LLM Benchmarks: Null Models Achieve High Win Rates,"Automatic LLM benchmarks, such as AlpacaEval 2.0, Arena-Hard-Auto, and MT-Bench, have become popular for evaluating language models due to their cost-effectiveness and scalability compared to human evaluation. Achieving high win rates on these benchmarks can significantly boost the promotional impact of newly released language models. This promotional benefit may motivate tricks, such as manipulating model output length or style to game win rates, even though several mechanisms have been developed to control length and disentangle style to reduce gameability. Nonetheless, we show that even a **""null model""** that always outputs a **constant** response (*irrelevant to input instructions*) can cheat automatic benchmarks and achieve top-ranked win rates: an $86.5\\%$ LC win rate on AlpacaEval 2.0; an $83.0$ score on Arena-Hard-Auto; and a $9.55$ score on MT-Bench. Moreover, the crafted cheating outputs are **transferable** because we assume that the instructions of these benchmarks (e.g., $805$ samples of AlpacaEval 2.0) are *private* and cannot be accessed. While our experiments are primarily proof-of-concept, an adversary could use LLMs to generate more imperceptible cheating responses, unethically benefiting from high win rates and promotional impact. Our findings call for the development of anti-cheating mechanisms for reliable automatic benchmarks. The code is available at https://github.com/sail-sg/Cheating-LLM-Benchmarks.",2025,0.613636140495949,0.6071040381972203,0.6666666666666666,0.625,af8cbb96-7fc5-47e0-bc89-1bb008cec60f,1,"[0.0, 0.0, 0.625, 0.625, 0.625, 0.875, 0.875, 0.875]","[0.95, 0.95, 0.95, 0.9, 0.9, 0.95, 0.9, 0.9]",Null Model,0.5384067357512954
542088,DEAL: High-Efficacy Privacy Attack on Retrieval-Augmented Generation Systems via LLM Optimizer,"Retrieval-Augmented Generation (RAG) technology provides a powerful means of combining private databases with large language models (LLMs). 
In a typical RAG system, a set of documents is retrieved from a private database and inserted into the final prompt, which is then fed into the LLM.
Existing research has shown that an attacker can use a simple manually designed attack suffix to induce LLM to output private documents in prompt with high probability.
However, in this paper, we demonstrate that the privacy leakage risk exhibited by using this simple manual attack suffix is significantly underestimated.
We propose a novel attack method called Documents Extraction Attack via LLM-Optimizer (DEAL). 
DEAL leverages an LLM as optimizer to iteratively refine attack strings, inducing the RAG model to reveal private data in its responses. 
Notably, our attack method does not require any knowledge about the target LLM, including its gradient information or model type. 
Instead, the attack can be executed solely through query access to the RAG model. 
We evaluate the effectiveness of our attack on multiple LLM architectures, including Qwen2, Llama3.1, and GPT-4o, across different attack tasks such as Entire Documents Extraction and Private Identity Information (PII) Extraction. 
Under the same permission setting as the existing method, the Mean Rouge-L Recall (MRR) of our method  can reach more than 0.95 on average in the Entire Documents Extraction task, and we can steal PII from the retrieved documents with close to 99\% accuracy in the PII Extraction task, highlighting the risk of privacy leakage in RAG systems.",2025,0.4772725537190714,0.4856836662012542,0.5333333333333333,0.5,01cd850f-28be-426d-9c87-f908196c4acd,0,"[0.25, 0.5, 0.5, 0.5]","[0.8, 1.0, 0.95, 0.95]",DEAL,0.4674784982215495
542092,iAgent: LLM Agent as a Shield between User and Recommender Systems,"Traditional recommender systems usually take the user-platform paradigm, where users are directly exposed under the control of the platform’s recommendation algorithms. However, the defect of recommendation algorithms may put users in very vulnerable positions under this paradigm. First, many sophisticated models are often designed with commercial objectives in mind, focusing on the platform’s
benefits, which may hinder their ability to protect and capture users’ true interests. Second, these models are typically optimized using data from all users, which may overlook individual user’s preferences. Due to these shortcomings, users may experience several disadvantages under the traditional user-platform direct exposure paradigm, such as lack of control over the recommender system, potential manipulation by the platform, echo chamber effects, or lack of personalization for less active users due to the dominance of active users during collaborative learning. Therefore, there is an urgent need to develop a new paradigm to protect user interests and alleviate these issues. Recently, some researchers have introduced LLM agents to simulate user behaviors, these approaches primarily aim to optimize platform-side performance, leaving core issues in recommender systems unresolved. To address these limitations, we propose a new user-agent-platform paradigm, where agent serves as the protective shield between user and recommender system
that enables indirect exposure. To this end, we first construct four recommendation datasets, denoted as InstructRec, along with user instructions for each record. To understand user’s intention, we design an Instruction-aware Agent (iAgent) capable of using tools to acquire knowledge from external environments. Moreover, we introduce an Individual Instruction-aware Agent ( i$^2$Agent), which incorporates a dynamic memory mechanism to optimize from individual feedback. Results on four InstructRec datasets demonstrate that i2Agent consistently achieves an average improvement of 16.6% over SOTA baselines across ranking metrics. Moreover, i$^2$Agent mitigates echo chamber effects and effectively alleviates the model bias in disadvantaged users (less-active), serving as a shield between user and recommender systems.",2025,0.4090907603306326,0.4048325931024568,0.4,0.25,ec255659-54ad-4b2c-a73d-6bc1d5681346,0,"[0.25, 0.25, 0.5, 0.5]","[0.95, 1.0, 0.95, 0.9]",iAgent,0.3575962171087842
542097,Event Camera Object Detection at Arbitrary Frequencies,"Event cameras offer unparalleled advantages for real-time perception in dynamic environments, thanks to their microsecond-level temporal resolution and asynchronous operation. However, existing event-based object detection methods are limited by fixed-frequency paradigms, which fail to fully exploit the high-temporal resolution and adaptability of event cameras. To address these limitations, we propose FlexEvent, a novel event camera object detection framework that enables detection at arbitrary frequencies. FlexEvent consists of two key components: FlexFuser, an adaptive event-frame fusion module that integrates high-frequency event data with rich semantic information from RGB frames, and FAL, a frequency-adaptive learning mechanism that generates frequency-adjusted labels to enhance model generalization across varying operational frequencies. This combination allows FlexEvent to detect objects with high accuracy in both fast-moving and static scenarios, while adapting to dynamic environments. Extensive experiments on large-scale event camera datasets demonstrate that our approach surpasses state-of-the-art methods, achieving significant improvements in both standard and high-frequency settings. Notably, FlexEvent maintains robust performance when scaling from 20 Hz to 90 Hz and delivers accurate detection up to 180 Hz, proving its effectiveness in extreme conditions. Our framework sets a new benchmark for event-based object detection and paves the way for more adaptable, real-time vision systems. The code will be made publicly available to facilitate future research.",2025,0.5909092396693675,0.5899342396169823,0.5333333333333333,0.5,7a66f570-321c-4ffe-b442-cb9e686662f3,0,"[0.5, 0.5, 0.625]","[0.95, 0.95, 0.95]",FlexEvent,0.5416666666666666
542102,Lotus: Diffusion-based Visual Foundation Model for High-quality Dense Prediction,"Leveraging the visual priors of pre-trained text-to-image diffusion models offers a promising solution to enhance zero-shot generalization in dense prediction tasks. However, existing methods often uncritically use the original diffusion formulation, which may not be optimal due to the fundamental differences between dense prediction and image generation. In this paper, we provide a systemic analysis of the diffusion formulation for the dense prediction, focusing on both quality and efficiency. And we find that the original parameterization type for image generation, which learns to predict noise, is harmful for dense prediction; the multi-step noising/denoising diffusion process is also unnecessary and challenging to optimize. Based on these insights, we introduce $\textbf{Lotus}$, a diffusion-based visual foundation model with a simple yet effective adaptation protocol for dense prediction. Specifically, Lotus is trained to directly predict annotations instead of noise, thereby avoiding harmful variance. We also reformulate the diffusion process into a single-step procedure, simplifying optimization and significantly boosting inference speed. Additionally, we introduce a novel tuning strategy called detail preserver, which achieves more accurate and fine-grained predictions. Without scaling up the training data or model capacity, Lotus achieves SoTA performance in zero-shot depth and normal estimation across various datasets. It also enhances efficiency, being significantly faster than most existing diffusion-based methods. Lotus' superior quality and efficiency also enable a wide range of practical applications, such as joint estimation, single/multi-view 3D reconstruction, etc.",2025,0.7363633685951387,0.7368321313203701,0.6666666666666666,0.625,c579838a-35f6-451c-801f-ff8b17700d3a,1,"[0.625, 0.625, 0.625, 0.625, 0.875]","[0.95, 1.0, 0.95, 0.95, 1.0]",Lotus,0.6841087995501827
542116,Flow-based Variational Mutual Information: Fast and Flexible Approximations,"Mutual Information (MI) is a fundamental measure of dependence between random variables, but its practical application is limited because it is difficult to calculate in many circumstances. Variational methods offer one approach by introducing an approximate distribution to create various bounds on MI, which in turn is an easier optimization problem to solve. In practice, the variational distribution chosen is often a Gaussian, which is convenient but lacks flexibility in modeling complicated distributions. In this paper, we introduce new classes of variational estimators based on Normalizing Flows that extend the previous Gaussian-based variational estimators. Our new estimators maintain many of the same theoretical guarantees while simultaneously enhancing the expressivity of the variational distribution. We experimentally verify that our new methods are effective on large MI problems where discriminative-based estimators, such as MINE and InfoNCE, are fundamentally limited. Furthermore, we compare against a diverse set of benchmarking tests to show that the flow-based estimators often perform as well, if not better, than the discriminative-based counterparts. Finally, we demonstrate how these estimators can be effectively utilized in the Bayesian Optimal Experimental Design setting for online sequential decision making.",2025,0.6818179338843877,0.6880520419698927,0.6,0.5,1fb10c9b-3a04-42a9-ba6f-98e6ac7e4fb4,1,"[0.5, 0.5, 0.625, 0.875]","[0.9, 0.9, 0.9, 1.0]",Normalizing Flows,0.6641363022941971
542121,Adversaries Can Misuse Combinations of Safe Models,"Developers try to evaluate whether an AI system can accomplish malicious tasks before releasing it; for example, they might test whether a model enables cyberoffense, user manipulation, or bioterrorism. In this work, we show that individually testing models for such misuse is inadequate; adversaries can misuse combinations of models even when each individual model is safe. The adversary accomplishes this by first decomposing tasks into subtasks, then solving each subtask with the best-suited model. For example, an adversary might solve challenging-but-benign subtasks with an aligned frontier model, and easy-but-malicious subtasks with a weaker misaligned model. We study two decomposition methods: manual decomposition where a human identifies a natural decomposition of a task, and automated decomposition where a weak model generates benign tasks for a frontier model to solve, then uses the solutions in-context to solve the original task. Using these decompositions, we empirically show that adversaries can create vulnerable code, explicit images, python scripts for hacking, and manipulative tweets at much higher rates with combinations of models than either individual model. Our work suggests that even perfectly-aligned frontier systems can enable misuse without ever producing malicious outputs, and that red-teaming efforts should extend beyond single models in isolation.",2025,0.443181657024852,0.4424504074355629,0.4,0.25,f8a6562d-ba26-4be7-a59d-f9eb5d44c7ed,0,"[0.25, 0.25, 0.5, 0.625]","[0.9, 1.0, 0.95, 0.95]",Model Decomposition,0.401777565325733
542135,Towards Understanding the Robustness of Diffusion-Based Purification: A Stochastic Perspective,"Diffusion-Based Purification (DBP) has emerged as an effective defense mechanism against adversarial attacks. The success of DBP is often attributed to the forward diffusion process, which reduces the distribution gap between clean and adversarial images by adding Gaussian noise. Although this explanation is theoretically grounded, the precise contribution of this process to robustness remains unclear. In this paper, through a systematic investigation, we propose that the intrinsic stochasticity in the DBP procedure is the primary factor driving robustness. To explore this hypothesis, we introduce a novel Deterministic White-Box (DW-box) evaluation protocol to assess robustness in the absence of stochasticity, and analyze attack trajectories and loss landscapes. Our results suggest that DBP models primarily leverage stochasticity to evade effective attack directions, and that their ability to purify adversarial perturbations can be weak. To further enhance the robustness of DBP models, we propose Adversarial Denoising Diffusion Training (ADDT), which incorporates classifier-guided adversarial perturbations into diffusion training, thereby strengthening the models' ability to purify adversarial perturbations. Additionally, we propose Rank-Based Gaussian Mapping (RBGM) to improve the compatibility of perturbations with diffusion models. Experimental results validate the effectiveness of ADDT. In conclusion, our study suggests that future research on DBP can benefit from the perspective of decoupling stochasticity-based and purification-based robustness.",2025,0.7499997272728265,0.7451790603605821,0.6666666666666666,0.625,df5ea128-9453-49ca-a0a5-83594880221f,1,"[0.625, 0.625, 0.625, 0.875]","[0.95, 0.95, 1.0, 0.9]",Stochasticity,0.671885190978491
542140,JetFormer: An autoregressive generative model of raw images and text,"Removing modeling constraints and unifying architectures across domains has been a key driver of the recent progress in training large multimodal models. However, most of these models still rely on many separately trained components such as modality-specific encoders and decoders. In this work, we further streamline joint generative modeling of images and text. We propose an autoregressive decoder-only transformer---JetFormer---which is trained to directly maximize the likelihood of raw data, without relying on any separately pretrained components, and can understand and generate both text and images. Specifically, we leverage a normalizing flow model to obtain a soft-token image representation that is jointly trained with an autoregressive multimodal transformer. The normalizing flow model serves as both an image encoder for perception tasks and an image decoder for image generation tasks during inference. JetFormer achieves text-to-image generation quality competitive with recent VQVAE- and VAE-based baselines. These baselines rely on pretrained image autoencoders, which are trained with a complex mixture of losses, including perceptual ones. At the same time, JetFormer demonstrates robust image understanding capabilities. To the best of our knowledge, JetFormer is the first model that is capable of generating high-fidelity images and producing strong log-likelihood bounds.",2025,0.7159088305786071,0.7156478780895291,0.6666666666666666,0.625,683ab7c3-66b4-41cf-a19d-f4951dc0bcee,1,"[0.5, 0.625, 0.625, 0.875]","[0.95, 0.9, 0.9, 0.95]",JetFormer,0.6602655440414509
542156,C-CLIP: Multimodal Continual Learning for Vision-Language Model,"Multimodal pre-trained models like CLIP need large image-text pairs for training but often struggle with domain-specific tasks. Since retraining with specialized and historical data incurs significant memory and time costs, it is important to continually learn new domains in the open world while preserving original performance. However, current continual learning research mainly focuses on single-modal scenarios, and the evaluation criteria are insufficient without considering image-text matching performance and the forgetting of zero-shot performance. This work introduces image-caption datasets from various domains and establishes a multimodal vision-language continual learning benchmark. Then, a novel framework named C-CLIP is proposed, which not only prevents forgetting but also enhances new task learning impressively. Comprehensive experiments demonstrate that our method has strong continual learning ability across different domain image-text datasets, and has little forgetting of the original capabilities of zero-shot prediction,  significantly outperforming existing methods.",2025,0.7499997272728265,0.7460394562298107,0.6666666666666666,0.625,2c3d6b20-fe68-4752-ab04-3b678fb9f4c4,1,"[0.625, 0.625, 0.625, 0.875]","[0.9, 0.95, 1.0, 0.9]",C-CLIP,0.67462988967128
542171,Efficient Masked AutoEncoder for Video Object Counting and A Large-Scale Benchmark,"The dynamic imbalance of the fore-background is a major challenge in video object counting, which is usually caused by the sparsity of target objects. This remains understudied in existing works and often leads to severe under-/over-prediction errors. To tackle this issue in video object counting, we propose a density-embedded Efficient Masked Autoencoder Counting (E-MAC) framework in this paper. To empower the model’s representation ability on density regression, we develop a new Density-Embedded Masked mOdeling (DEMO) method, which first takes the density map as an auxiliary modality to perform multimodal self-representation learning for image and density map. Although DEMO contributes to effective cross-modal regression guidance, it also brings in redundant background information, making it difficult to focus on the foreground regions. To handle this dilemma, we propose an efficient spatial adaptive masking derived from density maps to boost efficiency. Meanwhile, we employ an optical flow-based temporal collaborative fusion strategy to effectively capture the dynamic variations across frames, aligning features to derive multi-frame density residuals. The counting
accuracy of the current frame is boosted by harnessing the information from adjacent frames. In addition, considering that most existing datasets are limited to human-centric scenarios, we propose a large video bird counting dataset, DroneBird, in natural scenarios for migratory bird protection. Extensive experiments on three crowd datasets and our DroneBird validate our superiority against
the counterparts. The code and dataset are available.",2025,0.7159088305786071,0.7236844111516018,0.6666666666666666,0.625,82655fc2-d745-44e9-8ac4-7343775e880d,1,"[0.5, 0.625, 0.625, 0.875]","[0.8, 1.0, 1.0, 1.0]",Density-Embedded Masked mOdeling,0.6853607974594212
542178,6DGS: Enhanced Direction-Aware Gaussian Splatting for Volumetric Rendering,"Novel view synthesis has advanced significantly with the development of neural radiance fields (NeRF) and 3D Gaussian splatting (3DGS). However, achieving high quality without compromising real-time rendering remains challenging, particularly for physically-based rendering using ray/path tracing with view-dependent effects. Recently, N-dimensional Gaussians (N-DG) introduced a 6D spatial-angular representation to better incorporate view-dependent effects, but the Gaussian representation and control scheme are sub-optimal. In this paper, we revisit 6D Gaussians and introduce 6D Gaussian Splatting (6DGS), which enhances color and opacity representations and leverages the additional directional information in the 6D space for optimized Gaussian control. Our approach is fully compatible with the 3DGS framework and significantly improves real-time radiance field rendering by better modeling view-dependent effects and fine details. Experiments demonstrate that 6DGS significantly outperforms 3DGS and N-DG, achieving up to a 15.73 dB improvement in PSNR with a reduction of 66.5\% Gaussian points compared to 3DGS. The project page is: https://gaozhongpai.github.io/6dgs/.",2025,0.8181815206612653,0.815015977224559,0.8,0.625,1b08d518-e647-460c-8c90-f45da8b025d8,1,"[0.625, 0.625, 0.875, 0.875]","[0.95, 0.95, 0.95, 0.9]",6D Gaussian Splatting,0.7424537487828627
542188,Uncovering Challenges of Solving the Continuous Gromov-Wasserstein Problem,"Recently, the Gromov-Wasserstein Optimal Transport (GWOT) problem has attracted the special attention of the ML community. In this problem, given two distributions supported on two (possibly different) spaces, one has to find the most isometric map between them. In the discrete variant of GWOT, the task is to learn an assignment between given discrete sets of points. In the more advanced continuous formulation, one aims at recovering a parametric mapping between unknown continuous distributions based on i.i.d. samples derived from them. The clear geometrical intuition behind the GWOT makes it a natural choice for several practical use cases, giving rise to a number of proposed solvers. Some of them claim to solve the continuous version of the problem. At the same time, GWOT is notoriously hard, both theoretically and numerically. Moreover, all existing continuous GWOT solvers still heavily rely on discrete techniques. Natural questions arise: to what extent existing methods unravel GWOT problem, what difficulties they encounter, and under which conditions they are successful. Our benchmark paper is an attempt to answer these questions. We specifically focus on the continuous GWOT as the most interesting and debatable setup. We crash-test existing continuous GWOT approaches on different scenarios, carefully record and analyze the obtained results, and identify issues. Our findings experimentally testify that the scientific community is still missing a reliable continuous GWOT solver, which necessitates further research efforts. As the first step in this direction, we propose a new continuous GWOT method which does not rely on discrete techniques and partially solves some of the problems of the competitors.",2025,0.7090906512397632,0.7085065923749322,0.6666666666666666,0.625,dab44859-b01a-4320-9554-93d5c9e469a5,0,"[0.5, 0.625, 0.625, 0.625, 0.875]","[0.95, 0.9, 0.9, 0.95, 0.95]",Continuous Gromov-Wasserstein,0.6525055566781167
542228,Adaptive Energy Alignment for Accelerating Test-Time Adaptation,"In response to the increasing demand for tackling out-of-domain (OOD) scenarios, test-time adaptation (TTA) has garnered significant research attention in recent years. To adapt a source pre-trained model to target samples without getting access to their labels, existing approaches have typically employed entropy minimization (EM) loss as a primary objective function. In this paper, we propose an adaptive energy alignment (AEA) solution that achieves fast online TTA. We start from the re-interpretation of the EM loss by decomposing it into two energy-based terms with conflicting roles, showing that the EM loss can potentially hinder the assertive model adaptation. Our AEA addresses this challenge by strategically reducing the energy gap between the source and target domains during TTA, aiming to  effectively align the target domain with the source domains and thus to accelerate adaptation. We specifically propose two novel strategies, each contributing a necessary component for TTA: (i) aligning the energy level of each target sample with the energy zone of the source domain that the pre-trained model is already familiar with, and (ii) precisely guiding the direction of the energy alignment by matching the class-wise correlations between the source and target domains. Our approach demonstrates its effectiveness on various domain shift datasets including CIFAR10-C, CIFAR100-C, and TinyImageNet-C.",2025,0.7159088305786071,0.7151817395679724,0.8,0.875,7e59b5bd-da5e-4a8d-a95f-68e2b685d299,1,"[0.25, 0.625, 0.875, 0.875]","[0.95, 0.9, 1.0, 0.9]",Adaptive Energy Alignment,0.6647073580932151
542234,Regularized Diffusion Modeling for CAD Representation Generation,"Computer-Aided Design (CAD) has significant practical value in various industrial applications. However, achieving high-quality and diverse shape generation, as well as flexible conditional control, remains a challenge in the field of CAD model generation. To address these issues, we propose CADiffusion, a diffusion-based generative model with a hierarchical latent representation tailored to the complexities of CAD design processes. To enhance the performance and reliability of the model in generating accurate CAD models, we have developed a specialized decoder with regularization strategies that navigate through the noise space of the diffusion model, smoothing the results. This approach not only improves the diversity and quality of the generated CAD models but also enhances their practical applicability, marking a significant advancement in the integration of generative models and automated CAD systems.",2025,0.4363634776860081,0.431588726418074,0.5333333333333333,0.5,b6eedae2-57eb-412e-881b-18f5dec397e8,0,"[0.25, 0.25, 0.5, 0.5, 0.5]","[1.0, 0.95, 0.9, 0.9, 0.95]",CADiffusion,0.3800851413118035
542239,DISCO: Mitigating Bias in Deep Learning with Conditional DIStance COrrelation,"During prediction tasks, models can use any signal they receive to come up with
the final answer - including signals that are causally irrelevant. When predicting
objects from images, for example, the lighting conditions could be correlated to
different targets through selection bias, and an oblivious model might use these
signals as shortcuts to discern between various objects. A predictor that uses
lighting conditions instead of real object-specific details is obviously undesirable.
To address this challenge, we introduce a standard anti-causal prediction model
(SAM) that creates a causal framework for analyzing the information pathways
influencing our predictor in anti-causal settings. We demonstrate that a classifier
satisfying a specific conditional independence criterion will focus solely on the
direct causal path from label to image, being counterfactually invariant to the
remaining variables. Finally, we propose DISCO, a novel regularization strategy
that uses conditional distance correlation to optimize for conditional independence
in regression tasks. We can show that DISCO achieves competitive results in
different bias mitigation experiments, deeming it a valid alternative to classical
kernel-based methods.",2025,0.5113634504132908,0.4978784162616997,0.4,0.25,1a7d1516-e5e0-4109-bc70-0bf45501a216,0,"[0.25, 0.25, 0.5, 0.875]","[0.9, 0.9, 0.9, 0.8]",DISCO,0.4295295172863665
542246,"HOME-3: HIGH-ORDER MOMENTUM ESTIMATOR USING THIRD-POWER GRADIENT FOR CONVEX, SMOOTH NONCONVEX, AND NONSMOOTH NONCONVEX OPTIMIZATION","Momentum-based gradients are critical for optimizing advanced machine learning models, as they not only accelerate convergence but also help gradient-based optimizers overcome stationary points. While most state-of-the-art momentum techniques rely on lower-power gradients, such as the squared first-order gradient, there has been limited exploration into the potential of higher-power gradients—those raised to powers greater than two, such as the third-power first-order gradient. In this work, we introduce the concept of high-order momentum, where
momentum is constructed using higher-power gradients, with a specific focus on the third-power first-order gradient as a representative example. Our research offers both theoretical and empirical evidence of the benefits of this novel approach. From a theoretical standpoint, we demonstrate that incorporating third-power gradients into momentum can improve the convergence bounds of gradient-based optimizers
for both convex and smooth nonconvex problems. To validate these findings, we conducted extensive empirical experiments across convex, smooth nonconvex, and nonsmooth nonconvex optimization tasks. The results consistently showcase that high-order momentum outperforms traditional momentum-based optimizers, providing superior performance and more efficient optimization.",2025,0.3409089669421938,0.3336353794781427,0.2666666666666666,0.25,c222b06a-e078-4113-ab70-149878d6b4f4,0,"[0.25, 0.25, 0.25, 0.5]","[0.8, 1.0, 0.95, 0.8]",High-order momentum,0.287378086105457
542265,MSLC: Monte Carlo Tree Search Sampling Guided Local Construction for Solving Large-Scale Traveling Salesman Problem,"Neural solvers have achieved promising results in solving small-scale Travelling Salesman Problems (TSP), but inefficiencies arise when tackling larger instances. This paper proposes the MSLC (\textbf{M}onte Carlo Tree Search \textbf{S}ampling Guided \textbf{L}ocal \textbf{C}onstruction) framework, which innovatively integrates a predictive sampling module into the global coarse-grained selection module, MCTS, to achieve mutual integration with the fine-grained local construction module. This integration effectively balances coarse-grained exploration with fine-grained adjustment, thereby improving overall efficiency. This framework offers a novel way to combine autoregressive and non-autoregressive models. Experimental results demonstrate that MSLC effectively balances time and solution quality, outperforming state-of-the-art neural solvers. The performance gap of MSLC is reduced by at least 29.4\% (resp. 34.7\% or 28.5\%) on TSP-500 (resp. TSP-1000 or TSP-10000), compared to the SOTA neural methods.",2025,0.3409089669421938,0.3359268641728982,0.2666666666666666,0.25,819d818e-ef3c-4c39-b848-cb5d1472fca0,0,"[0.25, 0.25, 0.25, 0.5]","[1.0, 1.0, 0.95, 0.9]",MSLC,0.2932549372141985
542286,Q* Agent: Optimizing Language Agents with Q-Guided Exploration,"Language agents have become a promising solution to complex interactive tasks. One of the key ingredients to the success of language agents is the reward model on the trajectory of the agentic workflow, which provides valuable guidance during training or inference. However, due to the lack of annotations of intermediate interactions, most existing works use an outcome reward model to optimize policies across entire trajectories. This may lead to sub-optimal policies and hinder the overall performance. To address this, we propose Q\*Agent, leveraging an estimated Q value to generate intermediate annotations for open language agents. 
By introducing a reasoning tree and performing process reward modeling, Q\*Agent provides effective intermediate guidance for each step. This guidance aims to automatically annotate data in a step-wise manner.
Besides, we propose a Q-guided generation strategy that can significantly boost model performance by providing process guidance during inference.
Notably, even with almost half the annotated data, Q\*Agent retains strong performance, demonstrating its efficiency in handling limited supervision. We also empirically demonstrate that Q\*Agent can lead to more accurate decision making through qualitative analysis.",2025,0.5113634504132908,0.5035265339551418,0.5333333333333333,0.5,01d5679f-d619-44a4-a775-9a5458409d18,0,"[0.25, 0.5, 0.5, 0.625]","[1.0, 0.8, 0.95, 0.9]",Q*Agent,0.4337049458220379
542295,From Probability to Counterfactuals: the Increasing Complexity of Satisfiability in Pearl's Causal Hierarchy,"The framework of Pearl's Causal Hierarchy (PCH) formalizes three types of reasoning: probabilistic (i.e. purely observational), interventional, and counterfactual, that reflect the progressive sophistication of human thought regarding causation. We investigate the computational complexity aspects of reasoning in this framework focusing mainly on satisfiability problems expressed in probabilistic and causal languages across the PCH. That is, given a system of formulas in the standard probabilistic and causal languages, does there exist a model satisfying the formulas?  

Our main contribution is to prove the exact computational complexities showing that languages allowing addition and marginalization (via the summation operator) yield NP^{PP}-, PSPACE-, and NEXP-complete satisfiability problems, depending on the level of the PCH. These are the first results to demonstrate a strictly increasing complexity across the PCH: from probabilistic to causal and counterfactual reasoning. On the other hand, in the case of full languages, i.e.~allowing addition, marginalization, and multiplication, we show that the satisfiability for the counterfactual level remains the same as for the probabilistic and causal levels, solving an open problem in the field.",2025,0.6818179338843877,0.6788283804300236,0.7333333333333333,0.875,dbe4048a-7122-430b-9b37-745b76f8f007,1,"[0.25, 0.5, 0.875, 0.875]","[0.9, 0.95, 0.9, 0.9]",Satisfiability,0.6164174972314509
542322,Beyond Mere Token Analysis: A Hypergraph Metric Space Framework for Defending Against Socially Engineered LLM Attacks,"Recent jailbreak attempts on Large Language Models (LLMs) have shifted from algorithm-focused to human-like social engineering attacks, with persuasion-based techniques emerging as a particularly effective subset. These attacks evolve rapidly, demonstrate high creativity, and boast superior attack success rates. To combat such threats, we propose a promising approach to enhancing LLM safety by leveraging the underlying geometry of input prompt token embeddings using hypergraphs. This approach allows us to model the differences in information flow between benign and malicious LLM prompts.

In our approach, each LLM prompt is represented as a metric hypergraph, forming a compact metric space. We then construct a higher-order metric space over these compact metric hypergraphs using the Gromov-Hausdorff distance as a generalized metric. Within this space of metric hypergraph spaces, our safety filter learns to classify between harmful and benign prompts. Our study presents theoretical guarantees on the classifier's generalization error for novel and unseen LLM input prompts. Extensive empirical evaluations demonstrate that our method significantly outperforms both existing state-of-the-art generic defense mechanisms and naive baselines. Notably, our approach also achieves comparable performance to specialized defenses against algorithm-focused attacks.",2025,0.7499997272728265,0.7506017325541124,0.6666666666666666,0.625,c1cd8670-7c51-4484-8cef-49c795c5349c,1,"[0.625, 0.625, 0.625, 0.875]","[0.95, 1.0, 0.8, 0.95]",Hypergraph Metric Space,0.6900043335191146
542326,Hallo2: Long-Duration and High-Resolution Audio-Driven Portrait Image Animation,"Recent advances in latent diffusion-based generative models for portrait image animation, such as Hallo, have achieved impressive results in short-duration video synthesis. In this paper, we present updates to Hallo, introducing several design enhancements to extend its capabilities.First, we extend the method to produce long-duration videos. To address substantial challenges such as appearance drift and temporal artifacts, we investigate augmentation strategies within the image space of conditional motion frames. Specifically, we introduce a patch-drop technique augmented with Gaussian noise to enhance visual consistency and temporal coherence over long duration.Second, we achieve 4K resolution portrait video generation. To accomplish this, we implement vector quantization of latent codes and apply temporal alignment techniques to maintain coherence across the temporal dimension. By integrating a high-quality decoder, we realize visual synthesis at 4K resolution.Third, we incorporate adjustable semantic textual labels for portrait expressions as conditional inputs. This extends beyond traditional audio cues to improve controllability and increase the diversity of the generated content. To the best of our knowledge, Hallo2, proposed in this paper, is the first method to achieve 4K resolution and generate hour-long, audio-driven portrait image animations enhanced with textual prompts. We have conducted extensive experiments to evaluate our method on publicly available datasets, including HDTF, CelebV, and our introduced ''Wild'' dataset. The experimental results demonstrate that our approach achieves state-of-the-art performance in long-duration portrait video animation, successfully generating rich and controllable content at 4K resolution for duration extending up to tens of minutes.",2025,0.6818179338843877,0.6806929345162506,0.6666666666666666,0.625,63daf34a-ab27-438d-ad3b-a2f2e6e05205,1,"[0.625, 0.625, 0.625, 0.625]","[0.95, 0.95, 1.0, 0.95]",Hallo2,0.6249999999999999
542338,Identify Critical Nodes in Complex Network with Large Language Models,"Identifying critical nodes in networks is a classical combinatorial optimization task, and many methods struggle to strike a balance between adaptability and utility. Therefore, we propose an approach that empowers Evolutionary Algorithm (EA) with Large Language Models (LLMs), to generate a function called ""score_nodes"" which can further be used to identify crucial nodes based on their assigned scores. Our model consists of three main components: Manual Initialization, Population Management, and LLMs-based Evolution, and it evolves from initial populations with a set of designed node scoring functions created manually. LLMs leverage their strong contextual understanding and rich programming techniques to perform crossover and mutation operations on the individuals, generating new functions. These functions are then categorized, ranked, and eliminated to ensure the stable development of the populations while preserving diversity. Extensive experiments demonstrate the excellent performance of our method compared to other state-of-the-art algorithms. It can generate diverse and efficient node scoring functions to identify critical nodes in the network.",2025,0.4318176611572141,0.427863974680401,0.4,0.25,ed0490e4-73bc-4952-806d-42772ad4a026,0,"[0.25, 0.25, 0.25, 0.5, 0.5, 0.625]","[0.95, 0.9, 1.0, 0.95, 0.9, 0.9]",score_nodes,0.3793916033164728
542341,A Unified Approach to Routing and Cascading for LLMs,"The widespread applicability of large language models (LLMs) has increased the availability of many fine-tuned models of various sizes targeting specific tasks. Given a set of such specialized models, to maximize overall performance, it is important to figure out the optimal strategy for selecting the right model for a given user query. An effective strategy could drastically increase overall performance and even offer improvements over a single large monolithic model. Existing approaches typically fall into two categories: routing, where a single model is selected for each query, and cascading, which runs a sequence of increasingly larger models until a satisfactory answer is obtained. However, both have notable limitations: routing commits to an initial model without flexibility, while cascading requires executing every model in sequence, which can be inefficient. Additionally, the conditions under which these strategies are provably optimal remain unclear. In this work, we derive optimal strategies for both routing and cascading. Building on this analysis, we propose a novel approach called *cascade routing*, which combines the adaptability of routing with the cost-efficiency of cascading. Our experiments demonstrate that cascade routing consistently outperforms both routing and cascading across a variety of settings, improving both output quality and lowering computational cost, thus offering a unified and efficient solution to the model selection problem.",2025,0.5727270644628857,0.5709042433852984,0.6666666666666666,0.25,125e66f1-14a2-4d13-8bfd-ca31911de7e7,0,"[0.25, 0.25, 0.625, 0.625, 0.875]","[0.95, 0.9, 0.95, 0.95, 0.9]",cascade routing,0.5212416649828249
542372,OPTIMAL TRANSPORT BARYCENTER VIA NONCONVEX CONCAVE MINIMAX OPTIMIZATION,"The optimal transport barycenter (a.k.a. Wasserstein barycenter) is a fundamental notion of averaging that extends from the Euclidean space to the Wasserstein space of probability distributions. Computation of the \emph{unregularized} barycenter for discretized probability distributions on point clouds is a challenging task when the domain dimension $d > 1$. Most practical algorithms for approximating the barycenter problem are based on entropic regularization. In this paper, we introduce a nearly linear time $O(m \log{m})$ and linear space complexity $O(m)$ primal-dual algorithm, the Wasserstein-Descent $\dot{\mathbb{H}}^1$-Ascent (WDHA) algorithm, for computing the exact barycenter when the input probability density functions are discretized on an $m$-point grid. The key success of the WDHA algorithm hinges on alternating between two different yet closely related Wasserstein and Sobolev optimization geometries for the primal barycenter and dual Kantorovich potential subproblems. Under reasonable assumptions, we establish the convergence rate and iteration complexity of WDHA to its stationary point when the step size is appropriately chosen. Superior computational efficacy, scalability, and accuracy over the existing Sinkhorn-type algorithms are demonstrated on high-resolution (e.g., $1024 \times 1024$ images) 2D synthetic and real data.",2025,0.7090906512397632,0.7058491671585807,0.6666666666666666,0.625,e968210d-0a8f-4aae-8947-2a42f003faee,0,"[0.5, 0.625, 0.625, 0.625, 0.875]","[0.95, 0.9, 0.95, 0.9, 0.9]",Wasserstein-Descent,0.6407679217187832
542381,$InterLCM$: Low-Quality Images as Intermediate States of Latent Consistency Models for Effective Blind Face Restoration,"Diffusion priors have been used for blind face restoration (BFR) by fine-tuning diffusion models (DMs) on restoration datasets to recover low-quality images. However, the naive application of DMs presents several key limitations. 
(i) The diffusion prior has inferior semantic consistency (e.g., ID, structure and color.),  increasing the difficulty of optimizing the BFR model;
(ii) reliance on hundreds of denoising iterations, preventing the effective cooperation with perceptual losses, which is crucial for faithful restoration.
Observing that the latent consistency model (LCM) learns consistency noise-to-data mappings on the ODE-trajectory and therefore shows more semantic consistency in the subject identity, structural information and color preservation, 
we propose $\textit{InterLCM}$ to leverage the LCM for its superior semantic consistency and efficiency to counter the above issues. 
Treating low-quality images as the intermediate state of LCM, $\textit{InterLCM}$ achieves a balance between fidelity and quality by starting from earlier LCM steps. 
LCM also allows the integration of perceptual loss during training, leading to improved restoration quality, particularly in real-world scenarios.
To mitigate structural and semantic uncertainties, $\textit{InterLCM}$ incorporates a Visual Module to extract visual features and a Spatial Encoder to capture spatial details, enhancing the fidelity of restored images.
Extensive experiments demonstrate that $\textit{InterLCM}$ outperforms existing approaches in both synthetic and real-world datasets while also achieving faster inference speed. Code and models will be publicly available.",2025,0.5727270644628857,0.5717820649936505,0.6666666666666666,0.625,90bcc2f0-a91f-4029-a6b7-11551f6c64b9,1,"[0.25, 0.5, 0.625, 0.625, 0.625]","[0.95, 0.95, 0.95, 0.95, 0.95]",InterLCM,0.525
542402,Improving Graph Generation with Flow Matching and Optimal Transport,"Generating graph-structured data is crucial in various domains but remains challenging due to the complex interdependencies between nodes and edges. While diffusion models have demonstrated their superior generative capabilities, they often suffer from unstable training and inefficient sampling. To enhance generation performance and training stability, we propose GGFlow, a discrete flow matching generative model incorporating optimal transport for graph structures and it incorporates an edge-augmented graph transformer to enable the direct communications among edges. Additionally, GGFlow introduces a novel goal-guided generation framework to control the generative trajectory of our model towards desired properties. GGFlow demonstrates superior performance on both unconditional and conditional generation tasks, outperforming existing baselines and underscoring its effectiveness and potential for wider application.",2025,0.5795452438017296,0.5717820649936505,0.6666666666666666,0.625,a5d259e3-38b9-42da-98f4-8827901f332f,0,"[0.0, 0.625, 0.625, 0.875]","[1.0, 0.95, 0.8, 1.0]",GGFlow,0.5046134300491496
542404,Aligning Generative Denoising with Discriminative Objectives Unleashes Diffusion for Visual Perception,"With success in image generation, generative diffusion models are increasingly adopted for discriminative scenarios because generating pixels is a unified and natural perception interface. Although directly re-purposing their generative denoising process has established promising progress in specialist (e.g., depth estimation) and generalist models, the inherent gaps between a generative process and discriminative objectives are rarely investigated. For instance, generative models can tolerate deviations at intermediate sampling steps as long as the final distribution is reasonable, while discriminative tasks with rigorous ground truth for evaluation are sensitive to such errors. Without mitigating such gaps, diffusion for perception still struggles on tasks represented by multi-modal understanding (e.g., referring image segmentation). Motivated by these challenges, we analyze and improve the alignment between the generative diffusion process and perception objectives centering around the key observation: how perception quality evolves with the denoising process. (1) Notably, earlier denoising steps contribute more than later steps, necessitating a tailored learning objective for training: loss functions should reflect varied contributions of timesteps for each perception task. (2) Perception quality drops unexpectedly at later denoising steps, revealing the sensitiveness of perception to training-denoising distribution shift. We introduce diffusion-tailored data augmentation to simulate such drift in the training data. (3) We suggest a novel perspective to the long-standing question: why should a generative process be useful for discriminative tasks - interactivity. The denoising process can be leveraged as a controllable user interface adapting to users' correctional prompts and conducting multi-round interaction in an agentic workflow. Collectively, our insights enhance multiple generative diffusion-based perception models without architectural changes: state-of-the-art diffusion-based depth estimator, previously underplayed referring image segmentation models, and perception generalists. Our code is available at https://github.com/ziqipang/ADDP.",2025,0.7499997272728265,0.7487622279678756,0.6666666666666666,0.625,c579838a-35f6-451c-801f-ff8b17700d3a,1,"[0.625, 0.625, 0.625, 0.875]","[0.95, 0.95, 0.95, 0.95]",Diffusion-tailored data augmentation,0.6874999999999999
542406,N-ForGOT: Towards Not-forgetting and Generalization of Open Temporal Graph Learning,"Temporal Graph Neural Networks (TGNNs) lay emphasis on capturing node interactions over time but often overlook evolution in node classes and dynamic data distributions triggered by the continuous emergence of new class labels, known as the open-set problem. This problem poses challenges for existing TGNNs in preserving learned classes while rapidly adapting to new, unseen classes. To address this, this paper identifies two primary factors affecting model performance on the open temporal graph, backed by a theoretical guarantee:  (1) the forgetting of prior knowledge and (2) distribution discrepancies between successive tasks. Building on these insights, we propose N-ForGOT, which incorporates two plug-in modules into TGNNs to preserve prior knowledge and enhance model generalizability for new classes simultaneously. The first module preserves previously established inter-class connectivity and decision boundaries during the training of new classes to mitigate the forgetting caused by temporal evolutions of class characteristics. The second module introduces an efficient method for measuring distribution discrepancies with designed temporal Weisfeiler-Lehman subtree patterns, effectively addressing both structural and temporal shifts while reducing time complexity. Experimental results on four public datasets demonstrate that our method significantly outperforms state-of-the-art approaches in prediction accuracy, prevention of forgetting, and generalizability.",2025,0.7159088305786071,0.7083149092445724,0.6666666666666666,0.625,8c87611a-0e80-4a40-aef3-0c4967736f3d,1,"[0.5, 0.625, 0.625, 0.875]","[0.9, 0.8, 0.95, 0.8]",N-ForGOT,0.635292915050902
542407,Make LLMs better zero-shot reasoners: structure-oriented autonomous reasoning,"Zero-shot reasoning methods with Large Language Models (LLMs) offer significant advantages including great generalization to novel tasks and reduced dependency on human-crafted examples. 
However, the current zero-shot methods still have limitations in complex tasks, e.g., answering questions that require multi-step reasoning.
In this paper, we address this limitation by introducing a novel structure-oriented analysis method to help LLMs better understand the question and guide the problem-solving process of LLMs.
We first demonstrate how the existing reasoning strategies, Chain-of-Thought and ReAct, can benefit from our structure-oriented analysis. 
In addition to empirical investigations, we leverage the probabilistic graphical model to theoretically explain why our structure-oriented analysis can improve the LLM reasoning process. 
To further improve the reliability in complex question-answering tasks, we propose a multi-agent reasoning system, **S**tructure-oriented **A**utonomous **R**easoning **A**gents (SARA), that can better enforce the reasoning process following our structure-oriented analysis by refinement techniques and is equipped with external knowledge retrieval capability to reduce factual errors.
Extensive experiments verify the effectiveness of the proposed reasoning system. Surprisingly, in some cases, the system even surpasses few-shot methods.
Finally, the system not only improves reasoning accuracy in complex tasks but also demonstrates robustness against potential attacks that corrupt the reasoning process.",2025,0.5227274462809287,0.5160894027545737,0.4666666666666667,0.25,e2841036-bb35-40bf-9488-36086fbcf168,0,"[0.25, 0.25, 0.25, 0.625, 0.625, 0.875]","[0.95, 0.95, 0.95, 0.8, 0.9, 0.95]",SARA,0.4590466349689381
542408,Systematic Outliers in Large Language Models,"Outliers have been widely observed in Large Language Models (LLMs), significantly impacting model performance and posing challenges for model compression. Understanding the functionality and formation mechanisms of these outliers is critically important. Existing works, however, largely focus on reducing the impact of outliers from an algorithmic perspective, lacking an in-depth investigation into their causes and roles. In this work, we provide a detailed analysis of the formation process, underlying causes, and functions of outliers in LLMs. We define and categorize three types of outliers—activation outliers, weight outliers, and attention outliers—and analyze their distributions across different dimensions, uncovering inherent connections between their occurrences and their ultimate influence on the attention mechanism. Based on these observations, we hypothesize and explore the mechanisms by which these outliers arise and function, demonstrating through theoretical derivations and experiments that they emerge due to the self-attention mechanism's softmax operation. These outliers act as implicit context-aware scaling factors within the attention mechanism. As these outliers stem from systematic influences, we term them systematic outliers. Our study not only enhances the understanding of Transformer-based LLMs but also shows that structurally eliminating outliers can accelerate convergence and improve model compression. The code is avilable at \url{https://github.com/an-yongqi/systematic-outliers}.",2025,0.6818179338843877,0.6763942224961935,0.6666666666666666,0.875,5fa3ffa8-0105-48c7-8e4b-17635a89eea4,1,"[0.25, 0.5, 0.625, 0.875, 0.875]","[0.95, 1.0, 0.95, 0.9, 0.95]",Systematic Outliers,0.6054346334518879
542414,On the Convergence of Adam-Type Algorithms for Bilevel Optimization under Unbounded Smoothness,"Adam has become one of the most popular optimizers for training modern deep neural networks, such as transformers. However, its applicability is largely restricted to single-level optimization problems. In this paper, we aim to extend vanilla Adam to tackle bilevel optimization problems, which have important applications in machine learning, such as meta-learning. In particular, we study stochastic bilevel optimization problems where the lower-level function is strongly convex and the upper-level objective is nonconvex with potentially unbounded smoothness. This unbounded smooth objective function covers a broad class of neural networks, including transformers, which may exhibit non-Lipschitz gradients. In this work, we first introduce AdamBO, a single-loop Adam-type method that achieves $\widetilde{O}(\epsilon^{-4})$ oracle complexity to find $\epsilon$-stationary points, where the oracle calls involve stochastic gradient or Hessian/Jacobian-vector product evaluations. The key to our analysis is a novel randomness decoupling lemma that provides refined control over the lower-level variable. Additionally, we propose VR-AdamBO, a variance-reduced version with an improved oracle complexity of $\widetilde{O}(\epsilon^{-3})$. The improved analysis is based on a novel stopping time approach and a careful treatment of the lower-level error. We conduct extensive experiments on various machine learning tasks involving bilevel formulations with recurrent neural networks (RNNs) and transformers, demonstrating the effectiveness of our proposed Adam-type algorithms.",2025,0.5795452438017296,0.5698369168639769,0.6666666666666666,0.625,a9b31a41-7cd4-4458-b247-d53556cdb432,0,"[0.25, 0.625, 0.625, 0.625]","[0.95, 0.95, 0.8, 0.8]",AdamBO,0.5000247819191118
542418,Metric-Driven Attributions for Vision Transformers,"Attribution algorithms explain computer vision models by attributing the model response to pixels within the input. Existing attribution methods generate explanations by combining transformations of internal model representations such as class activation maps, gradients, attention, or relevance scores. The effectiveness of an attribution map is measured using attribution quality metrics. This leads us to pose the following question: if attribution methods are assessed using attribution quality metrics, why are the metrics not used to generate the attributions? In response to this question, we propose a Metric-Driven Attribution for explaining Vision Transformers (ViT) called MDA. Guided by attribution quality metrics, the method creates attribution maps by performing patch order and patch magnitude optimization across all patch tokens. The first step orders the patches in terms of importance and the second step assigns the magnitude to each patch while preserving the patch order. Moreover, MDA can provide a smooth trade-off between sparse and dense attributions by modifying the optimization objective. Experimental evaluation demonstrates the proposed MDA method outperforms $7$ existing ViT attribution methods by an average of $12\%$ across $12$ attribution metrics on the ImageNet dataset for the ViT-base $16 \times 16$, ViT-tiny $16 \times 16$, and ViT-base $32 \times 32$ models. Code is publicly available at https://github.com/chasewalker26/MDA-Metric-Driven-Attributions-for-ViT.",2025,0.5795452438017296,0.5717820649936505,0.6666666666666666,0.625,e175fa78-d8cd-4073-bdba-253e3913381a,1,"[0.25, 0.625, 0.625, 0.625]","[1.0, 0.9, 0.95, 0.9]",MDA,0.4952873203551428
542420,PathGen-1.6M: 1.6 Million Pathology Image-text Pairs Generation through Multi-agent Collaboration,"Vision Language Models (VLMs) like CLIP have attracted substantial attention in pathology, serving as backbones for applications such as zero-shot image classification and Whole Slide Image (WSI) analysis. Additionally, they can function as vision encoders when combined with large language models (LLMs) to support broader capabilities. Current efforts to train pathology VLMs rely on pathology image-text pairs from platforms like PubMed, YouTube, and Twitter, which provide limited, unscalable data with generally suboptimal image quality. In this work, we leverage large-scale WSI datasets like TCGA to extract numerous high-quality image patches. We then train a large multimodal model (LMM) to generate captions for extracted images, creating PathGen-1.6M, a dataset containing 1.6 million high-quality image-caption pairs. Our approach involves multiple agent models collaborating to extract representative WSI patches, generating and refining captions to obtain high-quality image-text pairs. Extensive experiments show that integrating these generated pairs with existing datasets to train a pathology-specific CLIP model, PathGen-CLIP, significantly enhances its ability to analyze pathological images, with substantial improvements across nine pathology-related zero-shot image classification tasks and three whole-slide image tasks. Furthermore, we construct 200K instruction-tuning data based on PathGen-1.6M and integrate PathGen-CLIP with the Vicuna LLM to create more powerful multimodal models through instruction tuning. Overall, we provide a scalable pathway for high-quality data generation in pathology, paving the way for next-generation general pathology models. Our dataset, code, and model are open-access at https://github.com/PathFoundation/PathGen-1.6M.",2025,0.886363314049704,0.884039329893202,0.9333333333333332,0.875,c1cd8a83-dd7e-4923-802f-b624446b7eb0,1,"[0.625, 0.875, 0.875, 0.875]","[1.0, 1.0, 0.95, 1.0]",PathGen-1.6M,0.8082115628970671
542437,Enhanced Diffusion Sampling via Extrapolation with Multiple ODE Solutions,"Diffusion probabilistic models (DPMs), while effective in generating high-quality samples, often suffer from high computational costs due to their iterative sampling process. To address this, we propose an enhanced ODE-based sampling method for DPMs inspired by Richardson extrapolation, which reduces numerical error and improves convergence rates. Our method, RX-DPM, leverages multiple ODE solutions at intermediate time steps to extrapolate the denoised prediction in DPMs. This significantly enhances the accuracy of estimations for the final sample while maintaining the number of function evaluations (NFEs). Unlike standard Richardson extrapolation, which assumes uniform discretization of the time grid, we develop a more general formulation tailored to arbitrary time step scheduling, guided by local truncation error derived from a baseline sampling method. The simplicity of our approach facilitates accurate estimation of numerical solutions without significant computational overhead, and allows for seamless and convenient integration into various DPMs and solvers. 
Additionally, RX-DPM provides explicit error estimates, effectively demonstrating the faster convergence as the leading error term's order increases. Through a series of experiments, we show that the proposed method improves the quality of generated samples without requiring additional sampling iterations.",2025,0.5795452438017296,0.5772276084697805,0.6666666666666666,0.625,f8660295-a377-40b0-be58-80ecbb7bfddf,1,"[0.25, 0.625, 0.625, 0.625]","[0.95, 0.9, 0.9, 1.0]",RX-DPM,0.528602348658697
542440,Benchmark Inflation: Revealing LLM Performance Gaps Using Retro-Holdouts,"The training data for many Large Language Models (LLMs) is contaminated with test data. This means that public benchmarks used to assess LLMs are compromised, suggesting a performance gap between benchmark scores and actual capabilities. Ideally, a private holdout set could be used to accurately verify scores. Unfortunately, such datasets do not exist for most benchmarks, and post-hoc construction of sufficiently similar datasets is non-trivial. To address these issues, we introduce a systematic methodology for (i) retrospectively constructing a holdout dataset for a target dataset, (ii) demonstrating the statistical indistinguishability of this retro-holdout dataset, and (iii) comparing LLMs on the two datasets to quantify the performance gap due to the dataset's public availability. Applying these methods to TruthfulQA, we construct and release Retro-Misconceptions, on which we evaluate twenty LLMs and find that some have inflated scores by as much as 16 percentage points. Our results demonstrate that public benchmark scores do not always accurately assess model properties, and underscore the importance of improved data practices in the field.",2025,0.443181657024852,0.4402406058929493,0.4,0.25,35e6a8aa-d85e-4953-9611-de25081942f0,0,"[0.25, 0.25, 0.5, 0.625]","[0.95, 1.0, 0.95, 0.95]",Retro-Holdouts,0.3938236377025036
542442,OSTQuant: Refining Large Language Model Quantization with Orthogonal and Scaling Transformations for Better Distribution Fitting,"Post-training quantization (PTQ) has emerged as a widely adopted technique for compressing and accelerating Large Language Models (LLMs).
The major challenge in LLM quantization is that uneven and heavy-tailed data distributions can expand the quantization range, thereby reducing bit precision for most values.
Recent methods attempt to eliminate outliers and balance inter-channel differences by employing linear transformations; however, they remain  heuristic and are often overlook optimizing the data distribution across the entire quantization space.
In this paper, we introduce Quantization Space Utilization Rate (QSUR), a novel metric that effectively assesses the quantizability of transformed data by measuring the space utilization of the data in the quantization space. We complement QSUR with mathematical derivations that examine the effects and limitations of various transformations, guiding our development of Orthogonal and Scaling Transformation-based Quantization (OSTQuant). OSTQuant employs a learnable equivalent transformation, consisting of an orthogonal transformation and a scaling transformation, to optimize the distributions of weights and activations across the entire quantization space. Futhermore, we propose the KL-Top loss function, designed to mitigate noise during optimization while retaining richer semantic information within the limited calibration data imposed by PTQ.
OSTQuant outperforms existing work on various LLMs and benchmarks. In the W4-only setting, it retains 99.5\% of the floating-point accuracy. In the more challenging W4A4KV4 configuration, OSTQuant reduces the performance gap by 32\% on the LLaMA-3-8B model compared to state-of-the-art methods. Code will be available.",2025,0.7090906512397632,0.6982591686615507,0.6666666666666666,0.625,26ed76d2-f48f-4ace-84bb-39564b70294e,1,"[0.5, 0.625, 0.625, 0.625, 0.875]","[1.0, 1.0, 0.95, 0.9, 0.8]",OSTQuant,0.6163119105196516
542443,The Deficit of New Information in Diffusion Models: A Focus on Diverse Samples,"Diffusion models are renowned for their state-of-the-art performance in generating high-quality images. Identifying samples with new information beyond the training data is essential for data augmentation, especially for enhancing model performance in diverse and unforeseen real-world scenarios. However, the investigation of new information in the generated samples has not been well explored. Our investigation through the lens of information theory reveals that diffusion models do not produce new information beyond what exists in the training data. Next, we introduce the concept of diverse samples (DS) to prove that generated images could contain information not present in the training data for diffusion models. Furthermore, we propose a method for identifying diverse samples among generated images by extracting deep features and detecting images that fall outside the boundary of real images. We demonstrate that diverse samples exist in the generated data of diffusion models, attributed to the estimation of forward and backward processes, but it can only produce a limited number of diverse samples, underscoring a notable gap in their capabilities in generating diverse samples. In addition, our experiment on the Chest X-ray dataset demonstrates that the diverse samples are more useful in improving classification accuracy than vanilla-generated samples. The source code is available at \url{https://github.com/lypz12024/diffusion-diverse-samples}.",2025,0.2727271735537551,0.2722771738065002,0.2666666666666666,0.25,a8dd19ca-f340-48dd-855e-8e17ca3512ac,0,"[0.25, 0.25, 0.25]","[1.0, 0.95, 0.95]",Diverse Samples,0.2500000000000004
542449,Scaling Stick-Breaking Attention: An Efficient Implementation and In-depth Study,"The self-attention mechanism traditionally relies on the softmax operator, necessitating positional embeddings like RoPE, or position biases to account for token order.
But current methods using still face length generalisation challenges.
We investigate an alternative attention mechanism based on the stick-breaking process in larger scale settings.
The method works as follows: For each token before the current, we determine a break point, which represents the proportion of the stick, the weight of the attention, to allocate to the current token.
We repeat this on the remaining stick, until all tokens are allocated a weight, resulting in a sequence of attention weights.
This process naturally incorporates recency bias, which has linguistic motivations for grammar parsing (Shen et al., 2017).
We study the implications of replacing the conventional softmax-based attention mechanism with stick-breaking attention.
We then discuss implementation of numerically stable stick-breaking attention and adapt Flash Attention to accommodate this mechanism.
When used as a drop-in replacement for current softmax+RoPE attention systems, we find that stick-breaking attention performs competitively with current methods on length generalisation and downstream tasks.
Stick-breaking also performs well at length generalisation, allowing a model trained with $2^{11}$ context window to perform well at $2^{14}$ with perplexity improvements.",2025,0.7499997272728265,0.751414207640751,0.6666666666666666,0.625,c7abe93b-a296-4242-ae72-06ddc2a8c7f8,1,"[0.625, 0.625, 0.625, 0.875]","[0.95, 0.95, 0.95, 1.0]",Stick-Breaking Attention,0.7024116347569956
542464,PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse,"Self-supervised learning (SSL) is a data-driven learning approach that utilizes the innate structure of the data to guide the learning process. In contrast to supervised learning, which depends on external labels, SSL utilizes the inherent characteristics of the data to produce its own supervisory signal. However, one frequent issue with SSL methods is representation collapse, where the model outputs a constant input-invariant feature representation. This issue hinders the potential application of SSL methods to new data modalities, as trying to avoid representation collapse wastes researchers' time and effort. This paper introduces a novel SSL algorithm for time-series data called Prediction of Functionals from Masked Latents (PFML). Instead of predicting masked input signals or their latent representations directly, PFML operates by predicting statistical functionals of the input signal corresponding to masked embeddings, given a sequence of unmasked embeddings. The algorithm is designed to avoid representation collapse, rendering it straightforwardly applicable to different time-series data domains, such as novel sensor modalities in clinical data. We demonstrate the effectiveness of PFML through complex, real-life classification tasks across three different data modalities: infant posture and movement classification from multi-sensor inertial measurement unit data, emotion recognition from speech data, and sleep stage classification from EEG data. The results show that PFML is superior to a conceptually similar pre-existing SSL method and competitive against the current state-of-the-art SSL method, while also being conceptually simpler and without suffering from representation collapse.",2025,0.2727271735537551,0.2722771738065002,0.2666666666666666,0.25,6eb4afd1-5c97-45a7-9086-59bf641b7983,0,"[0.25, 0.25, 0.25, 0.25]","[0.95, 0.95, 1.0, 0.9]",PFML,0.25
542470,Fragment-Augmented Diffusion for Molecular Conformation Generation,"Molecular conformer generation is a fundamental challenge in computational chemistry, particularly for large and complex molecules. 
In this work, we propose a novel approach called Fragment-Augmented Diffusion (FADiff), which integrates molecular fragmentations into diffusion models as a data augmentation strategy to enhance molecular conformation generation. By decomposing molecules into smaller, manageable fragments for the purpose of data augmentation, FADiff enhances the diffusion generation process, effectively capturing local structural variations while preserving the integrity of the entire molecule. Extensive experiments across multiple datasets demonstrate that FADiff consistently outperforms state-of-the-art methods, particularly in data-scarce scenarios, where the fragment-based augmentation approach significantly enhances model performance. We also provide a comprehensive analysis of different fragmentation rules and their impact on model performance, and theoretically validate FADiff's effectiveness in improving generalization. Overall, FADiff advances molecular conformation generation by enhancing the exploration of conformational space, offering a powerful tool for computational chemistry. The code is available at https://anonymous.4open.science/r/fragaug-5960/.",2025,0.4545456528924899,0.4554456523869995,0.5333333333333333,0.5,4f9a927a-1532-4d10-a890-429c1b54c016,0,"[0.25, 0.5, 0.5]","[0.9, 0.95, 0.9]",Fragment-Augmented Diffusion,0.4241248646697943
542480,CASE: Challenger Arm Sampling for Efficient In-Context Reasoning,"The in-context learning paradigm with LLMs has been instrumental in advancing applications that require complex reasoning over natural language. An optimal selection of few-shot examples (exemplars) is essential for constructing effective prompts under a limited budget.
In this paper, we frame the problem of exemplar selection for In-Context Reasoning (ICR) as a top-m best arms identification problem. A key challenge in this context is the exponentially large number of arms that need to be evaluated to identify the m-best arms. We propose CASE (Challenger Arm Sampling for Exemplar selection), a novel selective exploration strategy that maintains a shortlist of ``challenger'' arms, which are current candidates for the top-m arms. In each iteration, only the arms from this shortlist and the current top-m set are pulled, thereby reducing sample complexity and, consequently, the number of LLM evaluations. Furthermore, we model the scores of exemplar subsets (arms) using a parameterized linear scoring function, leading to a stochastic linear bandits setting. In this setting, CASE identifies the top-m arms with significantly fewer evaluations than existing state-of-the-art methods. CASE effectively works with black box LLMs and selects a static set of few-shot examples, resulting in an extremely efficient scheme for in-context reasoning. The exemplars selected with CASE show surprising performance gains of up to 15.19% compared to state-of-the-art exemplar selection methods. We release our code and data (https://anonymous.4open.science/r/CASE_exemplar_bandits-7403).",2025,0.6477270371901683,0.6475785846379041,0.6666666666666666,0.625,93996f76-4f92-4d1d-b1fb-25b1028f105c,0,"[0.5, 0.625, 0.625, 0.625]","[0.9, 0.95, 0.95, 0.9]",CASE,0.5977655440414507
542496,Delving into Temperature Scaling for Adaptive Conformal Prediction,"Conformal prediction, as an emerging uncertainty qualification technique, constructs prediction sets that are guaranteed to contain the true label with pre-defined probability. Previous works often employ temperature scaling to calibrate the classifier, assuming that confidence calibration can benefit conformal prediction. In this work, we empirically show that current confidence calibration methods (e.g., temperature scaling) normally lead to larger prediction sets in adaptive conformal prediction. Theoretically, we prove that a prediction with higher confidence could result in a smaller prediction set on expectation. Inspired by the analysis, we propose \textbf{Conformal Temperature Scaling} (ConfTS), a variant of temperature scaling that aims to improve the efficiency of adaptive conformal prediction. Specifically, ConfTS optimizes the temperature value by minimizing the gap between the threshold and the non-conformity score of the ground truth for a held-out validation dataset. In this way, the temperature value obtained would lead to an optimal set with high efficiency without violating the coverage. Experiments demonstrate that our method can effectively enhance adaptive conformal prediction methods in both efficiency and conditional coverage, reducing the average size of APS and RAPS by approximately 50$\%$ on ImageNet with error rate $\alpha=0.1$.",2025,0.5454543471075102,0.5427388034180588,0.6,0.625,5ac9e47a-31b3-49ea-aaff-50aeeca1b4bb,0,"[0.25, 0.5, 0.625, 0.625]","[0.95, 0.95, 0.9, 0.95]",Conformal Temperature Scaling,0.4924537487828628
542514,Evaluating the Instruction-following Abilities of Language Models using Knowledge Tasks,"In this work, we focus our attention on developing a benchmark for instruction-following where it is easy to verify both task performance as well as instruction-following capabilities. We adapt existing knowledge benchmarks and augment them with instructions that are a) conditional on correctly answering the knowledge task or b) use the space of candidate options in multiple-choice knowledge-answering tasks. This allows us to study model characteristics, such as their change in performance on the knowledge tasks in the presence of answer-modifying instructions and distractor instructions. In contrast to existing benchmarks for instruction following, we not only measure instruction-following capabilities but also use LLM-free methods to study task performance. We study a series of openly available large language models of varying parameter sizes (1B-405B) and closed source models namely GPT-4o-mini, GPT-4o. We find that even large-scale instruction-tuned LLMs fail to follow simple instructions in zero-shot settings. We release our dataset, the benchmark, code, and results for future work.",2025,0.2727271735537551,0.2722771738065002,0.2666666666666666,0.25,f639eefd-b50e-4e96-a43f-1ec367397f8f,0,"[0.25, 0.25, 0.25]","[0.95, 0.95, 0.95]",Instruction-following Benchmark,0.25000000000001
542529,Causal Effect Estimation with Mixed Latent Confounders and Post-treatment Variables,"Causal inference from observational data has attracted considerable attention among researchers. One main obstacle is the handling of confounders. As direct measurement of confounders may not be feasible, recent methods seek to address the confounding bias via proxy variables, i.e., covariates postulated to be conducive to the inference of latent confounders. However, the selected proxies may scramble both confounders and post-treatment variables in practice, which risks biasing the estimation by controlling for variables affected by the treatment. In this paper, we systematically investigate the bias due to latent post-treatment variables, i.e., latent post-treatment bias, in causal effect estimation. Specifically, we first derive the bias when selected proxies scramble both latent confounders and post-treatment variables, which we demonstrate can be arbitrarily bad. We then propose a Confounder-identifiable VAE (CiVAE) to address the bias. Based on a mild assumption that the prior of latent variables that generate the proxy belongs to a general exponential family with at least one invertible sufficient statistic in the factorized part, CiVAE individually identifies latent confounders and latent post-treatment variables up to bijective transformations. We then prove that with individual identification, the intractable disentanglement problem of latent confounders and post-treatment variables can be transformed into a tractable independence test problem despite arbitrary dependence may exist among them. Finally, we prove that the true causal effects can be unbiasedly estimated with transformed confounders inferred by CiVAE. Experiments on both simulated and real-world datasets demonstrate significantly improved robustness of CiVAE.",2025,0.7159088305786071,0.7174873826757658,0.6666666666666666,0.625,3ca73d7b-9a6d-40a6-a74a-b626206b0ee0,1,"[0.5, 0.625, 0.625, 0.875]","[0.9, 0.95, 0.9, 0.95]",Confounder-identifiable VAE,0.6682966321243524
542547,Large-Scale Dynamic Graph Generation via LLM-based Agent Simulation,"Graph generation is a fundamental task that has been extensively studied in social, technological, and scientific analysis. 
For modeling the dynamic graph evolution process, traditional rule-based methods struggle to capture community structures within graphs, while deep learning methods only focus on fitting training graphs. 
This limits existing graph generators to producing graphs that adhere to predefined rules or closely resemble training datasets, achieving poor performance in dynamic graph generation.
Given that graphs are abstract representations arising from pairwise interactions in human activities, a realistic simulation of human-wise interaction could provide deeper insights into the graph evolution mechanism.
With the increasing recognition of large language models (LLMs) in simulating human behavior, we introduce GraphAgent-Generator (GAG), a novel simulation-based framework for dynamic text-attributed graph generation. Without training or fine-tuning process of LLM, our framework effectively replicates seven macro-level structural characteristics in established network science theories while surpassing existing baselines in graph expansion tasks by 11\% on specific evaluation metrics. Through node classification task, we validate GAG effectively captures the intricate text-structure correlations in graph generation. 
Furthermore, GAG supports generating graphs with up to nearly 100,000 nodes or 10 million edges through large-scale LLM-based agent simulation with parallel acceleration, achieving a minimum speed-up of 90.4\%.
The source code is available at \url{https://anonymous.4open.science/r/GraphAgent-2206}.",2025,0.5181816297521347,0.5146332644290565,0.5333333333333333,0.5,c33c23ae-b955-4139-9d73-e4d783676be5,0,"[0.25, 0.5, 0.5, 0.5, 0.625]","[0.95, 0.8, 0.95, 0.95, 0.9]",GraphAgent-Generator,0.4640809531852174
542556,"Training on test proteins improves fitness, structure, and function prediction","Data scarcity and distribution shifts often hinder the ability of machine learning models to generalize when applied to proteins and other biological data. Self-supervised pre-training on large datasets is a common method to enhance generalization. However, striving to perform well on all possible proteins can limit model’s capacity to excel on any specific one, even though practitioners are often most interested in accurate predictions for the individual protein they study. To address this limitation, we propose an orthogonal approach to achieve generalization. Building on the prevalence of self-supervised pre-training, we introduce a method for self-supervised fine-tuning at test time, allowing models to adapt to the test protein of interest on the fly and without requiring any additional data. We study our test-time training (TTT) method through the lens of perplexity minimization and show that it consistently enhances generalization across different models, their scales, and datasets. Notably, our method leads to new state-of-the-art results on the standard benchmark for protein fitness prediction, improves protein structure prediction for challenging targets, and enhances function prediction accuracy.",2025,0.443181657024852,0.4401197148277793,0.4,0.25,76733b88-9a02-420c-afd1-e01a4caad4f8,0,"[0.25, 0.25, 0.5, 0.625]","[0.9, 0.95, 0.9, 0.9]",Test-time training,0.3955218715393134
542565,Graph Concept Bottleneck Models,"Concept Bottleneck Models (CBMs) provide explicit interpretations for deep neural networks through concepts and allow intervention with concepts to adjust final predictions. Existing CBMs assume concepts are conditionally independent given labels and isolated from each other, ignoring the hidden relationships among concepts. However, the set of concepts in CBMs often has an intrinsic structure where concepts are generally correlated: changing one concept will inherently impact its related concepts. To mitigate this limitation, we propose **Graph CBMs**: a new variant of CBM that facilitates concept relationships by constructing latent concept graphs, which can be combined with CBMs to enhance model performance while retaining their interpretability.  Empirical results on real-world image classification tasks demonstrate Graph CBMs are (1) superior in image classification tasks while providing more concept structure information for interpretability; (2) able to utilize concept graphs for more effective interventions; and (3) robust across different training and architecture settings.",2025,0.6477270371901683,0.6451781890736259,0.6666666666666666,0.625,d9681b36-91d6-4d84-b90e-110395767c77,0,"[0.5, 0.625, 0.625, 0.625]","[0.9, 0.8, 0.95, 0.8]",Graph CBMs,0.5901863376246153
542583,Noise Prompt Learning: Learning the Winning Tickets for Diffusion Sampling,"Text-to-image diffusion model is a popular paradigm that synthesizes personalized images by providing a text prompt and a random Gaussian noise. While people observe that some noises are winning tickets that can achieve better text-image alignment and higher human preference than others, we still lack a machine learning framework to obtain those winning noises. To learn winning noises for diffusion sampling, we mainly make three contributions in this paper. First, we identify a new concept termed the $\textit{noise prompt}$, which aims at turning a random Gaussian noise into a winning noise ticket by adding a small desirable perturbation derived from the text prompt. Following the concept, we first formulate the $\textit{noise prompt learning}$ framework that systematically learns ""prompted'' winning noise tickets associated with a text prompt for diffusion models. Second, we design a noise prompt data collection pipeline and collect a large-scale $\textit{noise prompt dataset}$ (NPD) that contains 100k pairs of random noises and winning noises with the associated text prompts. With the prepared NPD as the training dataset, we trained a small $\textit{noise prompt network}$ (NPNet) that can directly learn to transform a random noise ticket into a winning noise ticket. The learned winning noise perturbation can be considered as a kind of prompt for noise, as it is rich in semantic information and tailored to the given text prompt. Third, our extensive experiments demonstrate the impressive effectiveness and generalization of NPNet on improving the quality of synthesized images across various diffusion models, including SDXL, DreamShaper-xl-v2-turbo, and Hunyuan-DiT. Moreover, NPNet is a small and efficient controller that acts as a plug-and-play module with very limited additional inference and computational costs, as it just provides a winning noise instead of a random noise without accessing the original pipeline.",2025,0.5181816297521347,0.5170369273194203,0.5333333333333333,0.5,13972d7f-301b-4851-b10f-1d8c7862cc23,0,"[0.25, 0.5, 0.5, 0.5, 0.625]","[0.95, 0.9, 0.95, 0.95, 0.95]",Noise Prompt,0.4738070040407928
542611,A Scalable Temporal-Spatial Framework for Transaction Anomaly Detection in Ethereum Networks,"The rapid evolution of the Ethereum network necessitates sophisticated techniques to ensure its robustness against potential threats and to maintain transparency. While Graph Neural Networks (GNNs) have pioneered anomaly detection in such platforms, capturing the intricacies of both spatial and temporal transactional patterns has remained a challenge. This study presents a fusion of Graph Convolutional Networks (GCNs) with Temporal Random Walks (TRW) enhanced by probabilistic sampling to bridge this gap. Our approach, unlike traditional GCNs, leverages the strengths of TRW to discern complex temporal sequences in Ethereum transactions, thereby providing a more nuanced transaction anomaly detection mechanism. Extensive evaluations demonstrate that our TRW-GCN framework substantially advances the performance metrics over conventional GCNs in detecting irregularities such as suspiciously timed transactions, patterns indicative of token pump and dump schemes, or anomalous behavior in smart contract executions over time. As baseline algorithms for comparison, common unsupervised methods such as Isolation Forest, One-Class SVM, and DBSCAN (as classifier for TRW-GCN embedding) are employed; finally our novel TRW-GCN plus scoring method is compared with the state-of-the-art temporal graph attention algorithm.",2025,0.2727271735537551,0.2722771738065002,0.2666666666666666,0.25,cb7eb140-1d21-492c-88db-a2ff7e07f32f,0,"[0.25, 0.25, 0.25, 0.25]","[1.0, 1.0, 0.95, 0.95]",TRW-GCN,0.25
542636,Understanding Likelihood Over-optimisation in Direct Alignment Algorithms,"Direct Alignment Algorithms (DAAs), such as Direct Preference Optimisation (DPO) and Identity Preference Optimisation (IPO), have emerged as alternatives to online Reinforcement Learning from Human Feedback (RLHF) algorithms such as Proximal Policy Optimisation (PPO) for aligning language models to human preferences, without the need for explicit reward modelling. These methods generally aim to increase the likelihood of generating better (preferred) completions while discouraging worse (non-preferred) ones, while staying close to the original model's behaviour. In this work, we explore the relationship between completion likelihood and model performance in state-of-the-art DAAs, and identify a critical issue of likelihood over-optimisation. Contrary to expectations, we find that higher likelihood of better completions and larger margins between better and worse completion likelihoods do not necessarily lead to better performance, and may even degrade it. Our analysis reveals that while higher likelihood correlates with better memorisation of factual knowledge patterns, a slightly lower completion likelihood tends to improve output diversity, thus leading to better generalisation to unseen scenarios. Moreover, we identify two key indicators that signal when over-optimised output diversity begins to harm performance: ***Decreasing Entropy over Top-k Tokens*** and ***Diminishing Top-k Probability Mass***. Our experimental results validate that these indicators are reliable signs of declining performance under different regularisations, helping prevent over-optimisation and improve alignment with human preferences.",2025,0.5454543471075102,0.5427148430267638,0.4666666666666667,0.25,4c56c1c6-ca7b-4375-82b0-5a662a2fa778,0,"[0.25, 0.25, 0.625, 0.875]","[0.9, 0.95, 0.95, 0.9]",Likelihood Over-optimisation,0.4919689119170984
542659,Variational Inference with Unnormalized Priors,"Variational inference typically assumes normalized priors, limiting the expressiveness of generative models like Variational Autoencoders (VAEs). In this work, we propose a novel approach by replacing the prior 𝑝(𝑧) with an unnormalized energy-based distribution 
exp(−𝐸(𝑧))/𝑍, where 𝐸(𝑧) is the energy function and 𝑍 is the partition function. This leads to a variational lower bound that allows for two key innovations: (1) the incorporation of more powerful, flexible priors into the VAE framework, resulting in improved likelihood estimates and enhanced generative performance, and (2) the ability to train energy-based models (EBMs) without the need for computationally expensive Markov chain sampling, requiring only a small 𝑛 > 1 importance samples from the posterior distribution. Our approach bridges VAEs and EBMs, providing a scalable and efficient framework for leveraging unnormalized priors in probabilistic models.",2025,0.2727271735537551,0.2722771738065002,0.2666666666666666,0.25,b3031d97-6085-4309-806c-e6cf9bcfca06,0,"[0.25, 0.25, 0.25, 0.25, 0.25]","[0.95, 0.95, 0.95, 0.95, 1.0]",Unnormalized Priors,0.2499999999999999
542661,In-Context Transfer Learning: Demonstration Synthesis by Transferring Similar Tasks,"In-context learning (ICL) is an effective approach to help large language models (LLMs) adapt to various tasks by providing demonstrations of the target task. Considering the high cost of labeling demonstrations, many methods propose synthesizing demonstrations from scratch using LLMs. However, the quality of the demonstrations synthesized from scratch is limited by the capabilities and knowledge of LLMs. To address this, inspired by transfer learning, we propose In-Context Transfer Learning (ICTL), which synthesizes target task demonstrations by transferring labeled demonstrations from similar source tasks. ICTL consists of two steps: source sampling and target transfer. First, we define an optimization objective, which minimizes transfer error to sample source demonstrations similar to the target task.  Then, we employ LLMs to transfer the sampled source demonstrations to match the definition and format of the target task. Experiments on Super-NI show that ICTL outperforms synthesis from scratch by 2.0% on average, demonstrating the effectiveness of our method.",2025,0.5454543471075102,0.540875338440527,0.6,0.625,93996f76-4f92-4d1d-b1fb-25b1028f105c,0,"[0.25, 0.5, 0.625, 0.625]","[0.95, 0.95, 0.9, 0.9]",In-Context Transfer Learning,0.4839378238341968
542669,UV-Attack: Physical-World Adversarial Attacks on Person Detection via Dynamic-NeRF-based UV Mapping,"Recent works have attacked person detectors using adversarial patches or static-3D-model-based texture modifications. However, these methods suffer from low attack success rates when faced with significant human movements. The primary challenge stems from the highly non-rigid nature of the human body and clothing. Current attacks fail to model these 3D non-rigid deformations caused by varied actions.
Fortunately, recent research has shown significant progress in using NeRF for dynamic human modeling. 
In this paper, we introduce \texttt{UV-Attack}, a novel physical adversarial attack achieving high attack success rates in scenarios involving extensive and unseen actions. We address the challenges above by leveraging dynamic-NeRF-based UV mapping. Our method can generate human images across diverse actions and viewpoints and even create novel unseen actions by sampling from the SMPL parameter space. While dynamic NeRF models are capable of modeling human bodies, modifying their clothing textures is challenging due to the texture being embedded within neural network parameters.
To overcome this, \texttt{UV-Attack} generates UV maps instead of RGB images and modifies the texture stacks. This approach enables real-time texture edits and makes attacks more practical. Finally, we propose a novel Expectation over Pose Transformation loss (EoPT) to improve the evasion success rate on unseen poses and views.
Our experiments show that \texttt{UV-Attack} achieves a 92.7\% attack success rate against the FastRCNN model across varied poses in dynamic video settings, significantly outperforming the state-of-the-art AdvCaT attack, which only had a 28.5\% ASR. Moreover, we achieve 49.5\% ASR on the latest YOLOv8 detector in black-box settings. The code is available at https://github.com/PolyLiYJ/UV-Attack",2025,0.7909088033058898,0.7978015251878169,0.6666666666666666,0.625,4327a746-fb49-41c2-a2c4-3cb766be24de,1,"[0.625, 0.625, 0.625, 0.875, 0.875]","[0.8, 0.9, 0.95, 1.0, 1.0]",UV Mapping,0.760542259085529
542681,LightSAM: Parameter-Agnostic Sharpness-Aware Minimization,"Sharpness-Aware Minimization (SAM) optimizer enhances the generalization ability of the machine learning model by exploring the flat minima landscape through weight perturbations. Despite its empirical success, SAM introduces an additional hyper-parameter, the perturbation radius, which causes the sensitivity of SAM to it. Moreover, it has been proved that the perturbation radius and learning rate of SAM are constrained by problem-dependent parameters to guarantee convergence. These limitations indicate the requirement of parameter-tuning in practical applications. In this paper, we propose the algorithm LightSAM which sets the perturbation radius and learning rate of SAM adaptively, thus extending the application scope of SAM. LightSAM employs three popular adaptive optimizers, including AdaGrad-Norm, AdaGrad and Adam, to replace the SGD optimizer for weight perturbation and model updating, reducing sensitivity to parameters. Theoretical results show that under weak assumptions, LightSAM could converge ideally with any choices of perturbation radius and learning rate, thus achieving parameter-agnostic. We conduct preliminary experiments on several deep learning tasks, which together with the theoretical findings validate the the effectiveness of LightSAM.",2025,0.5454543471075102,0.5445543476130005,0.6,0.625,84dce633-7bb1-47ff-a1f2-da3b753dbf1a,0,"[0.25, 0.5, 0.625, 0.625]","[0.95, 0.95, 0.95, 0.95]",LightSAM,0.5
542686,Graph-Enhanced Learning for Predicting Optimal Drug Combinations Using Contrastive Embedding,"We present a groundbreaking unified theory for drug-drug interaction (DDI) aware domain adaptation (DA) in the context of drug synergy prediction. Our framework seamlessly integrates concepts from optimal transport, information geometry, and quantum information theory within the setting of abstract Banach spaces. We introduce a novel DDI-aware optimal transport problem, formulated as a geodesic equation on an infinite-dimensional Finsler manifold that encodes both DDI structure and optimal transport costs. This geometric formulation provides a unified perspective on DDI-aware domain adaptation, interpreting the process as the evolution of a transport map along a geodesic in a space that captures both domain discrepancy and drug interaction patterns. Our approach extends to a stochastic gradient flow on the space of probability measures, combining ideas from information geometry and stochastic analysis. We prove the existence of a unique invariant measure for this flow and establish its convergence properties using techniques from infinite-dimensional Markov processes and Γ-convergence. Our comprehensive mathematical framework not only unifies existing approaches to domain adaptation and DDI prediction but also opens new avenues for research at the intersection of these fields. By bridging the gap between abstract mathematical theories and practical drug synergy prediction, our work paves the way for more effective and theoretically grounded algorithms in drug discovery and personalized medicine. The proposed unified theory has far-reaching implications, potentially revolutionizing our understanding of cross-domain adaptation in complex biochemical systems and inspiring novel computational methods in pharmaceutical research.",2025,0.2045453801653163,0.2012487720299461,0.2666666666666666,0.25,4fda6d91-8287-4a98-982e-cde9335f03f1,0,"[0.0, 0.25, 0.25, 0.25]","[0.9, 0.8, 0.8, 0.95]",DDI-aware optimal transport,0.1803726953557179
542698,FuzzyCLIP: Clustering-Driven Stacked Prompt in Zero-Shot Anomaly Detection,"How to enhance the alignment of text and image features in CLIP model is a key challenge in zero-shot industrial anomaly detection tasks. Recent studies mostly rely on precise category prompts for pre-training, but this approach is prone to overfitting, which limits the generalization ability of the mode. To address this issue, we propose the concept of fuzzy prompts and introduce Clustering-Driven Stacked Prompts (CSP) along with the Ensemble Feature Alignment (EFA) module to improve the alignment between text and image features. This design significantly outperforms other methods in terms of training speed, stability, and final convergence results, showing remarkable efficiency in enhancing anomaly detection segmentation performance. What is even more surprising is that fuzzy stacked prompts exhibit strong generalization in classification tasks, enabling them to adapt to various anomaly classification tasks without any additional operations. Therefore, we further propose the Regulating Prompt Learning (RPL) module, which leverages the strong generalization ability of fuzzy stacked prompts to regularize prompt learning, thereby improving performance in anomaly detection classification tasks. We conducted extensive experiments on seven industrial anomaly detection datasets, which demonstrate that our method achieves state-of-the-art performance in zero-shot anomaly detection and segmentation tasks.",2025,0.5795452438017296,0.5728428568628007,0.5333333333333333,0.5,750f9695-d164-447e-a7ba-96e715dcc9e2,0,"[0.25, 0.5, 0.5, 0.875]","[0.95, 1.0, 1.0, 0.9]",Fuzzy Prompts,0.5088721482395356
542700,HALO: Human-Aligned End-to-end Image Retargeting with Layered Transformations,"Image retargeting aims to change the aspect-ratio of an image while maintaining its content and structure with less visual artifacts. 
Existing methods still generate many artifacts or lose a lot of original content or structure.  To address this, we introduce HALO, an end-to-end trainable solution for image retargeting. 
The core idea of HALO is to warp the input image to target resolution. 
Since humans are more sensitive to distortions in salient areas than non-salient areas of an image, HALO decomposes the input image into salient/non-salient layers and applies different wrapping fields to different layers. To further minimize the structure distortion in the output images, we propose perceptual structure similarity loss which measures the structure similarity between input and output images and aligns with human perception. Both quantitative results and a user study on the RetargetMe dataset show that our algorithm achieves SOTA. 
Especially, our method increases human preference by 13.21% compared with the second best method.",2025,0.4545456528924899,0.4506655543236526,0.5333333333333333,0.5,6354b7b5-e9cb-403f-9a30-0a09f2638f4e,0,"[0.25, 0.5, 0.5]","[1.0, 0.9, 1.0]",Layered Transformations,0.4028719229397074
542709,Disentangling Latent Shifts of In-Context Learning Through Self-Training,"In-context learning (ICL) has become essential in natural language processing, particularly with autoregressive large language models capable of learning from demonstrations provided within the prompt. However, ICL faces challenges with stability and long contexts, especially as the number of demonstrations grows, leading to poor generalization and inefficient inference. To address these issues, we introduce STICL (Self-Training ICL), an approach that disentangles the latent shifts of demonstrations from the latent shift of the query through self-training. STICL employs a teacher model to generate pseudo-labels and trains a student model using these labels, encoded in an adapter module. The student model exhibits weak-to-strong generalization, progressively refining its predictions over time. Our empirical results show that STICL improves generalization and stability, consistently outperforming traditional ICL methods and other disentangling strategies across both in-domain and out-of-domain data.",2025,0.6477270371901683,0.6415296749446189,0.6666666666666666,0.625,93996f76-4f92-4d1d-b1fb-25b1028f105c,0,"[0.25, 0.625, 0.625, 0.875]","[0.95, 0.9, 0.9, 0.9]",STICL,0.5701481173864896
542724,Explanations of GNN on Evolving Graphs via Axiomatic  Layer edges,"Graphs are ubiquitous in social networks, chemical molecules, and financial data, where Graph Neural Networks (GNNs) achieve superior predictive accuracy. Graphs can be
evolving, while understanding how GNN predictions respond to the evolution provides significant insight and trust. 
We explore the problem of explaining evolving GNN predictions due to continuously changing edge weights.
We introduce a layer edge-based explanation to balance
explanation fidelity and interpretability.
We propose a novel framework to address the challenges of axiomatic attribution and the entanglement of multiple computational graph paths due to continuous change of edge weights. We first design an axiomatic attribution of the evolution of the model prediction to message flows, then develop Shapley value to fairly map message flow contributions to layer edges.
We formulate a novel optimization problem to find the critical layer edges based on KL-divergence minimization. Extensive experiments on eight datasets for node classification, link prediction, and graph classification tasks with evolving graphs demonstrate the better fidelity and interpretability of the proposed method over the baseline methods. The code is available at https://github.com/yazhengliu/Axiomatic-Layer-Edges/tree/main.",2025,0.6818179338843877,0.6806929345162506,0.6666666666666666,0.625,df0eeceb-144e-4ecd-8d0f-c57b7cf2e4a5,1,"[0.625, 0.625, 0.625, 0.625]","[0.95, 0.8, 0.95, 0.95]",Axiomatic Attribution,0.625
542739,ABAS-RAL: Adaptive BAtch Size using Reinforced Active Learning,"Active learning reduces annotation costs by selecting the most informative samples, however fixed batch sizes used in traditional methods often lead to inefficient use of resources. We propose Adaptive BAtch Size using Reinforced Active Learning, a novel approach that dynamically adjusts batch sizes based on model uncertainty and performance. By framing the annotation process as a Markov Decision Process, the proposed method employs reinforcement learning to optimize batch size selection, using two distinct policies: one targeting precision and budget, and the other for adapting the batch size based on learning progress. The proposed method is evaluated on both CIFAR-10, CIFAR-100 and MNIST datasets. The performance is measured across multiple metrics, including precision, accuracy, recall, F1-score, and annotation budget. Experimental results demonstrate that the proposed method consistently reduces annotation costs while maintaining or improving performance compared to fixed-batch Active Learning methods, achieving higher sample selection efficiency without compromising model quality.",2025,0.3749998636364132,0.3643144823139639,0.2666666666666666,0.25,4ea9ef77-a39b-4e5e-bed4-047790493a1d,0,"[0.25, 0.25, 0.25, 0.625]","[0.9, 0.95, 0.9, 0.8]",Adaptive Batch Size,0.3116163503112722
542740,Open-World Planning via Lifted Regression with LLM-based Affordances for Embodied Agents,"Open-world planning is crucial for embodied AI agents that must make decisions with incomplete task-relevant knowledge. In fact, the main challenges lie in reasoning about objects and their affordances that are unknown to the agent. Large Language Models (LLMs), pre-trained on vast internet-scale data, have emerged as potential solutions for open-world planning. However, LLMs have limitations in long-horizon planning tasks and face problems related to interpretability, reliability, and cost-efficiency. Symbolic planning methods, on the other hand, offer structured and verifiable approaches to long-horizon tasks, but often struggle to generate feasible plans in an open-world setting. In this work, we propose a novel approach, called LLM-Regress, which combines the strengths of lifted symbolic regression planning with LLM-based affordances. The lifted representation allows us to generate plans capable of handling arbitrary unknown objects, while regression planning is the only planning paradigm that guarantees complete solutions using lifted representations. For such tasks, we leverage LLMs to supplement missing affordances knowledge for unknown objects. The regression nature of our approach enables the agent to focus on actions and objects relevant to the goal, thus avoiding the need for costly LLM calls for every decision. We evaluate our approach on the ALFWorld dataset and introduce a new ALFWorld-Afford dataset with higher planning complexity and more affordances types. The empirical results demonstrate that our method outperforms existing approaches in terms of success rates, planning duration, and number of LLM Tokens. Finally, we show that our approach is resilient to domain shifts in affordances and generalizes effectively to unseen tasks. This work underscores the importance of integrating symbolic reasoning with LLM knowledge for open-world decision-making in embodied AI.",2025,0.613636140495949,0.6126236410646255,0.6,0.5,a9b81b44-1906-43d0-91b4-5cb188297322,0,"[0.5, 0.5, 0.625, 0.625]","[0.95, 0.8, 0.95, 0.8]",LLM-Regress,0.5625
542775,Detecting Discrepancies Between Generated and Natural Images Using Uncertainty,"In this work, we propose a novel approach for detecting AI-generated images by leveraging predictive uncertainty to mitigate misuse and associated risks. The motivation arises from the fundamental assumption regarding the distributional discrepancy between natural and AI-generated images. **The feasibility of distinguishing natural images from AI-generated ones is grounded in the distribution discrepancy between them**. Predictive uncertainty offers an effective approach for capturing distribution shifts, thereby providing insights into detecting AI-generated images. Namely, as the distribution shift between training and testing data increases, model performance typically degrades, often accompanied by increased predictive uncertainty. Therefore, we propose to employ predictive uncertainty to reflect the discrepancies between AI-generated and natural images. In this context, the challenge lies in ensuring that the model has been trained over sufficient natural images to avoid the risk of determining the distribution of natural images as that of generated images. We propose to leverage large-scale pre-trained models to calculate the uncertainty as the score for detecting AI-generated images. Inspired by MC Dropout, we perturb pre-trained models and find that the uncertainty can be captured by perturbing the weights of pre-trained models. This leads to a simple yet effective method for detecting AI-generated images using large-scale vision models: images that induce high uncertainty are identified as AI-generated. Comprehensive experiments across multiple benchmarks demonstrate the effectiveness of our method.",2025,0.5454543471075102,0.5372932599419288,0.6,0.625,f3e44ca0-9975-4aee-b3d2-e4ed92c9975c,0,"[0.25, 0.5, 0.625, 0.625]","[1.0, 0.95, 0.9, 0.9]",Predictive Uncertainty,0.4631547699080418
542781,Multimodal Lego: Model Merging and Fine-Tuning Across Topologies and Modalities in Biomedicine,"Learning holistic computational representations in physical, chemical or biological systems requires the ability to process information from different distributions and modalities within the same model. Thus, the demand for multimodal machine learning models has sharply risen for modalities that go beyond vision and language, such as sequences, graphs, time series, or tabular data. While there are many available multimodal fusion and alignment approaches, most of them require end-to-end training, scale quadratically with the number of modalities, cannot handle cases of high modality imbalance in the training set, or are highly topology-specific, making them too restrictive for many biomedical learning tasks. This paper presents Multimodal Lego (MM-Lego), a general-purpose fusion framework to turn any set of encoders into a competitive multimodal model with no or minimal fine-tuning. We achieve this by introducing a wrapper for any unimodal encoder that enforces shape consistency between modality representations. It harmonises these representations by learning features in the frequency domain to enable model merging with little signal interference. We show that MM-Lego 1) can be used as a model merging method which achieves competitive performance with end-to-end fusion models without any fine-tuning, 2) can operate on any unimodal encoder, and 3) is a model fusion method that, with minimal fine-tuning, surpasses all benchmarks in five out of seven datasets.",2025,0.7159088305786071,0.7151817395679724,0.6666666666666666,0.625,d06665ce-69e1-4a73-a026-226ddf308a91,1,"[0.5, 0.625, 0.625, 0.875]","[0.95, 0.9, 0.95, 0.95]",MM-Lego,0.6581365628042842
542782,"Look, Compare and Draw:  Differential Query Transformer for Automatic Oil Painting","This work introduces a new approach to automatic oil painting that emphasizes the creation of dynamic and expressive brushstrokes. A pivotal challenge lies in mitigating the duplicate and common-place strokes, which often lead to less aesthetic outcomes. Inspired from the human painting process, i.e., observing, comparing, and drawing, we incorporate differential image analysis into a neural oil painting model, allowing the model to effectively concentrate on the incremental impact of successive brushstrokes. To operationalize this concept, we propose the Differential Query Transformer (DQ-Transformer), a new architecture that leverages differentially derived image representations enriched with positional encoding to guide the stroke prediction process. This integration enables the model to maintain heightened sensitivity to local details, resulting in more refined and nuanced stroke generation. Furthermore, we incorporate adversarial training into our framework, enhancing the accuracy of stroke prediction and thereby improving the overall realism and fidelity of the synthesized paintings. Extensive qualitative evaluations, complemented by a controlled user study, validate that our DQ-Transformer surpasses existing methods in both visual realism and artistic authenticity, typically achieving these results with fewer strokes. The stroke-by-stroke painting animations are available on our anonymous website: https://differential-query-painter.github.io/DQ-painter/ .",2025,0.4545456528924899,0.4459714958472285,0.6666666666666666,0.625,a8c205ca-ddda-4c39-91ce-296186c8d182,0,"[0.0, 0.625, 0.625]","[1.0, 0.95, 0.95]",Differential Query Transformer,0.3736250597800096
542784,Blind Coreset Selection: Efficient Pruning for Unlabeled Data,"Deep learning methods rely on massive data, resulting in substantial costs for storage, annotation, and model training.
Coreset selection aims to select a representative subset of the data to train models with lower cost while ideally performing on par with the full data training.
State-of-the-art coreset selection methods use carefully-designed criteria to quantify the importance of each data example using ground truth labels and dataset-specific training, then select examples whose scores lie in a certain range to construct a coreset.
These methods work well in their respective settings, however, they cannot consider candidate data that are initially unlabeled.
This limits the application of these methods, especially so considering that the majority of real-world data are unlabeled.
To that end, this paper explores the problem of coreset selection for unlabeled data.
We first motivate and formalize the problem of unlabeled coreset selection, which reduces annotation requirements to enable greater scale relative to label-based coreset selection.
We then develop an unlabeled coreset selection method, Blind Coreset Selection (BlindCS), that jointly considers overall data coverage on a distribution as well as the relative importance of each example based on redundancy.
Notably, BlindCS does not use any model- or dataset-specific training, which increases coreset generalization and reduces computation relative to training-based coreset selection.
We evaluate BlindCS on four datasets and confirm the advance over several state-of-the-art methods that use labels and training, leading to a strong baseline for future research in unlabeled coreset selection.
Notably, the BlindCS coreset for ImageNet achieves a higher accuracy than previous label-based coresets at a 90\% prune rate, while removing annotation requirements for 1.15 million images.
We will make our code publicly available with the final paper.",2025,0.3409089669421938,0.344884783191132,0.2666666666666666,0.25,ac11a68a-3c9c-43c7-a8c3-e61f5788dc21,0,"[0.25, 0.25, 0.25, 0.5]","[1.0, 0.8, 0.95, 1.0]",Blind Coreset Selection,0.3302577133005668
542789,$\Delta$-DiT: Accelerating Diffusion Transformers without training via Denoising Property Alignment,"Diffusion models are now commonly used for producing high-quality and diverse images, but the iterative denoising process is time-intensive, limiting their usage in real-time applications. As a result, various acceleration techniques have been developed, though these primarily target UNet-based architectures and are not directly applicable to Transformer-based diffusion models (DiT). To address the specific challenges of the DiT architecture, we first analyze the relationship between the depth of DiT blocks and the quality of image generation. While skipping blocks can lead to large degradations in generation quality, we propose the $\Delta$-Cache method, which captures and stores the incremental changes of different blocks, thereby mitigating the performance gap and maintaining closer alignment with the original results. Our analysis indicates that the shallow DiT blocks primarily define the global structure of images such as compositions, and outlines, while the deep blocks refine details. Based on this, we introduce a denoising property alignment method that selectively bypasses computations of different blocks at various timesteps while preserving performance. Comprehensive experiments on PIXART-$\alpha$ and DiT-XL demonstrate that $\Delta$-DiT achieves a $1.6\times$ speedup in 20-step generation and enhances performance in most cases. In the 4-step consistent model generation scenario, and with a more demanding $1.12\times$ acceleration, our approach significantly outperforms existing methods.",2025,0.613636140495949,0.6144631456508624,0.6,0.25,8265a13a-209b-467c-ad7a-bc408bd79c28,0,"[0.25, 0.5, 0.625, 0.875]","[0.95, 0.95, 0.8, 1.0]",$\Delta$-Cache,0.5824611641450914
542794,Riemann-Lebesgue Forest for Regression,"We propose a novel ensemble method called Riemann-Lebesgue Forest (RLF) for regression. The core idea in RLF is to mimic the way how a measurable function can be approximated by partitioning its range into a few intervals. With this idea in mind, we develop a new tree learner named Riemann-Lebesgue Tree (RLT) which has a chance to perform Lebesgue type cutting,i.e splitting the node from response Y
 at certain non-terminal nodes. In other words, we introduce the ""splitting type randomness"" in our ensemble method. We show that the optimal Lebesgue type cutting results in larger variance reduction in response Y than ordinary CART  cutting (an analogue of Riemann partition). Such property is beneficial to the ensemble part of RLF. We also generalize the asymptotic normality of RLF under different parameter settings. Two one-dimensional examples are provided to illustrate the flexibility of RLF. The competitive performance of RLF against original random forest  is demonstrated by experiments in simulation data and real world datasets.",2025,0.5795452438017296,0.5739896883188736,0.5333333333333333,0.5,415d522f-44ec-4d61-a7b3-7596b6498d56,0,"[0.25, 0.5, 0.5, 0.875]","[0.95, 0.95, 0.9, 0.9]",Riemann-Lebesgue Forest,0.511172279792746
542805,Leveraging Prior Experience: An Expandable Auxiliary Knowledge Base for Text-to-SQL,"Large Language Models (LLMs) exhibit impressive problem-solving skills across many tasks, but they still underperform compared to humans in various downstream applications, such as text-to-SQL. On the BIRD benchmark leaderboard, human performance achieves an accuracy of 92.96\%, whereas the top-performing method reaches only 72.39\%. Notably, these state-of-the-art (SoTA) methods predominantly rely on in-context learning to simulate human-like reasoning. However, they overlook a critical human skill: continual learning. Inspired by the educational practice of maintaining mistake notebooks during our formative years, we propose LPE-SQL ($\underline{\textbf{L}}$everaging $\underline{\textbf{P}}$rior $\underline{\textbf{E}}$xperience: An Expandable Auxiliary Knowledge Base for Text-to-$\underline{\textbf{SQL}}$), a novel framework designed to augment LLMs by enabling continual learning without requiring parameter fine-tuning. LPE-SQL consists of four modules that $\textbf{i)}$ retrieve relevant entries, $\textbf{ii)}$ efficient sql generation, $\textbf{iii)}$ generate the final result through a cross-consistency mechanism and $\textbf{iv)}$ log successful and failed tasks along with their reasoning processes or reflection-generated tips. Importantly, the core module of LPE-SQL is the fourth one, while the other modules employ foundational methods, allowing LPE-SQL to be easily integrated with SoTA technologies to further enhance performance. Our experimental results demonstrate that this continual learning approach yields substantial performance gains, with the smaller Llama-3.1-70B model with surpassing the performance of the larger Llama-3.1-405B model using SoTA methods.",2025,0.4090907603306326,0.4048325931024568,0.4,0.25,efae9ff1-1b64-4513-8f79-4f31c2d44eab,0,"[0.25, 0.25, 0.5, 0.5]","[0.95, 1.0, 0.95, 0.9]",LPE-SQL,0.3575962171087842
542807,Automatic Combination of Sample Selection Strategies for Few-Shot Learning,"In few-shot learning, such as meta-learning, few-shot fine-tuning or in-context learning, the selection of samples has a significant impact on the performance of the trained model. Although many sample selection strategies are employed and evaluated in typical supervised settings, their impact on the performance of few-shot learning is largely unknown. In this paper, we investigate the impact of 20 sample selection strategies on the performance of 5 representative few-shot learning approaches over 8 image and 6 text datasets. We propose a new method for Automatic Combination of SamplE Selection Strategies (ACSESS), to leverage the strengths and complementarity of the individual strategies in order to select more impactful samples. The experimental results show that our method consistently outperforms all individual selection strategies. We also show that the majority of existing strategies strongly depend on modality, dataset characteristics and few-shot learning approach, while improving performance especially on imbalanced and noisy datasets. Lastly, we show that sample selection strategies work well even on smaller datasets and provide larger benefit when selecting a lower number of shots, while frequently regressing to random selection with higher numbers of shots.",2025,0.5999997818182612,0.6004430494172179,0.5333333333333333,0.5,93a721fc-db5c-43e7-8fce-61ac6ceff194,0,"[0.5, 0.5, 0.5, 0.625, 0.625]","[0.9, 0.95, 0.95, 1.0, 0.95]",ACSESS,0.557289479176505
542824,Exploring Local Memorization in Diffusion Models via Bright Ending Attention,"Text-to-image diffusion models have achieved unprecedented proficiency in generating realistic images. However, their inherent tendency to memorize and replicate training data during inference raises significant concerns, including potential copyright infringement. In response, various methods have been proposed to evaluate, detect, and mitigate memorization. Our analysis reveals that existing approaches significantly underperform in handling local memorization, where only specific image regions are memorized, compared to global memorization, where the entire image is replicated. Also, they cannot locate the local memorization regions, making it hard to investigate locally. To address these, we identify a novel ""bright ending"" (BE) anomaly in diffusion models prone to memorizing training images. BE refers to a distinct cross-attention pattern observed in text-to-image diffusion models, where memorized image patches exhibit significantly greater attention to the final text token during the last inference step than non-memorized patches. This pattern highlights regions where the generated image replicates training data and enables efficient localization of memorized regions. Equipped with this, we propose a simple yet effective method to integrate BE into existing frameworks, significantly improving their performance by narrowing the performance gap caused by local memorization. Our results not only validate the successful execution of the new localization task but also establish new state-of-the-art performance across all existing tasks, underscoring the significance of the BE phenomenon.",2025,0.8636364132231226,0.8707728968766542,0.9333333333333332,0.875,3296a999-e1a6-4cec-8c50-a2370ebb2910,1,"[0.625, 0.875, 0.875]","[0.8, 0.95, 0.9]",Bright Ending,0.8199631039676829
542837,Solving New Tasks by Adapting Internet Video Knowledge,"Video generative models demonstrate great promise in robotics by serving as visual planners or as policy supervisors.  When pretrained on internet-scale data, such video models intimately understand alignment with natural language, and can thus facilitate generalization to novel downstream behavior through text-conditioning.  However, they may not be sensitive to the specificities of the particular environment the agent inhabits.  On the other hand, training video models on in-domain examples of robotic behavior naturally encodes environment-specific intricacies, but the scale of available demonstrations may not be sufficient to support generalization to unseen tasks via natural language specification.  In this work, we investigate different adaptation techniques that integrate in-domain information with large-scale pretrained video models, and explore the extent to which they enable novel text-conditioned generalization for robotic tasks, while also considering their independent data and resource considerations.  We successfully demonstrate across robotic environments that adapting powerful video models with small scales of example data can successfully facilitate generalization to novel behaviors.  In particular, we present a novel adaptation strategy, termed *Inverse Probabilistic Adaptation*, that not only consistently achieves strong generalization performance across robotic tasks and settings, but also exhibits robustness to the quality of adaptation data, successfully solving novel tasks even when only suboptimal in-domain demonstrations are available.",2025,0.6477270371901683,0.6471244263119947,0.6666666666666666,0.625,ebd4f08f-9b82-463c-8c1b-d5a98099fa7f,1,"[0.5, 0.625, 0.625, 0.625]","[0.9, 0.9, 0.95, 0.9]",Inverse Probabilistic Adaptation,0.5958956256921374
542838,Quality Diversity Imitation Learning,"Imitation learning (IL) has shown great potential in various applications, such as robot control. However, traditional IL methods are usually designed to learn only one specific type of behavior since demonstrations typically correspond to a single expert. In this work, we introduce the first generic framework for Quality Diversity Imitation Learning (QD-IL), which enables the agent to learn a broad range of skills from limited demonstrations. Our framework integrates the principles of quality diversity with adversarial imitation learning (AIL) methods, and can potentially improve any inverse reinforcement learning (IRL) method. Empirically, our framework significantly improves the QD performance of GAIL and VAIL on the challenging continuous control tasks derived from Mujoco environments. Moreover, our method even achieves 2x  expert performance in the Humanoid environment.",2025,0.4772725537190714,0.4764850541613754,0.5333333333333333,0.5,88a0a2b6-a796-4445-909c-5140997d1381,0,"[0.25, 0.5, 0.5, 0.5]","[0.9, 0.9, 0.9, 0.9]",Quality Diversity Imitation Learning,0.4375000000000001
542848,TFCounter: Polishing Gems for Training-Free Object Counting,"Object counting is a challenging task with broad application prospects in security surveillance, traffic management, and disease diagnosis. Existing object counting methods face a tri-fold challenge: achieving superior performance, maintaining high generalizability, and minimizing annotation costs. We develop a novel training-free class-agnostic object counter, TFCounter, which is prompt-context-aware via the cascade of the essential elements in large-scale foundation models. This approach employs an iterative counting framework with a dual prompt system to recognize a broader spectrum of objects varying in shape, appearance, and size. Besides, it introduces an innovative context-aware similarity module incorporating background context to enhance accuracy within messy scenes. To demonstrate cross-domain generalizability,  we collect a novel counting dataset named BIKE-1000, including exclusive 1000 images of shared bicycles from Meituan. Extensive experiments on FSC-147, CARPK, and BIKE-1000 datasets demonstrate that TFCounter outperforms existing leading training-free methods and exhibits competitive results compared to trained counterparts.",2025,0.3749998636364132,0.3695193327684489,0.4,0.0,6e77c126-91c8-4dcb-a59c-6aa22ac4326a,0,"[0.0, 0.25, 0.5, 0.625]","[1.0, 0.95, 0.95, 0.95]",TFCounter,0.316412002945508
542863,On the Diversity of Synthetic Data and its Impact on Training Large Language Models,"The rise of Large Language Models (LLMs) has accentuated the need for diverse, high-quality pre-training data. 
Synthetic data emerges as a viable solution to the challenges of data scarcity and inaccessibility.
While previous literature has focused predominantly on the quality and quantity of real data, our work enables the measurement of diversity in synthetic data and explores its impact on LLM performance. 
We study the downstream effects of synthetic data diversity during both the pre-training and fine-tuning stages by introducing a new diversity metric, LLM cluster-agent, designed to evaluate the diversity of synthetic datasets. 
Through a series of controlled experiments with models of 350M and 1.4B parameters, we demonstrate that the proposed cluster-based LLM scoring of diversity correlates positively with both pre-training and supervised fine-tuning performance. 
Our findings also reveal that synthetic data diversity in pre-training affects supervised fine-tuning more significantly than pre-training itself, even for smaller models. 
We hope this study advances our understanding of the optimal use of synthetic data in LLM training and opens new avenues for efficient data generation processes.",2025,0.6545452165290122,0.6420175956400801,0.6666666666666666,0.625,920c2510-b682-4291-bd20-b937415c07e7,0,"[0.25, 0.625, 0.625, 0.625, 0.875]","[0.95, 0.95, 0.8, 0.9, 0.8]",LLM cluster-agent,0.5587570332710408
542872,Improving Consistency Models with Generator-Induced Flows,"Consistency models imitate the multi-step sampling of score-based diffusion in a single forward pass of a neural network.
They can be learned in two ways: consistency distillation and consistency training. The former relies on the true velocity field of the corresponding differential equation, approximated by a pre-trained neural network.
In contrast, the latter uses a single-sample Monte Carlo estimate of this velocity field. The related estimation error induces a discrepancy between consistency distillation and training that, we show, still holds in the continuous-time limit. To alleviate this issue, we propose a novel flow that transports noisy data towards their corresponding outputs derived from the currently trained model - as a proxy of the true flow. Our empirical findings demonstrate that this approach mitigates the previously identified discrepancy.
Furthermore, we present theoretical and empirical evidence indicating that our generator-induced flow surpasses dedicated optimal transport-based consistency models in effectively reducing the noise-data transport cost.
Consequently, our method not only accelerates consistency training convergence but also enhances its overall performance.",2025,0.6818179338843877,0.6712383819329937,0.7333333333333333,0.875,a0de207a-9d83-4731-8136-6a8b3a947825,0,"[0.25, 0.5, 0.875, 0.875]","[0.95, 0.9, 0.8, 0.95]",Generator-Induced Flow,0.5955249125824517
542911,Inheritune: Training Smaller Yet More Attentive Language Models,"Large Language Models (LLMs) have achieved remarkable performance across various natural language processing tasks, primarily due to the transformer architecture and its self-attention mechanism. However, we observe that in standard decoder-style LLMs attention matrices degenerate to single-column for deeper layers. Layers in this state unable to learn anything meaningful and mostly redundant; we refer to these as lazy layers. The goal of this paper is to train smaller models by eliminating this structural inefficiency without compromising performance.

Motivated by this observation, we propose Inheritune, a simple yet effective training recipe for developing smaller, high-performing language models. Smaller models trained with Inheritune inherits early transformer layers from a larger pre-trained model, then retrains and progressively expands the smaller model until it matches or exceeds the performance of the larger model. We demonstrate that Inheritune enables the training of various sizes of GPT-2 models on datasets like OpenWebText-9B and FineWeb\_Edu. Models trained with Inheritune, despite having significantly fewer layers, match or even surpass the performance of their larger counterparts. For instance, our 16-layer GPT-2 medium variant achieves comparable performance to the standard 24-layer GPT-2 medium model.",2025,0.5454543471075102,0.5409722691144022,0.6,0.625,7d4f0021-2ca5-4455-bef4-e6f9d5528b53,0,"[0.25, 0.5, 0.625, 0.625]","[1.0, 0.9, 0.95, 0.95]",Inheritune,0.4790182693693706
542922,Dynamic Low-Rank Sparse Adaptation for Large Language Models,"Despite the efficacy of network sparsity in alleviating the deployment strain of Large Language Models (LLMs), it endures significant performance degradation. Applying Low-Rank Adaptation (LoRA) to fine-tune the sparse LLMs offers an intuitive approach to counter this predicament, while it holds shortcomings include: 1) The inability to integrate LoRA weights into sparse LLMs post-training, and 2) Insufficient performance recovery at high sparsity ratios. In this paper, we introduces dynamic $\textbf{Lo}$w-rank $\textbf{S}$parse $\textbf{A}$daptation $\textbf{(LoSA)}$, a novel method that seamlessly integrates low-rank adaptation into LLM sparsity within a unified framework, thereby enhancing the performance of sparse LLMs without increasing the inference latency. In particular, LoSA dynamically sparsifies the LoRA outcomes based on the corresponding sparse weights during fine-tuning, thus guaranteeing that the LoRA module can be integrated into the sparse LLMs post-training. Besides, to achieve the optimal sparse model architecture, LoSA leverages Representation Mutual Information (RMI) as an indicator to determine the importance of layers, thereby dynamically determining the optimal layer-wise sparsity rates during fine-tuning. Predicated on this, LoSA adjusts the rank of the LoRA module based on the variability in layer-wise reconstruction errors, allocating an appropriate fine-tuning for each layer to reduce the output discrepancies between dense and sparse LLMs. Extensive experiments tell that LoSA can efficiently boost the efficacy of sparse LLMs within a few hours, without introducing any additional inferential burden. For example, LoSA reduced the perplexity of sparse LLaMA-2-7B by $\textbf{68.73}$$\downarrow$ and increased zero-shot accuracy by $\textbf{16.32}$%$\uparrow$, achieving a $\textbf{2.60$\times$}$ speedup on CPU and $\textbf{2.23$\times$}$ speedup on GPU, requiring only $\textbf{45 minutes}$ of fine-tuning on $\textbf{a single}$ NVIDIA A100 80GB GPU. Code is available at https://github.com/wzhuang-xmu/LoSA.",2025,0.5727270644628857,0.5677273133213241,0.6666666666666666,0.25,4690aaab-c5fb-479d-941e-74eb02fe69f7,1,"[0.25, 0.25, 0.625, 0.625, 0.875]","[0.95, 0.95, 0.95, 0.95, 0.9]",LoSA,0.5082980565710986
542931,Linear Attention Sequence Parallelism,"Sequence parallelism (SP) serves as a prevalent strategy to handle long sequences that exceed the memory limit of a single device. However, for linear sequence modeling methods like linear attention, existing SP approaches do not take advantage of their right-product-first feature, resulting in sub-optimal communication efficiency and usability. In this paper, we introduce Linear Attention Sequence Parallelism (LASP), an efficient SP approach designed for linear attention-based transformer models. Specifically, we design an efficient point-to-point ring-style communication mechanism to leverage the right-product kernel trick of linear attention, which sharply decreases the communication overhead, comparing with existing SP methods. We enhance the computation efficiency of LASP by performing kernel fusion and intermediate state caching, making the implementation of LASP hardware-friendly on GPUs. Furthermore, we meticulously ensure the compatibility of sequence-level LASP with all types of batch-level data parallel methods, which is vital for distributed training on large clusters with very-long sequences. We also discuss the generalization of LASP on other linear sequence modeling methods. Extensive experiments on linear attention-based models are conducted with varying sequence lengths from 2K to 4096K. LASP scales sequence length up to 4096K on 128 GPUs, which is 8$\times$ longer than existing SP methods.",2025,0.5113634504132908,0.5077598994534853,0.5333333333333333,0.5,0f96964f-8c0f-433d-b5c3-a79f19a14c61,0,"[0.25, 0.5, 0.5, 0.625]","[0.95, 0.9, 0.95, 0.9]",LASP,0.4567033678756477
542932,Downstream Task Guided Masking Learning in Masked Autoencoders Using Multi-Level Optimization,"Masked Autoencoder (MAE) is a notable method for self-supervised pretraining in visual representation learning. It operates by randomly masking image patches and reconstructing these masked patches using the unmasked ones. A key limitation of MAE lies in its disregard for the varying informativeness of different patches, as it uniformly selects patches to mask. To overcome this, some approaches propose masking based on patch informativeness. However, these methods often do not consider the specific requirements of downstream tasks, potentially leading to suboptimal representations for these tasks. In response, we introduce the Multi-level Optimized Mask Autoencoder (MLO-MAE), a novel framework that leverages end-to-end feedback from downstream tasks to learn an optimal masking strategy during pretraining. Our experimental findings highlight MLO-MAE's significant advancements in visual representation learning. Compared to existing methods, it demonstrates remarkable improvements across diverse datasets and tasks, showcasing its adaptability and efficiency.",2025,0.4636361950413836,0.4625934727537678,0.5333333333333333,0.25,af66013a-04c7-49ee-ac94-9275b6bcfdae,0,"[0.25, 0.25, 0.5, 0.5, 0.625]","[1.0, 0.95, 1.0, 1.0, 0.95]",MLO-MAE,0.423568778160615
542937,Revealing and Reducing Gender Biases in Vision and Language Assistants (VLAs),"Pre-trained large language models (LLMs) have been reliably integrated with visual input for multimodal tasks. The widespread adoption of instruction-tuned image-to-text vision-language assistants (VLAs) like LLaVA and InternVL necessitates evaluating gender biases. We study gender bias in 22 popular open-source VLAs with respect to personality traits, skills, and occupations. Our results show that VLAs replicate human biases likely present in the data, such as real-world occupational imbalances. Similarly, they tend to attribute more skills and positive personality traits to women than to men, and we see a consistent tendency to associate negative personality traits with men. To eliminate the gender bias in these models, we find that fine-tuning-based debiasing methods achieve the best trade-off between debiasing and retaining performance on downstream tasks. We argue for pre-deploying gender bias assessment in VLAs and motivate further development of debiasing strategies to ensure equitable societal outcomes. Code is available at https://github.com/ExplainableML/vla-gender-bias.",2025,0.8181815206612653,0.8168315214195008,0.8,0.625,49a05c32-f0dd-485d-a595-b2642ce8444f,1,"[0.625, 0.625, 0.875, 0.875]","[0.95, 0.95, 0.95, 0.95]",Gender Bias Assessment,0.75
542947,DreamDistribution: Learning Prompt Distribution for Diverse In-distribution Generation,"The popularization of Text-to-Image (T2I) diffusion models enables the generation of high-quality images from text descriptions. However, generating diverse customized images with reference visual attributes remains challenging. This work focuses on personalizing T2I diffusion models at a more abstract concept or category level, adapting commonalities from a set of reference images while creating new instances with sufficient variations. We introduce a solution that allows a pretrained T2I diffusion model to learn a set of soft prompts, enabling the generation of novel images by sampling prompts from the learned distribution. These prompts offer text-guided editing capabilities and additional flexibility in controlling variation and mixing between multiple distributions. We also show the adaptability of the learned prompt distribution to other tasks, such as text-to-3D. Finally we demonstrate effectiveness of our approach through quantitative analysis including automatic evaluation and human assessment.",2025,0.6477270371901683,0.6480196736594706,0.6666666666666666,0.625,8dc97c49-3ddb-47f5-8699-7a7058e6ce83,1,"[0.5, 0.625, 0.625, 0.625]","[0.9, 0.95, 0.95, 0.95]",Prompt Distribution,0.5994096884128528
542951,REPOFILTER: Adaptive Retrieval Context Trimming for Repository-Level Code Completion,"Retrieval-Augmented Generation (RAG) has recently emerged as a promising approach for repository-level code completion by integrating cross-file knowledge with in-file preceding code to provide comprehensive contexts for generation. To better understand the contribution of the retrieved cross-file contexts, we introduce a likelihood-based metric to evaluate the impact of each retrieved code chunk on the completion. Our analysis reveals that, despite retrieving numerous chunks, only a small subset positively contributes to the target completion, while some chunks even degrade performance. To address this issue, we leverage this metric to construct a repository-level dataset where each retrieved chunk is labeled as positive, neutral, or negative based on its relevance to the target completion. We then propose an adaptive retrieval context trimming framework, REPOFILTER, trained on this dataset to mitigate the harmful effects of negative retrieved contexts in RAG-based code completion. Extensive evaluation on the RepoEval and CrossCodeLongEval benchmarks demonstrates that REPOFILTER consistently improves completion accuracy compared to approaches without filtering operations across various tasks. Additionally, REPOFILTER significantly reduces the length of the input prompt, enhancing computational efficiency while exhibiting strong generalizability across different models. These results underscore the potential of REPOFILTER to enhance the accuracy, efficiency, and attributability of RAG-based repository-level code completion.",2025,0.6272724991736367,0.6239688863427948,0.6666666666666666,0.625,bb831fc9-ee89-49b4-ab79-900128a3a58d,0,"[0.5, 0.5, 0.625, 0.625, 0.625]","[1.0, 1.0, 0.95, 0.9, 0.95]",REPOFILTER,0.563427388678399
542962,Invariant Convolutional Layers for Time Series,"Machine learning for time series has recently garnered considerable attention. Indeed, automatically extracting meaningful representations from large and complex time series data is becoming imperative for several real-world applications. Neural architectures tailored to time series are often built upon sequential modules, such as convolutional, commonly employed in text or vision. Unfortunately, the potential of standard layers in capturing invariant properties of time series remains relatively underexplored. For instance, convolutional layers often fail to capture underlying patterns in time series inputs that encompass strong deformations, such as linear trends. However, invariances to some deformations may be critical for solving complex time series tasks, such as classification, while guaranteeing good generalization properties.
To address these challenges, we mathematically formulate and technically design efficient *invariant convolutions* for specific group actions applicable to the case of time series.
We construct these convolutions by considering two sets of deformations commonly observed in time series, including (i) *offset shift and scaling* and (ii) *linear trend and scaling*.
We further combine the proposed invariant convolutions with standard (or variant) convolutions in a single embedding layer of an example architecture, the so-called *InvConvNet* method, and showcase the layer capacity to capture complex invariant time series properties.
Finally, *InvConvNet* is experimentally proven to achieve superior performance against common baselines in relevant time series tasks, including classification and anomaly detection.",2025,0.5454543471075102,0.5388822695282635,0.6,0.625,776e9cba-83e3-4ed3-a7eb-4531972bf32a,0,"[0.25, 0.5, 0.625, 0.625]","[0.95, 0.9, 0.95, 0.8]",Invariant Convolutions,0.4809131953926595
542964,Not All LLM-Generated Data Are Equal: Rethinking Data Weighting in Text Classification,"Synthetic data augmentation via Large Language Models (LLMs) allows researchers to leverage additional training data, thus enhancing the performance of downstream tasks, especially when real-world data is scarce. However, the generated data can deviate from the real-world data, and this misalignment can bring about deficient results while applying the trained model to applications. Therefore, we proposed efficient weighted-loss approaches to align synthetic data with real-world distribution by emphasizing high-quality and diversified data generated by LLMs using merely a tiny amount of real-world data. We empirically assessed the effectiveness of our methods on multiple text classification tasks, and the results showed that leveraging our approaches on a BERT-level model robustly outperformed standard cross-entropy and other data weighting approaches, providing potential solutions to effectively leveraging synthetic data from any suitable data generator.",2025,0.886363314049704,0.8924635856507751,0.9333333333333332,0.875,d904b6e4-8fb1-46b6-99de-c1626a1c3617,1,"[0.625, 0.875, 0.875, 0.875]","[0.8, 0.9, 0.9, 1.0]",Data Weighting,0.8381096059722776
542984,SimpleTM: A Simple Baseline for Multivariate Time Series Forecasting,"The versatility of large Transformer-based models has led to many efforts focused on adaptations to other modalities, including time-series data. For instance, one could start from a pre-trained checkpoint of a large language model and attach adapters to recast 
the new modality (e.g., time-series) as ``language''. Alternatively, one can use a suitably large Transformer-based model, and make some modifications for time-series data. These ideas offer good performance across available benchmarks. But temporal data are quite heterogeneous (e.g., wearable sensors, physiological measurements in healthcare), and unlike text/image corpus, much of it is not publicly available. So, these models need a fair bit of domain-specific fine-tuning to achieve good performance -- this is often expensive or difficult with limited resources. In this paper, we study and characterize the performance profile of a non-generalist approach: our SimpleTM model is specialized for multivariate time-series forecasting. By simple, we mean that the model is lightweight. It is restricted to tokenization based on textbook signal processing ideas (shown to be effective in vision) which are then allowed to attend/interact: via self-attention but also via ways that are a bit more general than dot-product attention, accomplished via basic geometric algebra operations. We show that even a single- or two-layer model gives results that are competitive with much bigger models, including large transformer-based architectures,
on most benchmarks commonly reported in the literature.",2025,0.7840906239670459,0.7813984591290181,0.8,0.875,82493b26-6a89-4a9f-9ccf-5505584d8f4a,1,"[0.5, 0.625, 0.875, 0.875]","[0.9, 0.95, 0.9, 0.9]",SimpleTM,0.7123131229235883
542985,Warfare: Breaking the Watermark Protection of AI-Generated Content,"AI-Generated Content (AIGC) is gaining great popularity, with many emerging commercial services and applications. These services leverage advanced generative models, such as latent diffusion models and large language models, to generate creative content (e.g., realistic images and fluent sentences) for users. The usage of such generated content needs to be highly regulated, as the service providers need to ensure the users do not violate the usage policies (e.g., abuse for commercialization, generating and distributing unsafe content).  
A promising solution to achieve this goal is watermarking, which adds unique and imperceptible watermarks on the content for service verification and attribution. Numerous watermarking approaches have been proposed recently. However, in this paper, we show that an adversary can easily break these watermarking mechanisms. Specifically, we consider two possible attacks. (1) Watermark removal: the adversary can easily erase the embedded watermark from the generated content and then use it freely bypassing the regulation of the service provider. (2) Watermark forging: the adversary can create illegal content with forged watermarks from another user, causing the service provider to make wrong attributions. We propose Warfare, a unified methodology to achieve both attacks in a holistic way. The key idea is to leverage a pre-trained diffusion model for content processing and a generative adversarial network for watermark removal or forging. We evaluate Warfare on different datasets and embedding setups. The results prove that it can achieve high success rates while maintaining the quality of the generated content. Compared to the inference process of existing diffusion model-based attacks, Warfare is 5,050~11,000x faster.",2025,0.5795452438017296,0.5717820649936505,0.6666666666666666,0.625,05a55458-aabc-499d-9303-544a76530a19,0,"[0.25, 0.625, 0.625, 0.625]","[1.0, 0.9, 0.9, 0.95]",Warfare,0.4952873203551426
542998,Physics-Assisted and Topology-Informed Deep Learning for Weather Prediction,"Weather prediction is crucial for decision-making in various social and economic sectors. The classical numerical weather prediction methods cannot incorporate the historical observations to enhance the underlying physical models, whereas the existing data-driven, deep learning-based weather prediction methods disregard either the $\textbf{physics}$ of the weather evolution or the $\textbf{topology}$ of the Earth's surface. In light of these disadvantages, we develop PASSAT, a novel Physics-ASSisted And Topology-informed deep learning model for weather prediction. PASSAT attributes the weather evolution to two key factors: (i) the advection process that can be characterized by the advection equation and the Navier-Stokes equation; (ii) the Earth-atmosphere interaction that is difficult to both model and calculate. PASSAT also takes the topology of the Earth's surface into consideration, other than simply treating it as a plane. Therefore, PASSAT numerically solves the advection equation and the Navier-Stokes equation on the spherical manifold, utilizes a spherical graph neural network to capture the Earth-atmosphere interaction, and generates the initial velocity fields that are critical to solving the advection equation from the same spherical graph neural network. These building blocks constitute a deep learning-based, $\textbf{physics-assisted}$ and $\textbf{topology-informed}$ weather prediction model. In the $5.625^\circ$-resolution ERA5 data set, PASSAT outperforms both the state-of-the-art deep learning-based weather prediction models and the operational numerical weather prediction model IFS T42.",2025,0.3409089669421938,0.3385069626718885,0.2666666666666666,0.25,ff532f41-ec85-447b-bb6a-51462f33a0d6,0,"[0.25, 0.25, 0.25, 0.5]","[0.9, 0.95, 0.95, 0.9]",PASSAT,0.3044689119170984
543006,Networked Communication for Decentralised Agents in Mean-Field Games,"We introduce networked communication to the mean-field game framework, in particular to oracle-free settings where $N$ decentralised agents learn along a single, non-episodic run of the empirical system. We prove that our architecture has sample guarantees bounded between those of the centralised- and independent-learning cases. We provide the order of the difference in these bounds in terms of network structure and number of communication rounds, and also contribute a policy-update stability guarantee. We discuss how the sample guarantees of the three theoretical algorithms do not actually result in practical convergence. We therefore show that in practical settings where the theoretical parameters are not observed (leading to poor estimation of the Q-function), our communication scheme significantly accelerates convergence over the independent case (and sometimes even the centralised case), without relying on the assumption of a centralised learner. We contribute further practical enhancements to all three theoretical algorithms, allowing us to present their first empirical demonstrations. Our experiments confirm that we can remove several of the theoretical assumptions of the algorithms, and display the empirical convergence benefits brought by our new networked communication. We additionally show that the networked approach has significant advantages, over both the centralised and independent alternatives, in terms of robustness to unexpected learning failures and to changes in population size.",2025,0.3818180429752571,0.3803004197424912,0.2666666666666666,0.25,0755e7d8-19fc-4bdb-aa26-61833e450957,0,"[0.0, 0.25, 0.25, 0.625, 0.625]","[0.95, 0.9, 0.9, 0.9, 0.95]",Networked Communication,0.3460433950223357
543008,FinRipple: Aligning Large Language Models with Financial Market for Event Ripple Effect Awareness,"Event studies have been fundamental in finance, focusing on analyzing the ripple effects of sudden market events. Accurately predicting these effects is crucial for informed decision-making and effective risk management. However, the dynamic complexity of financial markets and the lack of unified modeling tools make this task challenging. Previous models, constrained by simplistic assumptions and limited scopes, have struggled to address this complexity effectively. In contrast, large language models (LLMs), with their emergent reasoning abilities, offer a promising solution. In this paper, we introduce $\textbf{FinRipple}$, a novel training framework that enables LLMs to align with market behavior and develop the capability to analyze the ripple effects of sudden events. We first construct a time-varying financial knowledge graph (KG) that is both financially meaningful and noise-reduced to accurately represent the market state. These KGs are then integrated into the LLM using adapters as memory modules. Additionally, we align the LLM with market dynamics by integrating FinRipple with classic asset pricing theories through a reinforcement learning framework. This market-alignment process collects feedback that enhances the LLM's foundational ability to analyze financial events and explain market anomalies that traditional models fail to address. Our key contributions are as follows: (1) We are the first to define the underexplored task of ``event impact prediction''. Our framework not only establishes this task but also provides an open-source benchmark, creating a unified evaluation standard for both academia and industry; (2) FinRipple complements classic asset pricing models by combining strong theoretical foundations with AI-driven capabilities, offering an enhanced analysis of residuals unexplained by traditional models. We also demonstrate its potential for practical applications such as portfolio management; (3) We conduct a comprehensive analysis to ensure that the results generated by LLMs in our framework are more logically consistent and credible, thus improving the reliability of insights for financial decision-making.",2025,0.5795452438017296,0.5732153320365679,0.6666666666666666,0.625,5adc6e7a-4f6b-4a41-819c-afd0f1346f0a,0,"[0.25, 0.625, 0.625, 0.625]","[1.0, 0.95, 0.9, 0.95]",FinRipple,0.5024608648586162
543023,CompGS: Unleashing 2D Compositionality for Compositional Text-to-3D via Dynamically Optimizing 3D Gaussians,"Recent breakthroughs in text-guided image generation have significantly advanced the field of 3D generation. While generating a single high-quality 3D object is now feasible, generating multiple objects with reasonable interactions within a 3D space, a.k.a. compositional 3D generation, presents substantial challenges. This paper introduces CompGS, a novel generative framework that employs 3D Gaussian Splatting (GS) for efficient, compositional text-to-3D content generation. To achieve this goal, two core designs are proposed: (1) 3D Gaussians Initialization with 2D compositionality: We transfer the well-established 2D compositionality to initialize the Gaussian parameters on an entity-by-entity basis, ensuring both consistent 3D priors for each entity and reasonable interactions among multiple entities; (2) Dynamic Optimization: We propose a dynamic strategy to optimize 3D Gaussians using Score Distillation Sampling (SDS) loss. CompGS first automatically decomposes 3D Gaussians into distinct entity parts, enabling optimization at both the entity and composition levels. Additionally, CompGS optimizes across objects of varying scales by dynamically adjusting the spatial parameters of each entity, enhancing the generation of fine-grained details, particularly in smaller entities. Qualitative comparisons and quantitative evaluations on T3Bench demonstrate the effectiveness of CompGS in generating compositional 3D objects with superior image quality and semantic alignment over existing methods. CompGS can also be easily extended to controllable 3D editing, facilitating scene generation. We hope CompGS will provide new insights to the compositional 3D generation. Codes will be released to the research community.",2025,0.5999997818182612,0.5990097823743006,0.5333333333333333,0.5,4e70f926-0d81-4cf7-88f7-ddfa93161f52,0,"[0.5, 0.5, 0.5, 0.625, 0.625]","[1.0, 0.95, 0.9, 0.95, 0.95]",CompGS,0.5488484645092983
543033,Hough Voting-based Prompt Learning for Segment Anything Model,"Segment Anything Models (SAMs) like SEEM and SAM have achieved great performance on various downstream datasets at the cost of crafting spatial and semantic prompts. Previous prompt learning methods can learn prompts automatically but largely focus on learning semantic prompts, while how to learn effective spatial prompts that are important to SAMs is largely under-explored. Inspired by Hough Voting that detects a complex object by voting from its parts, we propose Hough Voting-based Spatial Prompt Learning (HoughSpaPL) that designs three types of voting mechanisms to learn three distinct spatial prompts for different subregions of the visual concept (e.g., things and stuff), which capture complementary spatial clues and vote together to guide SAMs to generate a precise segmentation mask for the visual concept. Following the same philosophy, we design Hough Voting-based Semantic Prompt Learning (HoughSemPL) that learns distinct semantic prompts for different sub-regions of the visual concept, which capture complementary semantic clues and vote together to predict a accurate semantic label for the generated mask. Extensive experiments show that our proposed techniques achieve superior prompt learning performance over popular segmentation datasets. Codes will be released.",2025,0.4090907603306326,0.4066481372973985,0.4,0.25,c9eb6a18-70ef-4e35-9425-cce99b226988,0,"[0.25, 0.25, 0.5, 0.5]","[0.95, 1.0, 0.95, 0.95]",HoughSpaPL,0.3650589101620029
543038,Overcoming label shift in targeted federated learning,"Federated learning enables multiple actors to collaboratively train models without sharing private data. This unlocks the potential for scaling machine learning to diverse applications. Existing algorithms for this task are well-justified when clients and the intended target domain share the same distribution of features and labels, but this assumption is often violated in real-world scenarios. One common violation is label shift, where the label distributions differ across clients or between clients and the target domain, which can significantly degrade model performance. To address this problem, we propose FedPALS, a novel model aggregation scheme that adapts to label shifts by leveraging knowledge of the target label distribution at the central server. Our approach ensures unbiased updates under stochastic gradient descent, ensuring robust generalization across clients with diverse, label-shifted data. Extensive experiments on image classification demonstrate that FedPALS consistently outperforms standard baselines by aligning model aggregation with the target domain. Our findings reveal that conventional federated learning methods suffer severely in cases of extreme client sparsity, highlighting the critical need for target-aware aggregation. FedPALS offers a principled and practical solution to mitigate label distribution mismatch, ensuring models trained in federated settings can generalize effectively to label-shifted target domains.",2025,0.4090907603306326,0.4112822947955852,0.2666666666666666,0.25,e5cb089e-5883-40d8-bd14-ceff0fcc2d60,0,"[0.25, 0.25, 0.25, 0.5, 0.625]","[0.95, 0.95, 0.9, 1.0, 0.95]",FedPALS,0.3890031906076595
543043,Rapid Grassmannian Averaging with Chebyshev Polynomials,"We propose new algorithms to efficiently average a collection of points on a Grassmannian manifold in both the centralized and decentralized settings. Grassmannian points are used ubiquitously in machine learning, computer vision, and signal processing to represent data through (often low-dimensional) subspaces. While averaging these points is crucial to many tasks (especially in the decentralized setting), existing methods unfortunately remain computationally expensive due to the non-Euclidean geometry of the manifold. Our proposed algorithms, Rapid Grassmannian Averaging (RGrAv) and Decentralized Rapid Grassmannian Averaging (DRGrAv), overcome this challenge by leveraging the spectral structure of the problem to rapidly compute an average using only small matrix multiplications and QR factorizations. We provide a theoretical guarantee of optimality and present numerical experiments which demonstrate that our algorithms outperform state-of-the-art methods in providing high accuracy solutions in minimal time. Additional experiments showcase the versatility of our algorithms to tasks such as $K$-means clustering on video motion data, establishing RGrAv and DRGrAv as powerful tools for generic Grassmannian averaging.",2025,0.3636358677687754,0.3613858690325012,0.2666666666666666,0.25,2421c1f3-1d1d-4745-afc1-c29256d92c46,0,"[0.25, 0.25, 0.5]","[0.9, 0.95, 0.9]",RGrAv,0.3258751353302058
543054,Enhancing Audio--Language Models through Self--Supervised Post--Training with Text--Audio Pairs,"Research on multi-modal contrastive learning strategies for audio and text has rapidly gained interest. Contrastively trained Audio-Language Models (ALMs), such as CLAP, which establish a unified representation across audio and language modalities, have enhanced the efficacy in various subsequent tasks by providing good text aligned audio encoders and vice versa. These improvements are evident in areas like zero-shot audio classification and audio retrieval, among others. However, the ability of these models to understand natural language and temporal relations is still a largely unexplored and open field for research. In this paper, we propose to equip the multi-modal ALMs with temporal understanding without loosing their inherent prior capabilities of audio-language tasks with a temporal instillation method $\textbf{TeminAL}$. We implement a two-stage training scheme TeminAL A \& B, where the model first learns to differentiate between multiple sounds in TeminAL A, followed by a phase that instills a sense of time, thereby enhancing its temporal understanding in TeminAL B. This approach results in an average performance gain of $5.28$\% in temporal understanding on the benchmark ESC-50 dataset, while the model remains competitive in zero-shot retrieval and classification tasks on the AudioCap/Clotho datasets. We also note the lack of proper evaluation techniques for contrastive ALMs and propose a strategy for evaluating ALMs in zero-shot settings. The general-purpose Zero-Shot Temporal Evaluation $\textbf{(ZSTE)}$ strategy , is used to evaluate various prior models. ZSTE demonstrates a general strategy to evaluate all ZS contrastive models. The model trained with TeminAL successfully outperforms current models on most downstream tasks.",2025,0.5113634504132908,0.5100884138438785,0.5333333333333333,0.5,1be2945a-ede1-4b79-9b1c-58242cc7f5ac,0,"[0.25, 0.5, 0.5, 0.625]","[1.0, 1.0, 0.95, 1.0]",TeminAL,0.4666057814485387
543061,WIQOR: A dataset for what-if analysis of Operations Research problems,"We formalize the mathematical program modification (MPM) task, in which the goal is to revise a mathematical program according to an inquiry expressed in natural language. These inquiries, which we refer to as what-if questions, express a desire to understand how the optimal solution to an optimization problem changes with the addition, deletion or revision of constraints. In detail, each MPM instance
is a triple consisting of: 1) a natural language specification that summarizes an optimization problem, 2) the canonical formulation of the problem, and 3) a natural language what-if question. The goal is to predict the updated canonical formulation with respect to the question. To support the study of this task, we construct WIQOR, a dataset of 1,946 MPM instances, derived from NL4OPT (Ramamonjison et al., 2023), but with the number of decision variables extended to more than 30 for some problems. In experiments, we observe that Llama 3.1 70B instruct under the in-context learning paradigm achieves 69% accuracy on the easiest test instances, but only 36% accuracy on the most complicated problems. We release WIQOR in the hopes of spurring additional study of MPM and ultimately enabling non-technical users to conduct what-if analyses without the help of technical experts.",2025,0.4999994545456529,0.4910714869165372,0.5333333333333333,0.25,b1495617-e0c8-4472-8684-3bcd13e2e7b5,0,"[0.25, 0.5, 0.625]","[1.0, 0.9, 0.9]",Mathematical Program Modification,0.4170051303976058
543063,MapEval: A Map-Based Evaluation of Geo-Spatial Reasoning in Foundation Models,"Recent advancements in foundation models have enhanced AI systems' capabilities in autonomous tool usage and reasoning.
However, their ability in location or map-based reasoning - which improves daily life by optimizing navigation, facilitating resource discovery, and streamlining logistics - has not been systematically studied. To bridge this gap, we introduce MapEval, a benchmark designed to assess diverse and complex map-based user queries with geo-spatial reasoning. MapEval features three task types (textual, API-based, and visual) that require collecting world information via map tools, processing heterogeneous geo-spatial contexts (e.g., named entities, travel distances, user reviews or ratings, images), and compositional reasoning, which all state-of-the-art foundation models find challenging. Comprising 700 unique multiple-choice questions about locations across 180 cities and 54 countries, MapEval evaluates foundation models' ability to handle spatial relationships, map infographics, travel planning, and navigation challenges. Using MapEval, we conducted a comprehensive evaluation of 28 prominent foundation models. While no single model excelled across all tasks, Claude-3.5-Sonnet, GPT-4o, and Gemini-1.5-Pro achieved competitive performance overall. However, substantial performance gaps emerged, particularly in MapEval, where agents with Claude-3.5-Sonnet outperformed GPT-4o and Gemini-1.5-Pro by 16% and 21%, respectively, and the gaps became even more amplified when compared to open-source LLMs.  Our detailed analyses provide insights into the strengths and weaknesses of current models, though all models still fall short of human performance by more than 20% on average, struggling with complex map images and rigorous geo-spatial reasoning. This gap highlights MapEval's critical role in advancing general-purpose foundation models with stronger geo-spatial understanding.",2025,0.4772725537190714,0.4755778266182522,0.5333333333333333,0.5,d4725a5e-410e-4d34-bd27-22a53052a54e,0,"[0.25, 0.5, 0.5, 0.5]","[0.95, 0.95, 0.95, 0.9]",MapEval,0.4337268743914313
543074,ViTally Consistent: Scaling Biological Representation Learning for Cell Microscopy,"Large-scale cell microscopy screens are used in drug discovery and molecular biology research to study the effects of millions of chemical and genetic perturbations on cells. To use these images in downstream analysis, we need models that can map each image into a feature space that represents diverse biological phenotypes consistently, in the sense that perturbations with similar biological effects have similar representations.
In this work, we present the largest foundation model for cell microscopy data to date, a new 1.9 billion-parameter ViT-G/8 MAE trained on over 8 billion microscopy image crops. Compared to a previous published ViT-L/8 MAE, our new model achieves a 60% improvement in linear separability of genetic perturbations and obtains the best overall performance on whole-genome biological relationship recall and replicate consistency benchmarks. We also show these performance trends hold on a public benchmark for measuring compound activity against target genes.
Beyond scaling, we developed two key methods that improve performance: (1) training on a curated and diverse dataset; and, (2) using biologically motivated linear probing tasks to search across each transformer block for the best candidate representation of whole-genome screens.
We find that many self-supervised vision transformers, pretrained on either natural or microscopy images, yield significantly more biologically meaningful representations of microscopy images in their intermediate blocks than in their typically used final blocks, therefore enabling significant cost and energy savings when deploying these large models in real-world applications. More broadly, our approach and results provide insights toward a general strategy for successfully building foundation models for large-scale biological image data.",2025,0.5795452438017296,0.5732153320365679,0.5333333333333333,0.5,b428a392-7e7b-430c-8b73-0717c6967199,0,"[0.25, 0.5, 0.5, 0.875]","[0.95, 1.0, 0.95, 0.9]",Biological Representation Learning,0.5096167603374433
543111,Deep Denoising Prior: You Only Need a Deep Gaussian Denoiser,"Gaussian denoising often serves as the initiation of research in the field of image denoising, owing to its prevalence and intriguing properties. However, deep Gaussian denoiser typically generalizes poorly to other types of noises, such as Poisson noise and real-world noise. In this paper, we reveal that deep Gaussian denoisers have an underlying ability to handle other noises with only ten iterations of self-supervised learning, which is referred to as \textit{deep denoiser prior}. Specifically, we first pre-train a Gaussian denoising model in a self-supervised manner. Then, for each test image, we construct a pixel bank based on the self-similarity and randomly sample pseudo-instance examples from it to perform test-time adaptation. Finally, we fine-tune the pre-trained Gaussian denoiser using the randomly sampled pseudo-instances. Extensive experiments demonstrate that our test-time adaptation method helps the pre-trained Gaussian denoiser rapidly improve performance in removing both in-distribution and out-of-distribution noise, achieving superior performance compared to existing single-image denoising methods while also significantly reducing computational time.",2025,0.4090907603306326,0.4031118013639997,0.4,0.25,4a844f03-2715-4fbd-b731-6fa5b9407920,0,"[0.25, 0.25, 0.5, 0.5]","[1.0, 1.0, 0.9, 0.95]",deep denoiser prior,0.349265194795961
543134,Test-Time Adversarial Defense with Opposite Adversarial Path and high Attack time cost,"Deep learning models are known to be vulnerable to adversarial attacks by injecting sophisticated designed perturbations to input data. Training-time defenses still exhibit a significant performance gap between natural accuracy and robust accuracy. In this paper, we investigate a new test-time adversarial defense method via diffusion-based recovery along opposite adversarial paths (OAPs). We present a purifier that can be plugged into a pre-trained model to resist adversarial attacks. Different from prior arts, the key idea is excessive denoising or purification by integrating the opposite adversarial direction with reverse diffusion to push the input image further toward the opposite adversarial direction. For the first time, we also exemplify the pitfall of conducting AutoAttack (Rand) for diffusion-based defense methods. Through the lens of time complexity, we examine the trade-off between the effectiveness of adaptive attack and its computation complexity against our defense. Experimental evaluation along with time cost analysis verifies the effectiveness of the proposed method.",2025,0.5909092396693675,0.5883136458784859,0.5333333333333333,0.5,8927d5e3-6396-4c01-822a-44b13fb5ecf7,0,"[0.5, 0.5, 0.625]","[0.95, 0.95, 0.9]",Opposite Adversarial Path,0.5348211328254389
543150,Causal Information Prioritization for Efficient Reinforcement Learning,"Current Reinforcement Learning (RL) methods often suffer from sample-inefficiency, resulting from blind exploration strategies that neglect causal relationships among states, actions, and rewards. Although recent causal approaches aim to address this problem, they lack grounded modeling of reward-guided causal understanding of states and actions for goal-orientation, thus impairing learning efficiency. To tackle this issue, we propose a novel method named Causal Information Prioritization (CIP) that improves sample efficiency by leveraging factored MDPs to infer causal relationships between different dimensions of states and actions with respect to rewards, enabling the prioritization of causal information. Specifically, CIP identifies and leverages causal relationships between states and rewards to execute counterfactual data augmentation to prioritize high-impact state features under the causal understanding of the environments. Moreover, CIP integrates a causality-aware empowerment learning objective, which significantly enhances the agent's execution of reward-guided actions for more efficient exploration in complex environments. 
To fully assess the effectiveness of CIP, we conduct extensive experiments across $39$ tasks in $5$ diverse continuous control environments, encompassing both locomotion and manipulation skills learning with pixel-based and sparse reward settings. Experimental results demonstrate that CIP consistently outperforms existing RL methods across a wide range of scenarios.",2025,0.727272826446245,0.7332380726261243,0.6666666666666666,0.5,1548108f-d7f2-4b3a-9b11-2e64953cceb7,1,"[0.5, 0.625, 0.875]","[0.9, 0.95, 1.0]",Causal Information Prioritization,0.7019328504466568
543151,HARDMath: A Benchmark Dataset for Challenging Problems in Applied Mathematics,"Advanced applied mathematics problems are underrepresented in existing Large Language Model (LLM) benchmark datasets. To address this, we introduce $\textbf{HARDMath}$, a dataset inspired by a graduate course on asymptotic methods, featuring challenging applied mathematics problems that require analytical approximation techniques. These problems demand a combination of mathematical reasoning, computational tools, and subjective judgment, making them difficult for LLMs. Our framework auto-generates a large number of problems with solutions validated against numerical ground truths. We evaluate both open- and closed-source LLMs on $\textbf{HARDMath-mini}$, a sub-sampled test set of 366 problems, as well as on 40 word problems formulated in applied science contexts. Even leading closed-source models like GPT-4 achieve only 43.8% overall accuracy with few-shot Chain-of-Thought prompting, and all models demonstrate significantly lower performance compared to results on existing mathematics benchmark datasets. We additionally conduct a detailed error analysis to gain insights into the failure cases of LLMs. These results demonstrate the limitations of current LLM performance on advanced graduate-level applied math problems and underscore the importance of datasets like $\textbf{HARDMath}$ to advance mathematical abilities of LLMs.",2025,0.7499997272728265,0.7343816367561116,0.6666666666666666,0.625,7966fc37-b276-4ff9-b4c7-80c5e0a51614,1,"[0.625, 0.625, 0.625, 0.875]","[0.95, 0.95, 0.95, 0.7]",HARDMath,0.6511374937841868
543159,Balanced Neural ODEs: nonlinear model order reduction and Koopman operator approximations,"Variational Autoencoders (VAEs) are a powerful framework for learning latent representations of reduced dimensionality, while Neural ODEs excel in learning transient system dynamics. This work combines the strengths of both to generate fast surrogate models with adjustable complexity reacting on time-varying inputs signals. By leveraging the VAE’s dimensionality reduction using a non-hierarchical prior, our method adaptively assigns stochastic noise, naturally complementing known NeuralODE training enhancements and enabling probabilistic
time series modeling. We show that standard Latent ODEs struggle with dimensionality reduction in systems with time-varying inputs. Our approach mitigates this by continuously propagating variational parameters through time, establishing fixed information channels in latent space. This results in a flexible and robust method that can learn different system complexities, e.g. deep neural networks or
linear matrices. Hereby, it enables efficient approximation of the Koopman operator without the need for predefining its dimensionality. As our method balances dimensionality reduction and reconstruction accuracy, we call it Balanced Neural ODE (B-NODE). We demonstrate the effectiveness of this methods on several academic and real-world test cases, e.g. a power plant or MuJoCo data.",2025,0.7159088305786071,0.719589362457552,0.6666666666666666,0.625,4e384e96-117d-4ade-8fc3-f812343d6ad6,1,"[0.5, 0.625, 0.625, 0.875]","[0.8, 0.9, 0.9, 0.9]",Balanced Neural ODE,0.6713348010437051
543192,"CulturalBench: a Robust, Diverse and Challenging Benchmark on Measuring (the Lack of) Cultural Knowledge of LLMs","To make large language models (LLMs) more helpful across diverse cultures, it is essential to have effective cultural knowledge benchmarks to measure and track our progress. Effective benchmarks need to be robust, diverse, and challenging. We introduce CulturalBench: a set of 1,227 human-written and human-verified questions for effectively assessing LLMs' cultural knowledge, covering 45 global regions including the underrepresented ones like Bangladesh, Zimbabwe, and Peru. Questions - each verified by five independent annotators - span 17 diverse topics ranging from food preferences to greeting etiquettes. We evaluate models on two setups: CulturalBench-Easy and CulturalBench-Hard which share the same questions but asked differently. We find that LLMs are sensitive to such difference in setups (e.g., GPT-4o with 27.3% difference). Compared to human performance (92.6% accuracy), CulturalBench-Hard is more challenging for frontier LLMs with the best performing model (GPT-4o) at only 61.5% and the worst (Llama3-8b) at 21.4%. Moreover, we find that LLMs often struggle with tricky questions that have multiple correct answers (e.g., What utensils do the Chinese usually use?), revealing a tendency to converge to a single answer. Our results also indicate that OpenAI GPT-4o substantially outperform other proprietary and open source models in questions related to all but one region (Oceania). Nonetheless, all models consistently underperform on questions related to South America and the Middle East.",2025,0.5454543471075102,0.5445543476130005,0.5333333333333333,0.5,cd374e34-d598-46f4-b4e7-37ce4073b10c,0,"[0.5, 0.5, 0.5, 0.5, 0.5]","[1.0, 0.9, 0.95, 0.95, 0.95]",CulturalBench,0.5
543204,Graph Neural Network Is A Mean Field Game,"In current graph neural networks (GNNs), it is a common practice to apply a pre-defined message passing heuristics to all graph data, even though the stereotypical relational inductive bias (e.g., graph heat diffusion) might not fit the unseen graph topology. Such gross simplification might be responsible for the lack of an in-depth understanding of graph learning principles, which challenges us to push the boundary from crafting application-specific GNNs to embracing a ""meta-learning"" paradigm. In this work, we ratchet the gear of GNN another notch forward by formulating GNN as a *mean field game*, that is, the best learning outcome occurs at the *Nash*-equilibrium when the learned graph inference rationale allows each graph node to find what is the best feature representations for not only the individual node but also the entire graph. Following this spirit, we formulate the search for novel GNN mechanism into a variational framework of *mean-field control* (MFC) problem, where the optimal relational inductive bias is essentially the critical point of mean-field information dynamics. Specifically, we seek for the best characteristic MFC functions of transportation mobility (controlling information exchange throughout the graph) and reaction mobility (controlling feature representation learning on each node), on the fly, that uncover the most suitable learning mechanism for a GNN instance by solving an MFC variational problem through the lens of *Hamiltonian flows* (formed in partial differential equations). In this context, our variational framework brings together existing GNN models into various mean-field games with distinct equilibrium states, each characterized by a unique MFC functional. Furthermore, we present an agnostic end-to-end deep model, coined *Nash-GNN* (in honor of Nobel laureate Dr. John Nash), to jointly carve the nature of the inductive bias and fine-tune the GNN hyper-parameters on top of the elucidated learning mechanism. *Nash-GNN* has achieved SOTA performance on diverse graph data including popular benchmark datasets and human connectomes. More importantly, the mathematical insight of mean-field games provides a new window to understand the foundational principles of graph learning as an interactive dynamical system, which allows us to reshape the idea of designing next-generation GNN models.",2025,0.5909092396693675,0.5881186954220405,0.5333333333333333,0.5,ac030846-56c9-4bbc-88e1-12530a0f1f36,0,"[0.5, 0.5, 0.625]","[0.9, 0.8, 0.8]",Nash-GNN,0.5352746825602487
543219,TIGeR: Unifying Text-to-Image Generation and Retrieval with Large Multimodal Models,"How humans can effectively and efficiently acquire images has always been a perennial question. A classic solution is *text-to-image retrieval* from an existing database; however, the limited database typically lacks creativity. By contrast, recent breakthroughs in *text-to-image generation* have made it possible to produce attractive and counterfactual visual content, but it faces challenges in synthesizing knowledge-intensive images. In this work, we rethink the relationship between text-to-image generation and retrieval, proposing a *unified* framework for both tasks with one single Large Multimodal Model (LMM). Specifically, we first explore the intrinsic discriminative abilities of LMMs and introduce an efficient generative retrieval method for text-to-image retrieval in a training-free manner. Subsequently, we unify generation and retrieval autoregressively and propose an autonomous decision mechanism to choose the best-matched one between generated and retrieved images as the response to the text prompt. To standardize the evaluation of unified text-to-image generation and retrieval, we construct TIGeR-Bench, a benchmark spanning both creative and knowledge-intensive domains. Extensive experiments on TIGeR-Bench and two retrieval benchmarks, *i.e.*, Flickr30K and MS-COCO, demonstrate the superiority of our proposed framework.",2025,0.7636360859505142,0.7684271745688763,0.6666666666666666,0.625,683ab7c3-66b4-41cf-a19d-f4951dc0bcee,1,"[0.5, 0.625, 0.625, 0.875, 0.875]","[0.9, 0.9, 0.8, 0.95, 0.95]",TIGeR-Bench,0.7247485957542898
543222,From Isolated Conversations to Hierarchical Schemas: Dynamic Tree Memory Representation for LLMs,"Recent advancements in large language models have significantly improved their context windows, yet challenges in effective long-term memory management remain. We introduce MemTree, an algorithm that leverages a dynamic, tree-structured memory representation to optimize the organization, retrieval, and integration of information, akin to human cognitive schemas. MemTree organizes memory hierarchically, with each node encapsulating aggregated textual content, corresponding semantic embeddings, and varying abstraction levels across the tree's depths. Our algorithm dynamically adapts this memory structure by computing and comparing semantic embeddings of new and existing information to enrich the model’s context-awareness. This approach allows MemTree to handle complex reasoning and extended interactions more effectively than traditional memory augmentation methods, which often rely on flat lookup tables. Evaluations on benchmarks for multi-turn dialogue understanding and document question answering show that MemTree significantly enhances performance in scenarios that demand structured memory management.",2025,0.6818179338843877,0.6806929345162506,0.6666666666666666,0.625,07254413-5ded-40ef-950a-5b47a32c6ca1,1,"[0.625, 0.625, 0.625, 0.625]","[0.9, 0.9, 0.95, 0.95]",MemTree,0.6250000000000001
543224,Learning 4D Embodied World Models,"In this paper, we present a 4D embodied world model, which takes in an image observation and language instruction as input and predicts a 4D dynamic mesh predicting how the scene will change as the embodied agent performs actions based on the given instructions. In contrast to previously learned world models which typically generate 2D videos, our 4D model provides detailed 3D information on precise configurations and shape of objects in a scene over time.
This allows us to effectively learn accurate inverse dynamic models for an embodied agent to execute a policy for interacting with the environment.
To construct a dataset to train such 4D world models,  we first annotate large-scale existing video robotics dataset using pretrained depth and normal prediction models to construct 3D consistent 4D models of each video. To efficiently learn generative models on this 4D data, we propose to train a video generative model on this annotated dataset, which jointly predicts RGB-DN (RGB, Depth, and Normal) for each video. We then present an algorithm to directly convert generated RGB, Depth and Normal images into high-quality dynamic 4D mesh models of the world. We illustrate how this enables us to predict high-quality meshes consistent across both time and space from embodied scenarios, render novel views for embodied scenes, as well as construct policies that substantially outperform those from prior 2D  and 3D models of the world. Our code, model, and dataset will be made publicly available.",2025,0.443181657024852,0.4415301105880969,0.4,0.25,7e9ede6d-fea9-433a-a60b-5f89867b4e61,0,"[0.25, 0.25, 0.5, 0.625]","[0.9, 0.95, 0.95, 0.9]",4D dynamic mesh,0.4022344559585493
543236,SimXRD-4M: Big Simulated X-ray Diffraction Data and Crystal Symmetry Classification Benchmark,"Powder X-ray diffraction (XRD) patterns are highly effective for crystal identification and play a pivotal role in materials discovery. While machine learning (ML) has advanced the analysis of powder XRD patterns, progress has been constrained by the limited availability of training data and established benchmarks. To address this, we introduce SimXRD, the largest open-source simulated XRD pattern dataset to date, aimed at accelerating the development of crystallographic informatics. We developed a novel XRD simulation method that incorporates comprehensive physical interactions, resulting in a high-fidelity database. SimXRD comprises 4,065,346 simulated powder XRD patterns, representing 119,569 unique crystal structures under 33 simulated conditions that reflect real-world variations. We benchmark 21 sequence models in both in-library and out-of-library scenarios and analyze the impact of class imbalance in long-tailed crystal label distributions. Remarkably, we find that: (1) current neural networks struggle with classifying low-frequency crystals, particularly in out-of-library situations; (2) models trained on SimXRD can generalize to real experimental data.",2025,0.7159088305786071,0.7151817395679724,0.6666666666666666,0.625,ac62730f-b5d3-4e50-8078-5a8ab85f01dd,1,"[0.5, 0.625, 0.625, 0.875]","[0.95, 0.9, 0.95, 0.95]",SimXRD,0.6581365628042842
543242,Reward-Augmented Data Enhances Direct Preference Alignment of LLMs,"Preference alignment in Large Language Models (LLMs) has significantly improved their ability to adhere to human instructions and intentions. However, existing direct alignment algorithms primarily focus on relative preferences and often overlook the qualitative aspects of responses, despite having access to preference data that includes reward scores from judge models during AI feedback. Striving to maximize the implicit reward gap between the chosen and the slightly inferior rejected responses can cause overfitting and unnecessary unlearning of the high-quality rejected responses. The unawareness of the reward scores also drives the LLM to indiscriminately favor the low-quality chosen responses and fail to generalize to responses with the highest rewards, which are sparse in data. To overcome these shortcomings, our study introduces reward-conditioned LLM policies that discern and learn from the entire spectrum of response quality within the dataset, helping extrapolate to more optimal regions. We propose an effective yet simple data relabeling method that conditions the preference pairs on quality scores to construct a reward-augmented dataset. This dataset is easily integrated with existing direct alignment algorithms and is applicable to any preference dataset. The experimental results across instruction-following benchmarks including AlpacaEval 2.0, MT-Bench, and Arena-Hard-Auto demonstrate that our approach consistently boosts the performance of DPO by a considerable margin across diverse models such as Zephyr, Mistral, Qwen2, Llama3.1, Gemma2, and SPPO. Additionally, on six academic benchmarks including GSM8K, GPQA, MUSR, TruthfulQA, BBH, and ARC, our method improves their average accuracy. When applying our method to on-policy data, the resulting DPO model outperforms various baselines and achieves state-of-the-art results on AlpacaEval 2.0. Through comprehensive ablation studies, we demonstrate that our method not only maximizes the utility of preference data but also mitigates the issue of unlearning, demonstrating its broad effectiveness beyond mere dataset expansion.",2025,0.5727270644628857,0.57062325334193,0.6666666666666666,0.625,e4396b6c-ac61-4d5a-bbfa-7a6802ffe22e,0,"[0.25, 0.5, 0.625, 0.625, 0.625]","[0.95, 0.95, 0.95, 0.95, 0.9]",Reward-Augmented Dataset,0.520228016163171
543243,Can Transformers Perform PCA ?,"Transformers demonstrate significant advantage as the building block of Large Language Models. Recent efforts are devoted to understanding the learning capacities of transformers at a fundamental level. This work attempts to understand the intrinsic capacity of transformers in performing dimension reduction from complex data. Theoretically, our results rigorously show that transformers can perform Principle Component Analysis (PCA) similar to the Power Method, given a supervised pre-training phase. Moreover, we show the generalization error of transformers decays by $n^{-1/5}$ in $L_2$. Empirically, our extensive experiments on the simulated and real world high dimensional datasets justify that a pre-trained transformer can successfully perform PCA by simultaneously estimating the first $k$ eigenvectors and eigenvalues. These findings demonstrate that transformers can efficiently extract low dimensional patterns from high dimensional data, shedding light on the potential benefits of using pre-trained LLM to perform inference on high dimensional data.",2025,0.4363634776860081,0.4416945660010761,0.5333333333333333,0.5,9ae85e0e-421a-43bd-9c3f-3679bda8c3b3,0,"[0.25, 0.25, 0.5, 0.5, 0.5]","[0.9, 0.8, 0.9, 0.95, 0.95]",Transformers,0.4213462364621338
543248,Are LLMs Better than Reported? Detecting Label Errors and Mitigating Their Effect on Model Performance,"NLP benchmarks rely on standardized datasets for training and evaluating models and are crucial for advancing the field. Traditionally, expert annotations ensure high-quality labels; however, the cost of expert annotation does not scale well with the growing demand for larger datasets required by modern models.
While crowd-sourcing provides a more scalable solution, it often comes at the expense of annotation precision and consistency. Recent advancements in large language models (LLMs) offer new opportunities to enhance the annotation process, particularly for detecting label errors in existing datasets. In this work, we consider the recent approach of LLM-as-a-judge, leveraging an ensemble of LLMs to flag potentially mislabeled examples.
Through a case study of four datasets from the TRUE benchmark, covering different tasks and domains, we empirically analyze the labeling quality of existing datasets, and compare expert, crowd-sourced, and our LLM-based annotations in terms of agreement, label quality, and efficiency, demonstrating the strengths and limitations of each annotation method. Our findings reveal a substantial number of label errors, which, when corrected, induce a significant upward shift in reported model performance. This suggests that many of the LLMs so-called mistakes are due to label errors rather than genuine model failures. Additionally, we discuss the implications of mislabeled data and propose methods to mitigate them in training to improve model performance.",2025,0.727272826446245,0.7260728265202324,0.6666666666666666,0.5,7d7d3e68-9d07-4bf6-9ee0-845786cfe47f,0,"[0.5, 0.625, 0.875]","[0.95, 0.95, 0.95]",LLM-as-a-judge,0.6666666666666667
543284,CipherPrune:  Efficient and Scalable Private Transformer Inference,"Private Transformer inference using cryptographic protocols offers promising solutions for privacy-preserving machine learning; however, it still faces significant runtime overhead (efficiency issues) and challenges in handling long-token inputs (scalability issues). We observe that the Transformer's operational complexity scales quadratically with the number of input tokens, making it essential to reduce the input token length. Notably, each token varies in importance, and many inputs contain redundant tokens. Additionally, prior private inference methods that rely on high-degree polynomial approximations for non-linear activations are computationally expensive. Therefore, reducing the polynomial degree for less important tokens can significantly accelerate private inference.  Building on these observations, we propose \textit{CipherPrune}, an efficient and scalable private inference framework that includes a secure encrypted token pruning protocol, a polynomial reduction protocol, and corresponding Transformer network optimizations. At the protocol level, encrypted token pruning adaptively removes unimportant tokens from encrypted inputs in a progressive, layer-wise manner. Additionally, encrypted polynomial reduction assigns lower-degree polynomials to less important tokens after pruning, enhancing efficiency without decryption. At the network level, we introduce protocol-aware network optimization via a gradient-based search to maximize pruning thresholds and polynomial reduction conditions while maintaining the desired accuracy. Our experiments demonstrate that CipherPrune reduces the execution overhead of private Transformer inference by approximately $6.1\times$ for 128-token inputs and $10.6\times$  for 512-token inputs, compared to previous methods, with only a marginal drop in accuracy. The code is publicly available at https://github.com/UCF-Lou-Lab-PET/cipher-prune-inference.",2025,0.7159088305786071,0.7217207481741093,0.6666666666666666,0.625,7b577d0a-2ac8-4449-86d4-25bc48f535ad,1,"[0.5, 0.625, 0.625, 0.875]","[0.8, 0.95, 0.95, 0.95]",CipherPrune,0.6785562542488103
543300,DRESSing Up LLM: Efficient Stylized Question-Answering via Style Subspace Editing,"We introduce DRESS, a novel approach for generating stylized large language model (LLM) responses through representation editing. Existing methods like prompting and fine-tuning are either insufficient for complex style adaptation or computationally expensive, particularly in tasks like NPC creation or character role-playing. Our approach leverages the over-parameterized nature of LLMs to disentangle a style-relevant subspace within the model's representation space to conduct representation editing, ensuring a minimal impact on the original semantics. By applying adaptive editing strengths, we dynamically adjust the steering vectors in the style subspace to maintain both stylistic fidelity and semantic integrity. We develop two stylized QA benchmark datasets to validate the effectiveness of DRESS, and the results demonstrate significant improvements compared to baseline methods such as prompting and ITI. In short, DRESS is a lightweight, train-free solution for enhancing LLMs with flexible and effective style control, making it particularly useful for developing stylized conversational agents. Codes and benchmark datasets are available at https://github.com/ArthurLeoM/DRESS-LLM.",2025,0.7159088305786071,0.7174873826757658,0.6666666666666666,0.625,ff411eb7-7654-4450-8c18-06bd180d97ec,1,"[0.5, 0.625, 0.625, 0.875]","[0.9, 0.95, 0.9, 0.95]",DRESS,0.6682966321243524
543329,Towards Generalization under Topological Shifts: A Diffusion PDE Perspective,"The capability of generalization is a cornerstone for the success of modern learning systems. For non-Euclidean data that particularly involves topological features, one important aspect neglected by prior studies is how learning-based models generalize under topological shifts. This paper makes steps towards understanding the generalization of graph neural networks operated on varying topologies through the lens of diffusion PDEs. Our analysis first reveals that the upper bound of the generalization error yielded by local diffusion equation models, which are intimately related to message passing over observed structures, would exponentially grow w.r.t. topological shifts. In contrast, extending the diffusion operator to a non-local counterpart that learns latent structures from data can in principle control the generalization error under topological shifts even when the model accommodates observed structures. On top of these results, we propose Advective Diffusion Transformer inspired by advective diffusion equations serving as a physics-inspired continuous model that synthesizes observed and latent structures for graph learning. The model demonstrates superiority in various downstream tasks across information networks, molecular screening and protein interactions.",2025,0.3409089669421938,0.3267326085678003,0.2666666666666666,0.25,887764b8-4081-4de6-bb6c-56412bbcd1fd,0,"[0.25, 0.25, 0.25, 0.5]","[0.95, 0.9, 0.95, 0.7]",Advective Diffusion Transformer,0.2780437439402053
543352,Goal-conditioned Reinforcement Learning with Subgoals Generated from Relabeling,"In goal-conditioned reinforcement learning (RL), the primary objective is to develop a goal-conditioned policy capable of reaching diverse desired goals, a process often hindered by sparse reward signals. To address the challenges associated with sparse rewards, existing approaches frequently employ hindsight relabeling, substituting original goals with achieved goals. However, these methods exhibit a tendency to prioritize the optimization of closer achieved goals during training, leading to the loss of potentially valuable information from the trajectory and low sample efficiency. Our key insight is that these achieved goals, generated from the same hindsight relabeling, can serve as effective subgoals to facilitate the learning of policies that reach possible long-horizon desired goals within the same trajectory. Leveraging this perspective, we propose a novel framework called Goal-Conditioned reinforcement learning with Q-BC (i.e, behavior cloning (BC)-regularized Q) and Subgoals (GCQS) for goal-conditioned RL. GCQS is a innovative goal-conditioned actor-critic framework that systematically exploits more trajectory information to improve policy learning and sample efficiency. Specifically, GCQS initially optimizes a Q-BC objective to facilitate learning policies that reach achieved goals effectively. Subsequently, these achieved goals are redefined as subgoals, which serve to enhance the goal-conditioned policies, thereby predicting better actions to reach the desired goals. Experimental results in simulated robotics environments demonstrate that GCQS significantly enhances sample efficiency and overall performance compared to existing goal-conditioned methods. Additionally, GCQS demonstrated competitive performance on long-horizon AntMaze tasks, achieving results comparable to such state-of-the-art subgoal-based methods.",2025,0.6477270371901683,0.6494180892241408,0.5333333333333333,0.5,1a8c5203-6610-409a-a4af-5c55c37abe31,0,"[0.5, 0.5, 0.5, 0.875]","[0.9, 0.95, 0.9, 0.95]",GCQS,0.6057966321243523
543356,ExploraCoder: Advancing code generation for multiple unseen APIs via planning and chained exploration,"Through training on publicly available source code libraries, large language models (LLMs) can invoke multiple encapsulated APIs to solve complex programming problems.
However, existing models inherently cannot generalize to use APIs that are unseen in their training corpora. As libraries continuously evolve, it becomes impractical to exhaustively retrain LLMs with new API knowledge. This limitation hampers LLMs from solving problems which require newly introduced or privately maintained libraries.
Human programmers often explore unfamiliar APIs by writing experimental code before invoking them for a more complex problem.
Inspired by this behavior, we propose $\textbf{ExploraCoder}$, a training-free framework that empowers LLMs to invoke multiple unseen APIs in code solution by (1) planning a complex problem into several API invocation subtasks, and (2) exploring correct API usage through a novel chain-of-API-exploration.
Concretely,  ExploraCoder guides the LLM to iteratively generate several experimental API invocations for each simple subtask, where the promising execution experience are exploited by subsequent subtasks. This forms a chained exploration trace that ultimately guides LLM in generating the final solution.
We evaluate ExploraCoder on Torchdata-Github benchmark as well as a newly constructed benchmark that involves more complex API interactions.
Experimental results demonstrate that ExploraCoder significantly improves performance for models lacking prior API knowledge, achieving an absolute increase of 11.24\% over niave RAG approaches and 14.07\% over pretraining methods in pass@10. Moreover, the integration of a self-debug mechanism further boosts ExploraCoder's performance on more challenging tasks. Comprehensive ablation and case studies provide further insights into the effectiveness of ExploraCoder.",2025,0.5454543471075102,0.5294277169450066,0.6666666666666666,0.625,b68197c1-e283-4206-91b8-2e827248f377,0,"[0.25, 0.625, 0.625]","[1.0, 0.8, 0.9]",ExploraCoder,0.4322696378492243
543358,Understanding Warmup-Stable-Decay Learning Rates: A River Valley Loss Landscape View,"Training language models currently requires pre-determining a fixed compute budget because the typical cosine learning rate schedule depends on the total number of steps. In contrast, the Warmup-Stable-Decay (WSD) schedule uses a constant learning rate to produce a main branch of iterates that can in principle continue indefinitely without a pre-specified compute budget. Then, given any compute budget, one can branch out from the main branch at a proper time with a rapidly decaying learning rate to produce a strong model. Empirically, WSD generates an intriguing, non-traditional loss curve: the loss remains elevated during the stable phase but sharply declines during the decay phase. Towards explaining this phenomenon, we conjecture that pretraining loss exhibits a river valley landscape, which resembles a deep valley with a river at its bottom. Under this assumption, we show that during the stable phase, the iterate undergoes large oscillations due to the high learning rate, yet it progresses swiftly along the river. During the decay phase, the rapidly dropping learning rate minimizes the iterate’s oscillations, moving it closer to the river and revealing true optimization progress. Therefore, the sustained high learning rate phase and fast decaying phase are responsible for progress in the river and the mountain directions, respectively, and are both critical. Our analysis predicts phenomenons consistent with empirical observations and shows that this landscape can naturally emerge from pretraining on a simple bi-gram dataset. Inspired by the theory, we introduce WSD-S, a variant of WSD that reuses previous checkpoints’ decay phases and keeps only one main branch, where we resume from a decayed checkpoint. WSD-S empirically outperforms WSD and Cyclic-Cosine in obtaining multiple pretrained language model checkpoints across various compute budgets in a single run for parameters scaling from 0.1B to 1.2B.",2025,0.4363634776860081,0.4313600135920765,0.5333333333333333,0.625,8484abec-ece5-4bb6-bf12-9e6606ef2a23,1,"[0.0, 0.25, 0.5, 0.625, 0.625]","[0.9, 0.9, 0.95, 0.8, 0.9]",Warmup-Stable-Decay,0.3899347145473402
543368,HyperPg - Prototypical Gaussians on the Hypersphere for Interpretable Deep Learning,"Prototype Learning methods provide an interpretable alternative to black-box deep learning models. Approaches such as ProtoPNet learn, which part of a test image ""look like""' known prototypical parts from training images, combining predictive power with the inherent interpretability of case-based reasoning. However, existing approaches have two main drawbacks: A) They rely solely on deterministic similarity scores without statistical confidence. B) The prototypes are learned in a black-box manner without human input.

This work introduces HyperPg, a new prototype representation leveraging Gaussian distributions on a hypersphere in latent space, with learnable mean and variance. HyperPg prototypes adapt to the spread of clusters in the latent space and output likelihood scores.The new architecture, HyperPgNet, leverages HyperPg to learn prototypes aligned with human concepts from pixel-level annotations. Consequently, each prototype represents a specific concept such as color, image texture, or part of the image subject. A concept extraction pipeline built on foundation models provides pixel-level annotations, significantly reducing human labeling effort.

Experiments on CUB-200-2011 and Stanford Cars datasets demonstrate that HyperPgNet outperforms other prototype learning architectures while using fewer parameters and training steps. Additionally, the concept-aligned HyperPg prototypes are learned transparently, enhancing model interpretability.",2025,0.2727271735537551,0.2687408378731014,0.2666666666666666,0.25,174ee501-f3a9-46ab-a217-2aedb3d4350b,0,"[0.0, 0.25, 0.25, 0.5]","[1.0, 0.95, 0.95, 0.95]",HyperPg,0.2301178203240059
543382,Recursive Abstractive Processing for Retrieval in Dynamic Datasets,"Recent retrieval-augmented models enhance basic methods by building a hierarchical structure over retrieved text chunks through recursive embedding, clustering, and summarization. The most relevant information is then retrieved from both the original text and generated summaries. However, such approaches face limitations with dynamic datasets, where adding or removing documents over time complicates the updating of hierarchical representations formed through clustering.
We propose a new algorithm to efficiently maintain the recursive-abstractive tree structure in dynamic datasets, without compromising performance. Additionally, we introduce a novel post-retrieval method that applies query-focused recursive abstractive processing to substantially improve context quality. Our method overcomes the limitations of other approaches by functioning as a black-box post-retrieval layer compatible with any retrieval algorithm.
Both algorithms are validated through extensive experiments on real-world datasets, demonstrating their effectiveness in handling dynamic data and improving retrieval performance.",2025,0.2727271735537551,0.2722771738065002,0.2666666666666666,0.25,7fd9abf4-898c-4046-a2f2-563f2bfde34f,0,"[0.25, 0.25, 0.25]","[0.95, 0.9, 0.9]",Recursive-Abstractive Tree Structure,0.2500000000007501
543388,Robust Conformal Prediction with a Single Binary Certificate,"Conformal prediction (CP) converts any model's output to prediction sets with a guarantee to cover the true label with (adjustable) high probability. Robust CP extends this guarantee to worst-case (adversarial) inputs. Existing baselines achieve robustness by bounding randomly smoothed conformity scores. In practice, they need expensive Monte-Carlo (MC) sampling (e.g. $\sim10^4$ samples per point) to maintain an acceptable set size. We propose a robust conformal prediction that produces smaller sets even with significantly lower MC samples (e.g. 150 for CIFAR10). Our approach binarizes samples with an adjustable (or automatically adjusted) threshold selected to preserve the coverage guarantee. Remarkably, we prove that robustness can be achieved by computing only one binary certificate, unlike previous methods that certify each calibration (or test) point. Thus, our method is faster and returns smaller robust sets. We also eliminate a previous limitation that requires a bounded score function.",2025,0.8181815206612653,0.8168315214195008,0.8,0.625,bbe61116-c0f0-4385-a25f-a4c0e452ab11,1,"[0.625, 0.625, 0.875, 0.875]","[0.95, 0.9, 0.9, 0.95]",Binary Certificate,0.75
543393,RESuM: A Rare Event Surrogate Model for  Physics Detector Design,"The experimental discovery of neutrinoless double-beta decay (NLDBD) would answer one of the most important questions in physics: Why is there more matter than antimatter in our universe? To maximize the chances of discovery, NLDBD experiments must optimize their detector designs to minimize the probability of background events contaminating the detector. Given that this probability is inherently low, design optimization either requires extremely costly simulations to generate sufficient background counts or contending with significant variance. In this work, we formalize this dilemma as a Rare Event Design (RED) problem: identifying optimal design parameters when the design metric to be minimized is inherently small. We then designed the Rare Event Surrogate Model (RESuM) for physics detector design optimization under RED conditions. RESuM uses a pre-trained Conditional Neural Process (CNP) model to incorporate additional prior knowledge into a Multi-Fidelity Gaussian Process model. We applied RESuM to optimize neutron shielding designs for the LEGEND NLDBD experiment, identifying an optimal design that reduces the neutron background by $(66.5 \pm 3.5)$% while using only 3.3% of the computational resources compared to traditional methods. Given the prevalence of RED problems in other fields of physical sciences, especially in rare-event searches, the RESuM algorithm has broad potential for accelerating simulation-intensive applications.",2025,0.6272724991736367,0.6033411676552144,0.6666666666666666,0.875,939bea09-3138-4208-a66c-2dfc64ab5ebc,1,"[0.0, 0.5, 0.625, 0.875, 0.875]","[1.0, 0.9, 0.8, 0.9, 0.8]",Rare Event Surrogate Model,0.4658031555745405
543397,MobileSafetyBench: Evaluating Safety of Autonomous Agents in Mobile Device Control,"Autonomous agents powered by large language models (LLMs) show promising potential in assistive tasks across various domains, including mobile device control. As these agents interact directly with personal information and device settings, ensuring their safe and reliable behavior is crucial to prevent undesirable outcomes. However, no benchmark exists for standardized evaluation of the safety of mobile device-control agents. In this work, we introduce MobileSafetyBench, a benchmark designed to evaluate the safety of device-control agents within a realistic mobile environment based on Android emulators. We develop a diverse set of tasks involving interactions with various mobile applications, including messaging and banking applications, challenging agents with managing risks encompassing misuse and negative side effects. These tasks include tests to evaluate the safety of agents in daily scenarios as well as their robustness against indirect prompt injection attacks. Our experiments demonstrate that baseline agents, based on state-of-the-art LLMs, often fail to effectively prevent risks while performing the tasks. To mitigate these safety concerns, we propose a prompting method that encourages agents to prioritize safety considerations. While this method shows promise in promoting safer behaviors, there is still considerable room for improvement to fully earn user trust. This highlights the urgent need for continued research to develop more robust safety mechanisms in mobile environments.",2025,0.443181657024852,0.435285161329671,0.4,0.0,609ad6a7-88f5-4cff-96de-9346627a006b,0,"[0.0, 0.25, 0.5, 0.875]","[1.0, 0.95, 0.9, 0.95]",MobileSafetyBench,0.3669699995433014
543398,A Generic Class-agnostic Object Counting Network with Adaptive Offset Deformable Convolution,"Class-agnostic object counting (CAC) aims at counting the number of objects in the unseen category in an image. In this paper, we design a generic class-agnostic object counting network with Adaptive Offset Deformable Convolution (AODC), which initially focus on the reference-less class-agnostic object counting task without any exemplar. Our method calculates the self-similarity maps of the image features and performing a 4D convolution on these maps, obtaining the adaptive offsets for the deformable convolution, so that the model can obtain complete information about the object at that location. Through this process, AODC is able to recognize objects of different scales in a same sample. In addition to this, we adopt our approach to both zero-shot setting and few-shot setting, the former with semantic text and the latter with visual exemplars as references. We conduct experiments on the few-shot object counting dataset FSC-147, as well as other large-scale datasets, and show that our method significantly outperforms state-of-the-art approaches on all the three settings.",2025,0.5227274462809287,0.5240736585992755,0.5333333333333333,0.5,6e77c126-91c8-4dcb-a59c-6aa22ac4326a,0,"[0.25, 0.5, 0.5, 0.5, 0.5, 0.625]","[0.9, 0.95, 0.95, 0.95, 0.95, 0.95]",Adaptive Offset Deformable Convolution,0.4882079223671651
543413,From Generalist to Specialist: Adapting Vision Language Models via Task-Specific Visual Instruction Tuning,"Large vision language models (VLMs) combine large language models with vision encoders, demonstrating promise across various tasks. However, they often underperform in task-specific applications due to domain gaps between pre-training and fine-tuning. We introduce VITask, a novel framework that enhances task-specific adaptability of VLMs by integrating task-specific models (TSMs). VITask employs three key strategies: exemplar prompting (EP), response distribution alignment (RDA), and contrastive response tuning (CRT) to improve the task-specific performance of VLMs by adjusting their response distributions. EP allows TSM features to guide VLMs, while RDA enables VLMs to adapt without TSMs during inference by learning from exemplar-prompted models. CRT further optimizes the ranking of correct image-response pairs, thereby reducing the risk of generating undesired responses. Experiments on 12 medical diagnosis datasets across 9 imaging modalities show that VITask outperforms both vanilla instruction-tuned VLMs and TSMs, showcasing its ability to integrate complementary features from both models effectively. Additionally, VITask offers practical advantages such as flexible TSM integration and robustness to incomplete instructions, making it a versatile and efficient solution for task-specific VLM tuning.",2025,0.7159088305786071,0.7169961946542188,0.6666666666666666,0.625,0d7a6b0d-5fd8-417e-a294-356750d2f438,0,"[0.5, 0.625, 0.625, 0.875]","[0.9, 0.95, 0.95, 0.95]",VITask,0.6656828140214215
543419,Evaluating Oversight Robustness with Incentivized Reward Hacking,"Scalable oversight aims to train systems to perform tasks that are hard for humans to specify, demonstrate and validate. 
As ground truth is not available for such tasks, evaluating scalable oversight techniques is challenging: existing methods measure the success of an oversight method based on whether it allows an artificially weak overseer to successfully supervise an AI to perform a task.
In this work, we additionally measure the robustness of scalable oversight techniques by testing their vulnerability to reward hacking by an adversarial supervisee.
In experiments on a synthetic domain, we show that adding an explicit reward hacking incentive to the model being trained leads it to exploit flaws in a weak overseer, and that scalable oversight methods designed to mitigate these flaws can make the optimization more robust to reward hacking.
We hope these experiments lay a foundation for future work to validate scalable oversight methods' ability to mitigate reward hacking in realistic settings.",2025,0.2954540743803366,0.2953945949713674,0.2666666666666666,0.25,9f8c47a4-97e9-4460-8a36-85bb5a224c18,0,"[0.0, 0.25, 0.25, 0.25, 0.25, 0.625]","[0.9, 0.8, 0.9, 0.9, 0.9, 0.9]",Reward Hacking,0.2721323994101537
543435,Adversarial Robustness of Graph Transformers,"Existing studies have shown that Message-Passing Graph Neural Networks (MPNNs) are highly susceptible to adversarial attacks. In contrast, despite the increasing importance of Graph Transformers (GTs), their robustness properties are unexplored. Thus, for the purpose of robustness evaluation, we design the first adaptive attacks for GTs. We provide general design principles for strong gradient-based attacks on GTs w.r.t. structure perturbations and instantiate our attack framework for five representative and popular GT architectures. Specifically, we study GTs with specialized attention mechanisms and Positional Encodings (PEs) based on random walks, pair-wise shortest paths, and the Laplacian spectrum. We evaluate our attacks on multiple tasks and threat models, including structure perturbations on node and graph classification and node injection for graph classification. Our results reveal that GTs can be catastrophically fragile in many cases. 
Consequently, we show how to leverage our adaptive attacks for adversarial training, substantially improving robustness.",2025,0.613636140495949,0.6126236410646255,0.6,0.5,1b70fd82-0d66-48bb-ba4b-5a01d49d14b0,0,"[0.5, 0.5, 0.625, 0.625]","[0.9, 0.95, 0.95, 0.9]",Graph Transformers,0.5625000000000001
543464,"If Optimizing for general parameters in chemistry is useful, why is it hardly done?","General parameters are highly desirable in the natural sciences — e.g., reaction
conditions that enable high yields across a range of related transformations. This
has a significant practical impact since those general parameters can be transfered
to related tasks without the need for laborious and time-intensive re-optimization.
While Bayesian optimization (BO) is widely applied to find optimal parameter
sets for specific tasks, it has remained underused in experiment planning towards
such general optima. In this work, we consider the real-world problem of condi-
tion optimization for chemical reactions to study whether performing generality-
oriented BO can accelerate the identification of general optima, and whether these
optima also translate to unseen examples. This is achieved through a careful for-
mulation of the problem as an optimization over curried functions, as well as
systematic benchmarking of generality-oriented strategies for optimization tasks
on real-world experimental data. Empirically, we find that for generality-oriented
optimization, simple optimization strategies that decouple parameter and task se-
lection perform comparably to more complex ones, and that effective optimization
is merely determined by an effective exploration of both parameter and task space.",2025,0.4090907603306326,0.4084157607097504,0.4,0.25,95daec0c-a6a2-4bed-9b7f-78fc53e88d86,0,"[0.25, 0.25, 0.5, 0.5]","[0.9, 0.95, 0.95, 0.9]",Generality-oriented Bayesian optimization,0.375
543466,Enhancing Robust Fairness via Confusional Spectral Regularization,"Recent research has highlighted a critical issue known as ``robust fairness"", where robust accuracy varies significantly across different classes, undermining the reliability of deep neural networks (DNNs). 
A common approach to address this has been to dynamically reweight classes during training, giving more weight to those with lower empirical robust performance. 
However, we find there is a divergence of class-wise robust performance between training set and testing set, which limits the effectiveness of these explicit reweighting methods, indicating the need for a principled alternative.
In this work, we derive a robust generalization bound for the worst-class robust error within the PAC-Bayesian framework, accounting for unknown data distributions. 
Our analysis shows that the worst-class robust error is influenced by two main factors: the spectral norm of the empirical robust confusion matrix and the information embedded in the model and training set. 
While the latter has been extensively studied, we propose a novel regularization technique targeting the spectral norm of the robust confusion matrix to improve worst-class robust accuracy and enhance robust fairness.
We validate our approach through comprehensive experiments on various datasets and models, demonstrating its effectiveness in enhancing robust fairness.",2025,0.8181815206612653,0.8193996397228436,0.8,0.625,cad56d58-51ca-41fa-86ae-250d2719b717,1,"[0.625, 0.625, 0.625, 0.875, 0.875, 0.875]","[0.95, 0.7, 0.95, 0.9, 0.9, 0.9]",Confusional Spectral Regularization,0.7491036593610563
543473,Memorization and the Orders of Loss: A Learning Dynamics Perspective,"Deep learning has become the de facto approach in nearly all learning tasks.
It has been observed that deep models tend to memorize and sometimes overfit data, which can lead to compromises in performance, privacy, and other critical metrics.
In this paper, we explore the theoretical foundations that connect memorization to various orders of sample loss, i.e., sample loss, sample loss gradient, and sample loss curvature, focusing on learning dynamics to understand what and how these models memorize. 
To this end, we introduce two proxies for memorization: Cumulative Sample Loss (CSL) and Cumulative Sample Gradient (CSG).
CSL represents the accumulated loss of a sample throughout training, while CSG is the gradient with respect to the input, aggregated over the training process. CSL and CSG exhibit remarkable similarity to stability-based memorization, as evidenced by considerably high cosine similarity scores. We delve into the theory behind these results, demonstrating that CSL and CSG represent the bounds for stability-based memorization and learning time. Additionally, we extend this framework to  include sample loss curvature and connect the three orders, namely, 
sample loss, sample loss gradient, and sample loss curvature, to learning time and memorization. 
The proposed proxy, CSL, is four orders of magnitude less computationally expensive than the stability-based method and can be obtained with zero additional overhead during training. We demonstrate the practical utility of the proposed proxies in identifying mislabeled samples and detecting duplicates where our metric achieves state-of-the-art performance. Thus, this paper provides a new tool for analyzing data as it scales in size, making it an important resource in practical applications.",2025,0.443181657024852,0.4424504074355629,0.2666666666666666,0.25,604adb2c-7ad1-41c8-9331-6795d1d191a3,0,"[0.25, 0.25, 0.25, 0.875]","[0.95, 0.95, 0.8, 0.9]",Cumulative Sample Loss,0.3974890490199287
543478,Fira: Can We Achieve Full-rank Training of LLMs Under Low-rank Constraint?,"Low-rank training has emerged as a promising approach for reducing memory usage in training Large Language Models (LLMs). Previous methods either rely on decomposing weight matrices (e.g., LoRA), or seek to decompose gradient matrices (e.g., GaLore) to ensure reduced memory consumption. However, both of them constrain the training in a low-rank subspace, thus inevitably leading to sub-optimal performance. This raises a question: whether it is possible to consistently preserve the low-rank constraint for memory efficiency, while achieving full-rank training (i.e., training with full-rank gradients of full-rank weights) to avoid inferior outcomes?
In this paper, we propose a new plug-and-play training framework for LLMs called Fira, as the first attempt to achieve this goal. First, we observe an interesting phenomenon during LLM training: the scaling impact of adaptive optimizers (e.g., Adam) on the gradient norm remains similar from low-rank to full-rank training. Based on this observation, we propose a norm-based scaling method, which utilizes the scaling impact of low-rank optimizers as substitutes for that of original full-rank optimizers to enable full-rank training. In this way, we can preserve the low-rank constraint in the optimizer while achieving full-rank training for better performance. Moreover, we find that there are sudden gradient rises during the optimization process, potentially causing loss spikes. To address this, we further put forward a norm-growth limiter to smooth the gradient via regulating the relative increase of gradient norms.
Extensive experiments on the pre-training and fine-tuning of LLMs show that Fira outperforms both LoRA and GaLore, achieving performance that is comparable to or even better than full-rank training. For instance, our Fira can reduce the memory usage of optimizer states by 61.1\%, while achieving improved performance for pre-training on the LLaMA 1B architecture. Notably, for pre-training on the LLaMA 7B architecture, our method uses an $8\times$ smaller rank than GaLore, yet outperforms it by a large margin.",2025,0.6477270371901683,0.645259872225768,0.6666666666666666,0.625,66705006-9d17-40a9-b693-a74387d72f97,0,"[0.5, 0.625, 0.625, 0.625]","[0.95, 0.95, 0.95, 0.8]",Fira,0.5892887491502382
543479,Demystifying amortized causal discovery with transformers,"Supervised learning for causal discovery from observational data often achieves competitive performance despite seemingly avoiding the explicit assumptions that traditional methods require for identifiability. In this work, we analyze CSIvA (Ke et al., 2023b) on bivariate causal models, a transformer architecture for amortized inference promising to train on synthetic data and transfer to real ones. First, we bridge the gap with identifiability theory, showing that the training distribution implicitly defines a prior on the causal model of the test observations: consistent with classical approaches, good performance is achieved when we have a good prior on the test data, and the underlying model is identifiable. Second, we find that CSIvA can not generalize to classes of causal models unseen during training: to overcome this limitation, we show that learning on datasets generated from different types of causal models, unambiguously identifiable in isolation, improves the test generalization. We analyze this empirical evidence with theory, illustrating that the ambiguous cases resulting from the mixture of identifiable causal models are unlikely to occur. Overall, we find that amortized causal discovery still adheres to identifiability theory, violating the previous hypothesis from Lopez-Paz et al. (2015) that supervised learning methods could overcome its restrictions.",2025,0.5454543471075102,0.540875338440527,0.6,0.625,be0f75c1-f5f9-401b-b14a-e02adbb424c3,0,"[0.25, 0.5, 0.625, 0.625]","[0.95, 0.95, 0.9, 0.9]",Amortized causal discovery,0.4839378238341968
543480,DCT-CryptoNets: Scaling Private Inference in the Frequency Domain,"The convergence of fully homomorphic encryption (FHE) and machine learning offers unprecedented opportunities for private inference of sensitive data. FHE enables computation directly on encrypted data, safeguarding the entire machine learning pipeline, including data and model confidentiality. However, existing FHE-based implementations for deep neural networks face significant challenges in computational cost, latency, and scalability, limiting their practical deployment. This paper introduces DCT-CryptoNets, a novel approach that operates directly in the frequency-domain to reduce the burden of computationally expensive non-linear activations and homomorphic bootstrap operations during private inference. It does so by utilizing the discrete cosine transform (DCT), commonly employed in JPEG encoding, which has inherent compatibility with remote computing services where images are generally stored and transmitted in this encoded format. DCT-CryptoNets demonstrates a substantial latency reductions of up to 5.3$\times$ compared to prior work on benchmark image classification tasks. Notably, it demonstrates inference on the ImageNet dataset within 2.5 hours (down from 12.5 hours on equivalent 96-thread compute resources). Furthermore, by *learning* perceptually salient low-frequency information DCT-CryptoNets improves the reliability of encrypted predictions compared to RGB-based networks by reducing error accumulating homomorphic bootstrap operations. DCT-CryptoNets also demonstrates superior scalability to RGB-based networks by further reducing computational cost as image size increases. This study demonstrates a promising avenue for achieving efficient and practical private inference of deep learning models on high resolution images seen in real-world applications.",2025,0.727272826446245,0.7268831233894805,0.6666666666666666,0.5,e1ac9491-4e02-45c9-bd26-eec8c93f738a,1,"[0.5, 0.625, 0.875]","[0.95, 0.9, 0.95]",DCT-CryptoNets,0.6700894335872806
543486,PREDICTING 3D STRUCTURE BY LATENT POSTERIOR SAMPLING,"The remarkable achievements of both generative models of 2D images and neural field representations for 3D scenes present a compelling opportunity to integrate the strengths of both approaches.
In this work, we propose a methodology that combines a NeRF-based representation of 3D scenes with probabilistic modeling and reasoning using diffusion models.
We view 3D reconstruction as a perception problem with inherent uncertainty that can thereby benefit from probabilistic inference methods.  
The core idea is to represent the 3D scene as a stochastic latent variable for which we can learn a prior and use it to perform posterior inference given a set of observations. 
We formulate posterior sampling using the score-based inference method of diffusion models in conjunction with a likelihood term computed from a reconstruction model that includes volumetric rendering. 
We train the model using a two-stage process: first we train the reconstruction model while auto-decoding the latent representations for a dataset of 3D scenes, and then we train the prior over the latents using a diffusion model.
By using the model to generate samples from the posterior we demonstrate that various 3D reconstruction tasks can be performed, differing by the type of observation used as inputs. 
We showcase reconstruction from single-view, multi-view, noisy images, sparse pixels, and sparse depth data. 
These observations vary in the amount of information they provide for the scene and we show that our method can model the varying levels of inherent uncertainty associated with each task.
Our experiments illustrate that this approach yields a comprehensive method capable of accurately predicting 3D structure from diverse types of observations.",2025,0.443181657024852,0.4370767451333178,0.4,0.25,a5511f57-93a8-44c3-b20d-9052d69e81cc,0,"[0.25, 0.25, 0.5, 0.625]","[0.95, 1.0, 0.95, 0.9]",Latent Posterior Sampling,0.3810388125980297
543509,A Bounding Box is Worth One Token: Interleaving Layout and Text in a Large Language Model for Document Understanding,"Recently, many studies have demonstrated that exclusively incorporating OCR-derived text and spatial layouts with large language models (LLMs) can be highly effective for document understanding tasks. However, existing methods that integrate spatial layouts with text have limitations, such as producing overly long text sequences or failing to fully leverage the autoregressive traits of LLMs. In this work, we introduce Interleaving Layout and Text in a Large Language Model (LayTextLLM)} for document understanding. In particular, LayTextLLM projects each bounding box to a single embedding and interleaves it with text, efficiently avoiding long sequence issues while leveraging autoregressive traits of LLMs. LayTextLLM not only streamlines the interaction of layout and textual data but also shows enhanced performance in Key Information Extraction (KIE) and Visual Question Answering (VQA). Comprehensive benchmark evaluations reveal significant improvements, with a 27.2% increase on KIE tasks and 12.0% on VQA tasks compared to previous state-of-the-art document understanding MLLMs, as well as a 15.1% improvement over other SOTA OCR-based LLMs on KIE tasks.",2025,0.5454543471075102,0.5391806853107554,0.6,0.625,ff572331-ffa6-4111-baa5-97f422bdecb8,0,"[0.25, 0.5, 0.625, 0.625]","[1.0, 0.95, 0.9, 0.95]",LayTextLLM,0.4721053517934694
543529,CarbonSense: A Multimodal Dataset and Baseline for Carbon Flux Modelling,"Terrestrial carbon fluxes provide vital information about our biosphere's health and its capacity to absorb anthropogenic CO$_2$
 emissions. The importance of predicting carbon fluxes has led to the emerging field of data-driven carbon flux modelling (DDCFM), which uses statistical techniques to predict carbon fluxes from biophysical data. However, the field lacks a standardized dataset to promote comparisons between models. To address this gap, we present CarbonSense, the first machine learning-ready dataset for DDCFM. CarbonSense integrates measured carbon fluxes, meteorological predictors, and satellite imagery from 385 locations across the globe, offering comprehensive coverage and facilitating robust model training. Additionally, we provide a baseline model using a current state-of-the-art DDCFM approach and a novel transformer based model. Our experiments illustrate the potential gains that multimodal deep learning techniques can bring to this domain. By providing these resources, we aim to lower the barrier to entry for other deep learning researchers to develop new models and drive new advances in carbon flux modelling.",2025,0.6818179338843877,0.6806929345162506,0.6666666666666666,0.625,7c35abd0-0e71-45ba-b075-9e45577a6f40,1,"[0.625, 0.625, 0.625]","[1.0, 0.9, 1.0]",CarbonSense,0.625
543530,LoRA-XS: Low-Rank Adaptation with Extremely Small Number of Parameters,"The rapid expansion of large language models (LLMs) has underscored the need for parameter-efficient fine-tuning methods, with LoRA (Low-Rank Adaptation) emerging as a popular solution. Although LoRA reduces the number of trainable parameters, serving multiple (task or user-specific) LoRA modules on top of a base model still creates significant storage challenges. To address this, using theoretical derivation, we introduce LoRA-XS (Low-Rank Adaptation with eXtremely Small number of parameters), a novel low-rank adaptation method that considerably reduces the trainable parameters while showing superior or competitive performance. LoRA-XS achieves this by inserting a small, trainable $r \times r$ weight matrix between frozen low-rank matrices, which are constructed by Singular Value Decomposition (SVD) of the original weight matrix. This lightweight matrix enables fine-tuning with drastically reduced storage requirements, making it feasible to deploy millions of personalized models while minimizing memory overhead. For instance, LoRA-XS achieves a remarkable reduction of trainable parameters by over 100x in 7B models compared to LoRA. Our evaluations across various benchmarks (including GLUE, GSM8K, MATH, and eight commonsense reasoning datasets) demonstrate that LoRA-XS performs competitively or better than LoRA and other recent methods like VeRA while being significantly more parameter efficient. We also provide an extensive ablation study on the importance of singular vectors in transformer weights, shedding light on the underlying mechanisms driving LoRA-XS’s enhanced efficiency. These findings suggest that LoRA-XS is not only a storage-efficient alternative, but also a powerful tool for scaling and personalizing LLMs at unprecedented scales.",2025,0.4545456528924899,0.4497430792587961,0.5333333333333333,0.5,75d20a88-0652-4373-b934-9ca1b5631802,0,"[0.25, 0.25, 0.5, 0.5, 0.5, 0.5]","[0.95, 1.0, 0.9, 0.9, 0.9, 0.95]",LoRA-XS,0.3964397528369419
543532,Rethinking the Bias of Foundation Model under Long-tailed Distribution,"Long-tailed learning has garnered increasing attention due to its practical significance. Among the various approaches, the fine-tuning paradigm has gained considerable interest with the advent of foundation models. However, most existing methods primarily focus on leveraging knowledge from these models, overlooking the inherent biases introduced by the imbalanced training data they rely on. In this paper, we examine how such imbalances affect long-tailed downstream tasks. Specifically, we refer to the biases in foundation models and downstream tasks as parameter imbalance and data imbalance, respectively. Through fine-tuning, we observe that parameter imbalance plays a more critical role, while data imbalance can be mitigated using existing re-balancing strategies. Moreover, we find that parameter imbalance cannot be effectively addressed by current re-balancing techniques, such as adjusting the logits, during training, unlike data imbalance. To tackle both imbalances simultaneously, we constitute a causal structure graph and view the partial semantic factor as the confounder, which brings spurious correlations between input samples and labels. To resolve the negative effects of this, we propose a novel backdoor adjustment method that learns the true causal effect between input samples and labels, rather than merely fitting the correlations in the data. Experimental results validate the effectiveness of our method.",2025,0.7159088305786071,0.7132899577643648,0.6666666666666666,0.625,68631335-102b-4ac5-8b4b-b6fb736bcf3b,0,"[0.5, 0.625, 0.625, 0.875]","[0.95, 0.9, 0.8, 0.9]",Parameter Imbalance,0.6473549265325894
543534,Policy Transfer via Latent Graph Planning,"We introduce a transfer learning framework for deep reinforcement learning that integrates graph-based planning with self-supervised representation learning to efficiently transfer knowledge across tasks. While standard reinforcement learning aims to learn policies capable of solving long-horizon tasks, the resulting policies often fail to generalize to novel tasks and environments. Our approach addresses this limitation by decomposing long-horizon tasks into sequences of transferable short-horizon tasks modeled by goal-conditioned policies. We utilize a planning graph to generate fine-grained sub-goals that guide these short-horizon policies to solve novel long-horizon tasks. Experimental results show that our method improves sample efficiency and demonstrates an improved ability to solve sparse-reward and long-horizon tasks compared to baseline methods in challenging single-agent and multi-agent scenarios. In particular, compared to the state-of-the-art, our method achieves the same or better expected policy reward while requiring fewer training samples when learning novel tasks.",2025,0.3409089669421938,0.3403464672581253,0.2666666666666666,0.25,b657595e-6e88-4456-a9f0-a661aedb76af,0,"[0.25, 0.25, 0.25, 0.5]","[0.9, 0.95, 1.0, 0.95]",Latent Graph Planning,0.3107110261302932
543537,FAN: Fourier Analysis Networks,"Despite the remarkable success achieved by neural networks, particularly those represented by MLP and Transformer, we reveal that they exhibit potential flaws in the modeling and reasoning of periodicity, i.e., they exhibit satisfactory performance within the domain of training period, but struggle to generalize to out of the domain (OOD). The inherent cause lies in the way that they tend to memorize the periodic data rather than genuinely understand the underlying principles of periodicity. In fact, periodicity is essential to various forms of reasoning and generalization, underpinning predictability across natural and engineered systems through recurring patterns in observations. In this paper, we propose FAN, a novel network architecture based on Fourier Analysis, which empowers the ability to efficiently model and reason about periodic phenomena, meanwhile maintaining general-purpose ability. By introducing Fourier Series, periodicity is naturally integrated into the structure and computational processes of FAN. On this basis, FAN is defined following two core principles: 1) its periodicity modeling capability scales with network depth and 2) the periodicity modeling available throughout the network, thus achieving more effective expression and prediction of periodic patterns. FAN can seamlessly replace MLP in various model architectures with fewer parameters and FLOPs, becoming a promising substitute to traditional MLP. Through extensive experiments, we demonstrate the superiority of FAN in periodicity modeling tasks, and the effectiveness and generalizability of FAN across a range of real-world tasks, including symbolic formula representation, time series forecasting, language modeling, and image recognition.",2025,0.443181657024852,0.4378511014156235,0.4,0.25,332e2716-84c3-409c-9f0f-f64d1822e863,0,"[0.25, 0.25, 0.5, 0.625]","[1.0, 0.9, 0.9, 0.9]",Fourier Analysis,0.3817898110661268
543538,ADIFF: Explaining audio difference using natural language,"Understanding and explaining differences between audio recordings is crucial for fields like audio forensics, quality assessment, and audio generation. This involves identifying and describing audio events, acoustic scenes, signal characteristics, and their emotional impact on listeners. This paper stands out as the first work to comprehensively study the task of explaining audio differences and then propose benchmark, baselines for the task. First, we present two new datasets for audio difference explanation derived from the AudioCaps and Clotho audio captioning datasets. Using Large Language Models (LLMs), we generate three levels of difference explanations: (1) concise descriptions of audio events and objects, (2) brief sentences about audio events, acoustic scenes, and signal properties, and (3) comprehensive explanations that include semantics and listener emotions. For the baseline, we use prefix tuning where audio embeddings from two audio files are used to prompt a frozen language model. Our empirical analysis and ablation studies reveal that the naive baseline struggles to distinguish perceptually similar sounds and generate detailed tier 3 explanations. To address these limitations, we propose ADIFF, which introduces a cross-projection module, position captioning, and a three-step training process to enhance the model’s ability to produce detailed explanations. We evaluate our model using objective metrics and human evaluation and show our model enhancements lead to significant improvements in performance over naive baseline and SoTA Audio-Language Model (ALM) Qwen Audio. Lastly, we conduct multiple ablation studies to study the effects of cross-projection, language model parameters, position captioning, third stage fine-tuning, and present our findings. Our benchmarks, findings, and strong baseline pave the way for nuanced and human-like explanations of audio differences.",2025,0.886363314049704,0.8857851711316493,0.9333333333333332,0.875,1be2945a-ede1-4b79-9b1c-58242cc7f5ac,1,"[0.625, 0.875, 0.875, 0.875]","[0.95, 0.95, 1.0, 0.95]",ADIFF,0.8174705449142813
543545,Enhancing Graph Of Thought: Enhancing Prompts with LLM Rationales and Dynamic Temperature Control,"We introduce Enhancing Graph of Thoughts (EGoT), a method designed to enhance the performance of large language models (LLMs) on complex reasoning tasks. EGoT automates the process of generating accurate responses using given data and a base prompt. The process consists of several steps: It obtains an initial response from the answering node using the base prompt. Evaluation node evaluates the response and generates reasoning for it, utilizing the score's probabilities to enhance evaluation accuracy. The reasoning from both the answering node and the evaluation node is aggregated to identify the problem in the response. This aggregated reasoning is incorporated into the base prompt to obtain an enhanced response. These steps are organized in a graph architecture, where the final leaf nodes are merged to produce a final response. As the graph descends, the temperature is lowered using Cosine Annealing and scoring, to explore diverse responses with earlier nodes and to focus on precise responses with later nodes. The minimum temperature in Cosine Annealing is adjusted based on scoring, ensuring that nodes with low scores continue to explore diverse responses, while those with high scores confirm accurate responses. In sorting 256 elements using GPT-4o mini, EGoT performs 88.31\% accuracy, while GoT (Graph of Thoughts) achieves 84.37\% accuracy. In the frozen lake problem using GPT-4o, EGoT averages 0.55 jumps or falls into the hole, while ToT (Tree of Thoughts) averages 0.89.",2025,0.7159088305786071,0.7166182739369754,0.6666666666666666,0.625,8720dbc0-6bb7-4964-980d-b911ce5b1fcb,1,"[0.5, 0.625, 0.625, 0.875]","[0.95, 0.8, 0.9, 0.95]",EGoT,0.6631963316011183
543546,HiBug2: Efficient and Interpretable Error Slice Discovery for Comprehensive Model Debugging,"Despite the significant success of deep learning models in computer vision, they often exhibit systematic failures on specific data subsets, known as error slices. Identifying and mitigating these error slices is crucial to enhancing model robustness and reliability in real-world scenarios. In this paper, we introduce HiBug2, an automated framework for error slice discovery and model repair. HiBug2 first generates task-specific visual attributes to highlight instances prone to errors through an interpretable and structured process. It then employs an efficient slice enumeration algorithm to systematically identify error slices, overcoming the combinatorial challenges that arise during slice exploration. Additionally, HiBug2 extends its capabilities by predicting error slices beyond the validation set, addressing a key limitation of prior approaches. Extensive experiments across multiple domains — including image classification, pose estimation, and object detection — show that HiBug2 not only improves the coherence and precision of identified error slices but also significantly enhances the model repair capabilities.",2025,0.6818179338843877,0.6806929345162506,0.6666666666666666,0.625,6f8dda96-64cb-42ab-a940-6d2049428b7b,1,"[0.625, 0.625, 0.625]","[0.95, 0.9, 0.9]",Error Slice Discovery,0.6250000000000002
543561,Collapsed Language Models Promote Fairness,"To mitigate societal biases implicitly encoded in recent successful pretrained language models, a diverse array of approaches have been proposed to encourage model fairness, focusing on prompting, data augmentation, regularized fine-tuning, and more. Despite the development, it is nontrivial to reach a principled understanding of fairness and an effective algorithm that can consistently debias language models. In this work, by rigorous evaluations of Neural Collapse -- a learning phenomenon happen in last-layer representations and classifiers in deep networks -- on fairness-related words, we find that debiased language models exhibit collapsed alignment between token representations and word embeddings. More importantly, this observation inspires us to design a principled fine-tuning method that can effectively improve fairness in a wide range of debiasing methods, while still preserving the performance of language models on standard natural language understanding tasks. We attach our code at https://github.com/Xujxyang/Fairness-NC-main",2025,0.7840906239670459,0.7795328159340958,0.8,0.875,2d3998d2-ce23-4eba-880b-8a0be560a69e,1,"[0.5, 0.625, 0.875, 0.875]","[0.95, 0.9, 0.9, 0.9]",Neural Collapse,0.703730620155039
543570,Dobi-SVD: Differentiable SVD for LLM Compression and Some New Perspectives,"Large language models (LLMs) have sparked a new wave of AI applications; however, their substantial computational costs and memory demands pose significant challenges to democratizing access to LLMs for a broader audience. Singular Value Decomposition (SVD), a technique studied for decades, offers a hardware-independent and flexibly tunable solution for LLM compression. In this paper, we present new directions using SVD: we first theoretically analyze the optimality of truncating weights and truncating activations, then we further identify three key issues on SVD-based LLM compression, including (1) How can we determine the optimal truncation position for each weight matrix in LLMs? (2) How can we efficiently update the weight matrices based on truncation position? (3) How can we address the inherent ""injection"" nature that results in the information loss of the SVD? We propose an effective approach, **Dobi-SVD**, to tackle the three issues. 
First, we propose a **differentiable** truncation-value learning mechanism, along with gradient-robust backpropagation, enabling the model to adaptively find the optimal truncation positions. Next, we utilize the Eckart-Young-Mirsky theorem to derive a theoretically **optimal** weight update formula through rigorous mathematical analysis. Lastly, by observing and leveraging the quantization-friendly nature of matrices after SVD decomposition, we reconstruct a mapping between truncation positions and memory requirements, establishing a **bijection** from truncation positions to memory. 
Experimental results show that with a 40\% parameter-compression rate, our method achieves a perplexity of 9.07 on the Wikitext2 dataset with the compressed LLama-7B model, a 78.7\% improvement over the state-of-the-art SVD for LLM compression method. 
We emphasize that Dobi-SVD is the first to achieve such a high-ratio LLM compression with minimal performance drop.  We also extend our Dobi-SVD to VLM compression, achieving a 20\% increase in throughput with minimal performance degradation. We hope that the inference speedup—up to 12.4x on 12GB NVIDIA Titan Xp GPUs and 3x on 80GB A100 GPUs for LLMs, and 1.2x on 80GB A100 GPUs for VLMs—will bring significant benefits to the broader community such as robotics.",2025,0.7090906512397632,0.7073292658753928,0.6666666666666666,0.625,ec4c2a29-b34a-4170-ad1f-bd4806964b07,1,"[0.5, 0.625, 0.625, 0.625, 0.875]","[1.0, 0.95, 0.8, 0.9, 0.95]",Dobi-SVD,0.6427126607134642
543576,Stealix: Model Stealing via Prompt Evolution,"Model stealing poses a significant security risk in machine learning by enabling attackers to replicate a black-box model without access to its training data, thus jeopardizing intellectual property and exposing sensitive information.
Recent methods that use pre-trained diffusion models for data synthesis improve efficiency and performance but rely heavily on manually crafted prompts, limiting automation and scalability, especially for attackers with little expertise.
To assess the risks posed by open-source pre-trained models, we propose a more realistic threat model that eliminates the need for prompt design skills or knowledge of class names.
In this context, we introduce Stealix, the first approach to perform model stealing without predefined prompts. Stealix uses two open-source pre-trained models to infer the victim model’s data distribution, and iteratively refines prompts through a genetic algorithm based on a proxy metric, progressively improving the precision and diversity of synthetic images.
Our experimental results demonstrate that Stealix significantly outperforms other methods, even those with access to class names or fine-grained prompts, while operating under the same query budget. These findings highlight the scalability of our approach and suggest that the risks posed by pre-trained generative models in model stealing may be greater than previously recognized.",2025,0.613636140495949,0.6126236410646255,0.6,0.5,ded41550-926d-436a-bfc8-f36e0e01df60,0,"[0.5, 0.5, 0.625, 0.625]","[0.9, 0.9, 0.9, 0.9]",Stealix,0.5625
543595,From Complex to Atomic: Enhancing Augmented Generation via Knowledge-Aware Dual Rewriting and Reasoning,"Recent advancements in Retrieval-Augmented Generation (RAG) systems have significantly enhanced the capabilities of large language models (LLMs) by incorporating external knowledge retrieval. However, the sole reliance on retrieval is often inadequate for mining deep, domain-specific knowledge and for performing logical reasoning from specialized datasets. To tackle these challenges, we present an approach, which is designed to extract, comprehend, and utilize domain knowledge while constructing a coherent rationale. At the heart of our approach lie four pivotal components: a knowledge atomizer that extracts atomic questions from raw data, a query proposer that generates subsequent questions to facilitate the original inquiry, an atomic retriever that locates knowledge based on atomic knowledge alignments, and an atomic selector that determines which follow-up questions to pose guided by the retrieved information. Through this approach, we implement a knowledge-aware task decomposition strategy that adeptly extracts multifaceted knowledge from segmented data and iteratively builds the rationale in alignment with the initial query and the acquired knowledge. We conduct comprehensive experiments to demonstrate the efficacy of our approach across various benchmarks, particularly those requiring multihop reasoning steps. The results indicate a significant enhancement in performance, up to 12.6\% over the second-best method, underscoring the potential of the approach in complex, knowledge-intensive applications.",2025,0.5454543471075102,0.5427148430267638,0.6,0.625,67f8f51e-fe55-4c42-a732-e8092b8a7a7e,0,"[0.25, 0.5, 0.625, 0.625]","[0.95, 0.9, 0.9, 0.95]",Knowledge Atomizer,0.4919689119170986
543602,Value Residual Learning For Alleviating  Attention Concentration In Transformers,"Transformers can capture long-range dependencies using self-attention, allowing tokens to attend to all others directly. However, stacking multiple attention layers leads to attention concentration. One natural way to address this issue is to use cross-layer attention, allowing information from earlier layers to be directly accessible to later layers. However, this approach is computationally expensive. To address this problem, we propose Transformer with residual value (ResFormer) which approximates cross-layer attention through adding a residual connection from the values of the the first layer to all subsequent layers. Based on this method, one variant is the Transformer with single layer value (SVFormer), where all layers share the same value embedding from first layer, reducing the $KV$ cache by nearly 50\%.  Comprehensive empirical evidence demonstrates that ResFormer mitigates attention concentration problem in deeper layers and enhances representation across most layers, outperforming the vanilla Transformer, DenseFormer, and NeuTRENO in training error as well as downstream tasks. SVFormer trains significantly faster than the vanilla Transformer and performs better than other methods like GQA and CLA, with performance influenced by sequence length and cumulative learning rate.",2025,0.3409089669421938,0.3441278526479499,0.2666666666666666,0.25,c7abe93b-a296-4242-ae72-06ddc2a8c7f8,0,"[0.25, 0.25, 0.25, 0.5]","[0.8, 0.9, 0.95, 0.95]",ResFormer,0.3263926632022365
543607,Integration of neural solver and problem-specific solver through bilevel approach: a case study of min-max capacitated vehicle routing problem,"In real-world operations with combinatorial structures like vehicle routing problems, similar optimization problems have to be solved repeatedly with slight parameter variations. 
A key challenge in such scenarios is achieving both high solution quality and fast computation time, while traditional methods like heuristics or branch-and-bound struggle to achieve both simultaneously.
In contrast, problem-specific solvers can effectively balance solution quality and computation speed for specific problems. 
However, since real-world problems have more complex structures, they can handle only subproblems.
To enhance the applicability of the problem-specific solvers, we propose a framework that integrates a problem-specific solver and a neural solver. 
Our framework decomposes the optimization problem into subproblems so that some of which can be solved by problem-specific solvers, such as the traveling salesperson problem. 
For the remaining portions of the problem, we utilize the similarities of the problems and design a neural solver.
By integrating two solvers, we can utilize the strengths of the problem-specific solver in balancing solution accuracy and computation speed, as well as the neural solver’s ability to infer a solution from the similarity of optimization problems.
Based on the case study with the min-max capacitated vehicle routing problem, we demonstrate that it outperforms the state-of-the-art solver regarding both high solution quality and short computation time.",2025,0.3409089669421938,0.3439285457567236,0.2666666666666666,0.25,974ae23b-bf42-40ed-9422-df0a4b3a62c8,0,"[0.25, 0.25, 0.25, 0.5]","[0.9, 0.95, 0.95, 1.0]",Bilevel Optimization,0.3316927567609226
543613,Convergent Privacy Loss of Noisy-SGD without Convexity and Smoothness,"We study the Differential Privacy (DP) guarantee of hidden-state Noisy-SGD algorithms over a bounded domain. Standard privacy analysis for Noisy-SGD assumes all internal states are revealed, which leads to a divergent R\'enyi DP bound with respect to the number of iterations. Ye & Shokri (2022) and Altschuler & Talwar (2022) proved convergent bounds for smooth (strongly) convex losses, and raise open questions about whether these assumptions can be relaxed. We provide positive answers by proving convergent R\'enyi DP bound for non-convex non-smooth losses, where we show that requiring losses to have H\""older continuous gradient is sufficient. We also provide a strictly better privacy bound compared to state-of-the-art results for smooth strongly convex losses. Our analysis relies on the improvement of shifted divergence analysis in multiple aspects, including forward Wasserstein distance tracking, identifying the optimal shifts allocation, and the  H\""older reduction lemma. Our results further elucidate the benefit of hidden-state analysis for DP and its applicability.",2025,0.7840906239670459,0.7851275673014718,0.8,0.875,b9002770-dbb0-4f82-8bfe-2c4150636b3c,1,"[0.5, 0.625, 0.875, 0.875]","[0.9, 0.9, 0.9, 0.95]",Convergent R\'enyi DP,0.7294781284606864
543615,MPHIL: Multi-Prototype Hyperspherical Invariant Learning for Graph Out-of-Distribution Generalization,"Out-of-distribution (OOD) generalization has emerged as a critical challenge in graph learning, as real-world graph data often exhibit diverse and shifting environments that traditional models fail to generalize across. A promising solution to address this issue is graph invariant learning (GIL), which aims to learn invariant representations by disentangling label-correlated invariant subgraphs from environment-specific subgraphs. However, existing GIL methods face two major challenges: (1) the difficulty of capturing and modeling diverse environments in graph data, and (2) the semantic cliff, where invariant subgraphs from different classes are difficult to distinguish, leading to poor class separability and increased misclassifications. To tackle these challenges, we propose a novel method termed Multi-Prototype Hyperspherical Invariant Learning (MPHIL), which introduces two key innovations: (1) invariant learning in hyperspherical space, enabling robust invariant feature extraction and prototypical learning in a highly discriminative space, and (2) class prototypes as intermediate variables, which eliminate the need for explicit environment modeling in GIL and mitigate the semantic cliff issue through multi-prototype-based classification. Derived from the theoretical framework of GIL, we introduce two novel objective functions: the invariant prototype matching loss to ensure samples are matched to the correct class prototypes, and the prototype separation loss to increase the distinction between prototypes of different classes in the hyperspherical space. Extensive experiments on 11 OOD generalization benchmark datasets demonstrate that MPHIL achieves state-of-the-art performance, significantly outperforming existing methods across graph data from various domains and with different distribution shifts. The source code of MPHIL is available at https://anonymous.4open.science/r/MPHIL-23C0/.",2025,0.5795452438017296,0.5781228558172563,0.5333333333333333,0.5,053da346-8735-489d-8f02-388a221a8203,0,"[0.5, 0.5, 0.5, 0.625]","[0.95, 0.9, 0.9, 0.9]",MPHIL,0.5291043743078626
543620,Cracking the Collective Mind: Adversarial Manipulation in Multi-Agent Systems,"Large Language Models (LLMs) have demonstrated significant capabilities across various domains such as healthcare, weather forecasting, finance, and law. These works have showcased the powerful abilities of individual LLMs. Recently, numerous studies have shown that coordinated multi-agent systems exhibit enhanced decision-making and reasoning capabilities through collaboration. However, since individual LLMs are susceptible to various adversarial attacks, a key vulnerability arises: Can an attacker manipulate the collective decision of such systems by accessing a single agent? To address this issue, we formulate it as a game with incomplete information, where agents lack full knowledge of adversarial strategies. We then propose a framework, M-Spoiler, which simulates a stubborn adversary in multi-agent debates during the training phase to tackle this problem. Through extensive experiments across various tasks, our findings confirm the risk of manipulation in multi-agent systems and demonstrate the effectiveness of our attack strategies. Additionally, we explore several defense mechanisms, revealing that our proposed attack method remains more potent than existing baselines, underscoring the need for further research on defensive strategies.",2025,0.3409089669421938,0.3403464672581253,0.2666666666666666,0.25,eeee5823-10a3-4467-90d5-1790a756d8a9,0,"[0.25, 0.25, 0.25, 0.5]","[0.95, 0.95, 0.95, 0.95]",M-Spoiler,0.3125
543643,Slot-Guided Adaptation of Pre-trained Diffusion Models for Object-Centric Learning and Compositional Generation,"We present SlotAdapt, an object-centric learning method that combines slot attention with pretrained diffusion models by introducing adapters for slot-based conditioning. Our method preserves the generative power of pretrained diffusion models, while avoiding their text-centric conditioning bias. We also incorporate an additional guidance loss into our architecture to align cross-attention from adapter layers with slot attention. This enhances the alignment of our model with the objects in the input image without using external supervision. Experimental results show that our method outperforms state-of-the-art techniques in object discovery and image generation tasks across multiple datasets, including those with real images. Furthermore, we demonstrate through experiments that our method performs remarkably well on complex real-world images for compositional generation, in contrast to other slot-based generative methods in the literature. The project page can be found at https://kaanakan.github.io/SlotAdapt/",2025,0.7159088305786071,0.7151817395679724,0.6666666666666666,0.625,9353030a-20cc-482c-ab6e-c7c09e2624c0,1,"[0.5, 0.625, 0.625, 0.875]","[0.95, 0.9, 0.95, 0.95]",SlotAdapt,0.6581365628042842
543672,Towards Graph Foundation Models: Learning Generalities Across Graphs via Task-trees,"Foundation models aim to create general, cross-task, and cross-domain machine learning models by pretraining on large-scale datasets to capture shared patterns or concepts (generalities), such as contours, colors, textures, and edges in images, or tokens, words, and sentences in text. However, discovering generalities across graphs remains challenging, which has hindered the development of graph foundation models. To tackle this challenge, in this paper, we propose a novel approach to learn generalities across graphs via task-trees. Specifically, we first define the basic learning instances in graphs as task-trees and assume that the generalities shared across graphs are, at least partially, preserved in the task-trees of the given graphs. To validate the assumption, we first perform a theoretical analysis of task-trees in terms of stability, transferability, and generalization. We find that if a graph neural network (GNN) model is pretrained on diverse task-trees through a reconstruction task, it can learn sufficient transferable knowledge for downstream tasks using an appropriate set of fine-tuning samples. To empirically validate the assumption, we further instantiate the theorems by developing a cross-task, cross-domain graph foundation model named Graph generality Identifier on task-Trees (GIT). The extensive experiments over 30 graphs from five domains demonstrate the effectiveness of GIT in fine-tuning, in-context learning, and zero-shot learning scenarios. Particularly, the general GIT model pretrained on large-scale datasets can be quickly adapted to specific domains, matching or even surpassing expert models designed for those domains.",2025,0.5795452438017296,0.5743926585361072,0.6666666666666666,0.625,d8678682-fe59-4dc6-b571-ee33d1d7e98d,0,"[0.25, 0.625, 0.625, 0.625]","[0.95, 0.95, 0.8, 0.95]",Task-trees,0.5178662474507137
543695,"MindDETR: Beyond Semantics, Exploring Positional Cues from Brain Activity","Decoding visual stimuli from brain recordings offers a unique opportunity to understand how the brain represents the world and seeks to interpret the connection between computer vision models and our visual system. Recent efforts mainly adopt diffusion models to reconstruct images from brain signals. However, while these methods generally capture correct semantic information, they often struggle with precise object localization. Additionally, the commonly used proxy task, image reconstruction from brain signals, mainly measures semantic consistency, to some extent neglecting positional information of the decoded signals. In this work, to encourage more accurate brain signal decoding, we propose to use object detection as the proxy task, aiming at decoding both the semantic and positional cues from brain recordings. Based on this task, we propose MindDETR, a brain recording-based object detection model with the DETR pipeline. After aligning feature representations with a pretrained image-based DETR model, our model demonstrates that accurately brain decoding at both semantic and positional levels is feasible, and our detection-based approach achieves significantly superior results than existing reconstruction-based approaches. This result suggests the effectiveness of applying object detection as a proxy task for brain signal decoding.  Our code will be publicly available.",2025,0.4999994545456529,0.5117797996475645,0.5333333333333333,0.25,612780b2-60ca-4de7-bb3c-b1ed2b946828,0,"[0.25, 0.5, 0.625]","[0.8, 0.95, 0.95]",MindDETR,0.4999702593385676
543712,Sanitizing LLMs: Retrospective Learning for Self-Correction of Inconsistent Samples via User Preferences,"With the advent of large language models (LLMs), using LLMs in conjunction with prompt-based tasks has demonstrated the ability to reduce the high cost and inefficiency of human annotations. Nonetheless, in unsupervised new downstream tasks that require user preferences to align data annotations with expectations, existing evaluation methods for prompt-based tasks become ineffective, especially when ground truth annotations are insufficient or missing. To fill this gap, we propose the novel Consistent and Inconsistent (CAI) Ratio, inspired by our experimental observation that LLMs underperform when the number of inconsistent samples—those with inconsistent predictions across LLMs and the student model—exceeds the number of consistent samples. By estimating the CAI ratio and identifying consistent and inconsistent samples with our proposed CAI identification approach, we aim to minimize inconsistency and enhance the accuracy of LLM-generated annotations for unsupervised data. To achieve this, we introduce Retrospective Learning (RetroL) with user preference, a data-centric approach that collaborates with the student model and LLMs, using a small number of human annotations as user preferences to resolve inconsistencies in the identified samples. Applied to eight domain-specific NLP datasets, our Retrospective Learning approach, leveraging CAI identification, significantly improved the accuracy of LLM-generated responses, with the CAI ratio increasing as the accuracy improved.",2025,0.5909092396693675,0.5916789917467343,0.5333333333333333,0.5,97d6b389-c8ba-4296-87a3-bf8f7f7b77f4,0,"[0.5, 0.5, 0.625]","[0.9, 0.8, 0.9]",CAI Ratio,0.5472084924736015
543729,Group rank for encrypted data,"Recently, there has been an increasing demand for privacy-preserving techniques in numerous machine learning algorithms, elevating it to a critical concern. One promising solution involves the application of homomorphic encryption (HE). This study focuses on obtaining statistics based on the ranks of HE-encrypted data as a vital tool for robust data analysis. However, computing ranks in HE comes with significant computational costs due to the necessity of comparison operations, and there is currently no efficient method available.
To address this gap, we propose an approximate rank method that exploits pairwise comparisons of data to derive ranks for encrypted information. This method effectively measures the association between two-dimensional ranks. Specifically, by utilizing approximate ranks of two variables, we estimate Spearman rank correlation without relying on perfect sorting and introduce a technique to reduce the number of required comparisons.
Numerical experiments have been conducted to validate our approach, demonstrating that the disparity in values between rank correlation and approximate rank correlation is not substantial. Notably, the processing of one block comprising 32,768 ciphertexts took approximately one minute, exhibiting observed linear complexity dependent on the number of blocks.",2025,0.4363634776860081,0.431588726418074,0.5333333333333333,0.5,b919e0e9-77bc-42ec-97da-04f686e8d398,0,"[0.25, 0.25, 0.5, 0.5, 0.5]","[0.95, 1.0, 0.95, 0.9, 0.9]",Approximate Rank Method,0.3800851413118035
543732,Learning anti-classes with one-cold cross entropy loss,"While softmax cross entropy loss is the standard objective for supervised classification, it primarily focuses on the ground truth classes, ignoring the relationships between the non-target, complementary classes. This leaves valuable information unexploited during optimization. In this work, we set explicit non-zero target distributions for the complementary classes, in order to address this limitation. Specifically, for each class, we define an *anti-class*, which consists of everything that is not part of the target class—this includes all complementary classes as well as out-of-distribution samples, and in general any instance that does not belong to the true class. Various distributions can be used as a target for the anti-classes. For example, by setting a uniform one-cold encoded distribution over the complementary classes as a target for each anti-class, we encourage the model to equally distribute activations across all non-target classes. This approach promotes a symmetric geometric structure of classes in the final feature space, increases the degree of neural collapse during training, addresses the independence deficit problem of neural networks and improves generalization. Our extensive evaluation demonstrates that our proposed framework consistently results in performance gains across multiple settings, including classification, open-set recognition, and out-of-distribution detection.",2025,0.613636140495949,0.6126236410646255,0.6,0.25,9af0ae75-695f-483e-843f-7382a95c1764,0,"[0.25, 0.5, 0.625, 0.875]","[0.95, 0.95, 0.95, 0.95]",anti-class,0.5624999999999999
543754,DocLayout-YOLO: Enhancing Document Layout Analysis through Diverse Synthetic Data and Global-to-Local Adaptive Perception,"Document Layout Analysis is crucial for real-world document understanding systems, but it encounters a challenging trade-off between speed and accuracy: multimodal methods leveraging both text and visual features achieve higher accuracy but suffer from significant latency, whereas unimodal methods relying solely on visual features offer faster processing speeds at the expense of accuracy. To address this dilemma, we introduce DocLayout-YOLO, a novel approach that enhances accuracy while maintaining speed advantages through document-specific optimizations in both pre-training and model design. For robust document pre-training, we introduce the Mesh-candidate BestFit algorithm, which frames document synthesis as a two-dimensional bin packing problem, generating the large-scale, diverse DocSynth-300K dataset. Pre-training on the resulting DocSynth-300K dataset significantly improves fine-tuning performance across various document types. In terms of model optimization, we propose a Global-to-Local Controllable Receptive Module that is capable of better handling multi-scale variations of document elements. Furthermore, to validate performance across different document types, we introduce a complex and challenging benchmark named DocStructBench. Extensive experiments on downstream datasets demonstrate that DocLayout-YOLO excels in both speed and accuracy. Code, data, and models will be made publicly available.",2025,0.5113634504132908,0.5069376223885896,0.5333333333333333,0.5,ff572331-ffa6-4111-baa5-97f422bdecb8,0,"[0.25, 0.5, 0.5, 0.625]","[1.0, 0.95, 0.9, 0.95]",DocLayout-YOLO,0.4486627563042241
543759,Multimodal Attributed Graphs: Benchmarking and Rethinking,"Associating unstructured data with structured information is crucial for real-world tasks that require relevance search. However, existing graph learning benchmarks often overlook the rich semantic information associated with each node, ignoring other available modalities such as the corresponding images. To bridge this gap, we introduce the Multimodal Graph Benchmark (MM-GRAPH), the first comprehensive multi-modal graph benchmark that incorporates both textual and visual information, going beyond the prior focus on just text-attributed graphs. MM-GRAPH consists of seven graph learning datasets of various scales that are appropriate for different learning tasks, and enable a comprehensive evaluation of graph learning algorithms in real-world scenarios thanks to their multimodal node features. To facilitate research on multimodal graph learning, we further provide an extensive study on the performance of various graph learning frameworks in the presence of features from various modalities. MM-GRAPH aims to foster research on multimodal attributed graphs and drive the development of more advanced and robust multimodal attributed graph learning algorithms. By providing a diverse set of datasets and benchmarks, MM-GRAPH enables researchers to evaluate and compare their models in realistic settings, ultimately leading to improved performance on real-world applications that rely on multimodal attributed graphs.",2025,0.4772725537190714,0.4773694104218989,0.5333333333333333,0.5,f771bf1d-aabc-4c65-9a0b-52dae79e376d,0,"[0.25, 0.5, 0.5, 0.5]","[0.95, 1.0, 0.95, 0.95]",MM-GRAPH,0.4424705449189985
543769,Neural Phylogeny: Fine-Tuning Relationship Detection among Neural Networks,"Given a collection of neural networks, can we determine which are parent models and which are child models fine-tuned from the parents?
In this work, we strive to answer this question
via introducing a new task termed as neural phylogeny detection, aimed at identifying the existence and direction of the fine-tuning relationship. Specifically, neural phylogeny detection attempts to identify all parent-child model pairs and determine, within each pair, which model is the parent and which is the child.
We present two approaches for neural phylogeny detection: a learning-free method and a learning-based method. First, we propose a metric that leverages the distance from network parameters to a fake initialization to infer fine-tuning directions. By integrating this metric with traditional clustering algorithms, we propose a series of efficient, learning-free neural phylogeny detection methods. Second, we introduce a transformer-based neural phylogeny detector, which significantly enhances detection accuracy through a learning-based manner. Extensive experiments, ranging from shallow fully-connected networks to open-sourced Stable Diffusion and LLaMA models, progressively validate the effectiveness of both methods. The results demonstrate the reliability of both the learning-free and the learning-based approaches across various learning tasks and network architectures, as well as their ability to detect cross-generational phylogeny between ancestor models and their fine-tuned descendants.",2025,0.6818179338843877,0.6806929345162506,0.6666666666666666,0.625,4fad3667-03b6-438b-97c5-39adac23c886,1,"[0.625, 0.625, 0.625, 0.625]","[0.9, 0.95, 0.95, 0.95]",neural phylogeny detection,0.6250000000000001
543772,Indirect Gradient Matching for Adversarial Robust Distillation,"Adversarial training significantly improves adversarial robustness, but superior performance is primarily attained with large models. 
This substantial performance gap for smaller models has spurred active research into adversarial distillation (AD) to mitigate the difference. 
Existing AD methods leverage the teacher’s logits as a guide.
In contrast to these approaches, we aim to transfer another piece of knowledge from the teacher, the input gradient.
In this paper, we propose a distillation module termed Indirect Gradient Distillation Module (IGDM) that indirectly matches the student’s input gradient with that of the teacher.
Experimental results show that IGDM seamlessly integrates with existing AD methods, significantly enhancing their performance.
Particularly, utilizing IGDM on the CIFAR-100 dataset improves the AutoAttack accuracy from 28.06\% to 30.32\% with the ResNet-18 architecture and from 26.18\% to 29.32\% with the MobileNetV2 architecture when integrated into the SOTA method without additional data augmentation.",2025,0.7840906239670459,0.7792147961950899,0.8,0.875,b5005da1-9e2c-4590-96fa-e9f58962fa86,1,"[0.5, 0.625, 0.875, 0.875]","[0.95, 1.0, 0.9, 0.95]",Indirect Gradient Distillation Module,0.7022407040436375
543776,CatVTON: Concatenation Is All You Need for Virtual Try-On with Diffusion Models,"Virtual try-on methods based on diffusion models achieve realistic effects but often require additional encoding modules, a large number of training parameters, and complex preprocessing, which increases the burden on training and inference. In this work, we re-evaluate the necessity of additional modules and analyze how to improve training efficiency and reduce redundant steps in the inference process. Based on these insights, we propose CatVTON, a simple and efficient virtual try-on diffusion model that transfers in-shop or worn garments of arbitrary categories to target individuals by concatenating them along spatial dimensions as inputs of the diffusion model. The efficiency of CatVTON is reflected in three aspects: (1) Lightweight network. CatVTON consists only of a VAE and a simplified denoising UNet, removing redundant image and text encoders as well as cross-attentions, and includes just 899.06M parameters. (2) Parameter-efficient training. Through experimental analysis, we identify self-attention modules as crucial for adapting pre-trained diffusion models to the virtual try-on task, enabling high-quality results with only 49.57M training parameters. (3) Simplified inference. CatVTON eliminates unnecessary preprocessing, such as pose estimation, human parsing, and captioning, requiring only a person image and garment reference to guide the virtual try-on process, reducing over 49% memory usage compared to other diffusion-based methods. Extensive experiments demonstrate that CatVTON achieves superior qualitative and quantitative results compared to baseline methods and demonstrates strong generalization performance in in-the-wild scenarios, despite being trained solely on public datasets with 73K samples.",2025,0.7159088305786071,0.7173457985453864,0.6666666666666666,0.625,8f93617a-d3ce-4002-b860-799cd8c90360,1,"[0.5, 0.625, 0.625, 0.875]","[0.95, 1.0, 0.95, 1.0]",CatVTON,0.67006309686221
543781,ProteinAdapter: Adapting Pre-trained Large Protein Models for Efficient Protein Representation Learning,"The study of proteins is crucial in various scientific disciplines, but understanding their intricate multi-level relationships remains challenging. Inspired by the sequence and structure understanding of Large Protein Models (LPMs), we introduce a new ProteinAdapter, to efficiently transfer the broad knowledge encapsulated in multiple LPMs, e.g., ESM-1b, to task-specific insights. ProteinAdapter could largely save labor-intensive analysis on the 3D position and the amino acid order. Specifically, (1) with a modest number of additional parameters, ProteinAdapter facilitates multi-level protein representation learning by integrating both sequence and geometric structure embeddings from LPMs; (2) based on the learned embedding, we further scale up the proposed ProteinAdapter to various tasks with a unified Multi-Scale Predictor, which optimally harnesses the learned embeddings through task-specific attention. Albeit simple, the proposed method is scalable to multiple downstream tasks without bells and whistles. Extensive experiments on over 20 tasks show that ProteinAdapter outperforms state-of-the-art methods under both single-task and multi-task scenarios.",2025,0.3272726082645061,0.3273120143936605,0.2666666666666666,0.25,0f15b022-b753-48a1-bd21-83705f56cde1,0,"[0.25, 0.25, 0.25, 0.25, 0.5]","[0.95, 0.9, 0.95, 0.95, 0.95]",ProteinAdapter,0.3023859919184144
543793,The OMG dataset: An Open MetaGenomic corpus for mixed-modality genomic language modeling,"Biological language model performance depends heavily on pretraining data quality, diversity, and size. While metagenomic datasets feature enormous biological diversity, their utilization as pretraining data has been limited due to challenges in data accessibility, quality filtering and deduplication. Here, we present the Open MetaGenomic (OMG) corpus, a genomic pretraining dataset totalling 3.1T base pairs and 3.3B protein coding sequences, obtained by combining two largest metagenomic dataset repositories (JGI's IMG and EMBL's MGnify). We first document the composition of the dataset and describe the quality filtering steps taken to remove poor quality data. We make the OMG corpus available as a mixed-modality genomic sequence dataset that represents multi-gene encoding genomic sequences with translated amino acids for protein coding sequences, and nucleic acids for intergenic sequences. We train the first mixed-modality genomic language model (gLM2) that leverages genomic context information to learn robust functional representations, as well as coevolutionary signals in protein-protein interfaces and genomic regulatory syntax. Furthermore, we show that deduplication in embedding space can be used to balance the corpus, demonstrating improved performance on downstream tasks. The OMG dataset is publicly hosted on the Hugging Face Hub at https://huggingface.co/datasets/tattabio/OMG and gLM2 is available at https://huggingface.co/tattabio/gLM2_650M.",2025,0.6818179338843877,0.684371943688724,0.6,0.5,0487751b-d3f5-4ebb-b632-22edd1d979c2,1,"[0.5, 0.5, 0.625, 0.875]","[0.9, 0.9, 0.95, 0.95]",OMG corpus,0.6410621761658032
543803,Auto-RAG: Autonomous Retrieval-Augmented Generation for Large Language Models,"Iterative retrieval refers to the process in which the model continuously queries the retriever during generation to enhance the relevance of the retrieved knowledge, thereby improving the performance of Retrieval-Augmented Generation (RAG). Existing work typically employs few-shot prompting or manually constructed rules to implement iterative retrieval. This introduces additional inference overhead and overlooks the remarkable reasoning capabilities of Large Language Models (LLMs). In this paper, we introduce Auto-RAG, an autonomous iterative retrieval model centered on the LLM's powerful decision-making capabilities.  Auto-RAG engages in multi-turn dialogues with the retriever, systematically planning retrievals and refining queries to acquire valuable knowledge. This process continues until sufficient external information is gathered, at which point the results are presented to the user. To this end, we develop a method for autonomously synthesizing reasoning-based decision-making instructions in iterative retrieval and fine-tuned the latest open-source LLMs.
The experimental results indicate that Auto-RAG is capable of autonomous iterative interaction with the retriever, effectively leveraging the remarkable reasoning and decision-making abilities of LLMs, which lead to outstanding performance across six benchmarks. Further analysis reveals that Auto-RAG can autonomously adjust the number of iterations based on the difficulty of the questions and the utility of the retrieved knowledge, without requiring any human intervention. Moreover, Auto-RAG expresses the iterative retrieval process in natural language, enhancing interpretability while providing users with a more intuitive experience.",2025,0.613636140495949,0.6126236410646255,0.6,0.5,67f8f51e-fe55-4c42-a732-e8092b8a7a7e,0,"[0.5, 0.5, 0.625, 0.625]","[0.95, 0.95, 0.9, 1.0]",Auto-RAG,0.5642889738697068
543804,Preference Optimization for Reasoning with Pseudo Feedback,"Preference optimization techniques, such as Direct Preference Optimization (DPO), are frequently employed to enhance the reasoning capabilities of large language models (LLMs) in domains like mathematical reasoning and coding, typically following supervised fine-tuning. These methods rely on high-quality labels for reasoning tasks to generate preference pairs; however, the availability of reasoning datasets with human-verified labels is limited.
In this study, we introduce a novel approach to generate pseudo feedback for reasoning tasks by framing the labeling of solutions to reason problems as an evaluation against associated \emph{test cases}. 
We explore two forms of pseudo feedback based on test cases: one generated by frontier LLMs and the other by extending self-consistency to multi-test-case.
We conduct experiments on both mathematical reasoning and coding tasks using pseudo feedback for preference optimization, and observe improvements across both tasks. Specifically, using Mathstral-7B as our base model, we improve MATH results from 58.3 to 68.6, surpassing both NuminaMath-72B and GPT-4-Turbo-1106-preview. In GSM8K and College Math, our scores increase from 85.6 to 90.3 and from 34.3 to 42.3, respectively. Building on Deepseek-coder-7B-v1.5, we achieve a score of 24.3 on LiveCodeBench (from 21.1), surpassing Claude-3-Haiku.",2025,0.8636364132231226,0.8589103249682525,0.9333333333333332,0.875,5443f568-c8c6-40c6-ae16-db38e1e2bc84,1,"[0.625, 0.875, 0.875]","[0.95, 0.9, 0.9]",Pseudo Feedback,0.7767502705698922
543805,SeedLoRA: A Fusion Approach to Efficient LLM Fine-Tuning,"Despite Low-Rank Adaptation (LoRA)'s popularity for fine-tuning large models, it often exhibits a noticeable performance gap compared to full fine-tuning, particularly in complex tasks such as mathematical reasoning and code generation. Motivated by this discrepancy, we propose a novel fusion approach for LoRA fine-tuned models. Our key insight is that LoRA models trained with different random seeds on the same task often exhibit complementary strengths. In contrast to existing research that typically focuses on fusing models trained on diverse tasks, we explore the potential of combining multiple LoRA models fine-tuned on the same task with different random seeds. This intra-task fusion method aims to leverage the strengths of various fine-tuned models to create a more robust and effective adaptation. To validate our approach, we conducted comprehensive experiments across three key areas: mathematical reasoning, code generation, and general instruction-tuning tasks. The results demonstrate that our fusion method significantly enhances LoRA's performance, outperforming both standalone LoRA models and current fusion methods. Notably, this advancement substantially narrows the gap between LoRA and full fine-tuning, thus offering a more effective approach to model adaptation without the GPU memory burden of full parameter fine-tuning.",2025,0.613636140495949,0.6099008693265606,0.6,0.5,75d20a88-0652-4373-b934-9ca1b5631802,0,"[0.5, 0.5, 0.625, 0.625]","[1.0, 0.95, 0.9, 0.9]",Intra-task fusion,0.5496298896712798
543807,U-shaped and Inverted-U Scaling behind Emergent Abilities of Large Language Models,"Large language models (LLMs) have been shown to exhibit *emergent abilities* in some downstream tasks, where model performance stagnates at first and then improves sharply and unpredictably with scale beyond a threshold. In this work, we investigate the phenomenon by grouping questions based on difficulty level and provide a possible explanation for emergent abilities. Specifically, we observe U-shaped scaling for hard questions and inverted-U scaling followed by steady improvement for easy questions. The two scaling patterns initially offset each other, causing stagnant overall performance. The performance starts to soar when the scaling pattern of easy questions reverts from inverse to standard scaling, leading to emergent abilities. Based on this finding, we propose a simple yet effective pipeline, called *Slice-and-Sandwich*, to predict the emergence threshold and model performance beyond the threshold. Our code is publicly available at https://github.com/tony10101105/ExpEmergence.",2025,0.7499997272728265,0.7515590590972161,0.6666666666666666,0.625,7c38bc9b-3a1c-4575-8798-4caa3cd93553,1,"[0.625, 0.625, 0.625, 0.875]","[0.9, 0.9, 0.9, 0.95]",Emergent abilities,0.7003737541528239
543810,RetinaGS: Scalable Training for Dense Scene Rendering with Billion-Scale 3D Gaussians,"In this work, we explore the possibility of training high-parameter 3D Gaussian splatting (3DGS) models on large-scale, high-resolution datasets. We design a general model parallel training method for 3DGS, named RetinaGS, which uses a proper rendering equation and can be applied to any scene and arbitrary distribution of Gaussian primitives. It enables us to explore the scaling behavior of 3DGS in terms of primitive numbers and training resolutions that were difficult to explore before and surpass previous state-of-the-art reconstruction quality. We observe a clear positive trend of increasing visual quality when increasing primitive numbers with our method. We also demonstrate the first attempt at training a 3DGS model with more than one billion primitives on the full MatrixCity dataset that attains a promising visual quality.",2025,0.5795452438017296,0.5781468162085512,0.5333333333333333,0.5,b753bc6f-18cb-42d4-8886-3224cdeebbcb,0,"[0.5, 0.5, 0.5, 0.625]","[1.0, 0.95, 0.95, 0.95]",RetinaGS,0.5287647275405007
543811,UNIQ: Offline Inverse  Q-learning for Avoiding Undesirable Demonstrations,"We address the problem of offline learning a policy that avoids undesirable demonstrations. Unlike conventional offline imitation learning approaches that aim to imitate expert or near-optimal demonstrations, our setting involves avoiding undesirable behavior (specified using undesirable demonstrations). To tackle this problem, unlike standard imitation learning where the aim is to minimize the distance between learning policy and expert demonstrations, we formulate the learning task as maximizing a statistical distance, in the space of state-action stationary distributions, between the learning policy and the undesirable policy. This significantly different approach results in a novel training objective that necessitates a new algorithm to address it. Our algorithm, UNIQ, tackles these challenges by building on the inverse Q-learning framework, framing the learning problem as a cooperative (non-adversarial) task. We then demonstrate how to efficiently leverage unlabeled data for practical training. Our method is evaluated on standard benchmark environments, where it consistently outperforms state-of-the-art baselines.",2025,0.443181657024852,0.4424504074355629,0.2666666666666666,0.25,c31aa0fd-f171-4009-8cdd-237c718de957,0,"[0.25, 0.25, 0.25, 0.875]","[0.95, 0.95, 0.95, 0.95]",Inverse Q-learning,0.40625
543839,Large Language Models can Become Strong Self-Detoxifiers,"Reducing the likelihood of generating harmful and toxic output is an essential task when aligning large language models (LLMs). Existing methods mainly rely on training an external reward model (i.e., another language model) or fine-tuning the LLM using self-generated data to influence the outcome. In this paper, we show that LLMs have the capability of self-detoxification without external reward model learning or retraining of the LM. We propose \textit{Self-disciplined Autoregressive Sampling (SASA)}, a lightweight controlled decoding algorithm for toxicity reduction of LLMs. SASA leverages the contextual representations from an LLM to learn linear subspaces from labeled data characterizing toxic v.s. non-toxic output in analytical forms. When auto-completing a response token-by-token, SASA dynamically tracks the margin of the current output to steer the generation away from the toxic subspace, by adjusting the autoregressive sampling strategy. Evaluated on LLMs of different scale and nature, namely Llama-3.1-Instruct (8B), Llama-2 (7B), and GPT2-L models with the RealToxicityPrompts, BOLD, and AttaQ benchmarks, SASA markedly enhances the quality of the generated sentences relative to the original models and attains comparable performance to state-of-the-art detoxification techniques, significantly reducing the toxicity level by only using the LLM's internal representations.",2025,0.6818179338843877,0.6806929345162506,0.6666666666666666,0.625,bcb70f18-5406-41a8-93c2-3845faa4fbd4,1,"[0.625, 0.625, 0.625, 0.625, 0.625]","[0.9, 1.0, 0.9, 0.95, 0.95]",Self-disciplined Autoregressive Sampling (SASA),0.625
543845,Has My System Prompt Been Used? Large Language Model Prompt Membership Inference,"Prompt engineering has emerged as a powerful technique for optimizing large language models (LLMs) for specific applications, enabling faster prototyping and improved performance, and giving rise to the interest of the community in protecting proprietary system prompts. In this work, we explore a novel perspective on prompt privacy through the lens of membership inference. We develop Prompt Detective, a statistical method to reliably determine whether a given system prompt was used by a third-party language model. Our approach relies on a statistical test comparing the distributions of two groups of generations corresponding to different system prompts. Through extensive experiments with a variety of language models, we demonstrate the effectiveness of Prompt Detective in both standard and challenging scenarios, including black-box settings. Our work reveals that even minor changes in system prompts manifest in distinct response distributions, enabling us to verify prompt usage with statistical significance.",2025,0.6818179338843877,0.6806929345162506,0.6666666666666666,0.875,6b63753a-45c7-4002-800e-5df463dfcbcf,0,"[0.25, 0.5, 0.625, 0.875, 0.875]","[0.95, 0.95, 0.9, 0.95, 0.95]",Prompt Detective,0.625
543856,Learning In-Distribution Representations for Anomaly Detection,"Anomaly detection involves identifying data patterns that deviate from the anticipated norm. Traditional methods struggle in high-dimensional spaces due to the curse of dimensionality. In recent years, self-supervised learning, particularly through contrastive objectives, has driven advances in anomaly detection by generating compact and discriminative feature spaces. However, vanilla contrastive learning faces challenges like class collision, especially when the In-Distribution (ID) consists primarily of normal, homogeneous data, where the lack of semantic diversity leads to increased overlap between positive and negative pairs. Existing methods attempt to address these issues by introducing hard negatives through synthetic outliers, Outlier Exposure (OE), or supervised objectives, though these approaches can introduce additional challenges. In this work, we propose the Focused In-distribution Representation Modeling (FIRM) loss, a novel multi-positive contrastive objective for anomaly detection. FIRM addresses class-collision by explicitly encouraging ID representations to be compact while promoting separation among synthetic outliers. We show that FIRM surpasses other contrastive methods in standard benchmarks, significantly enhancing anomaly detection compared to both traditional and supervised contrastive learning objectives. Our ablation studies confirm that FIRM consistently improves the quality of representations and shows robustness across a range of scoring methods. It performs particularly well in ensemble settings and benefits substantially from using OE. The code is available at \url{https://anonymous.4open.science/r/firm-8472/}.",2025,0.3818180429752571,0.3783215092432655,0.5333333333333333,0.5,92bf2d68-f179-4841-a50a-2e280c5c82a9,0,"[0.0, 0.25, 0.5, 0.5, 0.5]","[0.95, 1.0, 0.95, 0.9, 0.95]",FIRM,0.3365725771376913
543870,Self-Monitoring Large Language Models for Click-Through Rate Prediction,"Click-through rate (CTR) prediction tasks traditionally aim to model extensive user-
item feature interactions. Recent approaches fine-tune Large Language Models
(LLMs) using user-item features as input and click labels as output. However,
due to the sparsity of click labels, the attention mechanism may focus on a subset
of features rather than all features. This can hinder LLMs’ ability to accurately
match features to click labels, resulting in performance that does not consistently
exceed traditional state-of-the-art CTR approaches. To address this, we introduce
a SLLM4CTR framework which uses adaptive temperature and label matching loss
to improve fine-tuning and inference process of LLMs. The adaptive temperature
serves as a confidence score to calibrate CTR predictions by quantifying the LLMs’
attention to user-item features. The label matching loss clearly distinguish between
click-inducing and non-click-inducing features by constraining the representation
space of click labels. By combining these two designs, SLLM4CTR improves feature
utilization in LLMs and enhances the matching of user-item features to click
labels. Experimental results demonstrate
that SLLM4CTR significantly outperforms state-of-the-art baselines, including both
traditional and LLM-based CTR approaches. The code will be open-sourced.",2025,0.5181816297521347,0.5152769276679351,0.5333333333333333,0.5,c7789212-e001-446b-bb09-439368775c73,0,"[0.25, 0.5, 0.5, 0.5, 0.625]","[0.95, 0.9, 0.95, 0.95, 0.9]",SLLM4CTR,0.4662305516265912
543882,Bayesian Enhancement Models for One-to-Many Mapping in Image Enhancement,"Image enhancement is considered an ill-posed inverse problem due to its tendency to have multiple solutions. The loss of information makes accurately reconstructing the original image from observed data challenging. Also, the quality of the result is often subjective to individual preferences. This obviously poses a one-to-many mapping challenge.
To address this, we propose a Bayesian Enhancement Model (BEM) that leverages Bayesian estimation to capture inherent uncertainty and accommodate diverse outputs. 
To address the noise in predictions of Bayesian Neural Networks (BNNs) for high-dimensional images, we propose a two-stage approach. The first stage utilises a BNN to model reduced-dimensional image representations, while the second stage employs a deterministic network to refine these representations.
We further introduce a dynamic \emph{Momentum Prior} to overcome convergence issues typically faced by BNNs in high-dimensional spaces.
Extensive experiments across multiple low-light and underwater image enhancement benchmarks demonstrate the superiority of our method over traditional deterministic models, particularly in real-world applications lacking reference images, highlighting the potential of Bayesian models in handling one-to-many mapping problems.",2025,0.5113634504132908,0.5100655425612787,0.5333333333333333,0.5,a617debc-b27d-4a4c-ba5e-73975ed5f34c,0,"[0.25, 0.5, 0.5, 0.625]","[0.95, 0.95, 0.9, 0.95]",Bayesian Enhancement Model,0.4668634371957156
543904,ZETA: Leveraging $Z$-order Curves for Efficient Top-$k$ Attention,"Over recent years, the Transformer has become a fundamental building block for sequence modeling architectures. Yet at its core is the use of self-attention, whose memory and computational cost grow quadratically with the sequence length $N$, rendering it prohibitively expensive for long sequences. A promising approach is top-$k$ attention, which selects only the $k$ most relevant tokens and achieves performance comparable to vanilla self-attention while significantly reducing space and computational demands. However, causal masks require the current query token to only attend to past tokens, preventing existing top-$k$ attention methods from efficiently searching for the most relevant tokens in parallel, thereby limiting training efficiency. In this work, we propose ZETA, leveraging Z-Order Curves for Efficient Top-k Attention, to enable parallel querying of past tokens for entire sequences. We first theoretically show that the choice of key and query dimensions involves a trade-off between the curse of dimensionality and the preservation of relative distances after projection. In light of this insight, we propose reducing the dimensionality of keys and queries in contrast to values and further leveraging Z-order curves to map low-dimensional keys and queries into one-dimensional space, which permits parallel sorting, thereby largely improving the efficiency for top-$k$ token selection. Experimental results demonstrate that ZETA~matches the performance of standard attention on synthetic tasks Associative Recall and outperforms attention and its variants on Long-Range Arena and WikiText-103 language modeling.",2025,0.8181815206612653,0.8206129068093254,0.8,0.625,e55c70eb-2d82-4513-87bb-0ed9b466b154,1,"[0.625, 0.625, 0.875, 0.875]","[0.8, 0.95, 0.9, 0.95]",Z-Order Curves,0.760388282810199
543921,Longitudinal Ensemble Integration for sequential classification with multimodal data,"Effectively modeling multimodal longitudinal data is a pressing need in various application areas, especially biomedicine. Despite this, few approaches exist in the literature for this problem, with most not adequately taking into account the multimodality of the data. In this study, we developed multiple configurations of a novel multimodal and longitudinal learning framework, Longitudinal Ensemble Integration (LEI), for sequential classification. We evaluated LEI’s performance, and compared it against existing approaches, for the early detection of dementia, which is among the most studied multimodal sequential classification tasks. LEI outperformed these approaches due to its use of intermediate base predictions arising from the individual data modalities, which enabled their better integration over time. LEI’s design also enabled the identification of features that were consistently important across time for the effective prediction of dementia-related diagnoses. Overall, our work demonstrates the potential of LEI for sequential classification from longitudinal multimodal data.",2025,0.2727271735537551,0.2722771738065002,0.2666666666666666,0.25,6287600e-f99c-497e-952b-93d1806635d9,0,"[0.25, 0.25, 0.25, 0.25]","[0.95, 0.9, 0.95, 0.95]",Longitudinal Ensemble Integration,0.25
543934,LSTT:LONG SHORT-TERM TRANSFORMER FOR VIDEO SMALL OBJECT DETECTION,"Detecting small objects in video sequences is crucial, yet it poses significant challenges due to their limited visibility and dynamic nature, which complicates accurate identification and localization. Traditional methods often employ a uniform aggregation strategy across all frames, neglecting the unique spatiotemporal relationships of small objects, which results in insufficient feature extraction and diminished detection performance. This paper introduces a long short-term transformer network specifically designed for small object detection in videos. The model integrates features from both long-term and short-term frames: long-term frames capture global contextual information, enhancing the model’s ability to represent background scenes, while short-term frames provide dynamic information closely related to the current detection frame, thereby improving the feature representation of small objects. A dynamic query generation module optimizes query generation based on the implicit motion relationships of targets in shortterm frames, adapting to the current video framework. Additionally, the network employs a progressive sampling strategy—densely sampling short-term frames and sparsely sampling long-term frames—to effectively model video scenes. A spatio-temporal alignment encoder further enhances pixel-level features by accounting for temporal and spatial transformations. Extensive experiments on the VisDrone-VID and UAVDT datasets demonstrate the method’s effectiveness, with an average detection precision increase of 1.4% and 2.1%, respectively, highlighting its potential in small object video detection.",2025,0.3409089669421938,0.3403464672581253,0.2666666666666666,0.25,82655fc2-d745-44e9-8ac4-7343775e880d,0,"[0.25, 0.25, 0.25, 0.5]","[0.9, 1.0, 0.95, 0.95]",LSTT,0.3107110261302933
543953,Building Interactable Replicas of Complex Articulated Objects via Gaussian Splatting,"Building interactable replicas of articulated objects is a key challenge in computer vision. Existing methods often fail to effectively integrate information across different object states, limiting the accuracy of part-mesh reconstruction and part dynamics modeling, particularly for complex multi-part articulated objects. We introduce ArtGS, a novel approach that leverages 3D Gaussians as a flexible and efficient representation to address these issues. Our method incorporates canonical Gaussians with coarse-to-fine initialization and updates for aligning articulated part information across different object states, and employs a skinning-inspired part dynamics modeling module to improve both part-mesh reconstruction and articulation learning. Extensive experiments on both synthetic and real-world datasets, including a new benchmark for complex multi-part objects, demonstrate that ArtGS achieves state-of-the-art performance in joint parameter estimation and part mesh reconstruction. Our approach significantly improves reconstruction quality and efficiency, especially for multi-part articulated objects. Additionally, we provide comprehensive analyses of our design choices, validating the effectiveness of each component to highlight potential areas for future improvement.",2025,0.6818179338843877,0.6806929345162506,0.6666666666666666,0.625,56b5a13a-bf12-48fa-8e4d-869fed009f31,1,"[0.625, 0.625, 0.625, 0.625]","[0.95, 0.95, 0.95, 0.9]",ArtGS,0.625
543967,Action abstractions for amortized sampling,"As trajectories sampled by policies used by reinforcement learning (RL) and generative flow networks (GFlowNets) grow longer, credit assignment and exploration become more challenging, and the long planning horizon hinders mode discovery and generalization.
The challenge is particularly pronounced in entropy-seeking RL methods, such as generative flow networks, where the agent must learn to sample from a structured distribution and discover multiple high-reward states, each of which take many steps to reach.
To tackle this challenge, we propose an approach to incorporate the discovery of action abstractions, or high-level actions, into the policy optimization process.
Our approach involves iteratively extracting action subsequences commonly used across many high-reward trajectories and `chunking' them into a single action that is added to the action space.
In empirical evaluation on synthetic and real-world environments, our approach demonstrates improved sample efficiency performance in discovering diverse high-reward objects, especially on harder exploration problems.
We also observe that the abstracted high-order actions are potentially interpretable, capturing the latent structure of the reward landscape of the action space.
This work provides a cognitively motivated approach to action abstraction in RL and is the first demonstration of hierarchical planning in amortized sequential sampling.",2025,0.7909088033058898,0.7858910324968252,0.6666666666666666,0.625,e1dcf306-689f-4548-8047-55dab2d3ccaa,1,"[0.625, 0.625, 0.625, 0.875, 0.875]","[0.95, 0.95, 0.8, 0.8, 0.9]",Action Abstractions,0.7093236006177827
543970,Risk-Sensitive Variational Actor-Critic: A Model-Based Approach,"Risk-sensitive reinforcement learning (RL) with an entropic risk measure typically requires knowledge of the transition kernel or performs unstable updates w.r.t. exponential Bellman equations. As a consequence, algorithms that optimize this objective have been restricted to tabular or low-dimensional continuous environments. In this work we leverage the connection between the entropic risk measure and the RL-as-inference framework to develop a risk-sensitive variational actor-critic algorithm (rsVAC). Our work extends the variational framework to incorporate stochastic rewards and proposes a variational model-based actor-critic approach that modulates policy risk via a risk parameter.  We consider, both, the risk-seeking and risk-averse regimes and present rsVAC learning variants for each setting.  Our experiments demonstrate that this approach produces risk-sensitive policies and yields improvements in both tabular and risk-aware variants of complex continuous control tasks in MuJoCo.",2025,0.7499997272728265,0.74596430772984,0.7333333333333333,0.5,74f089e7-e32b-4e6e-b8ed-abf647023267,1,"[0.5, 0.5, 0.875, 0.875]","[0.95, 0.9, 0.9, 0.9]",rsVAC,0.6746262458471761
544001,Towards Dynamic Graph Neural Networks with Provably High-Order Expressive Power,"Dynamic Graph Neural Networks (DyGNNs) have garnered increasing research attention for learning representations on evolving graphs. 
Despite their effectiveness, the limited expressive power of existing DyGNNs hinders them from capturing important evolving patterns of dynamic graphs. 
Although some works attempt to enhance expressive capability with heuristic features, there remains a lack of DyGNN frameworks with provable and quantifiable high-order expressive power.
To address this research gap, we firstly propose the k-dimensional Dynamic WL tests (k-DWL) as the referencing algorithms to quantify the expressive power of DyGNNs. We demonstrate that the expressive power of existing DyGNNs is bounded by the 1-DWL test. 
To enhance the expressive power, we propose Dynamic Graph Neural Network with High-order expressive power (HopeDGN), which updates the representation of central node pair by aggregating the interaction history with neighbor node pairs. 
Our theoretical results demonstrate that HopeDGN can achieve expressive power equivalent to the 2-DWL test. 
We then present a Transformer-based implementation for the local variant of \model.
Experimental results show that HopeDGN achieved  performance improvement up to 3.12\% on seven datasets, demonstrating the effectiveness of HopeDGN.",2025,0.5181816297521347,0.5141398981901192,0.5333333333333333,0.5,3fd96ba7-9a4f-44af-ac1b-583ce68adb4a,0,"[0.25, 0.5, 0.5, 0.5, 0.625]","[1.0, 0.9, 0.9, 0.95, 0.95]",HopeDGN,0.4567701208617246
544008,CrossQuant: A Post-Training Quantization Method with Smaller Quantization Kernel for Precise Large Lanugage Model Compression,"Post-Training Quantization (PTQ) is an effective technique for compressing Large Language Models (LLMs). While many studies focus on quantizing both weights and activations, it is still a challenge to maintain the accuracy of LLM after activating quantization. To investigate the primary cause, we extend the concept of kernel from linear algebra to quantization functions to define a new term, ""quantization kernel"", which refers to the set of elements in activations that are quantized to zero. Through quantitative analysis of the quantization kernel, we find that these elements are crucial for maintaining the accuracy of quantized LLMs. With the decrease of quantization kernel, the precision of quantized LLMs increases. If the quantization kernel proportion is kept below 19\% for OPT models and below 1\% for LLaMA models, the precision loss from quantizing activations to INT8 becomes negligible. Motivated by the goal of developing a quantization method with small quantization kernel, we propose CrossQuant—a simple yet effective method for quantizing activations. CrossQuant cross-quantizes elements using row and column-wise absolute maximum vectors, achieving a quantization kernel of approximately 16\% for OPT models and less than 0.1\% for LLaMA models. Experimental results on LLMs (LLaMA, OPT) ranging from 6.7B to 70B parameters demonstrate that CrossQuant improves or maintains perplexity and accuracy in language modeling, zero-shot, and few-shot tasks.",2025,0.3409089669421938,0.3394621109976018,0.2666666666666666,0.25,26ed76d2-f48f-4ace-84bb-39564b70294e,0,"[0.25, 0.25, 0.25, 0.5]","[1.0, 0.9, 1.0, 0.95]",Quantization Kernel,0.3060102575817623
544009,Temporal Causal Discovery and Generative Prediction of Vehicular CO$_2$ emission,"Global warming from greenhouse gas emissions is humanity's largest environmental hazard. Greenhouse gases, like CO$_2$ emissions from transportation, notably cars, contribute to the greenhouse effect. Effective CO$_2$ emission monitoring is needed to regulate vehicle emissions. Few studies have predicted automobile CO$_2$ emissions using OBD port data. For precise and effective prediction, the system must capture the underlying cause-effect structure between vehicular parameters that may contribute to the emission of CO$_2$ in the transportation sector. Thus, we present a causal RNN-based generative deep learning architecture that predicts vehicle CO$_2$ emissions using OBD-II data while keeping the underlying causal structure. Most widely used real-life datasets lack causal relationships between features or components, so we use our proposed architecture to discover and learn the underlying causal structure as an adjacency matrix during training and employ that during forecasting. Our framework learns a sparse adjacency matrix by imposing a sparsity-encouraging penalty on model weights and allowing some weights to be zero. This matrix is capable of capturing the causal relationships between all variable pairs. In this work, we first train the model with widely used synthetic datasets with known causal structure among variables, then we apply it to the state-of-the-art OBD-II dataset to find the internal causal structure among the vehicular parameters and perform causal inference to predict CO$_2$ emission. Experimental results reveal that our causal discovery and forecasting method surpasses state-of-the-art methods for the tasks of causal discovery in terms of AUROC, forecasting on multivariate causal time series data, and OBD-II dataset in terms of MMD, RMSE, and MAE. After successful completion, we will release the code (Code for review - \href{https://anonymous.4open.science/r/causal-obd-co2-0A0C}{https://anonymous.4open.science/r/causal-obd-co2-0A0C}).",2025,0.4999994545456529,0.4983641587397706,0.5333333333333333,0.25,f72c1ad9-b128-413d-908f-e1258b790230,0,"[0.25, 0.5, 0.625]","[0.95, 0.9, 0.95]",Causal RNN,0.4549105664127194
544032,ARQ: A Mixed-Precision Quantization Framework for Accurate and Certifiably Robust DNNs,"Mixed precision quantization has become an important technique for
enabling the execution of deep neural networks (DNNs) on limited resource computing platforms.
Traditional quantization methods have primarily concentrated on maintaining
neural network accuracy, either ignoring the impact of quantization on the
robustness of the network, or using only empirical techniques for improving
robustness. In contrast, techniques for robustness certification, which can
provide strong guarantees about the robustness of DNNs have not been used
during quantization due to their high computation cost and/or scalability
issues. 

This paper introduces ARQ, an innovative mixed-precision quantization method that not only
preserves the clean accuracy of the smoothed classifiers but also maintains
their certified robustness. ARQ uses reinforcement learning to find accurate and robust
DNN quantization, while efficiently leveraging randomized smoothing,
a popular class of statistical DNN verification algorithms, to guide the search process. 
We compare ARQ with multiple state-of-the-art quantization techniques on
several DNN architectures commonly used in quantization studies: ResNet-20 on
CIFAR-10, ResNet-50 on ImageNet, and MobileNetV2 on ImageNet. 
We demonstrate that ARQ consistently performs better than these baselines
across all the benchmarks and the input perturbation levels. In many cases, the performance of ARQ quantized networks can reach that of the original DNN with floating-point weights, but with only 1.5% instructions.",2025,0.5113634504132908,0.4930427736548963,0.4,0.25,18a62c07-0859-4ea1-a53d-9d368f73447e,0,"[0.25, 0.25, 0.5, 0.875]","[0.95, 0.95, 1.0, 0.8]",ARQ,0.4187735856294469
544039,A GREAT Architecture for Edge-Based Graph Problems Like TSP,"In the last years, many neural network-based approaches have been proposed to tackle combinatorial optimization problems such as routing problems.
Many of these approaches are based on graph neural networks (GNNs) or related transformers, operating on the Euclidean coordinates representing the routing problems. 
However, GNNs are inherently not well suited to operate on dense graphs, such as in routing problems. Furthermore, models operating on Euclidean coordinates cannot be applied to non-Euclidean versions of routing problems that are often found in real-world settings.
To overcome these limitations, we propose a novel GNN-related edge-based neural model called Graph Edge Attention Network (GREAT).
We evaluate the performance of GREAT in the edge-classification task to predict optimal edges in the Traveling Salesman Problem (TSP). We can use such a trained GREAT model to produce sparse TSP graph instances, keeping only the edges GREAT finds promising. Compared to other, non-learning-based methods to sparsify TSP graphs, GREAT can produce very sparse graphs while keeping most of the optimal edges. Furthermore, we build a reinforcement learning-based GREAT framework which we apply to Euclidean and non-Euclidean asymmetric TSP. This framework achieves state-of-the-art results.",2025,0.2727271735537551,0.2722771738065002,0.2666666666666666,0.25,819d818e-ef3c-4c39-b848-cb5d1472fca0,0,"[0.25, 0.25, 0.25, 0.25]","[1.0, 1.0, 0.95, 0.95]",GREAT,0.25
544053,MAA: Meticulous Adversarial Attack against Vision-Language Pre-trained Models,"Current adversarial attacks for evaluating the robustness of vision-language pre-trained (VLP) models in multi-modal tasks suffer from limited transferability, where attacks crafted for a specific model often struggle to generalize effectively across different models, limiting their utility in assessing robustness more broadly. This is mainly attributed to the over-reliance on model-specific features and regions, particularly in the image modality. In this paper, we propose an elegant yet highly effective method termed Meticulous Adversarial Attack (MAA) to fully exploit model-independent characteristics and vulnerabilities of individual samples, achieving enhanced generalizability and reduced model dependence. MAA emphasizes fine-grained optimization of adversarial images by developing a novel resizing and sliding crop (RScrop) technique, incorporating a multi-granularity similarity disruption (MGSD) strategy. 
RScrop efficiently enriches the initial adversarial examples by generating more comprehensive, diverse, and detailed perspectives of the images, establishing a robust foundation for capturing representative and intrinsic visual characteristics. Building on this,  MGSD seeks to maximize %the layer- and component-wise feature% 
the embedding distance between adversarial examples and their original counterparts across different granularities and hierarchical levels within the architecture of VLP models, thereby amplifying the impact of the adversarial perturbations and enhancing the efficacy of attacks across every layer and component of the model. Extensive experiments across diverse VLP models, multiple benchmark datasets, and a variety of downstream tasks demonstrate that MAA significantly enhances the effectiveness and transferability of adversarial attacks. A large cohort of performance studies is conducted to generate insights into the effectiveness of various model configurations, guiding future advancements in this domain. The source code is provided in the supplementary material.",2025,0.6545452165290122,0.6555367018739204,0.5333333333333333,0.5,383cb6db-e75b-4293-8d20-e30b1c2ff2cc,0,"[0.5, 0.5, 0.5, 0.625, 0.875]","[0.95, 1.0, 0.8, 0.9, 0.95]",Meticulous Adversarial Attack,0.6021040733671882
544056,Hi-Mamba: Hierarchical Mamba for Efficient Image Super-Resolution,"State Space Models (SSM), such as Mamba, have shown strong representation ability in modeling long-range dependency with linear complexity, achieving successful applications from high-level to low-level vision tasks. However, SSM's sequential nature necessitates multiple scans in different directions to compensate for the loss of spatial dependency when unfolding the image into a 1D sequence. This multi-direction scanning strategy significantly increases the computation overhead and is unbearable for high-resolution image processing. To address this problem, we propose a novel Hierarchical Mamba network, namely, Hi-Mamba, for image super-resolution (SR).  Hi-Mamba consists of two key designs: (1) The Hierarchical Mamba Block (HMB) assembled by a Local SSM (L-SSM) and a Region SSM (R-SSM) both with the single-direction scanning, aggregates multi-scale representations to enhance the context modeling ability. (2) The Direction Alternation  Hierarchical Mamba Group (DA-HMG) allocates the isomeric single-direction scanning into cascading HMBs to enrich the spatial relationship modeling.
Extensive experiments demonstrate the superiority of Hi-Mamba across five benchmark datasets for efficient SR. For example, Hi-Mamba achieves a significant PSNR improvement of 0.29 dB on Manga109 for $\times3$ SR, compared to the strong lightweight MambaIR.",2025,0.3409089669421938,0.338600626019678,0.2666666666666666,0.25,0e15fef5-fe8c-4436-a234-dfb3501c2306,0,"[0.25, 0.25, 0.25, 0.5]","[1.0, 1.0, 0.95, 0.95]",Hierarchical Mamba,0.3032912687585266
544067,Stochastic variance-reduced Gaussian variational inference on the Bures-Wasserstein manifold,"Optimization in the Bures-Wasserstein space has been gaining popularity in the machine learning community since it draws connections between variational inference and Wasserstein gradient flows. The variational inference objective function of Kullback–Leibler divergence can be written as the sum of the negative entropy and the potential energy, making forward-backward Euler the method of choice. Notably, the backward step admits a closed-form solution in this case, facilitating the practicality of the scheme. However, the forward step is not exact since the Bures-Wasserstein gradient of the potential energy involves ""intractable"" expectations. Recent approaches propose using the Monte Carlo method -- in practice a single-sample estimator -- to approximate these terms, resulting in high variance and poor performance. We propose a novel variance-reduced estimator based on the principle of control variates. We theoretically show that this estimator has a smaller variance than the Monte-Carlo estimator in scenarios of interest. We also prove that variance reduction helps improve the optimization bounds of the current analysis. We demonstrate that the proposed estimator gains order-of-magnitude improvements over the previous Bures-Wasserstein methods.",2025,0.7159088305786071,0.7147275812420631,0.6666666666666666,0.625,75d03413-a6a3-4544-a465-dd467633906f,1,"[0.5, 0.625, 0.625, 0.875]","[0.95, 0.95, 0.95, 0.95]",Variance-reduced estimator,0.65625
544071,Explanation using Simulation,"In safety-critical domains, such as industrial systems, the lack of explainability in predictive `black-box' machine learning models can hinder trust and adoption. Standard explainability techniques, while powerful, often require deep expertise in data analytics and machine learning and fail to align with the sequential, dynamic nature of data in these environments. In this paper, we propose a novel explainability framework that leverages reinforcement learning (RL) to support model predictions with visual explanations based on dynamical system simulation. By training RL agents to simulate events that require prediction, we use these agents' critics to make classifications. Next, we employ the actors of the RL agents to simulate the potential future trajectories underlying these classifications, providing visual explanations that are more intuitive and align with the expertise of industrial domain experts. We demonstrate the applicability of this method through a case study involving monitoring a small industrial system for cyberattacks, showing how our framework generates actionable predictions that are supported with visual explanations. This approach aims to bridge the gap between advanced machine learning models and their real-world deployment in safety-critical environments.",2025,0.2727271735537551,0.2722771738065002,0.2666666666666666,0.25,28ecc43d-29eb-4166-b25a-6596b8f25cba,0,"[0.25, 0.25, 0.25, 0.25]","[0.8, 0.95, 0.95, 0.9]",Reinforcement Learning,0.2500000000000008
544083,DenseGrounding: Improving Dense Language-Vision Semantics for Ego-centric 3D Visual Grounding,"Enabling intelligent agents to comprehend and interact with 3D environments through natural language is crucial for advancing robotics and human-computer interaction. A fundamental task in this field is ego-centric 3D visual grounding, where agents locate target objects in real-world 3D spaces based on verbal descriptions. However, this task faces two significant challenges: (1) loss of fine-grained visual semantics due to sparse fusion of point clouds with ego-centric multi-view images, (2) limited textual semantic context due to arbitrary language descriptions. We propose DenseGrounding, a novel approach designed to address these issues by enhancing both visual and textual semantics. For visual features, we introduce the Hierarchical Scene Semantic Enhancer, which retains dense semantics by capturing fine-grained global scene features and facilitating cross-modal alignment. For text descriptions, we propose a Language Semantic Enhancer that leverage large language models to provide rich context and diverse language descriptions with additional context during model training. Extensive experiments show that DenseGrounding significantly outperforms existing methods in overall accuracy, achieving improvements of **5.81%** and **7.56%** when trained on the comprehensive full training dataset and smaller mini subset, respectively, further advancing the SOTA in ego-centric 3D visual grounding. Our method also achieves **1st place** and receives **Innovation Award** in the 2024 Autonomous Grand Challenge Multi-view 3D Visual Grounding Track, validating its effectiveness and robustness.",2025,0.6818179338843877,0.6806929345162506,0.6666666666666666,0.625,06f4aec6-9273-4c22-957b-31e8612a8235,1,"[0.625, 0.625, 0.625, 0.625, 0.625]","[0.9, 1.0, 0.95, 0.95, 0.95]",DenseGrounding,0.6250000000000001
544084,When to retrain a machine learning model,"A significant challenge in maintaining real-world machine learning models is responding to the continuous and unpredictable evolution of data. Most practitioners are faced with the difficult question: when should I retrain or update my machine learning model? This seemingly straightforward problem is particularly challenging for three reasons: 1) decisions must be made based on very limited information - we usually have access to only a few examples, 2) the nature, extent, and impact of the distribution shift are unknown, and 3) it involves specifying a cost ratio between retraining and poor performance, which can be hard to characterize. Existing works address certain aspects of this problem, but none offer a comprehensive solution. Distribution shift detection falls short as it cannot account for the cost trade-off; the scarcity of the data, paired with its unusual structure, makes it a poor fit for existing offline reinforcement learning methods, and the online learning formulation overlooks key practical considerations.
To address this, we present a principled formulation of the retraining problem and propose an uncertainty-based method that makes decisions by continually forecasting the evolution of model performance evaluated with a bounded metric. Our experiments, addressing classification tasks, show that the method consistently outperforms existing baselines on 7 datasets. We thoroughly assess its robustness to varying cost trade-off values and mis-specified cost trade-offs.",2025,0.5454543471075102,0.5409243483318122,0.6,0.625,fced44b5-f2d8-41d6-8429-52a4994ab96f,0,"[0.25, 0.5, 0.625, 0.625]","[0.95, 1.0, 0.9, 0.9]",Uncertainty-based retraining,0.485364788777078
544091,SwitchLoRA: Switched Low-Rank Adaptation Can Learn Full-Rank Information,"In the training of large language models, parameter-efficient techniques such as LoRA optimize memory usage and reduce communication overhead during the fine-tuning phase. However, applying such techniques directly during the pre-training phase results in poor performance, primarily because the premature implementation of low-rank training significantly reduces model accuracy. Existing methods like ReLoRA and GaLore have attempted to address this challenge by updating the low-rank subspace. However, they still fall short of achieving the accuracy of full-rank training because they must limit the update frequency to maintain optimizer state consistency, hindering their ability to closely approximate full-rank training behavior. In this paper, we introduce SwitchLoRA, a parameter-efficient training technique that frequently and smoothly replaces the trainable parameters of LoRA adapters with alternative parameters. SwitchLoRA updates the low-rank subspace incrementally, targeting only a few dimensions at a time to minimize the impact on optimizer states. This allows a higher update frequency, thereby enhancing accuracy by enabling the updated parameters to more closely mimic full-rank behavior during the pre-training phase. Our results demonstrate that SwitchLoRA actually surpasses full-rank training, reducing perplexity from 15.23 to 15.01 on the LLaMA 1.3B model while reducing communication overhead by 54\% on the LLaMA 1.3B model. Furthermore, after full fine-tuning the SwitchLoRA pre-trained model and the full-rank pre-trained model on the GLUE benchmark, the SwitchLoRA pre-trained model showed an average accuracy gain of about 1\% over the full-rank pre-trained model. This demonstrates enhanced generalization and reasoning capabilities of SwitchLoRA.",2025,0.5113634504132908,0.5072556421275956,0.5333333333333333,0.5,66705006-9d17-40a9-b693-a74387d72f97,0,"[0.25, 0.5, 0.5, 0.625]","[0.95, 0.9, 0.9, 0.9]",SwitchLoRA,0.4537306201550388
544092,DAS-GNN: Degree-Aware Spiking Graph Neural Networks for Graph Classification,"The recent integration of spiking neurons into graph neural networks has been gaining much attraction due to its superior energy efficiency. Especially because the sparse connection among graph nodes fits the nature of the spiking neural networks, spiking graph neural networks are considered strong alternatives to vanilla graph neural networks. However, there is still a large performance gap for graph tasks between the spiking neural networks and artificial neural networks. The gaps are especially large when they are adapted to graph classification tasks, where none of the nodes in the test set graphs are connected to the training set graphs. We diagnose the problem as the existence of neurons under starvation, caused by the sparse connections among the nodes and the neurons. To alleviate the problem, we propose DAS-GNN. Based on a set of observations on spiking neurons on graph classification tasks, we devise several techniques to utilize more neurons to deliver meaningful information to the connected neurons. Experiments on diverse datasets show significant improvements compared to the baselines, demonstrating the effectiveness of the DAS-GNN.",2025,0.727272826446245,0.7260728265202324,0.6666666666666666,0.5,a0df1535-9d58-4ea0-9c40-4f0be8d4b24c,0,"[0.5, 0.625, 0.875]","[0.95, 0.95, 0.95]",DAS-GNN,0.6666666666666667
544105,Minimax Optimal Reinforcement Learning with Quasi-Optimism,"In our quest for a reinforcement learning (RL) algorithm that is both practical and provably optimal, we introduce EQO (Exploration via Quasi-Optimism). Unlike existing minimax optimal approaches, EQO avoids reliance on empirical variances and employs a simple bonus term proportional to the inverse of the state-action visit count. Central to EQO is the concept of *quasi-optimism*, where estimated values need not be fully optimistic, allowing for a simpler yet effective exploration strategy. The algorithm achieves the sharpest known regret bound for tabular RL under the mildest assumptions, proving that fast convergence can be attained with a practical and computationally efficient approach. Empirical evaluations demonstrate that EQO consistently outperforms existing algorithms in both regret performance and computational efficiency, providing the best of both theoretical soundness and practical effectiveness.",2025,0.8181815206612653,0.815015977224559,0.8,0.625,08561405-f784-4714-a288-9e8712280ea5,1,"[0.625, 0.625, 0.875, 0.875]","[0.95, 0.95, 0.9, 0.95]",Quasi-optimism,0.7424537487828627
544167,Investigating the Pre-Training Dynamics of In-Context Learning: Task Recognition vs. Task Learning,"The emergence of in-context learning (ICL) is potentially attributed to two major abilities: task recognition (TR) for recognizing the task from demonstrations and utilizing pre-trained priors, and task learning (TL) for learning from demonstrations. However, relationships between the two abilities and how such relationships affect the emergence of ICL is unclear. In this paper, we take the first step by examining the pre-training dynamics of the emergence of ICL. With carefully designed metrics, we find that these two abilities are, in fact, competitive during pre-training. Moreover, we observe a negative correlation between the competition and the performance of ICL. Further analysis of common pre-training factors (i.e., model size, dataset size, and data curriculum) demonstrates possible ways to regulate the competition. Based on these insights, we propose a simple yet effective method to better integrate these two abilities for ICL at inference time. Through adaptive ensemble learning, the performance of ICL can be significantly boosted, enabling two small models to outperform a larger one with more than twice the parameters.",2025,0.7499997272728265,0.754281830835281,0.6666666666666666,0.625,3e327d92-e546-4efb-bcbe-37c916e9e8d7,1,"[0.625, 0.625, 0.625, 0.875]","[0.9, 0.9, 0.9, 1.0]",In-Context Learning,0.7168522267206481
544174,Adversarial Training of Two-Layer Polynomial and ReLU Activation Networks via Convex Optimization,"Training neural networks which are robust to adversarial attacks remains an important problem in deep learning, especially as heavily overparameterized models are adopted in safety-critical settings. Drawing from recent work which reformulates the training problems for two-layer ReLU and polynomial activation networks as convex programs, we devise a convex semidefinite program (SDP) for adversarial training of two-layer polynomial activation networks and prove that the convex SDP achieves the same globally optimal solution as its nonconvex counterpart. The convex adversarial SDP is observed to improve robust test accuracy against $\ell_\infty$
 attacks relative to the original convex training formulation on multiple datasets. Additionally, we present scalable implementations of adversarial training for two-layer polynomial and ReLU networks which are compatible with standard machine learning libraries and GPU acceleration. Leveraging these implementations, we retrain the final two fully connected layers of a Pre-Activation ResNet-18 model on the CIFAR-10 dataset with both polynomial and ReLU activations. The two `robustified' models achieve significantly higher robust test accuracies against $\ell_\infty$ attacks than a Pre-Activation ResNet-18 model trained with sharpness-aware minimization, demonstrating the practical utility of convex adversarial training on large-scale problems.",2025,0.4090907603306326,0.400852989930101,0.4,0.25,f3af024a-fe2e-46d3-b54f-0ff7029c9431,0,"[0.25, 0.25, 0.5, 0.5]","[0.95, 0.95, 0.9, 0.8]",Convex Adversarial SDP,0.3472146735955271
544194,Pathologies of Out-of-Distribution Detection,"There is a proliferation of out-of-distribution (OOD) detection methods in deep learning which aim to detect distribution shifts and improve model safety. These methods often rely on supervised learning to train models with in-distribution data and then use the models’ predictive uncertainty or features to identify OOD points. In this paper, we critically re-examine this popular family of OOD detection procedures, revealing deep-seated pathologies. In contrast to prior work, we argue that these procedures are fundamentally answering the wrong question for OOD detection, with no easy fix. Uncertainty-based methods incorrectly conflate high uncertainty with being OOD, and feature-based methods incorrectly conflate far feature-space distance with being OOD. Moreover, there is no reason
to expect a classifier trained only on in-distribution classes to be able to identify OOD points; for example, we should not necessarily expect a cat-dog classifier to be uncertain about the label of an airplane, which may share features with a cat that help distinguish cats from dogs, despite generally appearing nothing alike. We show how these pathologies manifest as irreducible errors in OOD detection and identify common settings where these methods are ineffective. Additionally, interventions to improve OOD detection such as feature-logit hybrid methods, scaling of model and data size, Bayesian (epistemic) uncertainty representation, and outlier exposure also fail to address the fundamental misspecification.",2025,0.5113634504132908,0.509599404039722,0.5333333333333333,0.5,83387ab2-693e-441e-ba9c-06f5a2fd958e,0,"[0.25, 0.5, 0.5, 0.625]","[0.95, 0.9, 0.9, 0.95]",Pathologies,0.4647344559585492
544226,Explainable Graph Representation Learning via Graph Pattern Analysis,"Explainable artificial intelligence (XAI) is an important area in the AI community, and interpretability is crucial for building robust and trustworthy AI models. While previous work has explored model-level and instance-level explainable graph learning, there has been limited investigation into explainable graph representation learning.
In this paper, we focus on representation-level explainable graph learning and ask a fundamental question: What specific information about a graph is captured in graph representations? Our approach is inspired by graph kernels, which evaluate graph similarities by counting substructures within specific graph patterns. Although the pattern counting vector can serve as an explainable representation, it has limitations such as ignoring node features and being high-dimensional.
To address these limitations, we introduce a framework for learning and explaining graph representations through graph pattern analysis. We start by sampling graph substructures of various patterns. Then, we learn the representations of these patterns and combine them using a weighted sum, where the weights indicate the importance of each graph pattern's contribution.
We also provide theoretical analyses of our methods, including robustness and generalization. In our experiments, we show how to learn and explain graph representations for real-world data using pattern analysis. Additionally, we compare our method against multiple baselines in both supervised and unsupervised learning tasks to demonstrate its effectiveness.",2025,0.6477270371901683,0.6462041294645289,0.6666666666666666,0.625,df0eeceb-144e-4ecd-8d0f-c57b7cf2e4a5,0,"[0.5, 0.625, 0.625, 0.625]","[0.95, 0.95, 0.9, 0.95]",Graph Pattern Analysis,0.5918634371957154
544231,Large-Scale Multi-Agent Reinforcement Learning for Traffic Signal Optimization,"We present a novel approach to Traffic Signal Control (TSC) in a multi-agent environment by modeling communication among agents as a sequence problem, enabling intersections within road networks to communicate with one another. Taking inspiration from point cloud processing and graph neural networks, we make our architecture capable of handling variable road network topologies, including differing numbers of intersections and intersection types, and demonstrate this by successfully training on real & randomly generated road networks and traffic demands. Furthermore, we demonstrate that even utilizing minimal state information can achieve competitive performance.",2025,0.3409089669421938,0.3376236955200603,0.2666666666666666,0.25,25967804-8ff0-47f6-a525-4dafeb4eac77,0,"[0.25, 0.25, 0.25, 0.5]","[0.95, 0.95, 0.95, 0.9]",Traffic Signal Control,0.3011806231742941
544232,Generative Visual Instruction Tuning,"We propose to use automatically generated instruction-following data to improve the zero-shot capabilities of a large multimodal model with additional support for generative and image editing tasks. We achieve this by curating a new multimodal instruction-following set using GPT-4V and existing datasets for image generation and editing. Using this instruction set and the existing LLaVA-Finetune instruction set for visual understanding tasks, we produce GenLLaVA, a Generative Large Language and Visual Assistant. GenLLaVA is built through a strategy that combines three types of large pretrained models through instruction finetuning: Mistral for language modeling, SigLIP for image-text matching, and StableDiffusion for text-to-image generation. Our model demonstrates visual understanding capabilities superior to LLaVA and additionally demonstrates competitive results with native multimodal models such as Unified-IO 2, paving the way for building advanced general-purpose visual assistants by effectively re-using existing multimodal models.",2025,0.443181657024852,0.4363415967640403,0.4,0.25,683ab7c3-66b4-41cf-a19d-f4951dc0bcee,0,"[0.25, 0.25, 0.5, 0.625]","[1.0, 1.0, 1.0, 0.9]",GenLLaVA,0.3801708633093525
544233,Overcoming Lookback Window Limitations: Exploring Longer Windows in Long-Term Time Series Forecasting,"Long-term time series forecasting (LTSF) aims to predict future trends based on historical data. While longer lookback windows theoretically provide more comprehensive insights, current Transformer-based models face the Lookback Window Limitation (LWL). On one hand, longer windows introduce redundant information, which can hinder model learning. On the other hand, Transformers tend to overfit temporal noise rather than extract meaningful temporal information when dealing with longer sequences, compounded by their quadratic complexity. In this paper, we aim to overcome LWL, enabling models to leverage more historical information for improved performance. Specifically, to mitigate information redundancy, we introduce the Information Bottleneck Filter (IBF), which applies information bottleneck theory to extract essential subsequences from the input. Additionally, to address the limitations of the Transformer architecture in handling long sequences, we propose the Hybrid-Transformer-Mamba (HTM), which combines the linear complexity and long-range modeling capabilities of Mamba with the Transformer's strength in modeling short sequences. We integrate these two model-agnostic modules into various existing methods and conduct experiments on seven datasets. The results demonstrate that incorporating these modules effectively overcomes the lookback window limitations. Notably, by combining them with the Patch strategy, we design the PIH (\textbf{P}atch-\textbf{I}BF-\textbf{H}TM), successfully extending the window length to 1024—a significantly larger window than previously achieved—and achieving state-of-the-art results, highlighting the potential of exploring even longer windows.",2025,0.3068180702479745,0.3027297420337144,0.2666666666666666,0.25,82493b26-6a89-4a9f-9ccf-5505584d8f4a,0,"[0.0, 0.25, 0.25, 0.625]","[1.0, 0.95, 0.9, 0.95]",Information Bottleneck Filter,0.2593737824345173
544244,Bridging and Modeling Correlations in Pairwise Data for Direct Preference Optimization,"Direct preference optimization (DPO), a widely adopted offline preference optimization algorithm, aims to align large language models (LLMs) with human-desired behaviors using pairwise preference data. However, the generation of the winning response and the losing response within pairwise data are typically isolated, leading to weak correlations between them as well as suboptimal alignment performance. To address this issue, we propose an effective framework for Bridging and Modeling Correlations in pairwise data, named BMC. Firstly, we increase the consistency and informativeness of the pairwise preference signals through targeted modifications, synthesizing a pseudo-winning response by improving the losing response with the winning response as a reference. Secondly, we identify that DPO alone is insufficient to model these correlations and capture nuanced variations. Therefore, we propose learning token-level correlations by dynamically leveraging the policy model's confidence during training. Comprehensive experiments on QA, math, and instruction-following tasks demonstrate the effectiveness of our approach, significantly surpassing competitive baselines, including DPO. Additionally, our in-depth quantitative analysis reveals the reasons behind our method's superior performance over DPO and showcases its versatility to other DPO variants.",2025,0.6818179338843877,0.6806929345162506,0.6666666666666666,0.625,e4396b6c-ac61-4d5a-bbfa-7a6802ffe22e,1,"[0.625, 0.625, 0.625]","[0.95, 0.95, 0.9]",BMC,0.625
544255,Neuro2Semantic: A Transfer Learning Framework for Semantic Reconstruction of Continuous Language from Human Intracranial EEG,"Decoding continuous language from neural signals remains a significant challenge in the intersection of neuroscience and artificial intelligence. We introduce Neuro2Semantic, a novel framework that reconstructs the semantic content of perceived speech from intracranial EEG (iEEG) recordings. Our approach consists of two phases: first, an LSTM-based adapter aligns neural signals with pre-trained text embeddings; second, a corrector module generates continuous, natural text directly from these aligned embeddings. This flexible method overcomes the limitations of previous decoding approaches and enables unconstrained text generation. Neuro2Semantic achieves remarkable performance with as little as 30 minutes of neural data, significantly outperforming a recent state-of-the-art method in low-data settings. These results highlight the potential for practical applications in brain-computer interfaces and neural decoding technologies.",2025,0.3409089669421938,0.3385069626718885,0.2666666666666666,0.25,fbff9167-23d7-4297-8b41-844f49aeea83,0,"[0.25, 0.25, 0.25, 0.5]","[0.9, 0.95, 0.95, 0.9]",Neuro2Semantic,0.3044689119170984
544263,SAGEPhos: Sage Bio-Coupled and Augmented Fusion for Phosphorylation Site Detection,"Phosphorylation site prediction based on kinase-substrate interaction plays a vital role in understanding cellular signaling pathways and disease mechanisms. Computational methods for this task can be categorized into kinase-family-focused and individual kinase-targeted approaches. Individual kinase-targeted methods have gained prominence for their ability to explore a broader protein space and provide more precise target information for kinase inhibitors. However, most existing individual kinase-based approaches focus solely on sequence inputs, neglecting crucial structural information. To address this limitation, we introduce SAGEPhos (Structure-aware kinAse-substrate bio-coupled and bio-auGmented nEtwork for Phosphorylation site prediction), a novel framework that modifies the semantic space of main protein inputs using auxiliary inputs at two distinct modality levels. At the inter-modality level, SAGEPhos introduces a Bio-Coupled Modal Fusion method, distilling essential kinase sequence information to refine task-oriented local substrate feature space, creating a shared semantic space that captures crucial kinase-substrate interaction patterns. Within the substrate's intra-modality domain, it focuses on Bio-Augmented Fusion, emphasizing 2D local sequence information while selectively incorporating 3D spatial information from predicted structures to complement the sequence space. Moreover, to address the lack of structural information in current datasets, we contribute a new, refined phosphorylation site prediction dataset, which incorporates crucial structural elements and will serve as a new benchmark for the field. Experimental results demonstrate that SAGEPhos significantly outperforms baseline methods, notably achieving almost 10\% and 12\% improvements in prediction accuracy and AUC-ROC, respectively. We further demonstrate our algorithm's robustness and generalization through stable results across varied data partitions and significant improvements in zero-shot scenarios. These results underscore the effectiveness of constructing a larger and more precise protein space in advancing the state-of-the-art in phosphorylation site prediction. We release the SAGEPhos models and code at https://github.com/ZhangJJ26/SAGEPhos.",2025,0.6818179338843877,0.6806929345162506,0.6666666666666666,0.625,31455c6f-939e-429b-9a9b-155f709713db,1,"[0.625, 0.625, 0.625]","[0.95, 0.8, 0.9]",SAGEPhos,0.625
544264,UniAdapt: A Universal Adapter for Knowledge Calibration,"Large Language Models (LLMs) require frequent updates to correct errors and keep pace with continuously evolving knowledge in a timely and effective manner. Recent research in *it model editing* has highlighted the challenges in balancing generalization and locality, especially in the context of *lifelong model editing*. We discover that inserting knowledge directly into the model often causes conflicts and potentially disrupts other unrelated pre-trained knowledge. To address this problem, we introduce UniAdapt, a universal adapter for knowledge calibration. Inspired by the Mixture of Experts architecture and Retrieval-Augmented Generation, UniAdapt is designed with a vector-assisted router that is responsible for routing inputs to appropriate experts. The router maintains a vector store, including multiple shards, to construct routing vectors based on semantic similarity search results. UniAdapt is fully model-agnostic and designed for seamless plug-and-play integration. Experimental results show that UniAdapt outperforms existing lifelong model editors and achieves exceptional results in most metrics.",2025,0.613636140495949,0.6117164135215023,0.6,0.5,c2c640cf-d1ee-4d37-bbf8-1ba3330f326c,0,"[0.5, 0.5, 0.625, 0.625]","[0.9, 1.0, 0.9, 0.95]",UniAdapt,0.5569474952827409
544291,Diffusion Bridge AutoEncoders for Unsupervised Representation Learning,"Diffusion-based representation learning has achieved substantial attention due to its promising capabilities in latent representation and sample generation. Recent studies have employed an auxiliary encoder to identify a corresponding representation from data and to adjust the dimensionality of a latent variable $\mathbf{z}$. Meanwhile, this auxiliary structure invokes an *information split problem*; the information of each data instance $\mathbf{x}_0$ is divided into diffusion endpoint $\mathbf{x}_T$ and encoded $\mathbf{z}$ because there exist two inference paths starting from the data. The latent variable modeled by diffusion endpoint $\mathbf{x}_T$ has some disadvantages. The diffusion endpoint $\mathbf{x}_T$ is computationally expensive to obtain and inflexible in dimensionality. To address this problem, we introduce Diffusion Bridge AuteEncoders (DBAE), which enables $\mathbf{z}$-dependent endpoint $\mathbf{x}_T$ inference through a feed-forward architecture. This structure creates an information bottleneck at $\mathbf{z}$, so $\mathbf{x}_T$ becomes dependent on $\mathbf{z}$ in its generation. This results in $\mathbf{z}$ holding the full information of data. We propose an objective function for DBAE to enable both reconstruction and generative modeling, with their theoretical justification. Empirical evidence supports the effectiveness of the intended design in DBAE, which notably enhances downstream inference quality, reconstruction, and disentanglement. Additionally, DBAE generates high-fidelity samples in the unconditional generation. Our code is
available at https://github.com/aailab-kaist/DBAE.",2025,0.8522724173554846,0.8508661681453132,0.9333333333333332,0.875,2ab4232d-494c-4c52-a234-b36f9f6c2fe0,1,"[0.5, 0.875, 0.875, 0.875]","[0.9, 0.9, 0.9, 0.9]",Diffusion Bridge AutoEncoders,0.7812499999964042
544327,Uncovering Gaps in How Humans and LLMs Interpret Subjective Language,"Humans often rely on subjective natural language to direct language models (LLMs); for example, users might instruct the LLM to write an *enthusiastic* blogpost, while developers might train models to be *helpful* and *harmless* using LLM-based edits. The LLM’s *operational semantics* of such subjective phrases---how it adjusts its behavior when each phrase is included in the prompt---thus dictates how aligned it is with human intent. In this work, we uncover instances of *misalignment* between LLMs' actual operational semantics and what humans expect. Our method, TED (thesaurus error detector), first constructs a thesaurus that captures whether two phrases have similar operational semantics according to the LLM. It then elicits failures by unearthing disagreements between this thesaurus and a human-constructed reference. TED routinely produces surprising instances of misalignment; for example, Mistral 7B Instruct produces more *harassing* outputs when it edits text to be *witty*, and Llama 3 8B Instruct produces *dishonest* articles when instructed to make the articles *enthusiastic*. Our results demonstrate that humans can uncover unexpected LLM behavior by scrutinizing relationships between abstract concepts, without supervising outputs directly.",2025,0.886363314049704,0.883061310284889,0.9333333333333332,0.875,c448aadf-bf94-49fd-8d55-aee9bdd73a0f,1,"[0.625, 0.875, 0.875, 0.875]","[0.95, 0.9, 0.9, 0.95]",Operational Semantics,0.8044689118962354
544345,Unify ML4TSP: Drawing Methodological Principles for TSP and Beyond from Streamlined Design Space of Learning and Search,"Despite the rich works on machine learning (ML) for combinatorial optimization (CO), a unified, principled framework remains lacking. This study utilizes the Travelling Salesman Problem (TSP) as a major case study, with adaptations demonstrated for other CO problems, dissecting established mainstream learning-based solvers to outline a comprehensive design space. We present ML4TSPBench, which advances a unified modular streamline incorporating existing technologies in both learning and search for transparent ablation, aiming to reassess the role of learning and discern which parts of existing techniques are genuinely beneficial and which are not. This further leads to the investigation of desirable principles of learning designs and the exploration of concepts guiding method designs. We demonstrate the desirability of principles such as joint probability estimation, symmetry solution representation, and online optimization for learning-based designs. Leveraging the findings, we propose enhancements to existing methods to compensate for their missing attributes, thereby advancing performance and enriching the technique library. From a higher viewpoint, we also uncover a performance advantage in non-autoregressive and supervised paradigms compared to their counterparts. The strategic decoupling and organic recompositions yield a factory of new TSP solvers, where we investigate synergies across various method combinations and pinpoint the optimal design choices to create more powerful ML4TSP solvers, thereby facilitating and offering a reference for future research and engineering endeavors.",2025,0.7159088305786071,0.7115506511780889,0.6666666666666666,0.625,e9b0a66c-b042-4eb5-a2e6-e6cea5ce497f,1,"[0.5, 0.625, 0.625, 0.875]","[0.95, 1.0, 0.9, 0.9]",ML4TSPBench,0.642497339224179
544350,POINTS: Improving Your Vision-language Model with Affordable Strategies,"In recent years, vision-language models have achieved significant advancements, excelling in tasks once deemed challenging, such as optical character recognition and geometric problem-solving. Despite these impressive achievements, several critical issues remain unaddressed: 1) Proprietary models rarely disclose detailed information about their architectures. In contrast, while open-source models provide visibility into their training strategies, detailed ablations of these strategies are highly anticipated. 2) Pre-training data is currently under-explored in open-source works, with most efforts empirically adding datasets from diverse sources, making the entire process elusive and cumbersome. 3) During the fine-tuning stage, the focus is often on adding and ablating more datasets, which frequently leads to diminishing returns. Therefore, refining data schemes is essential for further enhancing model performance.
To address these issues, we propose the following contributions in this paper: 1) We trained a robust baseline model, leveraging the latest technological advancements in vision-language models. Building upon existing advancements, we introduced effective improvements and conducted comprehensive ablation and validation for each technique incorporated into this strong baseline.
2) Inspired by recent work on large language models, we propose filtering pre-training data using perplexity, selecting the data with the lowest perplexity as the training set. This approach allowed us to train on a curated 1M dataset, resulting in highly competitive performance. 3) During the visual instruction tuning stage, we experimented with model soup on different datasets when further introducing more datasets into the training set brought marginal improvements. Integrating these innovations, we obtained a model with 9B parameters, performing competitively with a series of existing state-of-the-art models. Additionally, these strategies we propose are efficient and relatively lightweight, allowing the community to adopt them easily for their models.",2025,0.4772725537190714,0.4755778266182522,0.5333333333333333,0.5,0975960a-5a44-4623-b2b2-f44ea30ac769,0,"[0.25, 0.5, 0.5, 0.5]","[0.95, 0.95, 0.95, 0.9]",Perplexity,0.4337268743914313
544367,NeuralMark: Advancing White-Box Neural Network Watermarking,"As valuable digital assets, deep neural networks require ownership protection, making neural network watermarking (NNW) a promising solution. In this paper, we propose a *NeuralMark* method to advance white-box NNW, which can be seamlessly integrated into various network architectures. NeuralMark first establishes a hash mapping between the secret key and the watermark, enabling resistance to forging attacks. The watermark then functions as a filter to select model parameters for embedding, providing resilience against overwriting attacks. Furthermore, NeuralMark utilizes average pooling to defend against fine-tuning and pruning attacks. Theoretically, we analyze its security boundary. Empirically, we verify its effectiveness across 14 distinct Convolutional and Transformer architectures, covering five image classification tasks and one text generation task. The source codes are available at https://anonymous.4open.science/r/NeuralMark.",2025,0.5795452438017296,0.5776686974913471,0.5333333333333333,0.5,1bfe5036-1d0d-4580-ab60-79232442a25c,0,"[0.5, 0.5, 0.5, 0.625]","[0.9, 0.95, 0.95, 0.9]",NeuralMark,0.5272344559585492
544383,Unlocking the Power of Function Vectors for Characterizing and Mitigating Catastrophic Forgetting in Continual Instruction Tuning,"Catastrophic forgetting (CF) poses a significant challenge in machine learning, where a model forgets previously learned information upon learning new tasks. 
Despite the advanced capabilities of Large Language Models (LLMs), they continue to face challenges with CF during continual learning. The majority of existing research focuses on analyzing forgetting patterns through a singular training sequence, thereby overlooking the intricate effects that diverse tasks have on model behavior.
Our study explores CF across various settings, discovering that model forgetting is influenced by both the specific training tasks and the models themselves. To this end, we interpret forgetting by examining the function vector (FV), a compact representation of functions in LLMs, offering a model-dependent indicator for the occurrence of CF. Through theoretical and empirical analyses, we demonstrated that CF in LLMs primarily stems from biases in function activation rather than the overwriting of task processing functions.
Leveraging these insights, we propose a novel function vector guided training methodology, incorporating a regularization technique to stabilize the FV and mitigate forgetting. Empirical tests on four benchmarks confirm the effectiveness of our proposed training method, substantiating our theoretical framework concerning CF and model function dynamics.",2025,0.4772725537190714,0.4764850541613754,0.4666666666666667,0.0,4be19037-d88d-4fc3-9127-820df8f53ddd,1,"[0.0, 0.0, 0.875, 0.875]","[0.9, 0.95, 0.9, 0.95]",Function Vector,0.4374999999999999
544433,DocMIA: Document-Level Membership Inference Attacks against DocVQA Models,"Document Visual Question Answering (DocVQA) has introduced a new paradigm for end-to-end document understanding, and quickly became one of the standard benchmarks for multimodal LLMs. Automating document processing workflows, driven by DocVQA models, presents significant potential for many business sectors. However, documents tend to contain highly sensitive information, raising concerns about privacy risks associated with training such DocVQA models. One significant privacy vulnerability, exploited by the membership inference attack, is the possibility for an adversary to determine if a particular record was part of the model's training data. In this paper, we introduce two novel membership inference attacks tailored specifically to DocVQA models. These attacks are designed for two different adversarial scenarios: a white-box setting, where the attacker has full access to the model architecture and parameters, and a black-box setting, where only the model's outputs are available. Notably, our attacks assume the adversary lacks access to auxiliary datasets, which is more realistic in practice but also more challenging. Our unsupervised methods outperform existing state-of-the-art membership inference attacks across a variety of DocVQA models and datasets, demonstrating their effectiveness and highlighting the privacy risks in this domain.",2025,0.6818179338843877,0.6806929345162506,0.6666666666666666,0.625,02c1f8b5-5a1d-4082-8df5-04850fa16cde,1,"[0.625, 0.625, 0.625, 0.625, 0.625, 0.625]","[0.9, 0.9, 0.9, 0.9, 0.8, 1.0]",Membership Inference Attacks,0.6250000000000002
544458,Composing Unbalanced Flows for Flexible Docking and Relaxation,"Diffusion models have emerged as a successful approach for molecular docking, but they often cannot model protein flexibility or generate nonphysical poses. We argue that both these challenges can be tackled by framing the problem as a transport between distributions. Still, existing paradigms lack the flexibility to define effective maps between such complex distributions. To address this limitation, we propose Unbalanced Flow Matching, a generalization of Flow Matching (FM) that allows trading off sample efficiency with approximation accuracy and enables more accurate transport. Empirically, we apply Unbalanced FM on flexible docking and structure relaxation, demonstrating our ability to model protein flexibility and generate energetically favorable poses. On the PDBBind docking benchmark, our method FlexDock improves the docking performance while increasing the proportion of energetically favorable poses from 30% to 73%.",2025,0.7090906512397632,0.6867440224269262,0.9333333333333332,0.875,ef27f568-615b-42f3-ba41-5af8e6a1141c,1,"[0.0, 0.625, 0.875, 0.875, 0.875]","[1.0, 0.9, 0.9, 0.9, 0.8]",Unbalanced Flow Matching,0.5457746354520109
544460,Integrative Decoding: Improving Factuality via Implicit Self-consistency,"Self-consistency-based approaches, which involve repeatedly sampling multiple outputs and selecting the most consistent one as the final response, prove to be remarkably effective in improving the factual accuracy of large language models. Nonetheless, existing methods usually have strict constraints on the task format, largely limiting their applicability. In this paper, we present Integrative Decoding (ID), to unlock the potential of self-consistency in open-ended generation tasks. ID operates by constructing a set of inputs, each prepended with a previously sampled response, and then processes them concurrently, with the next token being selected by aggregating of all their corresponding predictions at each decoding step. In essence, this simple approach implicitly incorporates self-consistency in the decoding objective. Extensive evaluation shows that ID consistently enhances factuality over a wide range of language models, with substantial improvements on the TruthfulQA (+11.2%), Biographies (+15.4%) and LongFact (+8.5%) benchmarks. The performance gains amplify progressively as the number of sampled responses increases, indicating the potential of ID to scale up with repeated sampling.",2025,0.7499997272728265,0.7496694555109988,0.6666666666666666,0.625,1f56589b-01f3-49c2-b461-d08659291f2b,1,"[0.625, 0.625, 0.625, 0.875]","[0.9, 0.95, 0.95, 0.95]",Integrative Decoding,0.6912731256085687
544471,Generalist Policy for k-Server Problem on Graphs using Deep Reinforcement Learning with Action-Value Decomposition,"The online $k$-server problem on graphs is a fundamental computational problem that can model a wide range of practical problems, such as dispatching ambulances to serve accidents or dispatching taxis to serve ride requests. While most prior work on the $k$-server problem focused on online algorithms, reinforcement learning promises policies that require low computational effort during execution, which is critical in time-sensitive applications, such as ambulance dispatch. However, there exists no scalable reinforcement-learning approach for the $k$-server problem. To address this gap, we introduce a scalable computational approach for learning generalist policies. Besides scalability, the advantage of generalist policies is transferability: a generalist policy can be applied to an entire class of graphs without the need for retraining, which is crucial for practical applications, e.g., in ambulance dispatch problems where road conditions or demand distributions may change over time. We achieve scalability and transferability by introducing a novel architecture that decomposes the action-value into a global and a local term, estimated from a shared graph-convolution backbone. We evaluate our approach on a variety of graph classes, comparing to well-established baselines, demonstrating the performance and transferability of our generalist policies.",2025,0.3409089669421938,0.3376236955200603,0.2666666666666666,0.25,3d230c50-3d7f-465e-8925-fe31c9fec7e8,0,"[0.25, 0.25, 0.25, 0.5]","[0.95, 0.95, 0.95, 0.9]",Generalist Policies,0.3011806231742941
544479,Comprehensive Artistic Style Representation for Quantitative Evaluation,"Artistic style, a unique medium for artists to express creativity through elements like form, color, and composition, poses a challenge for computer vision due to its intricate patterns and nuanced aesthetics. Contemporary models, often reliant on specific datasets, face limitations in their generalizability and precision in identifying individual artists' styles. From an information theory perspective, we examine the limitations of fine-tuning and investigate techniques to disentangle content from style information. We note differences in artistic style representation between unimodal and multimodal models. As a result, we propose a plug-and-play approach designed to efficiently separate content information within Vision-Language Models (VLMs), preserving stylistic details. Furthermore, we present the WeART dataset, a large-scale art dataset with high-quality annotations, to evaluate the artistic style representation capabilities of models. Experimental results show that our method improves the performance of VLMs in style retrieval tasks across several datasets. We will publicly release the proposed dataset and code.",2025,0.2045453801653163,0.2006258018562768,0.2666666666666666,0.25,a8c205ca-ddda-4c39-91ce-296186c8d182,0,"[0.0, 0.25, 0.25, 0.25]","[1.0, 0.95, 0.95, 0.9]",Artistic Style Representation,0.1683072435587841
544480,Investigating Domain Gaps for Indoor 3D Object Detection,"As a fundamental task for indoor scene understanding, 3D object detection has been extensively studied, and the accuracy on indoor point cloud data has been substantially improved. However, existing researches have been conducted on limited datasets, where the training and testing sets share the same distribution. In this paper, we consider the task of adapting indoor 3D object detectors from one dataset to another, presenting a first benchmark with commonly used ScanNet and SUN RGB-D datasets, as well as our newly proposed large-scale SimRoom and SimHouse datasets by a 3D simulator with far greater number of objects and more precise annotations. Since indoor point cloud datasets are collected and constructed in different ways, the object detectors are likely to overfit to specific factors within each dataset, such as point cloud quality, room layout configuration, style and object size. We conduct experiments across datasets on different adaptation scenarios, analyzing the impact of different domain gaps on 3D object detectors. We observe that through our evaluated domain gap factors, synthetic-to-real adaptation is the most difficult adaptation hurdle to overcome. We also introduce several domain adaptation approaches to improve adaptation performances, providing a first baseline for domain adaptive indoor 3D object detection, hoping that future works may propose detectors with stronger generalization ability across domains.",2025,0.5454543471075102,0.5430905855266167,0.5333333333333333,0.5,519a486a-751e-472b-ab13-5f2b2a04fbbe,0,"[0.25, 0.5, 0.5, 0.625, 0.625]","[0.95, 0.9, 0.95, 0.95, 0.9]",Domain Adaptation,0.493736108304708
544495,Improving Ordinal Conformal Prediction by Stepwise Adaptive Posterior Alignment,"Ordinal classification (OC) is widely used in real-world applications to categorize instances into ordered discrete classes. In risk-sensitive scenarios, ordinal conformal prediction (OCP) is used to obtain a small contiguous prediction set containing ground-truth labels with a desired coverage guarantee. However, OC models often fail to accurately model the posterior distribution, which harms the prediction set obtained by OCP. Therefore, we introduce a new method called \textit{Adaptive Posterior Alignment Step-by-Step} (APASS), which reduces the distribution discrepancy to improve the downstream OCP performance. It is designed as a versatile, plug-and-play solution that is easily integrated into any OC model before OCP. APASS first employs an attention-based estimator to adaptively estimate the variance of the posterior distribution using the information in the calibration set, then utilizes a stepwise temperature scaling algorithm to align the posterior variance predicted by OC models to the better variance estimation. Extensive evaluations on 10 real-world datasets demonstrate that APASS consistently boosts the OCP performance of 5 popular OC models.",2025,0.4545456528924899,0.4546059495829803,0.5333333333333333,0.5,5ac9e47a-31b3-49ea-aaff-50aeeca1b4bb,0,"[0.25, 0.25, 0.5, 0.5, 0.5, 0.5]","[0.95, 0.9, 0.95, 0.9, 0.95, 0.95]",APASS,0.4200894335872806
544496,Agent-G: An Agentic Framework for Graph Retrieval Augmented Generation,"Given two knowledge sources, one containing unstructured documents and the other comprising structured graph knowledge bases, how can we effectively retrieve the relevant information to answer user questions?
While Retrieval-Augmented Generation (RAG) retrieves documents to assist the large language model (LLM) in question answering,
Graph RAG (GRAG) uses graph knowledge bases as an additional knowledge source.
However, there are many questions that require information from both sources, which complicates the scenario and makes hybrid retrieval essential.
The goal is to effectively leverage both sources to provide better answers to the questions.
Therefore, we propose Agent-G, a unified framework for GRAG, composed of an agent, a retriever bank, and a critic module.
Agent-G has the following advantages:
1) Agentic, it automatically improves the agent's action with self-reflection, 
2) Adaptive, it solves questions that require hybrid knowledge source with a single unified framework,
3) Interpretable, it justifies decision making and reduces hallucinations, and
4) Effective, it adapts to different GRAG settings and outperforms all baselines.

The experiments are conducted on two real-world GRAG benchmarks, namely STaRK and CRAG.
In STaRK, Agent-G shows relative improvements in Hit@1 of 47% in STaRK-MAG and 55% in TaRK-Prime.
In CRAG, Agent-G increases accuracy by 35% while reducing hallucination by 11%, both relatively.",2025,0.3409089669421938,0.3451396346258149,0.2666666666666666,0.25,348b7827-6871-4b79-ac0e-c334bf0f6c08,0,"[0.25, 0.25, 0.25, 0.5]","[0.8, 0.9, 0.9, 0.95]",Agent-G,0.3321062900303199
544520,Fantastic Copyrighted Beasts and How (Not) to Generate Them,"Recent studies show that image and video generation models can be prompted to reproduce copyrighted content from their training data, raising serious legal con- cerns about copyright infringement. Copyrighted characters (e.g., Mario, Batman) present a significant challenge: at least one lawsuit has already awarded damages based on the generation of such characters. Consequently, commercial services like DALL·E have started deploying interventions. However, little research has systematically examined these problems: (1) Can users easily prompt models to generate copyrighted characters, even if it is unintentional?; (2) How effective are the existing mitigation strategies? To address these questions, we introduce a novel evaluation framework with metrics that assess both the generated image’s similarity to copyrighted characters and its consistency with user intent, grounded in a set of popular copyrighted characters from diverse studios and regions. We show that state-of-the-art image and video generation models can still generate characters even if characters’ names are not explicitly mentioned, sometimes with only two generic keywords (e.g., prompting with “videogame, plumber” consistently gener- ates Nintendo’s Mario character). We also introduce semi-automatic techniques to identify such keywords or descriptions that trigger character generation. Using this framework, we evaluate mitigation strategies, including prompt rewriting and new approaches we propose. Our findings reveal that common methods, such as DALL·E’s prompt rewriting, are insufficient alone and require supplementary strategies like negative prompting. Our work provides empirical grounding for discussions on copyright mitigation strategies and offers actionable insights for model deployers implementing these safeguards.",2025,0.5454543471075102,0.5408241503318514,0.6,0.625,98a9b60d-88e1-4926-a751-5b7de97e40c0,1,"[0.25, 0.5, 0.625, 0.625]","[0.95, 0.9, 0.9, 0.9]",Copyright Infringement,0.4828349944629015
544527,OptiBench Meets ReSocratic: Measure and Improve LLMs for Optimization Modeling,"Large language models (LLMs) have exhibited their problem-solving abilities in mathematical reasoning. Solving realistic optimization (OPT) problems in application scenarios requires advanced and applied mathematics ability. However, current OPT benchmarks that merely solve linear programming are far from complex realistic situations. In this work, we propose **OptiBench**, a benchmark for End-to-end optimization problem-solving with human-readable inputs and outputs. **OptiBench** contains rich optimization problems, including linear and nonlinear programming with or without tabular data, which can comprehensively evaluate LLMs' solving ability. In our benchmark, LLMs are required to call a code solver to provide precise numerical answers.
Furthermore, to alleviate the data scarcity for optimization problems, and to bridge the gap between open-source LLMs on a small scale (e.g., Llama-3-8b) and closed-source LLMs (e.g., GPT-4), we further propose a data synthesis method namely ***ReSocratic***. Unlike general data synthesis methods that proceed from questions to answers, \ReSocratic first incrementally synthesizes formatted optimization demonstration with mathematical formulations step by step and then back-translates the generated demonstrations into questions. Based on this, we synthesize the ***ReSocratic-29k*** dataset. We further conduct supervised fine-tuning with ***ReSocratic-29k*** on multiple open-source models. Experimental results show that ***ReSocratic-29k*** significantly improves the performance of open-source models.",2025,0.772726628099408,0.7698016297422515,0.6666666666666666,0.625,b1495617-e0c8-4472-8684-3bcd13e2e7b5,1,"[0.625, 0.625, 0.875]","[0.95, 0.9, 0.9]",OptiBench,0.7008751353302056
544531,Do Vision Models Develop Human-Like Progressive Difficulty Understanding?,"When a human undertakes a test, their responses likely follow a pattern: if they answered an easy question (2x3) incorrectly, they would likely answer a more difficult one (2x3x4) incorrectly; and if they answered a difficult question correctly, they would likely answer the easy one correctly. Anything else hints at memorization. Do current visual recognition models exhibit a similarly structured learning capacity? In this work, we consider the task of image classification and study if those models' responses follow that pattern. Since real images aren't labeled with difficulty, we first create a dataset of 100 categories, 10 attributes, and 3 difficulty levels using recent generative models: for each category (e.g., dog) and attribute (e.g., occlusion), we generate images of increasing difficulty (e.g., a dog without occlusion, a dog only partly visible). We find that most of the models do in fact behave similarly to the aforementioned pattern around 80-90\% of the time. Using this property, we then present a new way to evaluate those models' image recognition ability. Instead of testing the model on every possible test image, we create an adaptive test akin to GRE, in which the model's performance on the current round of images determines the test images in the next round. This allows the model to skip over questions too easy/hard for itself, and helps us get its overall performance in fewer steps.",2025,0.3749998636364132,0.3735087379190618,0.4,0.0,e5b3dd72-e13c-425b-940f-c4d127099c94,0,"[0.0, 0.25, 0.5, 0.625]","[1.0, 0.95, 0.95, 1.0]",Progressive Difficulty,0.3391456343792633
544543,CMC-Bench: Towards a New Paradigm of Visual Signal Compression,"Ultra-low bitrate image compression is a challenging and demanding topic. With the development of Large Multimodal Models (LMMs), a Cross Modality Compression (CMC) paradigm of Image-Text-Image has emerged. Compared with traditional codecs, this semantic-level compression can **reduce image data size to 0.1% or even lower**, which has strong potential applications. However, CMC has certain defects in consistency with the original image and perceptual quality. To address this problem, we introduce CMC-Bench, a benchmark of the **cooperative performance of Image-to-Text (I2T) and Text-to-Image (T2I) models for image compression**. This benchmark covers 18,000 and 40,000 images respectively to verify 6 mainstream I2T and 12 T2I models, including 160,000 subjective preference scores annotated by human experts. At ultra-low bitrates, this paper proves that the combination of some I2T and T2I models has surpassed the most advanced visual signal codecs; meanwhile, it highlights where LMMs can be further optimized toward the compression task. We encourage LMM developers to participate in this test to promote the evolution of visual signal codec protocols.",2025,0.6818179338843877,0.6789253111038988,0.6,0.5,923e8e83-1012-490c-9001-ceb889b1cdb8,0,"[0.5, 0.5, 0.625, 0.875]","[1.0, 0.95, 0.95, 0.95]",CMC-Bench,0.615058910162003
544547,Learning positional encodings in transformers depends on initialization,"The attention mechanism is central to the transformer's ability to capture complex dependencies between tokens of an input sequence.
Key to the successful application of the attention mechanism in transformers is its choice of positional encoding (PE).
The PE provides essential information that distinguishes the position and order amongst tokens in a sequence.
Most prior investigations of PE effects on generalization were tailored to 1D input sequences, such as those presented in natural language, where adjacent tokens (e.g., words) are highly related.
In contrast, many real world tasks involve datasets with highly non-trivial positional arrangements, such as datasets organized in multiple spatial dimensions, or datasets for which ground truth positions are not known, such as in biological data.
Here we study the importance of learning accurate PE for problems which rely on a non-trivial arrangement of input tokens. 
Critically, we find that the choice of initialization of a learnable PE greatly influences its ability to learn accurate PEs that lead to enhanced generalization.
We empirically demonstrate our findings in a 2D relational reasoning task and a real world 3D neuroscience dataset, applying interpretability analyses to verify the learning of accurate PEs.
Overall, we find that a learned PE initialized from a small-norm distribution can 1) uncover interpretable PEs that mirror ground truth positions, 2) learn non-trivial and modular PEs in a real-world neuroscience dataset, and 3) lead to improved downstream generalization in both datasets.
Importantly, choosing an ill-suited PE can be detrimental to both model interpretability and generalization.
Together, our results illustrate the feasibility of learning identifiable and interpretable PEs for enhanced generalization.",2025,0.5795452438017296,0.5758291929051103,0.6666666666666666,0.625,14563037-8a2a-4f73-a2d3-e97e6a5f8b5a,0,"[0.25, 0.625, 0.625, 0.625]","[0.95, 0.9, 0.9, 0.95]",Positional Encoding,0.5192033678756478
544549,EventFlow: Forecasting Continuous-Time Event Data with Flow Matching,"Continuous-time event sequences, in which events occur at irregular intervals, are ubiquitous across a wide range of industrial and scientific domains. The contemporary modeling paradigm is to treat such data as realizations of a temporal point process, and in machine learning it is common to model temporal point processes in an autoregressive fashion using a neural network. While autoregressive models are successful in predicting the time of a single subsequent event, their performance can be unsatisfactory in forecasting longer horizons due to cascading errors. We propose $\texttt{EventFlow}$, a non-autoregressive generative model for temporal point processes. Our model builds on the flow matching framework in order to directly learn joint distributions over event times, side-stepping the autoregressive process. $\texttt{EventFlow}$ is likelihood-free, easy to implement and sample from, and either matches or surpasses the performance of state-of-the-art models in both unconditional and conditional generation tasks on a set of standard benchmarks.",2025,0.6477270371901683,0.6461790799645386,0.6666666666666666,0.625,0569d25f-aaf3-430f-a727-31118efeeae2,0,"[0.5, 0.625, 0.625, 0.625]","[0.9, 0.9, 0.95, 0.8]",EventFlow,0.5932959642261253
544552,Representing Signs as Signs: One-Shot ISLR to Facilitate Functional Sign Language Technologies,"Isolated Sign Language Recognition (ISLR) is crucial for scalable sign language technology, yet language-specific approaches limit current models. To address this, we propose a one-shot learning approach that generalises across languages and evolving vocabularies. Our method involves pretraining a model to embed signs based on essential features and using a dense vector search for rapid, accurate recognition of unseen signs. We achieve state-of-the-art results, including 50.8% one-shot MRR on a large dictionary containing 10,235 unique signs from a different language than the training set. Our approach is robust across languages and support sets, offering a scalable, adaptable solution for ISLR. Co-created with the Deaf and Hard of Hearing (DHH) community, this method aligns with real-world needs, and advances scalable sign language recognition.",2025,0.4090907603306326,0.4069977411885661,0.2666666666666666,0.25,020d0859-8766-4dbe-9d22-7935b93b36cf,0,"[0.25, 0.25, 0.25, 0.5, 0.625]","[0.95, 0.95, 1.0, 0.95, 0.95]",One-Shot ISLR,0.3669185872493266
544555,NEPENTHE: Entropy-Based Pruning as a Neural Network Depth's Reducer,"While deep neural networks are highly effective at solving complex tasks, their computational demands can hinder their usefulness in real-time applications and with limited-resources systems. Besides, it is a known fact that, for many downstream tasks, off-the-shelf models are over-parametrized. While classical structured pruning can reduce the network's width, the computation's critical path, namely the maximum number of layers encountered at forward propagation, apparently can not be reduced.

In this paper, we aim to reduce the depth of over-parametrized deep neural networks: we propose an e**N**tropy-bas**E**d **P**runing as a n**E**ural **N**etwork dep**TH**'s r**E**ducer (NEPENTHE) to alleviate deep neural networks' computational burden.
Based on our theoretical finding, NEPENTHE leverages ""unstructured'' pruning to bias sparsity enhancement in layers with low entropy to remove them entirely. We validate our approach on popular architectures such as MobileNet, Swin-T and RoBERTa, showing that, when in the overparametrization regime, some layers are linearizable (hence reducing the model's depth) with little to no performance loss. The code will be publicly available upon acceptance of the article.",2025,0.3749998636364132,0.3730545795931526,0.2666666666666666,0.25,86fc1871-3178-4dd7-904f-1fa445188a42,0,"[0.25, 0.25, 0.25, 0.625]","[0.95, 0.95, 1.0, 0.95]",NEPENTHE,0.3362941826215022
544558,Exploring The Forgetting in Adversarial Training: A Novel Method for Enhancing Robustness,"In recent years, there has been an explosion of research into developing robust deep neural networks against adversarial examples. As one of the most successful methods, Adversarial Training (AT)  has been widely studied before, but there is still a gap to achieve promising
clean and robust accuracy for many practical tasks. In this paper, we consider the AT problem from a new perspective which connects it to catastrophic forgetting in continual learning (CL). Catastrophic forgetting is a phenomenon in which neural networks forget old knowledge upon learning a new task. Although AT and CL are two different problems, we show that they actually share several  key properties in their training processes. Specifically, we conduct an empirical study and find that this forgetting phenomenon indeed occurs in adversarial robust training across multiple datasets (SVHN, CIFAR-10, CIFAR-100, and TinyImageNet) and perturbation models ($\ell_{\infty}$ and $\ell_{2}$). Based on this observation, we propose a novel method called Adaptive Multi-teachers Self-distillation (AMS), which leverages a carefully designed adaptive regularizer to mitigate the forgetting by aligning model outputs between new and old ``stages''. Moreover, our approach can be used  as a unified method to enhance multiple different AT algorithms. Our experiments demonstrate that our method can significantly enhance robust accuracy and meanwhile preserve high clean accuracy, under several popular adversarial attacks (e.g., PGD, CW, and Auto Attacks). As another benefit of our method, we discover that it can largely alleviate the robust overfitting issue of AT in our experiments.",2025,0.7363633685951387,0.7380149033633855,0.6666666666666666,0.625,b5005da1-9e2c-4590-96fa-e9f58962fa86,1,"[0.625, 0.625, 0.625, 0.625, 0.875]","[0.95, 0.95, 0.95, 0.9, 1.0]",Adaptive Multi-teachers Self-distillation,0.6907304938437119
544559,Learning Molecular Symmetry Breaking via Symmetry-adapted Neural Networks,"E(3)-equivariant neural networks have achieved remarkable performance in molecular modeling. However, the equivariance constraint limits the model's effectiveness in learning tasks involving symmetry breaking, particularly those that violate the celebrated Curie principle. Relaxing the equivariance constraint is essential for addressing these challenges. In this paper, we explore the intricate symmetry relationships between an object and its spontaneously symmetry-broken outcomes. We introduce a relaxed equivariance based on the molecule's inherent symmetries. Additionally, we develop SANN -- a symmetry-adapted neural network architecture that learns symmetry breaking through equivalence classes of atoms. SANN decomposes the molecular point cloud into sets of symmetry-equivalent atoms and performs message-passing both within and across these classes. We demonstrate the advantages of our method over invariant and equivariant models through synthetic tasks and show that SANN effectively learns both equivariance and symmetry breaking in various benchmark molecular modeling tasks.",2025,0.5795452438017296,0.5752334504488217,0.5333333333333333,0.5,2c023936-4f5b-4513-8c1c-e5e1ae8c39ee,0,"[0.5, 0.5, 0.5, 0.625]","[0.9, 0.9, 0.95, 0.8]",Symmetry-adapted Neural Networks,0.5205387834370907
544568,Linear Spherical Sliced Optimal Transport: A Fast Metric for Comparing Spherical Data,"Efficient comparison of spherical probability distributions becomes important in fields such as computer vision, geosciences, and medicine. Sliced optimal transport distances, such as spherical and stereographic spherical sliced Wasserstein distances, have recently been developed to address this need. These methods reduce the computational burden of optimal transport by slicing hyperspheres into one-dimensional projections, i.e., lines or circles. Concurrently, linear optimal transport has been proposed to embed distributions into $L^2$ spaces, where the $L^2$ distance approximates the optimal transport distance, thereby simplifying comparisons across multiple distributions. In this work, we introduce the Linear Spherical Sliced Optimal Transport (LSSOT) framework, which utilizes slicing to embed spherical distributions into $L^2$ spaces while preserving their intrinsic geometry, offering a computationally efficient metric for spherical probability measures. We establish the metricity of LSSOT and demonstrate its superior computational efficiency in applications such as cortical surface registration, 3D point cloud interpolation via gradient flow, and shape embedding. Our results demonstrate the significant computational benefits and high accuracy of LSSOT in these applications.",2025,0.5795452438017296,0.5659477097133248,0.6666666666666666,0.625,dab44859-b01a-4320-9554-93d5c9e469a5,1,"[0.0, 0.625, 0.625, 0.875]","[0.95, 0.7, 0.95, 0.9]",Linear Spherical Sliced Optimal Transport,0.4866725088517709
544592,GReaTer: Gradients Over Reasoning Makes Smaller Language Models Strong Prompt Optimizers,"The effectiveness of large language models (LLMs) is closely tied to the design of prompts, making prompt optimization essential for enhancing their performance across a wide range of tasks. Although recent advancements have focused on automating prompt engineering, many existing approaches rely exclusively on textual feedback, refining prompts based solely on inference errors identified by large, computationally expensive LLMs. Unfortunately, smaller models struggle to generate high-quality feedback, resulting in complete dependence on large LLM judgment. Moreover, these methods fail to leverage more direct and finer-grained information, such as gradients, due to operating purely in text space. To this end, we introduce, we introduce *GReaTer*, a novel prompt optimization technique that directly incorporates *gradient information over task-specific reasoning*. By utilizing task loss gradients, *GReaTer* enables self-optimization of prompts for smaller, lightweight language models (LM) without the need for costly closed-source LLMs, while maintaining reasonable prompt structures. This allows high-performance prompt optimization without dependence on massive LLMs, closing the gap between smaller models and the sophisticated reasoning often needed for prompt refinement. Extensive evaluations across diverse tasks demonstrate that \ours consistently outperforms previous methods, even those reliant on powerful LLMs. Additionally, *GReaTer*-optimized prompts frequently exhibit better transferability and, in some cases, boost task performance to levels comparable to or surpassing those achieved by larger language models, highlighting the effectiveness of *""gradient over reasoning""*-based prompt optimization. Code of *GReaTer* is available at: https://github.com/psunlpgroup/GreaTer",2025,0.772726628099408,0.7698016297422515,0.6666666666666666,0.625,a326eb30-19ac-40d4-a2be-ab39409c4895,1,"[0.625, 0.625, 0.875]","[0.9, 0.95, 0.9]",Gradient Over Reasoning,0.7008751353302056
544593,SGDF: A Method for Reducing Variance in Stochastic Gradient Descent via Filter Estimation,"In deep learning, stochastic gradient descent (SGD) and its momentum-based variants are widely used for optimization, but they typically suffer from slow convergence. Conversely, existing adaptive learning rate optimizers speed up convergence but often compromise generalization. To resolve this issue, we propose a novel optimization method designed to accelerate SGD's convergence without sacrificing generalization. Our approach reduces the variance of the historical gradient, improves first-order moment estimation of SGD by applying Wiener filter theory, and introduces a time-varying adaptive gain. Empirical results demonstrate that SGDF (SGD with Filter) effectively balances convergence and generalization compared to state-of-the-art optimizers.",2025,0.4363634776860081,0.4320918946352684,0.5333333333333333,0.5,c7780ae7-6ce5-4c3c-8d5b-5c2a281df2e4,0,"[0.25, 0.25, 0.5, 0.5, 0.5]","[0.9, 1.0, 0.95, 0.8, 0.95]",SGDF,0.3853512233278331
544609,Controlling Forgetting with Test-Time Data in Continual Learning,"Foundational vision-language models have shown impressive performance on various downstream tasks. Yet, there is still a pressing need to update these models later as new tasks or domains become available. Ongoing Continual Learning (CL) research provides techniques to overcome catastrophic forgetting of previous information when new knowledge is acquired. To date, CL techniques focus only on the supervised training sessions. This results in significant forgetting yielding inferior performance to even the prior model zero shot performance. In this work, we argue that test-time data hold great information that can be leveraged in a self supervised manner to refresh the model's memory of previous learned tasks and hence greatly reduce forgetting at no extra labelling cost. We study how unsupervised data can be employed online to improve models' performance on prior tasks upon encountering representative samples. We propose a simple yet effective student-teacher model with gradient based sparse parameters updates and show significant performance improvements and reduction in forgetting, which could alleviate the role of an offline episodic memory/experience replay buffer.",2025,0.7840906239670459,0.7827968746936882,0.8,0.875,bf74e818-d1b1-4cf5-85bf-acaa2f496313,0,"[0.5, 0.625, 0.875, 0.875]","[0.95, 0.95, 0.95, 0.95]",Test-Time Data,0.7187499999999999
544618,AnyECG: Foundational Models for Electrocardiogram Analysis,"Electrocardiogram (ECG), a non-invasive and affordable tool for cardiac monitoring, is highly sensitive in detecting acute heart attacks. However, due to the lengthy nature of ECG recordings, numerous machine learning methods have been developed for automated heart disease detection to reduce human workload. Despite these efforts, performance remains suboptimal. A key obstacle is the inherent complexity of ECG data, which includes heterogeneity (e.g., varying sampling rates), high levels of noise, demographic-related pattern shifts, and intricate rhythm-event associations. To overcome these challenges, this paper introduces AnyECG, a foundational model designed to extract robust representations from any real-world ECG data. Specifically, a tailored ECG Tokenizer encodes each fixed-duration ECG fragment into a token and, guided by proxy tasks, converts noisy, continuous ECG features into discrete, compact, and clinically meaningful local rhythm codes. These codes encapsulate basic morphological, frequency, and demographic information (e.g., sex), effectively mitigating signal noise. We further pre-train the AnyECG to learn rhythmic pattern associations across ECG tokens, enabling the capture of cardiac event semantics. By being jointly pre-trained on diverse ECG data sources, AnyECG is capable of generalizing across a wide range of downstream tasks where ECG signals are recorded from various devices and scenarios. Experimental results in anomaly detection, arrhythmia detection, corrupted lead generation, and ultra-long ECG signal analysis demon-
strate that AnyECG learns common ECG knowledge from data and significantly outperforms cutting-edge methods in each respective task.",2025,0.2727271735537551,0.2722771738065002,0.2666666666666666,0.25,50a8c3dc-4065-425e-8eb8-ea6b25db5adf,0,"[0.25, 0.25, 0.25, 0.25]","[1.0, 1.0, 1.0, 1.0]",AnyECG,0.25
544629,Towards Building Reliable Conditional Diffusion Models for Protein Generation,"Generating novel and functional protein sequences is critical to a wide
range of applications in biology. Recent advancements in conditional diffusion models have shown impressive empirical performance in protein generation tasks. However, reliable generations of protein remain an open research question in de novo protein design, especially when it comes to conditional diffusion models. Considering  the biological function of a protein is determined
by multi-level structures, we propose a novel multi-level conditional diffusion model that integrates both sequence-based
and structure-based information for efficient end-to-end protein design guided by
specified functions. By generating representations at different levels simultaneously, our framework can effectively model the inherent hierarchical relations between different levels, resulting in an informative and
discriminative representation of the generated protein. We also propose a Protein-MMD, a new reliable evaluation metric, to evaluate the quality of generated protein with conditional diffusion models. Our new metric is able to capture both distributional and functional similarities between real and generated protein sequences while ensuring conditional consistency. We experiment with standard datasets and the
results on protein generation tasks demonstrate the efficacy of the proposed generation framework and evaluation metric.",2025,0.3545453256198816,0.3494688416893383,0.2666666666666666,0.25,c66b4edb-8611-48d0-b486-4df576e42ad1,0,"[0.0, 0.25, 0.25, 0.5, 0.625]","[1.0, 0.95, 1.0, 0.95, 0.95]",Multi-level conditional diffusion model,0.3007098678661793
544635,GeoDream: Disentangling 2D and Geometric Priors for High-Fidelity and Consistent 3D Generation,"Text-to-3D generation by distilling pretrained large-scale text-to-image diffusion models has shown great promise but still suffers from inconsistent 3D geometric structures (Janus problems) and severe artifacts. The aforementioned problems mainly stem from 2D diffusion models lacking 3D awareness during the lifting. In this work, we present GeoDream, a novel method that incorporates explicit generalized 3D priors with 2D diffusion priors to enhance the capability of obtaining unambiguous 3D consistent geometric structures without sacrificing diversity or fidelity. Specifically, we first utilize a multi-view diffusion model to generate posed images and then construct cost volume from the predicted image, which serves as native 3D geometric priors, ensuring spatial consistency in 3D space. Subsequently, we further propose to harness 3D geometric priors to unlock the great potential of 3D awareness in 2D diffusion priors via a disentangled design. Notably, disentangling 2D and 3D priors allows us to refine 3D geometric priors further. We justify that the refined 3D geometric priors aid in the 3D-aware capability of 2D diffusion priors, which in turn provides superior guidance for the refinement of 3D geometric priors. Our numerical and visual comparisons demonstrate that GeoDream generates more 3D consistent textured meshes with high-resolution realistic renderings (i.e., 1024 * 1024) and adheres more closely to semantic coherence.",2025,0.3749998636364132,0.3743811139839378,0.2666666666666666,0.25,4e70f926-0d81-4cf7-88f7-ddfa93161f52,0,"[0.25, 0.25, 0.25, 0.625]","[0.95, 0.95, 0.95, 0.95]",GeoDream,0.34375
544638,Preference-based Credit Assignment for Reinforcement Learning with Delayed and Noised Rewards,"Credit assignment has been utilized as a common technique for determining the past key state-action pairs and assigning corresponding rewards that have strong relevance with the final outputs in reinforcement learning, especially for environments with delayed rewards. However, current reward function design methods rely heavily on domain knowledge and may not accurately reflect the actual reward that should be received, which will lead to noised reward assignments during the credit assignment process and deteriorate the performance of the agent. To address this issue, in this paper, by leveraging the benefits of Preference-based Reinforcement Learning (PbRL), we propose a novel trajectory preference-based credit assignment method, where each trajectory is assigned to one of three different preferences according to its related delayed reward and the entire trajectory space. Then, a causal Transformer framework is introduced to predict the relevance between the decisions at each timestep and the different trajectory preferences to guide the credit assignment. Despite the unavoidable noised reward related to each trajectory, we demonstrate that our method can still effectively guide agents to learn superior strategies. Experiments on the Mujoco task and the treatment of sepsis under extremely delayed reward setting show that our method can mitigate the adverse effects resulting from the delayed noised rewards and provide effective guidelines for agents.",2025,0.2727271735537551,0.2722771738065002,0.2666666666666666,0.25,46341e8a-9608-4173-a84c-3c99f6e7f86a,0,"[0.25, 0.25, 0.25, 0.25]","[0.95, 0.95, 0.9, 0.9]",Trajectory Preference-based Credit Assignment,0.25
544662,Generalization from Starvation: Hints of Universality in LLM Knowledge Graph Learning,"Motivated by interpretability and reliability, we investigate how neural networks represent knowledge during graph learning. We find hints of universality, where equivalent representations are learned across a range of model sizes (from $10^2$ to $10^9$ parameters) and contexts (MLP toy models, LLM in-context learning and LLM training). We show that these attractor representations optimize generalization to unseen examples by exploiting properties of knowledge graph relations (e.g. symmetry and meta-transitivity). We find experimental support for such universality by showing that LLMs and simpler neural networks can be successfully stitched, i.e., by stitching the first part of one model to the last part of another, mediated only by an affine or almost affine transformation. We hypothesize that this dynamic toward simplicity and generalization is driven by ``intelligence from starvation”: where overfitting is minimized by pressure to minimize the use of resources that are either scarce or competed for against other tasks.",2025,0.2727271735537551,0.2722771738065002,0.2666666666666666,0.25,8720dbc0-6bb7-4964-980d-b911ce5b1fcb,0,"[0.25, 0.25, 0.25, 0.25]","[0.9, 0.95, 0.95, 0.9]",Universality,0.2500000000000001
544670,Federated Continual Learning Goes Online: Uncertainty-Aware Memory Management for Vision Tasks and Beyond,"Given the ability to model more realistic and dynamic problems, Federated Continual Learning (FCL) has been increasingly investigated recently. A well-known problem encountered in this setting is the so-called catastrophic forgetting, for which the learning model is inclined to focus on more recent tasks while forgetting the previously learned knowledge. The majority of the current approaches in FCL propose generative-based solutions to solve said problem. However, this setting requires multiple training epochs over the data, implying an offline setting where datasets are stored locally and remain unchanged over time. Furthermore, the proposed solutions are tailored for vision tasks solely. To overcome these limitations, we propose a new approach to deal with different modalities in the online scenario where new data arrive in streams of mini-batches that can only be processed once. To solve catastrophic forgetting, we propose an uncertainty-aware memory-based approach.  Specifically, we suggest using an estimator based on the Bregman Information (BI) to compute the model's variance at the sample level. Through measures of predictive uncertainty, we retrieve samples with specific characteristics, and – by retraining the model on such samples – we demonstrate the potential of this approach to reduce the forgetting effect in realistic settings while maintaining data confidentiality and competitive communication efficiency compared to state-of-the-art approaches.",2025,0.7159088305786071,0.7071648104624139,0.6666666666666666,0.625,1ac77913-c8ff-466a-ac9a-3d1320da5550,1,"[0.5, 0.625, 0.625, 0.875]","[0.9, 0.95, 0.95, 0.8]",Uncertainty-Aware Memory Management,0.6337212441835698
544706,ZO-Offloading: Fine-Tuning LLMs with 100 Billion Parameters on a Single GPU,"Fine-tuning pre-trained LLMs typically requires a vast amount of GPU memory. Standard first-order optimizers like SGD face a significant challenge due to the large memory overhead from back-propagation as the size of LLMs increases, which necessitates caching activations during the forward pass and gradients during the backward pass. In contrast, zeroth-order (ZO) methods can estimate gradients with only two forward passes and without the need for activation caching. Additionally, CPU resources can be aggregated and offloaded to extend the memory and computational capacity of a single GPU.
To enable efficient fine-tuning of LLMs on a single GPU, we introduce ZO-Offloading, a framework that strategically utilizes both CPU and GPU resources for ZO. ZO-Offloading dynamically offloads model parameters to the CPU and retrieves them to the GPU as needed, ensuring continuous and efficient computation by reducing idle times and maximizing GPU utilization. Parameter updates are integrated with ZO's dual forward passes to minimize redundant data transfers, thereby improving the overall efficiency of the fine-tuning process. The ZO-Offloading framework also incorporates a novel low-bit precision technique for managing data transfers between the CPU and GPU in AMP mode, as well as asynchronous checkpointing for LLM fine-tuning.
With ZO-Offloading, for the first time, it becomes possible to fine-tune extremely large models, such as the OPT-175B with over $\textbf{175 billion}$ parameters, on a single GPU with just $\textbf{24GB}$ of memory—a feat unattainable with conventional methods. Moreover, our framework operates without any additional time cost compared to standard ZO methodologies.",2025,0.3749998636364132,0.3730545795931526,0.2666666666666666,0.25,49b85e3d-bbdb-43b5-9c31-071ca4ee95b3,0,"[0.0, 0.25, 0.25, 0.875]","[0.95, 1.0, 0.95, 0.95]",ZO-Offloading,0.3362941826215022
544742,Designing Deep Learning Programs with Large Language Models,"The process of utilizing deep neural architectures to solve tasks differs significantly from conventional programming due to its complexity and the need for specialized knowledge. While code generation technologies have made substantial progress, their application in deep learning programs requires a distinct approach. Although previous research has shown that large language model agents perform well in areas such as data science, neural architecture search, and hyperparameter tuning, the task of proposing and refining deep neural architectures at a high level remains largely unexplored. Current methods for automating the synthesis of deep learning programs often rely on basic code templates or API calls, which restrict the solution space to predefined architectures. In this paper, we aim to bridge the gap between traditional code generation and deep learning program synthesis by introducing the task of Deep Learning Program Design (DLPD), a task of designing an effective deep learning program for the task, along with appropriate architectures and techniques. We propose Deep Ones, a comprehensive solution for DLPD. Our solution includes a large-scale dataset and a lightweight benchmark specifically designed for DLPD. On our benchmark, Llama-3.1 8B, fine-tuned on our dataset, demonstrates better architecture suggestion capability than GPT-4o and better performance than Claude-3.5-Sonnet, showcasing that Deep Ones effectively addresses the challenge of DLPD. Deep Ones will be publicly available, including the dataset, benchmark, codes, and model weights.",2025,0.4090907603306326,0.4047367515372769,0.4,0.25,e44bc0f1-236d-4326-a51d-bc34e0d8a873,0,"[0.25, 0.25, 0.5, 0.5]","[0.95, 0.95, 0.9, 0.9]",Deep Learning Program Design,0.3589378238341969
544743,EEG-ImageNet: An Electroencephalogram Dataset and Benchmarks with Image Visual Stimuli of Multi-Granularity Labels,"Exploring how brain activity translates into visual perception offers valuable insights into the biological visual system's representation of the world. Recent advancements have enabled effective image classification and high-quality reconstruction using brain signals obtained through Functional Magnetic Resonance Imaging (fMRI) or magnetoencephalography (MEG). However, the cost and bulkiness of these technologies hinder their practical application. In contrast, Electroencephalography (EEG) presents advantages such as ease of use, affordability, high temporal resolution, and non-invasive operation, yet it remains underutilized in related research due to a shortage of comprehensive datasets. To fill this gap, we introduce EEG-ImageNet, a novel EEG dataset featuring recordings from 16 participants exposed to 4000 images sourced from the ImageNet dataset. This dataset offers five times the number of EEG-image pairs compared to existing benchmarks. EEG-ImageNet includes image stimuli labeled with varying levels of granularity, comprising 40 images with coarse labels and 40 with fine labels. We establish benchmarks for both object classification and image reconstruction based on this dataset. Experiments with several commonly used models show that the best-performing models can achieve object classification with an accuracy around 60% and image reconstruction with two-way identification around 64%. These findings highlight the dataset's potential to enhance EEG-based visual brain-computer interfaces, deepen our understanding of visual perception in biological systems, and suggest promising applications for improving machine vision models.",2025,0.443181657024852,0.4394346654584821,0.4,0.25,397a6037-759c-40b7-93c9-a5c9a0522e94,0,"[0.25, 0.25, 0.5, 0.625]","[1.0, 1.0, 1.0, 0.95]",EEG-ImageNet,0.3912404701397712
544744,WorldSimBench: Towards Video Generation  Models as World Simulators,"Recent advancements in predictive models have demonstrated exceptional capabilities in predicting the future state of objects and scenes. However, the lack of categorization based on inherent characteristics continues to hinder the progress of predictive model development. Additionally, existing benchmarks are unable to effectively evaluate higher-capability, highly embodied predictive models from an embodied perspective. In this work, we classify the functionalities of predictive models into a hierarchy and take the first step in evaluating World Simulators by proposing a dual evaluation framework called WorldSimBench. WorldSimBench includes Explicit Perceptual Evaluation and Implicit Manipulative Evaluation, encompassing human preference assessments from the visual perspective and action-level evaluations in embodied tasks, covering three representative embodied scenarios: Open-Ended Embodied Environment, Autonomous, Driving, and Robot Manipulation. In the Explicit Perceptual Evaluation, we introduce the HF-Embodied Dataset, a video assessment dataset based on fine-grained human feedback, which we use to train a Human Preference Evaluator that aligns with human perception and explicitly assesses the visual fidelity of World Simulater. In the Implicit Manipulative Evaluation, we assess the video-action consistency of World Simulators by evaluating whether the generated situation-aware video can be accurately translated into the correct control signals in dynamic environments. Our comprehensive evaluation offers key insights that can drive further innovation in video generation models, positioning World Simulators as a pivotal advancement toward embodied artificial intelligence.",2025,0.6818179338843877,0.6789253111038988,0.6,0.5,3fb59cc8-1274-4656-8249-e27795c051e5,0,"[0.5, 0.5, 0.625, 0.875]","[0.95, 1.0, 0.95, 0.95]",WorldSimBench,0.615058910162003
544770,RE-Adapt: Reverse Engineered Adaptation of Large Language Models,"We introduce RE-Adapt, an approach to fine-tuning large language models on new domains without degrading any pre-existing instruction-tuning. We reverse engineer an adapter which isolates what an instruction-tuned model has learned beyond its corresponding pretrained base model. Importantly, this requires no additional data or training. We can then fine-tune the base model on a new domain and readapt it to instruction following with the reverse engineered adapter. RE-Adapt and our low-rank variant LoRE-Adapt both outperform other methods of fine-tuning, across multiple popular LLMs and datasets, even when the models are used in conjunction with retrieval-augmented generation.",2025,0.4090907603306326,0.404833682211152,0.4,0.25,da3152f0-df4a-4ce9-93b8-3b52987d46dd,0,"[0.25, 0.25, 0.5, 0.5]","[0.95, 1.0, 0.9, 0.95]",RE-Adapt,0.3575962171087842
544775,Skinning-free Accurate 3D Garment Deformation via Image Transfer,"3D garment animation is key to a wide range of applications including digital humans, virtual try-on, and extended reality. This paper addresses the task of predicting 3D garment deformation from a posed body mesh. Existing learning-based methods mostly rely on linear blend skinning to decompose garment deformation into low-frequency posed garment shape and high-frequency wrinkles. However, due to the lack of explicit skinning supervision, they often produce misaligned garment positions with undesired artifacts during garment re-posing, which corrupt the high-frequency signals. These skinning-based methods consequently fail to recover accurate wrinkle patterns. To tackle this issue, we present a skinning-free approach that re-formulates the high-low frequency decomposition by estimating posed (i) vertex position for low-frequency posed garment shape, and (ii) vertex normal for high-frequency local wrinkle details. In this way, each frequency modality can be effectively decoupled and directly supervised by the geometry of the deformed garment. Moreover, we propose to encode both vertex attributes as texture images, so that 3D garment deformation can be equivalently achieved via 2D image transfer. This enables us to leverage powerful pretrained image encoders to recover high-fidelity visual details representing fine wrinkles. In addition, we model body-garment interaction via cross-attention between dense body and garment image patches, which refines the naive skinning on sparse joints. Finally, we propose a multimodal fusion to incorporate constraints from both frequency modalities and optimize deformed 3D garments from transferred images. Extensive experiments show that our method significantly improves deformation accuracy on various garment types and recovers finer wrinkles than state-of-the-art methods.",2025,0.443181657024852,0.4370767451333178,0.4,0.25,caf0bffc-d6a4-4476-8f71-8675bcbe2493,0,"[0.25, 0.25, 0.5, 0.625]","[1.0, 0.95, 0.95, 0.9]",Image Transfer,0.3810388125980297
544782,Layerwise Recurrent Router for  Mixture-of-Experts,"The scaling of large language models (LLMs) has revolutionized their capabilities in various tasks, yet this growth must be matched with efficient computational strategies. 
The Mixture-of-Experts (MoE) architecture stands out for its ability to scale model size without significantly increasing training costs. 
Despite their advantages, current MoE models often display parameter inefficiency. 
For instance, a pre-trained MoE-based LLM with 52 billion parameters might perform comparably to a standard model with 6.7 billion. 
Being a crucial part of MoE, 
current routers in different layers independently assign tokens without leveraging historical routing information, potentially leading to suboptimal token-expert combinations and the parameter inefficiency problem.
To alleviate this issue, we introduce the Layerwise Recurrent Router for Mixture-of-Experts (RMoE). 
RMoE leverages a Gated Recurrent Unit (GRU) to establish dependencies between routing decisions across consecutive layers.
Such layerwise recurrence can be efficiently parallelly computed for input tokens and introduces negotiable costs.
Our extensive empirical evaluations demonstrate that RMoE-based language models consistently outperform a spectrum of baseline models. 
Furthermore, RMoE integrates a novel computation stage orthogonal to existing methods, allowing seamless compatibility with other MoE architectures. 
Our analyses attribute RMoE's gains to its effective cross-layer information sharing, which also improves expert selection and diversity.",2025,0.6477270371901683,0.6502403662890364,0.6666666666666666,0.625,bd3b2e37-0e1f-46bd-8ff1-a1bf1cf4a582,1,"[0.25, 0.625, 0.625, 0.875]","[0.95, 0.95, 0.9, 1.0]",Layerwise Recurrent Router,0.615626217565483
544789,GraphRouter: A Graph-based Router for LLM Selections,"The rapidly growing number and variety of Large Language Models (LLMs)
present significant challenges in efficiently selecting the appropriate LLM for
a given query, especially considering the trade-offs between performance and
computational cost. Current LLM selection methods often struggle to generalize
across new LLMs and different tasks because of their limited ability to leverage
contextual interactions among tasks, queries, and LLMs, as well as their depen-
dence on a transductive learning framework. To address these shortcomings, we
introduce a novel inductive graph framework, named as GraphRouter, which
fully utilizes the contextual information among tasks, queries, and LLMs to en-
hance the LLM selection process. GraphRouter constructs a heterogeneous
graph comprising task, query, and LLM nodes, with interactions represented as
edges, which efficiently captures the contextual information between the query’s
requirements and the LLM’s capabilities. Through an innovative edge prediction
mechanism, GraphRouter is able to predict attributes (the effect and cost of
LLM response) of potential edges, allowing for optimized recommendations that
adapt to both existing and newly introduced LLMs without requiring retraining.
Comprehensive experiments across three distinct effect-cost weight scenarios have
shown that GraphRouter substantially surpasses existing routers, delivering a
minimum performance improvement of 12.3%. In addition, it achieves enhanced
generalization across new LLMs settings and supports diverse tasks with at least a
9.5% boost in effect and a significant reduction in computational demands. This
work endeavors to apply a graph-based approach for the contextual and adaptive
selection of LLMs, offering insights for real-world applications.",2025,0.7499997272728265,0.7451790603605821,0.6666666666666666,0.625,125e66f1-14a2-4d13-8bfd-ca31911de7e7,1,"[0.625, 0.625, 0.625, 0.875]","[0.95, 1.0, 0.95, 0.9]",GraphRouter,0.6718851909784911
544810,Double Descent Meets Out-of-Distribution Detection: Theoretical Insights and Empirical Analysis of the role of model complexity,"While overparameterization is known to benefit generalization, its impact on Out-Of-Distribution (OOD) detection is less understood. This paper investigates the influence of model complexity in OOD detection. We propose an expected OOD risk metric to evaluate classifiers confidence on both training and OOD samples. Leveraging Random Matrix Theory, we derive bounds for the expected OOD risk of binary least-squares classifiers applied to Gaussian data. We show that the OOD risk depicts an infinite peak, when the number of parameters is equal to the number of samples, which we associate with the double descent phenomenon. Our experimental study on different OOD detection methods across multiple neural architectures extends our theoretical insights and highlights a double descent curve. Our observations suggest that overparameterization does not necessarily lead to better OOD detection. Using the Neural Collapse framework, we provide insights to better understand this behavior. To facilitate reproducibility, our code will be made publicly available upon publication.",2025,0.7159088305786071,0.7222903520217124,0.6666666666666666,0.625,98510aac-0480-4007-979a-100e73f3609a,0,"[0.5, 0.625, 0.625, 0.875]","[0.8, 0.95, 0.9, 0.95]",Expected OOD risk,0.6822831362084589
544811,End-to-End Reinforcement Learning for Traffic Signal Control: Real-Time Video to Signal Decisions,"Efficient traffic management at urban intersections is vital for reducing congestion
and improving safety. This paper presents MD3DQN, the first End-to-End novel
reinforcement learning model using surveillance video for real-time traffic signal
control. The model features two main components: an image reception module,
capturing traffic data from cameras positioned on signal poles, and a multi-agent
decision module, where each agent manages a traffic phase. These components
are connected via a bridge module for seamless integration.

Our novel Entropy Attention Mechanism enhances the multi-decision turn-based
traffic signal control by leveraging uncertainty and signal phase delays, leading
to more optimized decisions. Results show MD3DQN improved cumulative reward
by an average of 85.2% over Fixed-time 40 and 54.4% over DQN-VTP.
The entropy mechanism contributed to a 41.8% improvement upon ablation study,
demonstrating its impact on faster convergence and better performance.",2025,0.218181738843004,0.2178217390452002,0.2666666666666666,0.25,25967804-8ff0-47f6-a525-4dafeb4eac77,0,"[0.0, 0.25, 0.25, 0.25, 0.25]","[0.95, 0.95, 0.9, 0.95, 1.0]",MD3DQN,0.201151535490702
544818,Wolf: Accurate Video Captioning with a World Summarization Framework,"We propose Wolf, a WOrLd summarization Framework for accurate video captioning. Wolf is an automated captioning framework that adopts a mixture-of-experts approach, leveraging complementary strengths of Vision Language Models (VLMs). By utilizing both image and video models, our framework captures different levels of information and summarizes them efficiently. Our approach can be applied to enhance video understanding, auto-labeling, and captioning. To evaluate caption quality, we introduce CapScore, an LLM-based metric to assess the similarity and quality of generated captions compared to the ground truth captions. We further build four human-annotated datasets in three domains: autonomous driving, general scenes, and robotics, to facilitate comprehensive comparisons. We show that Wolf achieves superior captioning performance compared to state-of-the-art approaches from the research community (VILA1.5, CogAgent) and commercial solutions (Gemini-Pro-1.5, GPT-4V). For instance, in comparison with GPT-4V, Wolf improves CapScore (caption quality) by 55.6% and CapScore (caption similarity) by 77.4% on challenging driving videos. Finally, we establish a benchmark for video captioning and introduce a leaderboard, aiming to accelerate advancements in video understanding, captioning, and data alignment.",2025,0.5113634504132908,0.5136966309511622,0.4,0.25,d73bb42f-ba01-4913-91ba-e93027635033,0,"[0.25, 0.25, 0.5, 0.875]","[0.9, 0.95, 0.95, 0.95]",World Summarization Framework,0.4819559396299903
544826,Knowledge Entropy Decay during Language Model Pretraining Hinders New Knowledge Acquisition,"In this work, we investigate how a model's tendency to broadly integrate its parametric knowledge evolves throughout pretraining, and how this behavior affects overall performance, particularly in terms of knowledge acquisition and forgetting. We introduce the concept of knowledge entropy, which quantifies the range of memory sources the model engages with; high knowledge entropy indicates that the model utilizes a wide range of memory sources, while low knowledge entropy suggests reliance on specific sources with greater certainty. Our analysis reveals a consistent decline in knowledge entropy as pretraining advances. We also find that the decline is closely associated with a reduction in the model's ability to acquire and retain knowledge, leading us to conclude that diminishing knowledge entropy (smaller number of active memory sources) impairs the model's knowledge acquisition and retention capabilities. We find further support for this by demonstrating that increasing the activity of inactive memory sources enhances the model's capacity for knowledge acquisition and retention.",2025,0.8522724173554846,0.8495047822762808,0.9333333333333332,0.875,a1edc1d4-8205-4077-93b0-cbadd95a9ed0,1,"[0.5, 0.875, 0.875, 0.875]","[0.95, 0.95, 0.9, 0.95]",Knowledge Entropy,0.775590311587141
544828,3D-MolT5: Leveraging Discrete Structural Information for Molecule-Text Modeling,"The integration of molecular and natural language representations has emerged as a focal point in molecular science, with recent advancements in Language Models (LMs) demonstrating significant potential for comprehensive modeling of both domains. However, existing approaches face notable limitations, particularly in their neglect of three-dimensional (3D) information, which is crucial for understanding molecular structures and functions. While some efforts have been made to incorporate 3D molecular information into LMs using external structure encoding modules, significant difficulties remain, such as insufficient interaction across modalities in pre-training and challenges in modality alignment. To address the limitations, we propose \textbf{3D-MolT5}, a unified framework designed to model molecule in both sequence and 3D structure spaces. The key innovation of our approach lies in mapping fine-grained 3D substructure representations into a specialized 3D token vocabulary. This methodology facilitates the seamless integration of sequence and structure representations in a tokenized format, enabling 3D-MolT5 to encode molecular sequences, molecular structures, and text sequences within a unified architecture. Leveraging this tokenized input strategy, we build a foundation model that unifies the sequence and structure data formats. We then conduct joint pre-training with multi-task objectives to enhance the model's comprehension of these diverse modalities within a shared representation space. Thus, our approach significantly improves cross-modal interaction and alignment, addressing key challenges in previous work. Further instruction tuning demonstrated that our 3D-MolT5 has strong generalization ability and surpasses existing methods with superior performance in multiple downstream tasks, such as nearly 70\% improvement on the molecular property prediction task compared to state-of-the-art methods. Our code is available at \url{https://github.com/QizhiPei/3D-MolT5}.",2025,0.6818179338843877,0.6806929345162506,0.6666666666666666,0.625,2172e192-f7de-4644-a978-139dd3e81102,1,"[0.625, 0.625, 0.625, 0.625]","[0.9, 1.0, 0.95, 0.95]",3D token vocabulary,0.6250000000000001
544839,BadJudge: Backdoor Vulnerabilities of LLM-As-A-Judge,"This paper proposes a novel backdoor threat attacking the LLM-as-a-Judge evaluation regime, where the adversary controls both the candidate and evaluator model. The backdoored evaluator victimizes benign users by unfairly assigning inflated scores to adversary. A trivial single token backdoor poisoning 1% of the evaluator training data triples the adversary's score with respect to their legitimate score. We systematically categorize levels of data access corresponding to three real-world settings, (1) web poisoning, (2) malicious annotator, and (3) weight poisoning. These regimes reflect a weak to strong escalation of data access that highly correlates with attack severity. Under the weakest assumptions - web poisoning (1), the adversary still induces a 20% score inflation.  Likewise, in the (3) weight poisoning regime, the stronger assumptions enable the adversary to inflate their scores from 1.5/5 to 4.9/5. The backdoor threat generalizes across different evaluator architectures, trigger designs, evaluation tasks, and poisoning rates. By poisoning 10% of the evaluator training data, we control toxicity judges (Guardrails) to misclassify toxic prompts as non-toxic 89% of the time,  and document reranker judges in RAG to rank the poisoned document first 97% of the time. LLM-as-a-Judge is uniquely positioned at the intersection of ethics and technology, where social implications of mislead model selection and evaluation constrain the available defensive tools. Amidst these challenges, model merging emerges as a principled tool to offset the backdoor, reducing ASR to near 0% whilst maintaining SOTA performance. Model merging's low computational cost and convenient integration into the current LLM Judge training pipeline position it as a promising avenue for backdoor mitigation in the LLM-as-a-Judge setting.",2025,0.7840906239670459,0.7823176668677887,0.8,0.875,f8a6562d-ba26-4be7-a59d-f9eb5d44c7ed,1,"[0.5, 0.625, 0.875, 0.875]","[0.95, 0.8, 0.9, 0.9]",Backdoor Vulnerabilities,0.7107629980803388
544841,"Keep Your Friends Close, and Your Enemies Farther: Distance-aware Voxel-wise Contrastive Learning for Semi-supervised Multi-organ Segmentation","Voxel-wise contrastive learning (VCL) is a prominent approach in semi-supervised medical image segmentation. Based on the initially generated pseudo-labels, VCL pulls voxels with the same pseudo-labels toward their prototypes while pushes those with different labels apart, thereby learns effective representations for the segmentation task. However, in multi-organ segmentation (MoS), the complex anatomical structures of certain organs often lead to many unreliable pseudo-labels. Directly applying VCL can introduce confirmation bias, resulting in poor segmentation performance. A common practice is to first transform these unreliable pseudo-labels into more reliable complementary ones, which represent classes that voxels are least likely to belong to, and then push voxels away from the prototypes of their complementary labels.  However, we find that in this approach, if voxels with unreliable pseudo-labels are originally close in feature space, they can end up far apart after being pushed away from their complementary prototypes. This disruption of the semantic relationships among voxels can be detrimental to the MoS task. In this paper, we propose DVCL, a novel distance-aware VCL method for semi-supervised MoS. DVCL is based on the observation that voxels close to each other in the feature space ('neighbors') likely belong to the same semantic category, while distant ones ('outsiders') likely belong to different categories. In DVCL, we first identify neighbors and outsiders for all voxels with unreliable pseudo-labels, and then pull their neighbors into the same clusters while pushing outsiders away. In this way, neighbors of unreliable voxels remain their neighbors and outsiders remain outsiders. This approach helps maintain useful semantic relationships among unreliable voxels while still enjoying the advantages of VCL. We conduct extensive experiments on four datasets to validate the effectiveness.  Extensive experiments on four datasets demonstrate the superior performance of DVCL compared to state-of-the-art methods.",2025,0.443181657024852,0.4380874380024875,0.4,0.25,9a0cfa93-e4df-488e-aff6-09fc6c6d3497,0,"[0.25, 0.25, 0.5, 0.625]","[1.0, 1.0, 0.95, 0.95]",DVCL,0.3832281718963164
544891,Reinforcement Learning for Control of Non-Markovian Cellular Population Dynamics,"Many organisms and cell types, from bacteria to cancer cells, exhibit a remarkable ability to adapt to fluctuating environments. Additionally, cells can leverage memory of past environments to better survive previously-encountered stressors. From a control perspective, this adaptability poses significant challenges in driving cell populations toward extinction, and is thus an open question with great clinical significance. In this work, we focus on drug dosing in cell populations exhibiting phenotypic plasticity. For specific dynamical models switching between resistant and susceptible states, exact solutions are known. However, when the underlying system parameters are unknown, and for complex memory-based systems, obtaining the optimal solution is currently intractable. To address this challenge, we apply reinforcement learning (RL) to identify informed dosing strategies to control cell populations evolving under novel non-Markovian dynamics. We find that model-free deep RL is able to recover exact solutions and control cell populations even in the presence of long-range temporal dynamics.  To further test our approach in more realistic settings, we demonstrate performant RL-based control strategies in environments with dynamic memory strength.",2025,0.886363314049704,0.8821028946330901,0.9333333333333332,0.875,1c27a54c-1db5-4eae-8246-7be34702c3c6,1,"[0.625, 0.875, 0.875, 0.875]","[0.95, 0.9, 0.9, 0.9]",Non-Markovian Dynamics,0.7996262458165371
544929,DiSciPLE: Learning Interpretable Programs for Scientific Discovery,"Creating hypotheses for new observations is a key step in the scientific process of understanding a problem in any domain. A good hypothesis that is interpretable, reliable (good at predicting unseen observations), and data-efficient; is useful for scientists aiming to make novel discoveries. This paper introduces an automatic way of learning such interpretable and reliable hypotheses in a data-efficient manner. We propose DiSciPLE (Discovering Scientific Programs using LLMs and Evolution), an evolutionary algorithm that leverages common sense and prior knowledge of large language models (LLMs) to create hypotheses as Python programs. Additionally, we propose two improvements: a program critic and a program simplifier to further improve our method to produce good hypotheses. We evaluate our method on four different real-world tasks in two scientific domains and show significantly better results. For example, we can learn programs with 35% lower error than the closest non-interpretable baseline for population density estimation",2025,0.443181657024852,0.4378511014156235,0.4,0.25,252ad76a-d173-4f34-81a0-e73ec89c2d57,0,"[0.25, 0.25, 0.5, 0.625]","[0.95, 0.95, 0.9, 0.9]",DiSciPLE,0.3861722797927463
544952,Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data,"Foundation models have recently been shown to be strong data compressors. However, when accounting for their excessive parameter count, their compression ratios are actually inferior to standard compression algorithms. Moreover, naively reducing the number of parameters may not necessarily help as it leads to worse predictions and thus weaker compression. In this paper, we conduct a large-scale empirical study to investigate whether there is a sweet spot where competitive compression ratios with pre-trained vanilla transformers are possible. To this end, we train families of models on 165GB of raw byte sequences of either text, image, or audio data (and all possible combinations of the three) and then compress 1GB of out-of-distribution (OOD) data from each modality. We find that relatively small models (i.e., millions of parameters) can outperform standard general-purpose compression algorithms (gzip, LZMA2) and even domain-specific compressors (PNG, JPEG~2000, FLAC) &mdash; even when factoring in parameter count. We achieve, e.g., the lowest compression ratio of 0.49 on OOD audio data (vs. 0.54 for FLAC). To study the impact of model- and dataset scale, we conduct extensive ablations and hyperparameter sweeps, and we investigate the effect of unimodal versus multimodal training. We find that even small models can be trained to perform well on multiple modalities, but, in contrast to previously reported results with large-scale foundation models, transfer to unseen modalities is generally weak.",2025,0.5454543471075102,0.5445543476130005,0.6,0.625,3d4a4edb-866c-48e5-99bb-c0ae4e599994,0,"[0.25, 0.5, 0.625, 0.625]","[0.95, 0.9, 0.95, 0.95]",Byte-Level Compression,0.5
544962,GPU-Accelerated Counterfactual Regret Minimization,"Counterfactual regret minimization is a family of algorithms of no-regret learning dynamics capable of solving large-scale imperfect information games. We propose implementing this algorithm as a series of dense and sparse matrix and vector operations, thereby making it highly parallelizable for a graphical processing unit, at a cost of higher memory usage. Our experiments show that our implementation performs up to about 401.2 times faster than OpenSpiel's Python implementation and, on an expanded set of games, up to about 203.6 times faster than OpenSpiel's C++ implementation and the speedup becomes more pronounced as the size of the game being solved grows.",2025,0.4363634776860081,0.4435972388916359,0.5333333333333333,0.5,fd7f3a77-7108-4e93-b403-4b51b3a7d0d8,0,"[0.25, 0.25, 0.5, 0.5, 0.5]","[0.95, 0.7, 0.9, 0.95, 0.95]",Counterfactual Regret Minimization,0.418116555867959
544968,From Attention to Prediction Maps: Per-Class Gradient-Free Transformer Explanations,"The Vision Transformer (ViT) has become a standard model architecture in computer vision, especially for classification tasks. As such, explaining ViT predictions has attracted significant research efforts in recent years. Many methods rely on attention maps, which highlight \emph{where} in the image the network directs its attention. In this paper, we introduce Prediction~Maps -- a novel explanation method that complements attention maps by revealing \emph{what} the network sees. Prediction maps visualize how each patch token within a given layer is associated with each possible class. This is done by utilizing the classification head at the output of the network, originally trained to be fed with the class token at the last layer. Specifically, to obtain the prediction map of a particular layer, we apply the classification head to every patch token within that layer. We show that prediction maps provide complementary information to attention maps and illustrate that combining them leads to state-of-the-art explainability performance. Furthermore, since our proposed method is neither gradient- nor perturbation-based, it offers superior computational and memory efficiency compared to competing methods.",2025,0.4772725537190714,0.4624987202972831,0.4666666666666667,0.25,e175fa78-d8cd-4073-bdba-253e3913381a,0,"[0.25, 0.25, 0.625, 0.625]","[0.95, 1.0, 0.8, 0.9]",Prediction Maps,0.3818933811730862
544972,Understanding Impact of Human Feedback via Influence Functions,"In Reinforcement Learning from Human Feedback (RLHF), it is crucial to learn suitable reward models from human feedback to align large language models (LLMs) with human intentions. However, human feedback can often be noisy, inconsistent, or biased, especially when evaluating complex responses. Such feedback can lead to misaligned reward signals, potentially causing unintended side effects during the RLHF process. To address these challenges, we explore the use of influence functions to measure the impact of human feedback on the performance of reward models. We propose a compute-efficient approximation method that enables the application of influence functions to LLM-based reward models and large-scale preference datasets. In our experiments, we demonstrate two key applications of influence functions: (1) detecting common forms of labeler bias in human feedback datasets and (2) guiding labelers to refine their strategies to align more closely with expert feedback. By quantifying the impact of human feedback on reward models, we believe that influence functions can enhance feedback interpretability and contribute to scalable oversight in RLHF, helping labelers provide more accurate and consistent feedback.",2025,0.727272826446245,0.7252244108466513,0.6666666666666666,0.5,4d18a509-ed64-4634-af6a-e35b0253f0e0,0,"[0.5, 0.5, 0.625, 0.625, 0.875, 0.875]","[0.9, 0.95, 0.9, 0.8, 1.0, 0.8]",Influence Functions,0.6740603216303899
544977,InstructBrush: Learning Attention-based Visual Instruction for Image Editing,"Diffusion-based image editing methods have garnered significant attention in image editing. However, despite encompassing a wide range of editing priors, these methods are helpless when handling editing tasks that are challenging for users to accurately describe. We propose InstructBrush, an inversion method for instruction-based image editing methods to bridge this gap. It extracts editing effects from example image pairs as editing instructions to guide the editing of new images. Two key techniques are introduced into InstructBrush, Attention-based Instruction Optimization and Transformation-oriented Instruction Initialization, to address the limitations of the previous method in terms of inversion effects and instruction generalization. To explore the ability of visual prompt editing methods to guide image editing in open scenarios, we establish a Transformation-Oriented Paired Benchmark (TOP-Bench). Quantitatively and qualitatively, our approach achieves superior performance in editing and is more semantically consistent with the target editing effects. The code and benchmark will be released upon acceptance.",2025,0.5795452438017296,0.5795092911862789,0.5333333333333333,0.5,e776bf2d-85f0-4dc3-a5cd-179a1c82fc4c,0,"[0.5, 0.5, 0.5, 0.625]","[0.9, 0.9, 0.95, 0.95]",InstructBrush,0.5352655440414508
544986,Generalizable Human Gaussians from Single-View Image,"In this work, we tackle the task of learning 3D human Gaussians from a single image, focusing on recovering detailed appearance and geometry including unobserved regions. We introduce a single-view generalizable Human Gaussian Model (HGM), which employs a novel generate-then-refine pipeline with the guidance from human body prior and diffusion prior. Our approach uses a ControlNet to refine rendered back-view images from coarse predicted human Gaussians, then uses the refined image along with the input image to reconstruct refined human Gaussians. To mitigate the potential generation of unrealistic human poses and shapes, we incorporate human priors from the SMPL-X model as a dual branch, propagating image features from the SMPL-X volume to the image Gaussians using sparse convolution and attention mechanisms. Given that the initial SMPL-X estimation might be inaccurate, we gradually refine it with our HGM model. We validate our approach on several publicly available datasets. Our method surpasses previous methods in both novel view synthesis and surface reconstruction. Our approach also exhibits strong generalization for cross-dataset evaluation and in-the-wild images.",2025,0.6818179338843877,0.6806929345162506,0.6666666666666666,0.625,a81ce036-cb81-4c59-852f-4d1aadddae6f,1,"[0.625, 0.625, 0.625]","[1.0, 0.95, 1.0]",Human Gaussian Model,0.6250000000000001
544992,Budgeted Online Continual Learning by Adaptive Layer Freezing and Frequency-based Sampling,"The majority of online continual learning (CL) advocates single-epoch training and imposes restrictions on the size of replay memory. However, single-epoch training would incur a different amount of computations per CL algorithm, and the additional storage cost to store logit or model in addition to replay memory is largely ignored in calculating the storage budget. Arguing different computational and storage budgets hinder fair comparison among CL algorithms in practice, we propose to use floating point operations (FLOPs) and total memory size in Byte as a metric for computational and memory budgets, respectively, to compare and develop CL algorithms in the same ‘total resource budget.’ To improve a CL method in a limited total budget, we propose adaptive layer freezing that does not update the layers for less informative batches to reduce computational costs with a negligible loss of accuracy. In addition, we propose a memory retrieval method that allows the model to learn the same amount of knowledge as using random retrieval in fewer iterations. Empirical validations on the CIFAR-10/100, CLEAR-10/100, and ImageNet-1K datasets demonstrate that the proposed approach outperforms the state-of-the-art methods within the same total budget. Furthermore, we validate its effectiveness in the Multi-modal Concept incremental Learning setup with multimodal large language models, such as LLaVA-1.5-7B. Code is available at https://github.com/snumprlab/budgeted-cl.",2025,0.886363314049704,0.8867403194573625,0.9333333333333332,0.875,95e4c6a5-92d5-4411-9fbf-3ce85fe0cadc,1,"[0.625, 0.875, 0.875, 0.875]","[0.9, 0.95, 0.9, 0.95]",Adaptive Layer Freezing,0.8205310875824223
545005,MixMax: Distributional Robustness in Function Space via Optimal Data Mixtures,"Machine learning models are often required to perform well across several pre-defined settings, such as a set of user groups. Worst-case performance is a common metric to capture this requirement, and is the objective of group distributionally robust optimization (group DRO). Unfortunately, these methods struggle when the loss is non-convex in the parameters, or the model class is non-parametric. Here, we make a classical move to address this: we reparameterize group DRO from parameter space to function space, which results in a number of advantages. First, we show that group DRO over the space of bounded functions admits a minimax theorem. Second, for cross-entropy and mean squared error, we show that the minimax optimal mixture distribution is the solution of a simple convex optimization problem. Thus, provided one is working with a model class of universal function approximators, group DRO can be solved by a convex optimization problem followed by a classical risk minimization problem. We call our method MixMax. In our experiments, we found that MixMax matched or outperformed the standard group DRO baselines, and in particular, MixMax improved the performance of XGBoost over the only baseline, data balancing, for variations of the ACSIncome and CelebA annotations datasets.",2025,0.7840906239670459,0.7798791524991777,0.8,0.875,e51d57a8-c66b-42bd-8b44-126f46ff8b01,1,"[0.5, 0.625, 0.875, 0.875]","[0.95, 0.8, 0.8, 0.95]",MixMax,0.7083415939730373
545025,In-context Time Series Predictor,"Recent Transformer-based large language models (LLMs) demonstrate in-context learning ability to perform various functions based solely on the provided context, without updating model parameters. To fully utilize the in-context capabilities in time series forecasting (TSF) problems, unlike previous Transformer-based or LLM-based time series forecasting methods, we reformulate ""time series forecasting tasks"" as input tokens by constructing a series of (lookback, future) pairs within the tokens. This method aligns more closely with the inherent in-context mechanisms and is more parameter-efficient without the need of using pre-trained LLM parameters. Furthermore, it addresses issues such as overfitting in existing Transformer-based TSF models, consistently achieving better performance across full-data, few-shot, and zero-shot settings compared to previous architectures.",2025,0.7159088305786071,0.7128368885471509,0.8,0.875,9fcdb5e0-ca0e-44ae-b9a0-70583f46aa5d,1,"[0.25, 0.625, 0.875, 0.875]","[0.95, 0.8, 0.95, 0.9]",In-context learning,0.6457992880068533
545048,Denoising with a Joint-Embedding Predictive Architecture,"Joint-embedding predictive architectures (JEPAs) have shown substantial promise in self-supervised representation learning, yet their application in generative modeling remains underexplored. Conversely, diffusion models have demonstrated significant efficacy in modeling arbitrary probability distributions. In this paper, we introduce Denoising with a Joint-Embedding Predictive Architecture (D-JEPA), pioneering the integration of JEPA within generative modeling. By recognizing JEPA as a form of masked image modeling, we reinterpret it as a generalized next-token prediction strategy, facilitating data generation in an auto-regressive manner. Furthermore, we incorporate diffusion loss to model the per-token probability distribution, enabling data generation in a continuous space. We also adapt flow matching loss as an alternative to diffusion loss, thereby enhancing the flexibility of D-JEPA. Empirically, with increased GFLOPs, D-JEPA consistently achieves lower FID scores with fewer training epochs, indicating its good scalability. Our base, large, and huge models outperform all previous generative models across all scales on ImageNet conditional generation benchmarks. Beyond image generation, D-JEPA is well-suited for other continuous data modeling, including video and audio.",2025,0.7159088305786071,0.7097361960918424,0.6666666666666666,0.625,0e019fb2-ec3e-460a-a877-e81574a72697,1,"[0.5, 0.625, 0.625, 0.875]","[1.0, 0.9, 0.95, 0.9]",D-JEPA,0.6313923297896608
545050,BA-LoRA: Bias-Alleviating Low-Rank Adaptation to Mitigate Catastrophic Inheritance in Large Language Models,"Large language models (LLMs) have demonstrated remarkable proficiency across various natural language processing (NLP) tasks. However, adapting LLMs to downstream applications requires computationally intensive and memory-demanding fine-tuning procedures. To alleviate these burdens, parameter-efficient fine-tuning (PEFT) techniques have emerged as a promising approach to tailor LLMs with minimal computational overhead. While PEFT methods offer substantial advantages, they do not fully address the pervasive issue of bias propagation from pre-training data. This work introduces Bias-Alleviating Low-Rank Adaptation (BA-LoRA), a novel PEFT method designed to counteract bias inheritance. BA-LoRA incorporates three distinct regularization terms: (1) a consistency regularizer, (2) a diversity regularizer, and (3) a singular value decomposition regularizer. These regularizers aim to enhance the models' consistency, diversity, and generalization capabilities during fine-tuning. We conduct extensive experiments on natural language understanding (NLU) and natural language generation (NLG) tasks using prominent LLMs such as LLaMA, Mistral, and Gemma. The results demonstrate that BA-LoRA outperforms LoRA and its state-of-the-art variants. Moreover, our method effectively mitigates the adverse effects of pre-training bias, leading to more reliable and robust model outputs.",2025,0.6272724991736367,0.6192105704533525,0.6666666666666666,0.25,75d20a88-0652-4373-b934-9ca1b5631802,0,"[0.25, 0.25, 0.625, 0.875, 0.875]","[0.95, 0.95, 0.95, 0.9, 0.9]",BA-LoRA,0.5449333198625987
545070,Multi-View and Multi-Scale Alignment for Contrastive Language-Image Pre-training in Mammography,"Contrastive Language-Image Pre-training (CLIP) shows promise in medical image analysis but requires substantial data and computational resources. Due to these restrictions, existing CLIP applications in medical imaging focus mainly on modalities like chest X-rays that have abundant image-report data available, leaving many other important modalities under-explored. Here, we propose one of the first adaptations of the full CLIP model to mammography, which presents significant challenges due to labeled data scarcity, high-resolution images with small regions of interest, and data imbalance. We first develop a specialized supervision framework for mammography that leverages its multi-view nature. Furthermore, we design a symmetric local alignment module to better focus on detailed features in high-resolution images. Lastly, we incorporate a parameter-efficient fine-tuning approach for large language models pre-trained with medical knowledge to address data limitations. Our multi-view and multi-scale alignment (MaMA) method outperforms state-of-the-art baselines for three different tasks on two large real-world mammography datasets, EMBED and RSNA-Mammo, with only 52% model size compared with the largest baseline. The code is attached in the supplement file and will be released on GitHub upon acceptance.",2025,0.443181657024852,0.4411576354143296,0.4,0.25,c1cd8a83-dd7e-4923-802f-b624446b7eb0,0,"[0.25, 0.25, 0.5, 0.625]","[1.0, 1.0, 0.95, 1.0]",MaMA,0.3998173443456163
545089,SVIP: Towards Verifiable Inference of Open-source Large Language Models,"Open-source Large Language Models (LLMs) have recently demonstrated remarkable capabilities in natural language understanding and generation, leading to widespread adoption across various domains. However, their increasing model sizes render local deployment impractical for individual users, pushing many to rely on decentralized computing service providers for inference through a blackbox API. This reliance introduces a new risk: a computing provider may stealthily substitute the requested LLM with a smaller, less capable model without consent from users, thereby delivering inferior outputs while benefiting from cost savings. In this paper, we formalize the problem of verifiable inference for LLMs. Existing verifiable computing solutions based on cryptographic or game-theoretic techniques are either computationally uneconomical or rest on strong assumptions. We introduce $\texttt{SVIP}$, a secret-based verifiable LLM inference protocol that leverages intermediate outputs from LLMs as unique model identifiers. By training a proxy task on these outputs and requiring the computing provider to return both the generated text and the processed intermediate outputs, users can reliably verify whether the computing provider is acting honestly. In addition, the integration of a secret mechanism further enhances the security of our protocol. We thoroughly analyze our protocol under multiple strong and adaptive adversarial scenarios. Our extensive experiments demonstrate that $\texttt{SVIP}$ is accurate, generalizable, computationally efficient, and resistant to various attacks. Notably, $\texttt{SVIP}$ achieves false negative rates below  $5\\%$ and false positive rates below  $3\\%$, while requiring less than $0.01$ seconds per query for verification.",2025,0.4090907603306326,0.4108466513174948,0.2666666666666666,0.25,269251f7-436b-4639-a6ee-e615d4886a8d,0,"[0.25, 0.25, 0.625]","[0.95, 0.9, 0.95]",SVIP,0.3852683007618416
545092,Towards Generalization Bounds of GCNs for Adversarially Robust Node Classification,"Adversarially robust generalization of Graph Convolutional Networks (GCNs) has garnered significant attention in various security-sensitive application areas, driven by intrinsic adversarial vulnerability. Albeit remarkable empirical advancement, theoretical understanding of the generalization behavior of GCNs subjected to adversarial attacks remains elusive. To make progress on the mystery, we establish unified high-probability generalization bounds for GCNs in the context of node classification, by leveraging adversarial Transductive Rademacher Complexity (TRC) and developing a novel contraction technique on graph convolution. Our bounds capture the interaction between generalization error and adversarial perturbations, revealing the importance of key quantities in mitigating the negative effects of perturbations, such as low-dimensional feature projection, perturbation-dependent norm regularization, normalized graph matrix, proper number of network layers, etc. Furthermore, we provide TRC-based bounds of popular GCNs with $\ell_r$-norm-additive perturbations for arbitrary $r\geq 1$. A comparison of theoretical results demonstrates that specific network architectures (e.g., residual connection) can help alleviate the cumulative effect of perturbations during the forward propagation of deep GCNs. Experimental results on benchmark datasets validate our theoretical findings.",2025,0.7499997272728265,0.7507073760975492,0.6666666666666666,0.625,1b70fd82-0d66-48bb-ba4b-5a01d49d14b0,1,"[0.625, 0.625, 0.625, 0.875]","[0.9, 0.9, 0.8, 0.9]",Transductive Rademacher Complexity,0.6935339204174821
545104,Privacy as a Free Lunch: Crafting Initial Distilled Datasets through the Kaleidoscope,"The advancement of deep learning necessitates stringent data privacy guarantees.
Dataset distillation has shown potential in preserving differential privacy while maintaining training efficiency.
This study first identifies that data generated by state-of-the-art dataset distillation methods strongly resembles to real data, indicating severe privacy leakage.
We define this phenomenon as explicit privacy leakage.
We theoretically analyze that although distilled datasets can ensure differential privacy to some extent, a high \IPC can weaken both differential privacy and explicit privacy.
Furthermore, we reveal that the primary source of privacy leakage in distilled data stems from the common approach of initializing distilled images as real data.
To address this, we propose a plug-and-play module, Kaleidoscopic Transformation (KT), designed to introduce enhanced strong perturbations to the selected real data during the initialization phase.
Extensive experiments demonstrate that our method ensures both differential privacy and explicit privacy, while preserving the generalization performance of the distilled data.
Our code will be publicly available.",2025,0.5113634504132908,0.503354454781296,0.5333333333333333,0.5,31d5dd72-af7f-46b1-95fd-707103cc16fd,0,"[0.0, 0.5, 0.5, 0.875]","[1.0, 0.9, 0.95, 0.95]",Kaleidoscopic Transformation,0.4276810256735947
545113,Versatile Motion-Language Models for Multi-turn Interactive Agents,"Recent advancements in large language models (LLMs) have greatly enhanced their ability to generate natural and contextually relevant text, making AI interactions more human-like. However, generating and understanding interactive human-like motion, where two individuals engage in coordinated movements, remains a challenge due to the complexity of modeling these coordinated interactions. Furthermore, a versatile model is required to handle diverse interactive scenarios, such as chat systems that follow user instructions or adapt to their assigned role while adjusting interaction dynamics. To tackle this problem, we introduce VIM, short for the Versatile Interactive Motion language model, which integrates both language and motion modalities to effectively understand, generate, and control interactive motions in multi-turn conversational contexts. To address the scarcity of multi-turn interactive motion data, we introduce a synthetic dataset called INTER-MT2; where we utilize pre-trained models to create diverse instructional datasets with interactive motion. Our approach first trains a motion tokenizer that encodes interactive motions into residual discrete tokens. In the pre-training stage, the model learns to align motion and text representations with these discrete tokens. During the instruction fine-tuning stage, VIM adapts to multi-turn conversations using INTER-MT2. We evaluate the versatility of our method across motion-related tasks—motion-to-text, text-to-motion, reaction generation, motion editing, and reasoning about motion sequences. The results highlight VIM’s versatility and effectiveness in handling complex interactive motion synthesis.",2025,0.4772725537190714,0.4738330744885001,0.5333333333333333,0.5,313299ab-8c0d-491c-b1ec-4701d5991a4d,0,"[0.25, 0.5, 0.5, 0.5]","[1.0, 0.9, 0.95, 1.0]",VIM,0.4246325973979806
545124,Hardware-Aware Parallel Prompt Decoding for Memory-Efficient Acceleration of LLM Inference,"The auto-regressive decoding of Large Language Models (LLMs) results in significant overheads in their hardware performance. 
While recent research has investigated various speculative decoding techniques for multi-token generation, these efforts have primarily focused on improving processing speed such as throughput.
Crucially, they often neglect other metrics essential for real-life deployments, such as memory consumption and training cost.
To overcome these limitations, we propose a novel parallel prompt decoding that requires only $0.0002$\% trainable parameters, enabling efficient training on a single A100-40GB GPU in just 16 hours. 
Inspired by the human natural language generation process,  PPD approximates outputs generated at future timesteps in parallel by using multiple prompt tokens. This approach partially recovers the missing conditional dependency information necessary for multi-token generation, resulting in up to a 28\% higher acceptance rate for long-range predictions.
Furthermore, we present a hardware-aware two-stage tree pruning algorithm that adaptively optimizes this decoding scheme to fully leverage the computational capacities on different GPUs.
Through extensive experiments across LLMs ranging from MobileLlama to Vicuna-13B on a wide range of benchmarks, our approach demonstrates up to 2.49$\times$ speedup and maintains a minimal runtime memory overhead of just $0.0004\%$.
More importantly, our parallel prompt decoding can serve as an orthogonal optimization for synergistic integration with existing speculative decoding,
showing up to $1.22\times$ further speed improvement. Our code will be open-sourced upon acceptance of the paper.",2025,0.443181657024852,0.4363895175466302,0.5333333333333333,0.5,b1820e4b-e869-4ca2-bef4-21c1d32c8e0e,0,"[0.0, 0.5, 0.5, 0.625]","[0.95, 0.9, 0.9, 0.9]",Parallel Prompt Decoding,0.3783568660022151
545141,Offline-to-Online Reinforcement Learning with Classifier-Free Diffusion Generation,"Offline-to-online Reinforcement Learning (O2O RL) aims to perform online fine-tuning on an offline pre-trained policy to minimize costly online interactions. Existing methods have used offline data or online data to generate new data for data augmentation, which has led to performance improvement during online fine-tuning. However, they have not fully analyzed and utilized both types of data simultaneously. Offline data helps prevent agents from settling too early on suboptimal policies by providing diverse data, while online data improves training stability and speeds up convergence. In this paper, we propose a data augmentation approach, Classifier-Free Diffusion Generation (CFDG). Considering the differences between offline data and online data, we use conditional diffusion to generate both types of data for augmentation in the online phase, aiming to improve the quality of sample generation. Experimental results show that CFDG outperforms replaying the two data types or using a standard diffusion model to generate new data. Our method is versatile and can be integrated with existing offline-to-online RL algorithms. By implementing CFDG to popular methods IQL, PEX and APL, we achieve a notable 15% average improvement in empirical performance on the D4RL benchmark like MuJoCo and AntMaze.",2025,0.2727271735537551,0.2722771738065002,0.2666666666666666,0.25,d3c28adb-a1d9-4aaa-a2df-3985b65bdb23,0,"[0.25, 0.25, 0.25]","[0.95, 0.95, 0.95]",Classifier-Free Diffusion Generation,0.25000000000001
545156,Certifiably Robust RAG against Retrieval Corruption Attacks,"Retrieval-augmented generation (RAG) has been shown vulnerable to retrieval corruption attacks: an attacker can inject malicious passages into retrieval results to induce inaccurate responses. In this paper, we propose RobustRAG as the first defense framework against retrieval corruption attacks. The key insight of RobustRAG is an isolate-then-aggregate strategy: we isolate passages into disjoint groups, generate LLM responses based on the concatenated passages from each isolated group, and then securely aggregate these responses for a robust output. To instantiate RobustRAG, we design keyword-based and decoding-based algorithms for securely aggregating unstructured text responses. Notably, RobustRAG can achieve certifiable robustness: we can formally prove and certify that, for certain queries, RobustRAG can always return accurate responses, even when an adaptive attacker has full knowledge of our defense and can arbitrarily inject a small number of malicious passages. We evaluate RobustRAG on open-domain QA and long-form text generation datasets and demonstrate its effectiveness and generalizability.",2025,0.4999994545456529,0.5,0.5333333333333333,0.25,59282b5a-c383-46ed-8de1-faab3f5c06c8,0,"[0.25, 0.5, 0.625]","[0.9, 0.95, 0.9]",RobustRAG,0.4620624323348972
545157,BodyGen: Advancing Towards Efficient Embodiment Co-Design,"Embodiment co-design aims to optimize a robot's morphology and control policy simultaneously. 
While prior work has demonstrated its potential for generating environment-adaptive robots, this field still faces persistent challenges in optimization efficiency due to the (i) combinatorial nature of morphological search spaces and (ii) intricate dependencies between morphology and control.
We prove that the ineffective morphology representation and unbalanced reward signals between the design and control stages are key obstacles to efficiency.
To advance towards efficient embodiment co-design, we propose **BodyGen**, which utilizes (1) topology-aware self-attention for both design and control, enabling efficient morphology representation with lightweight model sizes; (2) a temporal credit assignment mechanism that ensures balanced reward signals for optimization. With our findings, BodyGen achieves an average **60.03%** performance improvement against state-of-the-art baselines. We provide codes and more results on the website: https://genesisorigin.github.io.",2025,0.886363314049704,0.883061310284889,0.9333333333333332,0.875,87830301-f4f7-47a5-b1b9-68bb9d898350,1,"[0.625, 0.875, 0.875, 0.875]","[0.95, 0.9, 0.9, 0.95]",BodyGen,0.8044689118962354
545158,KEEP: Towards a Knowledge-Enhanced Explainable Prompting Framework for Vision-Language Models,"Large-scale vision-language models (VLMs) embedded with expansive representations and visual concepts have showcased significant potential in the computer vision community. Efficiently adapting VLMs such as CLIP, to downstream tasks has garnered growing attention, with prompt learning emerging as a representative approach. However, most existing prompt-based adaptation methods, which rely solely on coarse-grained textual prompts, suffer from limited performance and interpretability when handling tasks that require domain-specific knowledge. This results in a failure to satisfy the stringent trustworthiness requirements of Explainable Artificial Intelligence (XAI) in high-risk scenarios like healthcare. To address this issue, we propose a Knowledge-Enhanced Explainable Prompting (KEEP) framework that leverages fine-grained domain-specific knowledge to enhance the adaptation process across various domains, facilitating bridging the gap between the general domain and other specific domains. We present to our best knowledge the first work to incorporate retrieval augmented generation and domain-specific foundation models to provide more reliable image-wise knowledge for prompt learning in various domains, alleviating the lack of fine-grained annotations, while offering both visual and textual explanations. Extensive experiments and explainability analyses conducted on eight datasets of different domains, demonstrate that our method simultaneously achieves superior performance and interpretability, shedding light on the effectiveness of the collaboration between foundation models and XAI. The code will be made publically available.",2025,0.4772725537190714,0.4755778266182522,0.5333333333333333,0.5,e175fa78-d8cd-4073-bdba-253e3913381a,0,"[0.25, 0.5, 0.5, 0.5]","[0.95, 0.9, 0.95, 0.95]",Knowledge-Enhanced Prompting,0.4337268743914313
545159,Laplace-Transform-Filters render spectral Graph Neural Networks transferable,"We introduce a new point of view on transferability of graph neural networks based on the intrinsic notion of information diffusion within graphs. This notion is adapted to considering graphs to be similar if their overall rough structures are similar, while their fine-print articulation may differ. Transferability of graph neural networks is then considered between graphs that are similar from this novel perspective on transferability. After carefully analysing transferability of single filters, the transferability properties of entire networks are relegated to the transferability characteristics of the filters employed inside their convolutional blocks. A rigorous analysis establishes our main theoretical finding: Spectral convolutional networks are transferable between graphs whose overall rough structures align, if  their filters arise as Laplace transforms of certain generalized functions. Numerical experiments illustrate and validate the theoretical findings in practice.",2025,0.4909089123967591,0.4772169352045673,0.5333333333333333,0.25,a541d8bb-16be-4423-9482-f74828aad815,0,"[0.25, 0.25, 0.5, 0.625, 0.625]","[1.0, 1.0, 0.95, 0.8, 0.9]",Laplace-Transform-Filters,0.3965041167437472
545211,Data-Evolution Learning,"Recent advancements in machine learning have been driven by models trained on large-scale, high-quality datasets. However, the practical application of these models faces two significant challenges: the infeasibility of acquiring precise labels in real-world settings and the substantial computational burden imposed by training large models. While existing approaches—such as self-supervised learning, weak supervision, noisy label learning, and dataset distillation—address these challenges from a model-centric perspective, they often overlook the potential benefits of optimizing the data itself.
This paper introduces a novel data-centric learning paradigm where both the dataset and the model co-evolve during the learning process.
We formalize this paradigm and propose a Data-evolution Learning Algorithm (DeLA), which offers three key advantages: optimized dataset generation, versatile dataset compatibility, and effective utilization of prior knowledge.
Extensive experiments demonstrate that DeLA enables the creation of optimized datasets for reuse in subsequent training, effectively addressing diverse datasets with varying target types. Moreover, DeLA accelerates learning by utilizing architecture-agnostic, open-source prior models for efficient data creation.
Notably, DeLA frequently outperforms traditional SOTA model-centric methods in self-supervised and noisy label learning.
Furthermore, its simplicity enables implementation in only two lines of PyTorch code, offering significant potential for advancements in representation learning.
Our code will be made publicly available.",2025,0.2045453801653163,0.2014099601168396,0.1333333333333333,0.0,f1e3bb0f-19cf-492d-a8f5-ed51cc608b01,0,"[0.0, 0.0, 0.25, 0.5]","[0.9, 0.95, 0.9, 0.9]",Data-evolution Learning Algorithm,0.1746262494470135
545214,Multi-Agent Decision S4: Leveraging State Space Models for Offline Multi-Agent Reinforcement Learning,"Goal-conditioned sequence-based supervised learning with transformers has shown promise in offline reinforcement learning (RL) for single-agent settings. However, extending these methods to offline multi-agent RL (MARL) remains challenging. Existing transformer-based MARL approaches either train agents independently, neglecting multi-agent system dynamics, or rely on centralized transformer models, which face scalability issues. Moreover, transformers inherently struggle with long-term dependencies and computational efficiency. Building on the recent success of Structured State Space Sequence (S4) models, known for their parameter efficiency, faster inference, and superior handling of long context lengths, we propose a novel application of S4-based models to offline MARL tasks. Our method utilizes S4's efficient convolutional view for offline training and its recurrent dynamics for fast on-policy fine-tuning. To foster scalable cooperation between agents, we sequentially expand the decision-making process, allowing agents to act one after another at each time step. This design promotes bi-directional cooperation, enabling agents to share information via their S4 latent states or memory with minimal communication. Gradients also flow backward through this shared information, linking the current agent's learning to its predecessor. Experiments on challenging MARL benchmarks, including Multi-Robot Warehouse (RWARE) and StarCraft Multi-Agent Challenge (SMAC), demonstrate that our approach significantly outperforms state-of-the-art offline RL and transformer-based MARL baselines across most tasks.",2025,0.443181657024852,0.4410890215665304,0.4,0.25,178c1174-117a-4505-a785-e3339e9491bd,0,"[0.25, 0.25, 0.5, 0.625]","[0.9, 1.0, 0.9, 0.95]",S4-based MARL,0.396027541012583
545242,Executing Arithmetic: Fine-Tuning Large Language Models as Turing Machines,"Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide range of natural language processing and reasoning tasks. However, their performance in the foundational domain of arithmetic remains unsatisfactory. When dealing with arithmetic tasks, LLMs often memorize specific examples rather than learning the underlying computational logic, limiting their ability to generalize to new problems. In this paper, we propose a Composable Arithmetic Execution Framework (CAEF) that enables LLMs to learn to execute step-by-step computations by emulating Turing Machines, thereby gaining a genuine understanding of computational logic. Moreover, the proposed framework is highly scalable, allowing composing learned operators to significantly reduce the difficulty of learning complex operators. In our evaluation, CAEF achieves nearly $100\\%$ accuracy across seven common mathematical operations on the LLaMA 3.1-8B model, effectively supporting computations involving operands with up to 100 digits, a level where GPT-4o falls short noticeably in some settings.",2025,0.5113634504132908,0.5100655425612787,0.5333333333333333,0.5,9f233a45-53b1-448a-87cb-d7c90c90238d,0,"[0.25, 0.5, 0.5, 0.625]","[0.95, 0.95, 0.9, 0.95]",Composable Arithmetic Execution Framework,0.4668634371957156
545262,Astral: training physics-informed neural networks with error majorants,"The primal approach to physics-informed learning is a residual minimization. We argue that residual is, at best, an indirect measure of the error of approximate solution and propose to train with error majorant instead. Since error majorant provides a direct upper bound on error, one can reliably estimate how close PiNN is to the exact solution and stop the optimization process when the desired accuracy is reached. We call loss function associated with error majorant **Astral**: neur**A**l a po**ST**erio**R**i function**A**l **L**oss. To compare Astral and residual loss functions, we illustrate how error majorants can be derived for various PDEs and conduct experiments with diffusion equations (including anisotropic and in the L-shaped domain), convection-diffusion equation, temporal discretization of Maxwell's equation, magnetostatics and nonlinear elastoplasticity problems. The results indicate that Astral loss is competitive to the residual loss, typically leading to faster convergence and lower error (e.g., for Maxwell's equations, we observe an order of magnitude better relative error and training time). The main benefit of using Astral loss comes from its ability to estimate error, which is impossible with other loss functions. Our experiments indicate that the error estimate obtained with Astral loss is usually tight enough, e.g., for a highly anisotropic equation, on average, Astral overestimates error by a factor of $1.5$, and for convection-diffusion by a factor of $1.7$. We further demonstrate that Astral loss is better correlated with error than residual and is a more reliable predictor (in a statistical sense) of the error value. Moreover, unlike residual, the error indicator obtained from Astral loss has a superb spatial correlation with error. Backed with the empirical and theoretical results, we argue that one can productively use Astral loss to perform reliable error analysis and approximate PDE solutions with accuracy similar to standard residual-based techniques.",2025,0.5909092396693675,0.5907445364862304,0.5333333333333333,0.5,a68d1900-0862-44cf-a903-0b7f25fcbfb2,0,"[0.5, 0.5, 0.625]","[0.9, 0.95, 0.95]",Astral,0.5450894335872805
545270,LoRanPAC: Low-rank Random Features and Pre-trained Models for Bridging Theory and Practice in Continual Learning,"The goal of continual learning (CL) is to train a model that can solve multiple tasks presented sequentially. Recent CL approaches have achieved strong performance by leveraging large pre-trained models that generalize well to downstream tasks. However, such methods lack theoretical guarantees, making them prone to unexpected failures. Conversely, principled CL approaches often fail to achieve competitive performance. In this work, we aim to bridge this gap between theory and practice by designing a simple CL method that is theoretically sound and highly performant. Specifically, we lift pre-trained features into a higher dimensional space and formulate an over-parametrized minimum-norm least-squares problem. We find that the lifted features are highly ill-conditioned, potentially leading to large training errors (numerical instability) and increased generalization errors. We address these challenges by continually truncating the singular value decomposition of the lifted features. Our approach, termed LoRanPAC, is stable with respect to the choice of hyperparameters, can handle hundreds of tasks, and outperforms state-of-the-art CL methods on multiple datasets. Importantly, our method satisfies a recurrence relation throughout its continual learning process, which allows us to prove it maintains small training and test errors by appropriately truncating a fraction of SVD factors. This results in a stable continual learning method with strong empirical performance and theoretical guarantees. Code available: \url{https://github.com/liangzu/loranpac}.",2025,0.613636140495949,0.6156709671938678,0.6,0.5,bf74e818-d1b1-4cf5-85bf-acaa2f496313,1,"[0.5, 0.5, 0.625, 0.625]","[0.7, 0.9, 0.8, 0.95]",LoRanPAC,0.5715016718205592
545275,Off-policy Evaluation with Deeply-abstracted States,"Off-policy evaluation (OPE) is crucial  for assessing a target policy's impact offline before its deployment. However, achieving accurate OPE in  large state spaces remains challenging. This paper studies state abstractions -- originally designed for policy learning -- in the context of OPE. Our contributions are three-fold: (i) We define a set of irrelevance conditions central to learning state abstractions for OPE, and derive a backward-model-irrelevance condition for achieving irrelevance in  (marginalized) importance sampling ratios by constructing a time-reversed Markov decision process (MDP) based on the standard MDP. (ii) We propose a novel iterative procedure that sequentially projects the original state space into a smaller space, resulting in a deeply-abstracted state, which substantially simplify the sample complexity of OPE arising from high cardinality. (iii) We prove the Fisher consistencies of various OPE estimators when applied to our proposed abstract state spaces.",2025,0.4090907603306326,0.400852989930101,0.4,0.25,40db92d5-3d5f-4c80-9b29-3a492f3a06c8,0,"[0.25, 0.25, 0.5, 0.5]","[0.95, 0.95, 0.9, 0.8]",Deeply-abstracted States,0.3472146735955271
545279,Seeking Flat Minima with Mean Teacher on Semi- and Weakly-Supervised Domain Generalization for Object Detection,"Object detectors do not work well when domains largely differ between training and testing data. To overcome this domain gap in object detection without requiring expensive annotations, we consider two problem settings: semi-supervised domain generalizable object detection (SS-DGOD) and weakly-supervised DGOD (WS-DGOD). In contrast to the conventional domain generalization for object detection that requires labeled data from multiple domains, SS-DGOD and WS-DGOD require labeled data only from one domain and unlabeled or weakly-labeled data from multiple domains for training. In this paper, we show that object detectors can be effectively trained on the two settings with the same Mean Teacher learning framework, where a student network is trained with pseudo-labels output from a teacher on the unlabeled or weakly-labeled data. We provide novel interpretations of why the Mean Teacher learning framework works well on the two settings in terms of the relationships between the generalization gap and flat minima in parameter space. On the basis of the interpretations, we also show that incorporating a simple regularization method into the Mean Teacher learning framework leads to flatter minima. The experimental results demonstrate that the regularization leads to flatter minima and boosts the performance of the detectors trained with the Mean Teacher learning framework on the two settings.",2025,0.613636140495949,0.6126236410646255,0.6,0.5,5addfdc4-8097-40a4-aa99-d3b8ecd6d88e,0,"[0.5, 0.5, 0.625, 0.625]","[0.95, 0.95, 0.95, 0.95]",Flat Minima,0.5624999999999999
545294,Ouroboros3D: Image-to-3D Generation via 3D-aware Recursive Diffusion,"Existing image-to-3D creation methods typically split the task into multi-view image generation and 3D reconstruction, leading to two main limitations: (1) multi-view bias, where geometric inconsistencies arise because multi-view diffusion models ensure image-level rather than 3D consistency; (2) misaligned reconstruction data, since reconstruction models trained on mostly synthetic data misalign when processing generated multi-view images during inference. To address these issues, we propose Ouroboros3D, a unified framework that integrates multi-view generation and 3D reconstruction into a recursive diffusion process. By incorporating a 3D-aware feedback mechanism, our multi-view diffusion model leverages the explicit 3D information from the reconstruction results of the previous denoising process as conditions, thus modeling consistency at the 3D geometric level. Furthermore, through joint training of both the multi-view diffusion and reconstruction models, we alleviate reconstruction bias due to data misalignment and enable mutual enhancement within the multi-step recursive process. Experimental results demonstrate that Ouroboros3D outperforms methods that treat these stages separately and those that combine them only during inference, achieving superior multi-view consistency and producing 3D models with higher geometric realism.",2025,0.5454543471075102,0.5445543476130005,0.5333333333333333,0.5,dce1cfd2-d6ca-41b3-9343-40609d3da9fc,0,"[0.5, 0.5, 0.5, 0.5]","[0.9, 0.9, 0.95, 0.95]",Ouroboros3D,0.5
545295,CycleResearcher: Improving Automated Research via Automated Review,"The automation of scientific discovery has been a long-standing goal within the research community, driven by the potential to accelerate knowledge creation. While significant progress has been made using commercial large language models (LLMs) as research assistants or idea generators, the possibility of automating the entire research process with open-source LLMs remains largely unexplored. This paper explores the feasibility of using open-source post-trained LLMs as autonomous agents capable of performing the full cycle of automated research and review, from literature review and manuscript preparation to peer review and paper refinement. Our iterative preference training framework consists of CycleResearcher, which conducts research tasks, and CycleReviewer, which simulates the peer review process, providing iterative feedback via reinforcement learning. To train these models, we develop two new datasets, Review-5k and Research-14k, reflecting real-world machine learning research and peer review dynamics. Our results demonstrate that CycleReviewer achieves promising performance with a 26.89\% reduction in mean absolute error (MAE) compared to individual human reviewers in predicting paper scores, indicating the potential of LLMs to effectively assist expert-level research evaluation. In research, the papers generated by the CycleResearcher model achieved a score of 5.36 in simulated peer reviews, showing some competitiveness in terms of simulated review scores compared to the preprint level of 5.24 from human experts, while still having room for improvement compared to the accepted paper level of 5.69. This work represents a significant step toward fully automated scientific inquiry, providing ethical safeguards and exploring AI-driven research capabilities. The code, dataset and model weight are released at https://wengsyx.github.io/Researcher.",2025,0.7499997272728265,0.752344306466474,0.6666666666666666,0.625,ceca425f-7e1b-4e75-8c67-8a032ecc8dd2,1,"[0.625, 0.625, 0.625, 0.875]","[0.9, 0.95, 0.95, 1.0]",CycleResearcher,0.7066927567609226
545301,Naturality-Guided Hyperedge Disentanglement for Message Passing Hypergraph Neural Network,"Hypergraph data structure has been widely used to store information or meaning derived from group interactions, meaning that each hyperedge inherently contains the context of their interactions. For example, a set of genes or a genetic pathway can be represented as a hyperedge to express the interaction of multiple genes that collaboratively perform a biological function (i.e., interaction context). However, most existing hypergraph neural networks cannot reflect the interaction context of each hyperedge due to their limited capability in capturing important or relevant factors therein. In this paper, we propose a \textbf{simple but effective} hyperedge disentangling method, \textbf{Natural-HNN}, that captures inherent hyperedge types or the interaction context of an hyperedge. We devised a novel guidance for hyperedge disentanglement based on the naturality condition in the category theory. In our experiments, we applied our model to hypergraphs of genetic pathways for the cancer subtype classification task, and showed that our model outperforms baselines by capturing the functional semantic similarity of genetic pathways.",2025,0.5909092396693675,0.5785889943388131,0.5333333333333333,0.25,a6c06f2b-91e2-4e01-8f9a-68a2c87bc514,0,"[0.25, 0.5, 0.875]","[1.0, 0.9, 0.9]",Naturality-HNN,0.4838071825566482
545302,No Preference Left Behind: Group Distributional Preference Optimization,"Preferences within a group of people are not uniform but follow a distribution. While existing alignment methods like Direct Preference Optimization (DPO) attempt to steer models to reflect human preferences, they struggle to capture the distributional pluralistic preferences within a group. These methods often skew toward dominant preferences, overlooking the diversity of opinions, especially when conflicting preferences arise. To address this issue, we propose Group Distributional Preference Optimization (GDPO), a novel framework that aligns language models with the distribution of preferences within a group by incorporating the concept of beliefs that shape individual preferences. GDPO calibrates a language model using statistical estimation of the group's belief distribution and aligns the model with belief-conditioned preferences, offering a more inclusive alignment framework than traditional methods. In experiments using both synthetic controllable opinion generation and real-world movie review datasets, we show that DPO fails to align with the targeted belief distributions, while GDPO consistently reduces this alignment gap during training. Additionally, our evaluation metrics demonstrate that GDPO outperforms existing approaches in aligning with group distributional preferences, marking a significant advance in pluralistic alignment.",2025,0.5454543471075102,0.5445543476130005,0.6666666666666666,0.625,a5baca48-2438-4096-9b23-cabd1841a9d0,1,"[0.25, 0.625, 0.625]","[0.9, 0.9, 0.9]",Group Distributional Preference Optimization,0.5
545310,Making Transformer Decoders Better Differentiable Indexers,"Retrieval aims to find the top-k items most relevant to a query/user from a large dataset. Traditional retrieval models represent queries/users and items as embedding vectors and use Approximate Nearest Neighbor (ANN) search for retrieval. Recently, researchers have proposed a generative-based retrieval method that represents items as token sequences and uses a decoder model for autoregressive training. Compared to traditional methods, this approach uses more complex models and integrates index structure during training, leading to better performance. However, these methods remain two-stage processes, where index construction is separate from the retrieval model, limiting the model's overall capacity. Additionally, existing methods construct indices by clustering pre-trained item representations in Euclidean space. However, real-world scenarios are more complex, making this approach less accurate. To address these issues, we propose a \underline{U}nified framework for \underline{R}etrieval and \underline{I}ndexing, termed \textbf{URI}. URI ensures strong consistency between index construction and the retrieval model, typically a Transformer decoder. URI simultaneously builds the index and trains the decoder, constructing the index through the decoder itself. It no longer relies on one-sided item representations in Euclidean space but constructs the index within the interactive space between queries and items. Experimental comparisons on three real-world datasets show that URI significantly outperforms existing methods.",2025,0.6818179338843877,0.6806929345162506,0.6666666666666666,0.625,c5775ee1-bdca-4eb9-aaf8-f1a96289ccbe,1,"[0.625, 0.625, 0.625, 0.625, 0.625]","[0.95, 0.9, 0.9, 0.9, 0.9]",URI,0.6250000000000001
545321,TimeBridge: Non-Stationarity Matters for Long-term Time Series Forecasting,"Non-stationarity poses significant challenges for multivariate time series forecasting due to the inherent short-term fluctuations and long-term trends that can lead to spurious regressions or obscure essential long-term relationships. Most existing methods either eliminate or retain non-stationarity without adequately addressing its distinct impacts on short-term and long-term modeling. Eliminating non-stationarity is essential for avoiding spurious regressions and capturing local dependencies in short-term modeling, while preserving it is crucial for revealing long-term cointegration across variates. In this paper, we propose TimeBridge, a novel framework designed to bridge the gap between non-stationarity and dependency modeling in long-term time series forecasting. By segmenting input series into smaller patches, TimeBridge applies Integrated Attention to mitigate short-term non-stationarity and capture stable dependencies within each variate, while Cointegrated Attention preserves non-stationarity to model long-term cointegration across variates. Extensive experiments show that TimeBridge consistently achieves state-of-the-art performance in both short-term and long-term forecasting. Additionally, TimeBridge demonstrates exceptional performance in financial forecasting on the CSI 500 and S\&P 500 indices, further validating its robustness and effectiveness. The code is available in the supplementary material.",2025,0.4999994545456529,0.4991744556090187,0.2666666666666666,0.25,cf8401f6-2fbd-4290-8670-75187c1669d6,0,"[0.25, 0.25, 0.875]","[0.95, 0.95, 0.95]",TimeBridge,0.4583333333333332
545322,Robotic Programmer: Video Instructed Policy Code Generation for Robotic Manipulation,"Zero-shot generalization across various robots, tasks and environments remains a significant challenge in robotic manipulation. Policy code generation methods use executable code to connect high-level task descriptions and low-level action sequences, leveraging the generalization capabilities of large language models and atomic skill libraries. In this work, we propose Robotic Programmer (RoboPro), a robotic foundation model, enabling the capability of perceiving visual information and following free-form instructions to perform robotic manipulation with policy code in a zero-shot manner. To address low efficiency and high cost in collecting runtime code data for robotic tasks, we devise Video2Code to synthesize executable code from extensive videos in-the-wild with off-the-shelf vision-language model and code-domain large language model. Extensive experiments show that RoboPro achieves the state-of-the-art zero-shot performance on robotic manipulation in both simulators and real-world environments. Specifically, the zero-shot success rate of RoboPro on RLBench surpasses the state-of-the-art model GPT-4o by 11.6\%, which is even comparable to a strong supervised training baseline. Furthermore, RoboPro is robust to different robotic configurations, and demonstrates broad visual understanding in general VQA tasks.",2025,0.443181657024852,0.4424504074355629,0.4,0.25,65beaf2a-41cb-4209-af90-e5401b0a0e10,0,"[0.25, 0.25, 0.5, 0.625]","[0.95, 0.95, 0.95, 0.95]",RoboPro,0.40625
545341,Counterfactual Techniques for Enhancing Customer Retention,"In this paper, we introduce a novel counterfactual reasoning method using eBERT embeddings to convert customers from an e-commerce company who frequently add items to their cart but don’t proceed to checkout. We demonstrate that our method i) outperforms existing techniques such as DiCE, GANs, and CFRL in key metrics such as coverage, while also maintaining a low latency; ii) balances high coverage and low latency by adjusting the number of nearest unlike neighbors, highlighting a trade-off between these competing goals; and iii) allows customization of  mutable features, improving the practical applicability of our counterfactual explanations.",2025,0.3749998636364132,0.3769993312872611,0.2666666666666666,0.25,c1282407-3f55-4eba-90cf-4ff67184026d,0,"[0.0, 0.25, 0.25, 0.875]","[1.0, 0.95, 0.95, 1.0]",eBERT embeddings,0.3575630968622101
545353,Spiking GS: Towards High-Accuracy and Low-Cost Surface Reconstruction via Spiking Neuron-based Gaussian Splatting,"3D Gaussian Splatting is capable of reconstructing 3D scenes in minutes. Despite recent advances in improving surface reconstruction accuracy, the reconstructed results still exhibit bias and suffer from inefficiency in storage and training. This paper provides a different observation on the cause of the inefficiency and the reconstruction bias, which is attributed to the integration of the low-opacity parts (LOPs) of the generated Gaussians. We show that LOPs consist of Gaussians with overall low-opacity (LOGs) and the low-opacity tails (LOTs) of Gaussians. We propose Spiking GS to reduce such two types of LOPs by integrating spiking neurons into the Gaussian Splatting pipeline. Specifically, we introduce global and local full-precision integrate-and-fire spiking neurons to the opacity and representation function of flattened 3D Gaussians, respectively. Furthermore, we enhance the density control strategy with spiking neurons' thresholds and an new criterion on the scale of Gaussians. Our method can represent more accurate reconstructed surfaces at a lower cost. The code is available at https://anonymous.4open.science/r/SpikingGS-D721.",2025,0.5795452438017296,0.5781228558172563,0.5333333333333333,0.5,f48f6133-138c-4ba9-baaf-8206ce062ea0,0,"[0.5, 0.5, 0.5, 0.625]","[0.95, 1.0, 0.8, 0.9]",Spiking GS,0.5266802375152778
545361,MF-LAL: Drug Compound Generation Using Multi-Fidelity Latent Space Active Learning,"Current generative models for drug discovery primarily use molecular docking as an oracle to guide the generation of active compounds. However, such models are often not useful in practice because even compounds with high docking scores do not consistently show experimental activity. More accurate methods for activity prediction exist, such as molecular dynamics based binding free energy calculations, but they are too computationally expensive to use in a generative model. To address this challenge, we propose Multi-Fidelity Latent space Active Learning (MF-LAL), a generative modeling framework that integrates a set of oracles with varying cost-accuracy tradeoffs. We train a surrogate model for each oracle and use these surrogates to generate compounds with high predicted activity. Unlike previous approaches that separately learn the surrogate model and generative model, MF-LAL combines the generative and multi-fidelity surrogate models into a single framework, allowing for more accurate activity prediction and higher quality samples. We train MF-LAL with a novel active learning algorithm to further reduce computational cost. Our experiments on two disease-relevant proteins show that MF-LAL produces compounds with significantly better binding free energy scores than other single and multi-fidelity approaches.",2025,0.5727270644628857,0.5714988967328917,0.5333333333333333,0.5,d06fbe99-dd7f-4503-a9c0-115fec8b5b15,0,"[0.25, 0.5, 0.5, 0.5, 0.875]","[0.95, 0.95, 0.95, 1.0, 0.95]",MF-LAL,0.5233837174498653
545366,Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling,"Efficiently modeling sequences with infinite context length has long been a challenging problem. Previous approaches have either suffered from quadratic computational complexity or limited extrapolation ability in length generalization. In this
work, we present Samba, a simple hybrid architecture that layer-wise combines
Mamba, a selective State Space Model (SSM), with Sliding Window Attention
(SWA). Samba selectively compresses a given sequence into recurrent hidden
states while still maintaining the ability to precisely recall recent memories with the
attention mechanism. We scale Samba up to 3.8B parameters with 3.2T training
tokens and demonstrate that it significantly outperforms state-of-the-art models
across a variety of benchmarks. Pretrained on sequences of 4K length, Samba
shows improved perplexity in context lengths of up to 1M in zero-shot. When
finetuned on 4K-length sequences, Samba efficiently extrapolates to a 256K context length with perfect memory recall on the Passkey Retrieval task, and exhibits
superior retrieval extrapolation on the challenging Phonebook task compared to
full-attention models. As a linear-time sequence model, Samba achieves a 3.73×
higher throughput compared to Transformers with grouped-query attention for user
prompts of 128K length, and a 3.64× speedup when generating 64K tokens with
unlimited streaming.",2025,0.772726628099408,0.7745817278055985,0.6666666666666666,0.625,837f6127-ce40-4a5d-b773-03f3cb871505,1,"[0.625, 0.625, 0.875]","[0.95, 0.95, 1.0]",Samba,0.7255499760879962
545373,LANE: Label-Aware Noise Elimination for Fine-Grained Text Classification,"We propose Label-Aware Noise Elimination (LANE), a new approach that improves the robustness of deep learning models in fine-grained text classification when trained under increased label noise. LANE leverages the semantic relations between classes and monitors the training dynamics of the model on each training example to dynamically lower the importance of training examples that may have noisy labels. We test the effectiveness of LANE in fine-grained text classification and benchmark our approach on a wide variety of datasets with various number of classes and various amounts of label noise. LANE considerably outperforms strong baselines on all datasets, obtaining significant improvements ranging from an average improvement of 2.4% in F1 on manually annotated datasets to a considerable average improvement of 4.5% F1 on datasets with injected noisy labels. We carry out comprehensive analyses of LANE and identify the key components that lead to its success.",2025,0.5454543471075102,0.536884844181219,0.6,0.625,cadab05b-0c9d-4748-a3a8-474b16dd89fd,0,"[0.25, 0.5, 0.625, 0.625]","[0.95, 0.9, 0.8, 0.9]",Label-Aware Noise Elimination,0.4701365291806456
545378,An efficient implementation for solving the all pairs minimax path problem in an undirected dense graph,"We provide an efficient $ O(n^2) $ implementation for solving the all pairs minimax path problem or  widest path problem in an undirected dense graph. It is a code implementation of the Algorithm 4 (MMJ distance by Calculation and Copy) in a previous paper. The distance matrix is also called the all points path distance (APPD). We conducted experiments to test the implementation and algorithm, compared it with several other algorithms for solving the APPD matrix.  Result shows Algorithm 4 works good for solving the widest path or minimax path APPD matrix.  It can drastically improve the efficiency for computing the APPD matrix.  There are several theoretical outcomes which claim the APPD matrix can be solved accurately in $ O(n^2) $ . However, they are impractical because there is no code implementation of these algorithms. It seems Algorithm 4 is the first algorithm that has an actual code implementation for solving the APPD matrix of minimax path or widest path problem in $ O(n^2) $, in an undirected dense graph.",2025,0.0,0.0,0.0,0.0,8821863b-ce01-4b1b-b9d8-8176fde9c0e6,0,"[0.0, 0.0, 0.0, 0.0]","[1.0, 1.0, 0.95, 0.95]",APPD,0.0212075462007768
545388,A Quantum Circuit-Based Compression Perspective for Parameter-Efficient Learning,"Quantum-centric supercomputing presents a compelling framework for large-scale hybrid quantum-classical tasks. Although quantum machine learning (QML) offers theoretical benefits in various applications, challenges such as large-size data encoding in the input stage and the reliance on quantum resources in the inference stage limit its practicality for tasks like fine-tuning large language models (LLMs). Quantum parameter generation, a novel approach of QML, addresses these limitations by using quantum neural networks (QNNs) to generate classical model weights (parameters) exclusively during training, thereby decoupling inference from quantum hardware. In this work, we introduce Quantum Parameter Adaptation (QPA) in the framework of quantum parameter generation, which integrates QNNs with a classical multi-layer perceptron mapping model to generate parameters for fine-tuning methods. Using Gemma-2 and GPT-2 as case studies, QPA demonstrates significant parameter reduction for parameter-efficient fine-tuning methods, such as Low-Rank Adaptation (LoRA), while maintaining comparable or improved performance in text generation tasks. Specifically, QPA reduces the number of  parameters to $52.06\%$ of the original LoRA for GPT-2 with a slight performance gain of $0.75\%$, and to $16.84\%$ for Gemma-2, with a marginal performance improvement of $0.07\%$. These results highlight QPA’s ability to achieve efficient parameter reduction without sacrificing performance in the quantum parameter generation framework. This work showcases the potential of quantum-enhanced parameter reduction, offering a scalable quantum-classical solution for fine-tuning LLMs while preserving the feasibility of inference on classical hardware.",2025,0.6818179338843877,0.6806929345162506,0.6666666666666666,0.625,51fd5af6-48d5-4f1e-96da-bc66461ad8f2,1,"[0.625, 0.625, 0.625, 0.625, 0.625]","[0.9, 0.95, 0.9, 0.95, 0.95]",Quantum Parameter Adaptation,0.6250000000000001
545390,Mini-batch Coresets for Memory-efficient Language Model Training on Data Mixtures,"Training with larger mini-batches improves the convergence rate and can yield superior performance. However, training with large mini-batches becomes prohibitive for Large Language Models (LLMs), due to the large GPU memory requirement. To address this problem, an effective approach is finding small mini-batch coresets that closely match the gradient of larger mini-batches. However, this approach becomes infeasible and ineffective for LLMs, due to the highly imbalanced mixture of sources in language data, use of the Adam optimizer, and the very large gradient dimensionality of LLMs. In this work, we address the above challenges by proposing *Coresets for Training LLMs* (CoLM). First, we show that mini-batch coresets found by gradient matching do not contain representative examples of the small sources w.h.p., and thus including all examples of the small sources in the mini-batch coresets is crucial for optimal performance. Second, we normalize the gradients by their historical exponential to find mini-batch coresets for training with Adam. Finally, we leverage zeroth-order methods to find smooth gradient of the last *V*-projection matrix and sparsify it to keep the dimensions with the largest normalized gradient magnitude. We apply CoLM to fine-tuning Phi-2, Phi-3, Zephyr, and Llama-3 models with LoRA on MathInstruct and SuperGLUE benchmark. Remarkably, CoLM reduces the memory requirement of fine-tuning by 2x and even outperforms training with 4x larger mini-batches. Moreover, CoLM seamlessly integrates with existing memory-efficient training methods like LoRA, further reducing the memory requirements of training LLMs.",2025,0.7840906239670459,0.7841582605627206,0.8,0.875,6245b570-d65c-484f-a7ba-97a0824a1efd,1,"[0.5, 0.625, 0.875, 0.875]","[0.95, 0.9, 0.95, 0.95]",Coresets for Training LLMs,0.724409688412853
545402,DeMo: Decoupled Momentum Optimization,"Training large scale neural networks typically involves sharing the gradients between all accelerators, which necessitates specialized high-speed interconnects. Taking cues from signal processing, we show that it is not necessary to share or synchronize the full optimizer states and model parameters during training. By decoupling the momentum and allowing divergence in the optimizer states across accelerators, it is possible to even improve convergence compared to previous state of the art optimizers.
From this, we introduce a Decoupled Momentum optimization algorithm (DeMo) that reduces the communication requirements by several orders of magnitude, potentially enabling future training of large neural networks on slow internet bandwidths with heterogeneous networking hardware. Furthermore, our method is agnostic to the network topology and neural network architecture, and supports scalable clock-synchronous distributed training with negligible compute and memory overhead.
Empirically, we show that models trained with DeMo match or surpass the performance of equal models trained with AdamW, entirely bypassing the need for high-speed interconnects for pre-training large scale foundation models.",2025,0.218181738843004,0.2154278781330934,0.2666666666666666,0.25,0c8e13d2-04b4-4e46-b519-cd0ea6f19bef,0,"[0.0, 0.25, 0.25, 0.25, 0.25]","[0.95, 0.8, 0.95, 0.9, 0.95]",Decoupled Momentum,0.1914871473735896
545405,Refuse Whenever You Feel Unsafe: Improving Safety in LLMs via Decoupled Refusal Training,"This study addresses a critical gap in safety tuning practices for Large Language Models (LLMs) by identifying and tackling a refusal position bias within safety tuning data, which compromises the models' ability to appropriately refuse generating unsafe content.  We introduce a novel approach, Decoupled Refusal Training (DeRTa), designed to empower LLMs to refuse compliance to harmful prompts at any response position, significantly enhancing their safety capabilities. DeRTa incorporates two novel components: (1) Maximum Likelihood Estimation (MLE) with Harmful Response Prefix, which trains models to recognize and avoid unsafe content by appending a segment of harmful response to the beginning of a safe response, and (2) Reinforced Transition Optimization (RTO), which equips models with the ability to transition from potential harm to safety refusal consistently throughout the harmful response sequence. Our empirical evaluation, conducted using LLaMA3 and Mistral model families across six attack scenarios, demonstrates that our method not only improves model safety without compromising performance but also surpasses well-known models such as GPT-4 in defending against attacks. Importantly, our approach successfully defends recent advanced attack methods that have jailbroken GPT-4 and LLaMA3-70B-Instruct.",2025,0.4363634776860081,0.4344846664386799,0.5333333333333333,0.5,c1cd8670-7c51-4484-8cef-49c795c5349c,0,"[0.25, 0.25, 0.5, 0.5, 0.5]","[0.95, 0.95, 0.95, 0.95, 0.9]",Decoupled Refusal Training,0.3952280161631711
545415,Efficient Sparse Single-stage 3D Visual Grounding with Text-guided Pruning,"In this paper, we propose an efficient sparse convolution-based architecture called ESS3D for 3D visual grounding. Conventional 3D visual grounding methods are difficult to meet the requirements of real-time inference due to the two-stage or point-based architecture. Inspired by the success of multi-level fully sparse convolutional architecture in 3D object detection, we aim to build a new 3D visual grounding framework following this technical route. However, as in visual grounding task the 3D scene representation should be deeply interacted with text features, sparse convolution-based architecture is inefficient for this interaction due to the large amount of voxel features. To this end, we propose text-guided pruning (TGP) and completion-based addition (CBA) to deeply fuse 3D scene representation and text features in an efficient way by gradual region pruning and target completion. Specifically, TGP iteratively sparsifies the 3D scene representation and thus efficiently interacts the voxel features with text features by cross-attention. To mitigate the affect of pruning on delicate geometric information, CBA adaptively fixes the over-pruned region by voxel completion with negligible computational overhead. Compared with previous single-stage methods, ESS3D achieves top inference speed and surpasses previous fastest method by 100\% FPS. ESS3D also achieves state-of-the-art accuracy even compared with two-stage methods, with $+1.13$ lead of Acc@0.5 on ScanRefer, and $+5.4$ and $+5.0$ leads on NR3D and SR3D respectively. The code will be released soon.",2025,0.6818179338843877,0.6769115491264259,0.6,0.5,06f4aec6-9273-4c22-957b-31e8612a8235,0,"[0.5, 0.5, 0.625, 0.875]","[0.95, 0.95, 0.8, 0.9]",Text-guided pruning,0.607602956405735
545428,Risk-Sensitive Diffusion: Robustly Optimizing Diffusion Models with Noisy Samples,"Diffusion models are mainly studied on image data. However, non-image data (e.g., tabular data) are also prevalent in real applications and tend to be noisy due to some inevitable factors in the stage of data collection, degrading the generation quality of diffusion models. In this paper, we consider a novel problem setting where every collected sample is paired with a vector indicating the data quality: risk vector. This setting applies to many scenarios involving noisy data and we propose risk-sensitive SDE, a type of stochastic differential equation (SDE) parameterized by the risk vector, to address it. With some proper coefficients, risk-sensitive SDE can minimize the negative effect of noisy samples on the optimization of diffusion models. We conduct systematic studies for both Gaussian and non-Gaussian noise distributions, providing analytical forms of risk-sensitive SDE. To verify the effectiveness of our method, we have conducted extensive experiments on multiple tabular and time-series datasets, showing that risk-sensitive SDE permits a robust optimization of diffusion models with noisy samples and significantly outperforms previous baselines.",2025,0.7159088305786071,0.7190415407838533,0.6666666666666666,0.625,7af81181-4e4e-45ac-ba76-19635dc67337,1,"[0.5, 0.625, 0.625, 0.875]","[0.9, 0.8, 0.9, 0.95]",Risk-Sensitive SDE,0.6754022542564451
545467,Measuring LLM Confidence through Stable Explanations,"In many high-risk machine learning applications it is essential for a model to indicate when it is uncertain about a prediction. While large language models (LLMs) can reach and even surpass human-level accuracy on a variety of benchmarks, their overconfidence in incorrect responses is still a well-documented failure mode. Traditional methods for ML uncertainty quantification can be difficult to directly adapt to LLMs due to the computational cost of implementation and closed-source nature of many models. A variety of black-box methods have recently been proposed, but these often rely on heuristics such as self-verbalized confidence. We instead propose a framework for measuring an LLM's uncertainty with respect to the distribution of generated explanations for an answer. While utilizing explanations is not a new idea in and of itself, by interpreting each possible model+explanation pair as a test-time classifier we can calculate a posterior answer distribution over the most likely of these classifiers. We demonstrate how a specific instance of this framework using explanation entailment as our classifier likelihood improves confidence score metrics (in particular AURC and AUROC) over baselines across five different datasets. We believe these results indicate that our framework is both a well-principled and effective way of quantifying uncertainty in LLMs.",2025,0.613636140495949,0.6117164135215023,0.6,0.5,0a44d0c9-5058-418e-a6d5-d48f828313e3,0,"[0.5, 0.5, 0.625, 0.625]","[0.95, 0.95, 0.95, 0.9]",Explanation Entailment,0.5587268743914313
545490,Targeted Attack Improves Protection against Unauthorized Diffusion Customization,"Diffusion models build a new milestone for image generation yet raising public concerns, for they can be fine-tuned on unauthorized images for customization. Protection based on adversarial attacks rises to encounter this unauthorized diffusion customization, by adding protective watermarks to images and poisoning diffusion models. However, current protection, leveraging untargeted attacks, does not appear to be effective enough. In this paper, we propose a simple yet effective improvement for the protection against unauthorized diffusion customization by introducing targeted attacks. We show that by carefully selecting the target, targeted attacks significantly outperform untargeted attacks in poisoning diffusion models and degrading the customization image quality. Extensive experiments validate the superiority of our method on two mainstream customization methods of diffusion models, compared to existing protections. To explain the surprising success of targeted attacks, we delve into the mechanism of attack-based protections and propose a hypothesis based on our observation, which enhances the comprehension of attack-based protections. To the best of our knowledge, we are the first to both reveal the vulnerability of diffusion models to targeted attacks and leverage targeted attacks to enhance protection against unauthorized diffusion customization.",2025,0.886363314049704,0.8867403194573625,0.9333333333333332,0.875,2ed1baea-728d-4012-b0ba-07aadb833183,1,"[0.625, 0.875, 0.875, 0.875]","[0.9, 0.95, 0.9, 0.95]",Targeted Attacks,0.8205310875824223
545491,Unveiling Neural Combinatorial Optimization Model Representations Through Probing,"Neural combinatorial optimization (NCO) models have achieved remarkable performance, yet their learned underlying representations remain largely unclear. This hinders real-world application, as industrial stakeholders may want a deeper understanding of NCO models before committing resources. In this paper, we make the first step towards interpreting NCO models by investigating embeddings learned by various architectures through three probing tasks. Specifically, we analyze representative and state-of-the-art attention-based models, including AM, POMO, and LEHD, on the representative Traveling Salesman Problem and Capacitated Vehicle Routing Problem. Our findings reveal that NCO models encode linear representations of Euclidean distances between nodes, while also capturing additional knowledge that help avoid making myopic decisions. Furthermore, we show that architectural choices affect the ability of deep models to accurately represent Euclidean distances and to incorporate non-myopic decision-making strategies. We also verify to what extent NCO models understand the feasibility of constraints. Our work represents an initial effort to interpret NCO models, enhance understanding of why certain architectures outperform others, and demonstrate probing as a valuable tool for analyzing their internal mechanisms.",2025,0.5795452438017296,0.5772276084697805,0.5333333333333333,0.5,e9b0a66c-b042-4eb5-a2e6-e6cea5ce497f,0,"[0.5, 0.5, 0.5, 0.625]","[0.95, 0.95, 0.95, 0.9]",Probing,0.5255903115871472
545497,Multi-Task Consistency-based Detection of Adversarial Attacks,"Deep Neural Networks (DNNs) have found successful deployment in numerous vision perception systems. However, their susceptibility to adversarial attacks has prompted concerns regarding their practical applications, specifically in the context of autonomous driving. Existing research on defenses often suffers from cost inefficiency, rendering their deployment impractical for resource-constrained applications. In this work, we propose an efficient and effective adversarial attack detection scheme leveraging the multi-task perception within a complex vision system. Adversarial perturbations are detected by the inconsistencies between the inference outputs of multiple vision tasks, e.g., objection detection and instance segmentation. To this end, we developed a consistency score metric to measure the inconsistency between vision tasks. Next, we designed an approach to select the best model pairs for detecting this inconsistency effectively. Finally, we evaluated our defense by implementing PGD attacks across multiple vision models on the BDD100k validation dataset. The experimental results demonstrated that our defense achieved a ROC-AUC performance of 99.9% detection within the considered attacker model.",2025,0.3749998636364132,0.3717628966806145,0.2666666666666666,0.25,8927d5e3-6396-4c01-822a-44b13fb5ecf7,0,"[0.25, 0.25, 0.25, 0.625]","[1.0, 0.95, 1.0, 0.95]",Consistency Score,0.3299369031377899
545521,Training Semi-Supervised Deep Learning Models with Heuristic Early Stopping Rules,"Semi-supervised learning (SSL), especially when combined with deep learning (DL) models, is a useful  technique when there is a substantial amount of unlabeled data. This is particularly relevant in healthcare applications, such as mHealth, where data is often collected through smartphones. Labels are typically obtained via self-reported questions delivered by the device and tend to have a high rate of non-response i.e., missing labels. Despite its benefit, there is a lack of objective methodology on how to train semi-supervised deep learning (SSDL) models. In this study, we propose a framework for early-stopping in SSDL that terminates learning to prevent overfitting and before the performance starts to deteriorate. Our approach focuses on three aspects: model stability, generalizability, and high-confidence pseudo-label (i.e., label assigned to unlabeled data during SSL). We first monitor changes in learned weights of the model to assess convergence, using weight stabilization. We also track cross-entropy loss, identifying which iteration of the SSL algorithm minimizes validation loss and improves generalizability. Lastly, we use a sliding window method to assess our confidence in the pseudo-labels, retaining only the most reliable labels during training. Combining these criteria, this SSDL framework can be used to train deep learning models in the context of SSL with an objective criteria that prevents overfitting and improves generalizability. We apply this SSDL training strategy to mHealth data (device sensor data and self-reported data) collected from participants in a clinical trial, which consists of 4,700 observations, 62% of which are unlabeled. Using this objective early stopping criteria for training, we achieve improvements in accuracy and F1 scores, compared to the benchmark model where the early stopping criteria is not applied.",2025,0.2727271735537551,0.2722771738065002,0.2666666666666666,0.25,b2ff86f2-730e-46b8-963f-8f38c0c681c9,0,"[0.25, 0.25, 0.25, 0.25]","[0.95, 0.95, 0.95, 0.95]",Heuristic Early Stopping,0.25
545530,Searching for Optimal Solutions with LLMs via Bayesian Optimization,"Scaling test-time compute to search for optimal solutions is an important step towards building generally-capable language models that can reason. Recent work, however, shows that tasks of varying complexity require distinct search strategies to solve optimally, thus making it challenging to design a one-size-fits-all approach. Prior solutions either attempt to predict task difficulty to select the optimal search strategy, often infeasible in practice, or use a static, pre-defined strategy, e.g., repeated parallel sampling or greedy sequential search, which is sub-optimal. In this work, we argue for an alternative view using the probabilistic framework of Bayesian optimization (BO), where the search strategy is adapted dynamically based on the evolving uncertainty estimates of solutions as search progresses. To this end, we introduce Bayesian-OPRO (BOPRO)––a generalization of a recent method for in-context optimization, which iteratively samples from new proposal distributions by modifying the prompt to the LLM with a subset of its previous generations selected to explore or exploit different parts of the search space. We evaluate our method on word search, molecule optimization, and a joint hypothesis+program search task using a 1-D version of the challenging Abstraction and Reasoning Corpus (1D-ARC). Our results show that BOPRO outperforms all baselines in word search (≥10 points) and molecule optimization (higher quality and 17% fewer invalid molecules), but trails a best-k prompting strategy in program search. Our analysis reveals that despite the ability to balance exploration and exploitation using BOPRO, failure is likely due to the inability of code representation models in distinguishing sequences with low edit-distances.",2025,0.7159088305786071,0.7147275812420631,0.6666666666666666,0.625,b710b5ba-23a8-44d6-bc01-891e01424abb,1,"[0.5, 0.625, 0.625, 0.875]","[0.9, 0.9, 0.9, 0.9]",Bayesian-OPRO,0.6562500000000002
545546,The Effect of Personalization in FedProx: A Fine-grained Analysis on Statistical Accuracy and Communication Efficiency,"FedProx is a simple yet effective federated learning method that enables model personalization via regularization. Despite remarkable success in practice, a rigorous analysis of how such a regularization provably improves the statistical accuracy of each client's local model hasn't been fully established. Setting the regularization strength heuristically presents a risk, as an inappropriate choice may even degrade accuracy. This work fills in the gap by analyzing the effect of regularization on statistical accuracy, thereby providing a theoretical guideline for setting the regularization strength for achieving personalization. We prove that by adaptively choosing the regularization strength under different statistical heterogeneity, FedProx can consistently outperform pure local training and achieve a minimax-optimal statistical rate. In addition, to shed light on resource allocation, we design an algorithm, provably showing that stronger personalization reduces communication complexity without increasing the computation cost overhead. Finally, our theory is validated on both synthetic and real-world datasets and its generalizability is verified in a non-convex setting.",2025,0.5727270644628857,0.5536298903703187,0.6666666666666666,0.625,caa90de6-ff82-4e4e-bddd-7cd57e46eaba,0,"[0.0, 0.5, 0.625, 0.625, 0.875]","[1.0, 0.8, 0.9, 1.0, 0.8]",Personalization,0.4506156294659082
545564,ActSafe: Active Exploration with Safety Constraints for Reinforcement Learning,"Reinforcement learning (RL) is ubiquitous in the development of modern AI systems. However, state-of-the-art RL agents require extensive, and potentially
unsafe, interactions with their environments to learn effectively. These limitations
confine RL agents to simulated environments, hindering their ability to learn
directly in real-world settings. In this work, we present ActSafe, a novel
model-based RL algorithm for safe and efficient exploration. ActSafe learns
a well-calibrated probabilistic model of the system and plans optimistically
w.r.t. the epistemic uncertainty about the unknown dynamics, while enforcing
pessimism w.r.t. the safety constraints. Under regularity assumptions on the
constraints and dynamics, we show that ActSafe guarantees safety during
learning while also obtaining a near-optimal policy in finite time. In addition, we
propose a practical variant of ActSafe that builds on latest model-based RL advancements and enables safe exploration even in high-dimensional settings such
as visual control. We empirically show that ActSafe obtains state-of-the-art
performance in difficult exploration tasks on standard safe deep RL benchmarks
while ensuring safety during learning.",2025,0.7840906239670459,0.7746481634360073,0.8,0.875,c9016982-1752-4ae2-b275-fa485047f2f8,1,"[0.5, 0.625, 0.875, 0.875]","[0.95, 0.9, 0.9, 0.8]",ActSafe,0.6884324934067709
545571,Frequency-Conditioned Diffusion Models for Time Series Generation,"Time series data, commonly used in fields like climate studies, finance, and healthcare, usually faces challenges such as missing data and privacy concerns. Recently, diffusion models have emerged as effective tools for generating high-quality data, but applying them to time series is still difficult, especially for capturing long-range dependencies and complex information. In this paper, we introduce a new diffusion model that uses frequency domain information to improve time series data generation. In particular, we apply Fourier analysis to adaptively separate low-frequency global trends from high-frequency details, which helps the model better understand important patterns during the denoising process. Finally, our approach uses a specialized frequency encoder to integrate this information, enhancing the model's ability to capture both global and local features. Through exhaustive experiments on various public datasets, our model shows an impressive performance in generating time series data for diverse tasks like forecasting and imputation, outperforming existing methods in accuracy and flexibility.",2025,0.4772725537190714,0.4764850541613754,0.5333333333333333,0.5,f583bfe8-00af-4647-a643-b3a26010379a,0,"[0.25, 0.5, 0.5, 0.5]","[0.95, 1.0, 0.95, 0.9]",Frequency-Conditioned Diffusion Models,0.4392889738697067
545587,SaMer: A Scenario-aware Multi-dimensional Evaluator for Large Language Models,"Evaluating the response quality of large language models (LLMs) for open-ended questions poses a significant challenge, especially given the subjectivity and multi-dimensionality of ""quality"" in natural language generation. Existing LLM evaluators often neglect that different scenarios require distinct evaluation criteria. In this work, we propose **SaMer**, a scenario-aware multi-dimensional evaluator designed to provide both overall and fine-grained assessments of LLM-generated responses. Unlike fixed-dimension evaluation approaches, SaMer adapts to different scenarios by automatically identifying and prioritizing relevant evaluation dimensions tailored to the given query. To achieve this, we construct a large-scale fine-grained preference dataset spanning multiple real-world scenarios, each with distinct evaluation dimensions. We then leverage a text embedding model combined with three specialized heads to predict the appropriate evaluation dimensions and corresponding scores, as well as the respective weights that contribute to the overall score. The resulting model offers fine-grained and interpretable evaluations and shows robust adaptability across diverse scenarios. Extensive experiments on eight single rating and pairwise comparison datasets demonstrate that SaMer outperforms existing baselines in a variety of evaluation tasks, showcasing its robustness, versatility, and generalizability.",2025,0.772726628099408,0.7745817278055985,0.6666666666666666,0.625,54fc6269-da97-44a5-a257-4edc1b9c3093,1,"[0.625, 0.625, 0.875]","[0.95, 0.95, 1.0]",SaMer,0.7255499760879962
545609,DRL: DISCRIMINATIVE REPRESENTATION LEARNING FOR CLASS INCREMENTAL LEARNING,"Non-rehearsal class incremental learning (CIL) is pivotal in real-world scenarios such as data streaming applications and data security. 
Despite the remarkable progress in research on CIL, it remains an extremely challenging task due to three  conundrums: increasingly large model complexity, non-smooth representation shift during incremental learning and inconsistency between stage-wise sub-problem optimization and global inference. In this work, we propose the Discriminative Representation Learning (\emph{DRL}) method to deal with these challenges specifically. To conduct incremental learning effectively and yet efficiently, our \emph{DRL} is built upon a pre-trained large model with excellent representation learning capability, and increasingly augments the model by learning a lightweight adapter with a small amount of parameter learning overhead in each incremental learning stage. While the adapter is responsible for adapting the model to new classes of data involved in current learning stage, it can inherit and propagate the representation capability from the current model via parallel connection between them. As a result, such design can guarantee a smooth representation shift between different stages of incremental learning. Furthermore, to alleviate the issue of the training-inference inconsistency induced by the stage-wise sub-optimization, we design the Margin-CE loss, which imposes a hard margin between classification boundaries to push for more discriminative representation learning, thereby narrowing down the gap between stage-wise local optimization over a subset of data and global inference on all classes of data. Extensive experiments on six benchmarks reveal that our \emph{DRL} consistently outperforms other state-of-the-art methods throughout the entire CIL period while maintaining high efficiency in both training and inference phases.",2025,0.4772725537190714,0.4755778266182522,0.5333333333333333,0.5,ff984f15-2f17-426b-9a87-be0d5626e854,0,"[0.25, 0.5, 0.5, 0.5]","[0.95, 0.95, 0.9, 0.95]",Discriminative Representation Learning,0.4337268743914313
545618,EDU-RAG: A RAG Benchmark with Web-enhanced Content in Education Domain. Can RAG Help AI Tutor?,"Hallucination has been a persistent challenge when using Large Language Models (LLMs). Retrieval-Augmented Generation (RAG) has emerged as a popular approach to mitigate this issue by maintaining context and coherence in generated outputs, as well as incorporating customized knowledge. In this paper, we propose a benchmark dataset for evaluating LLM performance in the domain of middle-school science question answering, using textbook questions augmented with real-world web search results. We assess the performance of various LLMs, including GPT-4o, Llama2-7b, and Llama3-8b, with and without the application of RAG. Our goal is to determine whether RAG can reduce hallucinations stemming from the inherent biases of pre-trained LLMs or from the retrieval of irrelevant knowledge, even when relevant information is accessible. The dataset and methodology introduced here provide a robust foundation for advancing the evaluation and development of RAG techniques in mitigating hallucinations across diverse LLMs.",2025,0.1818184793387348,0.1783883805171523,0.2666666666666666,0.25,1822dd00-806c-4cda-92c5-2137ff61b863,0,"[0.0, 0.25, 0.25]","[1.0, 0.95, 0.95]",RAG Benchmark,0.1494502068618578
545622,TextSquare: Scaling up Text-Centric Visual Instruction Tuning,"Text-centric visual question answering (VQA) has made great strides with the development of Multimodal Large Language Models (MLLMs), yet open-source models still fall short of leading models like GPT4V and Gemini. A key contributing factor to this disparity is the absence of extensive, high-quality instruction tuning data. To this end, we introduce a new approach for creating a massive, high-quality instruction-tuning dataset, Square-10M, generated by leveraging the versatile multimodal capabilities of closed-source MLLMs. The data construction process, termed Square, consists of four steps: Self-Questioning, Answering, Reasoning, and Evaluation. Our experiments with Square-10M led to three key findings: 1) Our model, TextSquare, considerably surpasses open-source previous state-of-the-art text-centric MLLMs and sets a new standard on OCRBench (62.2%). It even outperforms top-tier models like GPT4V and Gemini on six out of ten text-centric benchmarks. 2) We demonstrate the importance of VQA reasoning data in offering comprehensive contextual insights for specific questions, which not only improves accuracy but also substantially mitigates hallucinations. Specifically, TextSquare scores an average of 75.1% across four general VQA and hallucination evaluation datasets, outperforming previous state-of-the-art models. 3) Notably, the phenomenon observed in scaling text-centric VQA datasets reveals a vivid pattern: an exponential increase of instruction tuning data volume is directly proportional to the improvement in model performance, thereby validating the necessity of the dataset scale and the high quality of Square-10M.",2025,0.3409089669421938,0.3403464672581253,0.2666666666666666,0.25,98b25e78-21ec-4e67-8428-4bbbc07a353e,0,"[0.25, 0.25, 0.25, 0.5]","[0.95, 0.95, 0.95, 0.95]",Square-10M,0.3125
545628,Rethinking Pre-Training in Tabular Data:  A Neighborhood Embedding Perspective,"Pre-training is prevalent in deep learning for vision and text data, acquiring knowledge from other datasets to improve the downstream tasks. However, when it comes to tabular data, the inherent heterogeneity in the attribute and label spaces among datasets makes it hard to learn shareable knowledge and encode it in a model. We propose **Tab**ular data **P**re-**T**raining via **M**eta-representation (TabPTM), aiming to pre-train a general tabular model over a set of heterogeneous datasets. The key is to embed data instances from any dataset into a common feature space, in which an instance is represented by its distance to a fixed number of nearest neighbors and their labels. Such a meta-representation standardizes heterogeneous tasks into homogeneous local prediction problems, enabling training a model to infer the label (or the score to each possible label) of an input instance based on its neighborhood information. As such, the pre-trained TabPTM can be directly applied to new datasets without further fine-tuning, regardless of their diverse attributes and labels. Extensive experiments on 72 tabular datasets validate TabPTM's effectiveness (with and without fine-tuning) in both tabular classification and regression tasks.",2025,0.6818179338843877,0.6779146182347291,0.5333333333333333,0.5,cef41053-9ba4-4fd9-8c88-10058631ed1f,0,"[0.5, 0.5, 0.875]","[0.95, 0.7, 0.8]",TabPTM,0.601380508185587
545642,Cross-Cultural Recipe Transformation via Neural Network and Encoder-Based Models,"Every cuisine has a culinary fingerprint characterized by its idiosyncratic ingredient composition. Transforming the culinary signature of a recipe is a creative endeavor. Traditionally, such fusion recipes have arisen from creative human interventions as a product of trial and error. Herein, we present a framework to transform the culinary signature of a recipe from one regional cuisine to another. A clustering-based computational strategy was developed, which replaces the ingredients of a recipe, one at a time, to achieve the transformation of the cuisine. We used a neural network-based Word2Vec-Doc2Vec model and three encoder-based BERT models to capture the context of an ingredient within the culinary landscape. The performance of recipe transformation strategies was evaluated by scoring their success at ‘Recipe Transformation’ and manually assessing the most frequent ingredient replacements for every fusion experiment. We observe that the encoder-based models perform better at transforming recipes with fewer ingredient replacements needed, suggesting that BERT-based models are better at providing more meaningful ingredient replacements to transform the culinary signature of recipes. The percentage of successful recipe transformations in the case of Word2Vec-Doc2Vec, BERT-Mean Pooling, BERT-CLS Pooling, and BERT-SBERT model are 99.95%, 43.1%, 41.65%, and 41.45% respectively, indicating that the neural network-based model can better cluster the cuisine-wise ingredient embeddings. On the other hand, for a successful recipe transformation, the average percentage of ingredients replaced for Word2Vec-Doc2Vec, BERT-Mean Pooling, BERT-CLS Pooling, and BERT-SBERT model are 77%, 52.3%, 51.6% and 51.5%, respectively. Our study shows a way forward for implementing cross-cultural fusion of recipes.",2025,0.2727271735537551,0.2722771738065002,0.2666666666666666,0.25,c53436c0-2e97-4264-b9f9-82972b7998c6,0,"[0.25, 0.25, 0.25, 0.25]","[0.95, 1.0, 0.9, 0.95]",Culinary Fingerprint,0.25
545645,ReGRAF: Training free Prompt Refinement via Gradient Flow for Segmentation,"Visual Foundation Models (VFMs) such as the Segment Anything Model (SAM) have significantly advanced segmentation, object detection, and image classification tasks. 
However, SAM and its fine-tuned variants necessitate substantial manual effort for prompt generation and additional training for specific applications. 
Recent methods have addressed these limitations by integrating SAM into one-shot and few-shot segmentation, enabling auto-prompting through semantic alignment between query and support images. 
Despite these advancements, they still generate inadequate prompts that degrade segmentation quality. 
To tackle this limitation, we introduce ReGRAF (Refinement via GRAdient Flow), a training-free method that refines prompts through gradient flow derived from SAM's mask decoder. ReGRAF seamlessly integrates into SAM-based auto-prompting frameworks and is theoretically proven to refine segmentation masks with high efficiency and precision. Extensive evaluations demonstrate that ReGRAF consistently improves segmentation quality across various benchmarks, effectively mitigating false positives without requiring additional training or architectural modifications.",2025,0.5795452438017296,0.5790431526647223,0.5333333333333333,0.5,c9eb6a18-70ef-4e35-9425-cce99b226988,0,"[0.5, 0.5, 0.5, 0.625]","[0.95, 0.95, 0.9, 0.95]",ReGRAF,0.5331365628042843
545660,Closed-Form Interpretation of Neural Network Latent Spaces with Symbolic Gradients,"It has been demonstrated in many scientific fields that artificial neural networks, like autoencoders or Siamese networks, encode meaningful concepts in their latent spaces. However, there does not exist a comprehensive framework for retrieving this information in a human-readable form without prior knowledge. In order to extract these concepts, we introduce a framework for finding closed-form interpretations of neurons in latent spaces of artificial neural networks. The interpretation framework is based on embedding trained neural networks into an equivalence class of functions that encode the same concept. We interpret these neural networks by finding an intersection between the equivalence class and human-readable equations defined by a symbolic search space. The effectiveness of our approach is demonstrated by retrieving invariants of matrices and conserved quantities of dynamical systems from latent spaces of Siamese neural networks.",2025,0.5454543471075102,0.5445543476130005,0.5333333333333333,0.5,432efa97-b3d3-4ecb-87db-43d53fe1f6c0,0,"[0.5, 0.5, 0.5]","[0.9, 0.95, 0.8]",Closed-Form Interpretations,0.5
545662,Interpretable Language Modeling via Induction-head Ngram Models,"Recent large language models (LLMs) have excelled across a wide range of tasks, but their use in high-stakes and compute-limited settings has intensified the demand for interpretability and efficiency. We address this need by proposing Induction-head ngram models (Induction-Gram), a method that builds an efficient, interpretable LM by bolstering modern ngram models with a hand-engineered ``induction head''. This induction head uses a custom neural similarity metric to efficiently search the model's input context for potential next-word completions. This process enables Induction-Gram to provide ngram-level grounding for each generated token. Moreover, experiments show that this simple method significantly improves next-word prediction over baseline interpretable models (up to 26%p) and can be used to speed up LLM inference for large models through speculative decoding. We further study Induction-Gram in a natural-language neuroscience setting, where the goal is to predict the next fMRI response in a sequence. It again provides a significant improvement over interpretable models (20% relative increase in the correlation of predicted fMRI responses), potentially enabling deeper scientific investigation of language selectivity in the brain.",2025,0.7840906239670459,0.7746481634360073,0.8,0.875,7594ef13-8b44-42db-a548-9516216ca77a,0,"[0.5, 0.625, 0.875, 0.875]","[0.95, 0.9, 0.8, 0.9]",Induction-Gram,0.6884324934067709
545678,$\alpha$-Divergence Loss Function for Neural Density Ratio Estimation,"Density ratio estimation (DRE) is a fundamental machine learning technique for capturing relationships between two probability distributions. State-of-the-art DRE methods estimate the density ratio using neural networks trained with loss functions derived from variational representations of $f$-divergence.
   However, existing methods face optimization challenges, such as overfitting due to lower-unbounded loss functions, biased mini-batch gradients, vanishing training loss gradients, and high sample requirements for Kullback-Leibler (KL) divergence loss functions.
   To address these issues, we focus on $\alpha$-divergence, which provides a suitable variational representation of $f$-divergence.
   Subsequently, a novel loss function for DRE, the $\alpha$-divergence loss function ($\alpha$-Div), is derived.
      $\alpha$-Div is concise but offers stable and effective optimization for DRE.
   The boundedness of $\alpha$-divergence provides the potential for successful DRE with data exhibiting high KL-divergence.
      Our numerical experiments demonstrate the effectiveness in optimization using $\alpha$-Div.
   However, the experiments also show that the proposed loss function offers no significant advantage over the KL-divergence loss function in terms of RMSE for DRE. This indicates that the accuracy of DRE is
 primarily determined by the amount of KL-divergence in the data and is less dependent on $\alpha$-divergence.",2025,0.2727271735537551,0.2722771738065002,0.2666666666666666,0.25,1d0e5f85-3656-49ad-9427-71aa8597f525,0,"[0.25, 0.25, 0.25]","[0.95, 0.95, 0.95]",$\alpha$-divergence,0.25000000000001
545686,Federated Few-Shot Class-Incremental Learning,"This study proposes a challenging yet practical Federated Few-Shot Class-Incremental Learning (FFSCIL) problem, where clients only hold very few samples for new classes.  We develop a novel Unified Optimized Prototype Prompt (UOPP) model to simultaneously handle catastrophic forgetting, over-fitting, and prototype bias in FFSCIL. UOPP utilizes task-wise prompt learning to mitigate task interference and over-fitting, unified static-dynamic prototypes to achieve a stability-plasticity balance, and adaptive dual heads for enhanced inferences. Dynamic prototypes represent new classes in the current few-shot task and are rectified to deal with prototype bias. Our comprehensive experimental results show that UOPP significantly outperforms state-of-the-art (SOTA) methods on three datasets with improvements up to 76% on average accuracy and 90% on harmonic mean accuracy respectively. Our extensive analysis shows UOPP robustness in various numbers of local clients and global rounds, low communication costs, and moderate running time. The source code of UOPP is publicly available at https://github.com/anwarmaxsum/FFSCIL.",2025,0.7090906512397632,0.7082103548098307,0.6666666666666666,0.625,1ac77913-c8ff-466a-ac9a-3d1320da5550,1,"[0.5, 0.625, 0.625, 0.625, 0.875]","[0.95, 0.95, 0.9, 0.95, 0.95]",Unified Optimized Prototype Prompt,0.6511929959592072
545693,Memory Proxy Maps for Visual Navigation,"Visual navigation takes inspiration from humans, who navigate in previously unseen environments using vision without detailed environment maps. Inspired by this, we introduce a novel no-RL, no-graph, no-odometry approach to visual navigation using feudal learning to build a three tiered agent. Key to our approach is a memory proxy map (MPM), an intermediate representation of the environment
learned in a self-supervised manner by the high-level manager agent that serves as a simplified memory, approximating what the agent has seen. We demonstrate that recording observations in this learned latent space is an effective and efficient memory proxy that can remove the need for graphs and odometry in visual navigation tasks. For the mid-level manager agent, we develop a waypoint network
(WayNet) that outputs intermediate subgoals, or waypoints, imitating human waypoint selection during local navigation. For the low-level worker agent, we learn a classifier over a discrete action space that avoids local obstacles and moves the agent towards the WayNet waypoint. The resulting feudal navigation network offers a novel approach with no RL, no graph, no odometry, and no metric map; all while achieving SOTA results on the image goal navigation task.",2025,0.5113634504132908,0.5105197008871879,0.5333333333333333,0.5,5bbba171-3454-4a0c-85ff-f90001642943,0,"[0.25, 0.5, 0.5, 0.625]","[0.95, 0.95, 0.95, 0.95]",Memory Proxy Map,0.4687499999999999
545696,Step-by-Step Reasoning for Math Problems  via Twisted Sequential Monte Carlo,"Augmenting the multi-step reasoning abilities of Large Language Models (LLMs) has been a persistent challenge. Recently, verification has shown promise in improving solution consistency by evaluating generated outputs. However, current verification approaches suffer from sampling inefficiencies, requiring a large number of samples to achieve satisfactory performance. Additionally, training an effective verifier often depends on extensive process supervision, which is costly to acquire. In this paper, we address these limitations by introducing a novel verification method based on Twisted Sequential Monte Carlo (TSMC). TSMC sequentially refines its sampling effort to focus exploration on promising candidates, resulting in more efficient generation of high-quality solutions. We apply TSMC to LLMs by estimating the expected future rewards at partial solutions. This approach results in a more straightforward training target that eliminates the need for step-wise human annotations. We empirically demonstrate the advantages of our method across multiple math benchmarks, and also validate our theoretical analysis of both our approach and existing verification methods.",2025,0.7636360859505142,0.7620635124626708,0.6666666666666666,0.625,4712013a-fe7b-4a0a-9f04-9ea2f083ed3d,1,"[0.5, 0.625, 0.625, 0.875, 0.875]","[0.8, 0.9, 0.95, 0.8, 0.9]",Twisted Sequential Monte Carlo,0.6971000727454946
545699,LLM-Mediated Guidance of MARL Systems,"In complex multi-agent environments, achieving efficient learning and desirable behaviours is a significant challenge for Multi-Agent Reinforcement Learning (MARL) systems. This work explores the potential of combining MARL with Large Language Model (LLM)-mediated interventions to guide agents toward more desirable behaviours. Specifically, we investigate how LLMs can be used to interpret and facilitate interventions that shape the learning trajectories of multiple agents. We experimented with two types of interventions, referred to as controllers: a Natural Language (NL) Controller and a Rule-Based (RB) Controller. The NL Controller, which uses an LLM to simulate human-like interventions, showed a stronger impact than the RB Controller. Our findings indicate that agents particularly benefit from early interventions, leading to more efficient training and higher performance. Both intervention types outperform the baseline without interventions, highlighting the potential of LLM-mediated guidance to accelerate training and enhance MARL performance in challenging environments.",2025,0.4363634776860081,0.4381963488720101,0.2666666666666666,0.25,0986f348-585f-463b-ae98-b4c4bd3ed667,0,"[0.25, 0.25, 0.25, 0.625, 0.625]","[0.95, 0.95, 0.95, 0.95, 1.0]",LLM-mediated interventions,0.4145465429512122
545701,Familiarity-Aware Evidence Compression for Retrieval-Augmented Generation,"Retrieval-augmented generation (RAG) improves large language models (LMs) by incorporating non-parametric knowledge through evidence retrieved from external sources. However, it often struggles to cope with inconsistent and irrelevant information that can distract the LM from its tasks, especially when multiple evidence pieces are required. While compressing the retrieved evidence with a compression model aims to address this issue, the compressed evidence may still be unfamiliar to the target model used for downstream tasks, potentially failing to utilize the evidence effectively. We propose FaviComp (Familiarity-aware Evidence Compression), a novel training-free evidence compression technique that makes retrieved evidence more familiar to the target model, while seamlessly integrating parametric knowledge from the model. Specifically, FaviComp proactively composes the compressed evidence in a way to lower the perplexity of the target model by combining decoding probabilities from both the compression model and the target model to generate context that is more familiar to the target model. This approach balances the integration of parametric and non-parametric knowledge, which is especially helpful in complex tasks where the retrieved evidence set may not contain all the necessary information. Experimental results show that FaviComp consistently outperforms most recent evidence compression baselines across multiple open-domain QA datasets, improving accuracy by up to 23.91% while achieving high compression rates. Additionally, we demonstrate the effective integration of both parametric and non-parametric knowledge during evidence compression.",2025,0.5113634504132908,0.5082510874750322,0.5333333333333333,0.5,67f8f51e-fe55-4c42-a732-e8092b8a7a7e,0,"[0.25, 0.5, 0.5, 0.625]","[0.95, 0.95, 0.95, 0.9]",FaviComp,0.4593171859785785
545704,ASTrA: Adversarial Self-supervised Training with Adaptive-Attacks,"Existing self-supervised adversarial training (self-AT) methods rely on hand-crafted adversarial attack strategies for PGD attacks, which fail to adapt to the evolving learning dynamics of the model and do not account for instance-specific characteristics of images. This results in sub-optimal adversarial robustness and limits the alignment between clean and adversarial data distributions. To address this, we propose $\textit{ASTrA}$ ($\textbf{A}$dversarial $\textbf{S}$elf-supervised $\textbf{Tr}$aining with $\textbf{A}$daptive-Attacks), a novel framework introducing a learnable, self-supervised attack strategy network that autonomously discovers optimal attack parameters through exploration-exploitation in a single training episode. ASTrA leverages a reward mechanism based on contrastive loss, optimized with REINFORCE, enabling adaptive attack strategies without labeled data or additional hyperparameters. We further introduce a mixed contrastive objective to align the distribution of clean and adversarial examples in representation space. ASTrA achieves state-of-the-art results on CIFAR10, CIFAR100, and STL10 while integrating seamlessly as a plug-and-play module for other self-AT methods. ASTrA shows scalability to larger datasets, demonstrates strong semi-supervised performance, and is resilient to robust overfitting, backed by explainability analysis on optimal attack strategies. Project page for source code and other details at https://prakashchhipa.github.io/projects/ASTrA.",2025,0.727272826446245,0.7260728265202324,0.6666666666666666,0.5,7bebad63-576e-4896-995a-17c58e896154,1,"[0.5, 0.625, 0.875]","[0.9, 0.9, 0.9]",ASTrA,0.6666666666666666
545707,ConsisSR: Delving Deep into Consistency in Diffusion-based Image Super-Resolution,"Real-world image super-resolution (Real-ISR) aims at restoring high-quality (HQ) images from low-quality (LQ) inputs corrupted by unknown and complex degradations. In particular, pretrained text-to-image (T2I) diffusion models provide strong generative priors to reconstruct credible and intricate details. However, T2I generation focuses on semantic consistency while Real-ISR emphasizes pixel-level reconstruction, which hinders existing methods from fully exploiting diffusion priors. To address this challenge, we introduce ConsisSR to handle both semantic and pixel-level consistency. Specifically, compared to coarse-grained text prompts, we exploit the more powerful CLIP image embedding and effectively leverage both modalities through our Hybrid Prompt Adapter (HPA) for semantic guidance. Secondly, we introduce Time-aware Latent Augmentation (TALA) to mitigate the inherent gap between T2I generation and Real-ISR consistency requirements. By randomly mixing LQ and HQ latent inputs, our model not only handle timestep-specific diffusion noise but also refine the accumulated latent representations. Last but not least, our GAN-Embedding strategy employs the pretrained Real-ESRGAN model to refine the diffusion start point. This accelerates the inference process to 10 steps while preserving sampling quality, in a training-free manner. Our method demonstrates state-of-the-art performance among both full-scale and accelerated models. The code will be made publicly available.",2025,0.5113634504132908,0.5074255430840509,0.5333333333333333,0.5,053e710b-f4b0-463a-bfed-9d29e396d93b,0,"[0.25, 0.5, 0.5, 0.625]","[1.0, 0.95, 0.95, 0.95]",ConsisSR,0.4513530927835051
545710,TopInG: Topologically Interpretable Graph Learning via Persistent Rationale Filtration,"Graph Neural Networks (GNNs) have shown remarkable performance in various scientific domains, but their lack of interpretability limits their applicability in critical decision-making processes. Recently, intrinsic interpretable GNNs have been studied to provide insights into model predictions by identifying rationale substructures in graphs. However, existing methods face challenges when the underlying rationale subgraphs are complicated and variable. To address this challenge,
we propose TopIng, a novel topological framework to interpretable GNNs that leverages persistent homology to identify persistent rationale subgraphs.
Our method introduces a rationale filtration learning technique that models the generating procedure of rationale subgraphs, and enforces the persistence of topological gap between rationale subgraphs and complement random graphs by a novel self-adjusted topological constraint, topological discrepancy. We show that our topological discrepancy is a lower bound of a Wasserstein distance on graph distributions with Gromov-Hausdorff metric. 
We provide theoretical guarantees showing that our loss is uniquely optimized by the ground truth under certain conditions.
Through extensive experiments on varaious synthetic and real datasets, we demonstrate that TopIng effectively addresses key challenges in interpretable GNNs including handling variiform rationale subgraphs, balancing performance with interpretability, and avoiding spurious correlations. 
Experimental results show that our approach improves state-of-the-art methods up to 20%+ on both predictive accuracy and interpretation quality.",2025,0.6477270371901683,0.6447675950955257,0.6666666666666666,0.625,df0eeceb-144e-4ecd-8d0f-c57b7cf2e4a5,0,"[0.5, 0.625, 0.625, 0.625]","[0.95, 0.9, 0.8, 0.95]",Topological Discrepancy,0.5868036683988817
545716,HOI-Diff: Text-Driven Synthesis of 3D Human-Object Interactions using Diffusion Models,"We address the problem of generating realistic 3D human object interactions (HOIs) driven by textual prompts. To this end, we take a modular design and decompose the complex task into simpler subtasks. We first develop a dual-branch diffusion model (DBDM) to generate both human and object motions conditioned on the input text, and encourage coherent motions by a cross-attention communication module between the human and object motion generation branches. We also develop an affordance prediction diffusion model (APDM) to predict the contacting area between the human and object during the interactions driven by the textual prompt. The APDM is independent of the results by the DBDM and thus can correct potential errors by the latter. Moreover, it stochastically generates the contacting points to diversify the generated motions. Finally, we incorporate the estimated contacting points into the classifier-guidance to achieve accurate and close contact between humans and objects. To train and evaluate our approach, we annotate BEHAVE dataset with text descriptions. Experimental results on BEHAVE and OMOMO demonstrate that our approach produces realistic HOIs with various interactions and different types of objects.",2025,0.5795452438017296,0.5781468162085512,0.5333333333333333,0.5,313299ab-8c0d-491c-b1ec-4701d5991a4d,0,"[0.5, 0.5, 0.5, 0.625]","[0.95, 1.0, 0.95, 0.95]",HOI-Diff,0.5287647275405007
545723,Automated Feature Engineering by Prompting,"Automated feature engineering (AutoFE) liberates data scientists from the burden
of manual feature construction, a critical step for tabular data prediction. While the
semantic information of datasets provides valuable context for feature engineering,
it has been underutilized in most existing works. In this paper, we introduce
AutoFE by Prompting (FEBP), a novel AutoFE algorithm that leverages large language
models (LLMs) to process dataset descriptions and automatically generate
features. Incorporating domain knowledge, the LLM iteratively refines feature
construction through in-context learning of top-performing example features and
provides semantic explanations. Our experiments on real-world datasets demonstrate
the superior performance of FEBP over state-of-the-art AuoFE methods. We
also conduct ablation study to verify the impact of dataset semantic information
and examine the behavior of our LLM-based feature search process.",2025,0.5795452438017296,0.5776686974913471,0.5333333333333333,0.5,1d5e9f54-d2f8-4a6f-8f27-a9c0234ec2dc,0,"[0.5, 0.5, 0.5, 0.625]","[0.95, 0.9, 0.95, 0.9]",FEBP,0.5272344559585492
545729,An EEG dataset of word-level brain responses for semantic text relevance,"Electroencephalography (EEG) can enable non-invasive, real-time measurement of brain activity in response to human language processing. Previously released EEG datasets focus on brain signals measured either during completely natural reading or in full psycholinguistic experimental settings. Since reading is commonly performed when considering certain content as more semantically relevant than other, we release a novel dataset for semantic text relevance containing $23{,}270$ time-locked (${\sim}0.7s$) word-level EEG recordings acquired from participants who read both text that was semantically relevant and irrelevant to self-selected topics. Using these data, we present benchmark experiments with two evaluation protocols: participant-independent and participant-dependent on two prediction tasks (word relevance and sentence relevance). We report the performance of five well known models on these tasks. Our dataset and code are openly released. Altogether, our dataset paves the way for advancing research on language relevance and psycholinguistics, brain input and feedback-based recommendation and retrieval systems, and development of brain-computer interface (BCI) devices for online detection of language relevance.",2025,0.5113634504132908,0.5069376223885896,0.5333333333333333,0.5,397a6037-759c-40b7-93c9-a5c9a0522e94,0,"[0.25, 0.5, 0.5, 0.625]","[1.0, 0.95, 0.9, 0.95]",semantic text relevance,0.4486627563042241
545733,ARB-LLM: Alternating Refined Binarizations for Large Language Models,"Large Language Models (LLMs) have greatly pushed forward advancements in natural language processing, yet their high memory and computational demands hinder practical deployment. Binarization, as an effective compression technique, can shrink model weights to just 1 bit, significantly reducing the high demands on computation and memory. However, current binarization methods struggle to narrow the distribution gap between binarized and full-precision weights, while also overlooking the column deviation in LLM weight distribution. To tackle these issues, we propose ARB-LLM, a novel 1-bit post-training quantization (PTQ) technique tailored for LLMs. To narrow the distribution shift between binarized and full-precision weights, we first design an alternating refined binarization (ARB) algorithm to progressively update the binarization parameters, which significantly reduces the quantization error. Moreover, considering the pivot role of calibration data and the column deviation in LLM weights, we further extend ARB to ARB-X and ARB-RC. In addition, we refine the weight partition strategy with column-group bitmap (CGB), which further enhance performance. Equipping ARB-X and ARB-RC with CGB, we obtain ARB-LLM$_{\text{X}}$ and ARB-LLM$ _{\text{RC}} $ respectively, which significantly outperform state-of-the-art (SOTA) binarization methods for LLMs.
As a binary PTQ method, our ARB-LLM$ _{\text{RC}} $ is the first to surpass FP16 models of the same size. Code: https://github.com/ZHITENGLI/ARB-LLM.",2025,0.8181815206612653,0.8065568699887385,0.9333333333333332,0.875,26ed76d2-f48f-4ace-84bb-39564b70294e,1,"[0.5, 0.875, 0.875]","[0.95, 0.8, 0.9]",Alternating Refined Binarization,0.7099882601205313
545762,NECOMIMI: Neural-Cognitive Multimodal EEG-informed Image Generation with Diffusion Models,"NECOMIMI (NEural-COgnitive MultImodal EEG-Informed Image Generation with Diffusion Models) introduces a novel framework for generating images directly from EEG signals using advanced diffusion models. Unlike previous works that focused solely on EEG-image classification through contrastive learning, NECOMIMI extends this task to image generation. The proposed NERV EEG encoder demonstrates state-of-the-art (SoTA) performance across multiple zero-shot classification tasks, including 2-way, 4-way, and 200-way, and achieves top results in our newly proposed Category-based Assessment Table (CAT) Score, which evaluates the quality of EEG-generated images based on semantic concepts. A key discovery of this work is that the model tends to generate abstract or generalized images, such as landscapes, rather than specific objects, highlighting the inherent challenges of translating noisy and low-resolution EEG data into detailed visual outputs. Additionally, we introduce the CAT Score as a new metric tailored for EEG-to-image evaluation and establish a benchmark on the ThingsEEG dataset. This study underscores the potential of EEG-to-image generation while revealing the complexities and challenges that remain in bridging neural activity with visual representation.",2025,0.3409089669421938,0.3403464672581253,0.2666666666666666,0.25,397a6037-759c-40b7-93c9-a5c9a0522e94,0,"[0.25, 0.25, 0.25, 0.5]","[0.95, 0.95, 0.95, 0.95]",NECOMIMI,0.3125
545799,Socrates Loss for training ad-hoc calibrated selective classifiers,"Model reliability is paramount for critical real-world applications. To enhance reliability, it is essential to quantify uncertainty in model predictions, as achieved through Confidence Calibration and Selective Classification. Confidence Calibration ensures prediction confidences accurately reflect the actual likelihood of correctness, while Selective Classification allows a model to abstain from making predictions when uncertain. Although related, existing methods address each aspect separately, or both through post-hoc approaches. Only one method, Confidence-aware Contrastive Learning for Selective Classification (CCL-SC), combines both in an ad-hoc manner. Despite being a powerful calibrator, CCL-SC has some drawbacks, including the absence of an additional unknown class, the use of two different losses (detrimental for calibration), and its cumbersome implementation. In the pursuit of reliable models and motivated by the idea of creating an ad-hoc calibrated selective classifier with an unknown class, we first empirically analyze the Self-Adaptive Training (SAT) method, a leading approach in ad-hoc selective classification. We identify that while SAT excels in selective classification, it falls short in confidence calibration, especially when training for a small number of epochs (e.g., <=100). To address this, we introduce an original approach that uses an unknown class and a unique novel loss, Socrates loss, which serves as a classifier and a calibrator with a unified optimization goal. This approach mitigates overfitting and ensures theoretically well-calibrated predictions across all epochs, addressing the drawbacks of both CCL-SC and SAT, without the need for post-hoc processing or additional data. We integrate our approach into the SAT implementation and extend it to provide selective classification and confidence calibration metrics. We show empirically that our approach matches or improves the selective classification error rate of SAT and CCL-SC, while producing well-calibrated models in an ad-hoc manner through the evaluation on 6 image benchmark datasets across two architectures, VGG-16 and ResNet-34.",2025,0.2727271735537551,0.2722771738065002,0.2666666666666666,0.25,a7c707f1-e1a4-4917-8a08-1234907c3d9d,0,"[0.25, 0.25, 0.25, 0.25]","[0.9, 0.95, 0.95, 0.9]",Socrates loss,0.2500000000000001
545801,InterMask: 3D Human Interaction Generation via Collaborative Masked Modeling,"Generating realistic 3D human-human interactions from textual descriptions remains a challenging task. Existing approaches, typically based on diffusion models, often produce results lacking realism and fidelity. In this work, we introduce *InterMask*, a novel framework for generating human interactions using collaborative masked modeling in discrete space. InterMask first employs a VQ-VAE to transform each motion sequence into a 2D discrete motion token map. Unlike traditional 1D VQ token maps, it better preserves fine-grained spatio-temporal details and promotes *spatial awareness* within each token. Building on this representation, InterMask utilizes a generative masked modeling framework to collaboratively model the tokens of two interacting individuals. This is achieved by employing a transformer architecture specifically designed to capture complex spatio-temporal inter-dependencies. During training, it randomly masks the motion tokens of both individuals and learns to predict them. For inference, starting from fully masked sequences, it progressively fills in the tokens for both individuals. With its enhanced motion representation, dedicated architecture, and effective learning strategy, InterMask achieves state-of-the-art results, producing high-fidelity and diverse human interactions. It outperforms previous methods, achieving an FID of $5.154$ (vs $5.535$ of in2IN) on the InterHuman dataset and $0.399$ (vs $5.207$ of InterGen) on the InterX dataset. Additionally, InterMask seamlessly supports reaction generation without the need for model redesign or fine-tuning.",2025,0.6818179338843877,0.6806929345162506,0.6666666666666666,0.625,313299ab-8c0d-491c-b1ec-4701d5991a4d,1,"[0.625, 0.625, 0.625]","[0.9, 0.95, 1.0]",InterMask,0.6249999999999999
545818,VidEgoThink: Assessing Egocentric Video Understanding Capabilities for Embodied AI,"Recent advancements in Multi-modal Large Language Models (MLLMs) have opened new avenues for applications in Embodied AI.
Building on previous work, EgoThink, we introduce VidEgoThink, a comprehensive benchmark for evaluating egocentric video understanding capabilities. To bridge the gap between MLLMs and low-level control in Embodied AI, we design four key interrelated tasks: video question-answering, hierarchy planning, visual grounding and reward modeling. To minimize manual annotation costs, we develop an automatic data generation pipeline based on the Ego4D dataset, leveraging the prior knowledge and multimodal capabilities of GPT-4o. Three human annotators then filter the generated data to ensure diversity and quality, resulting in the VidEgoThink benchmark. We conduct extensive experiments with three types of models: API-based MLLMs, open-source image-based MLLMs, and open-source video-based MLLMs. Experimental results indicate that all MLLMs, including GPT-4o, perform poorly across all tasks related to egocentric video understanding. These findings suggest that foundation models still require significant advancements to be effectively applied to first-person scenarios in Embodied AI. In conclusion, VidEgoThink reflects a research trend towards employing MLLMs for egocentric vision, akin to human capabilities, enabling active observation and interaction in the complex real-world environments.",2025,0.4636361950413836,0.4628711954710504,0.5333333333333333,0.25,09f209ba-7384-45b6-ae26-bb9843abe438,0,"[0.25, 0.25, 0.5, 0.5, 0.625]","[0.9, 1.0, 0.95, 0.95, 0.95]",VidEgoThink,0.4209696257825441
545839,Permute-and-Flip: An optimally stable and watermarkable decoder for LLMs,"In this paper, we propose a new decoding method called Permute-and-Flip (PF) decoder. It enjoys stability properties similar to the standard sampling decoder, but is provably up to 2x better in its quality-stability tradeoff than sampling and never worse than any other decoder. We also design a cryptographic watermarking scheme analogous to Aaronson (2023)'s Gumbel watermark, but naturally tailored for PF decoder. The watermarking scheme does not change the distribution to sample, while allowing arbitrarily low false positive rate and high recall whenever the generated text has high entropy. Our experiments show that the PF decoder (and its watermarked counterpart) significantly outperform(s) naive sampling (and its Gumbel watermarked counterpart) in terms of perplexity, while retaining the same stability (and detectability), hence making it a promising new approach for LLM decoding. The code is available at https://github.com/XuandongZhao/pf-decoding",2025,0.7363633685951387,0.7285657963236047,0.6666666666666666,0.625,c6f9c124-94b7-4975-8217-8e5c0cfb915d,1,"[0.625, 0.625, 0.625, 0.625, 0.875]","[0.9, 0.95, 0.95, 0.95, 0.8]",Permute-and-Flip,0.6542738304934527
545873,Learning 3D Perception from Others' Predictions,"Accurate 3D object detection in real-world environments requires a huge amount of annotated data with high quality. Acquiring such data is tedious and expensive, and often needs repeated effort when a new sensor is adopted or when the detector is deployed in a new environment. We investigate a new scenario to construct 3D object detectors: *learning from the predictions of a nearby unit that is equipped with an accurate detector.* For example, when a self-driving car enters a new area, it may learn from other traffic participants whose detectors have been optimized for that area. This setting is label-efficient, sensor-agnostic, and communication-efficient: nearby units only need to share the predictions with the ego agent (e.g., car). Naively using the received predictions as ground-truths to train the detector for the ego car, however, leads to inferior performance. We systematically study the problem and identify viewpoint mismatches and mislocalization (due to synchronization and GPS errors) as the main causes, which unavoidably result in false positives, false negatives, and inaccurate pseudo labels. We propose a distance-based curriculum, first learning from closer units with similar viewpoints and subsequently improving the quality of other units' predictions via self-training. We further demonstrate that an effective pseudo label refinement module can be trained with a handful of annotated data, largely reducing the data quantity necessary to train an object detector. We validate our approach on the recently released real-world collaborative driving dataset, using reference cars' predictions as pseudo labels for the ego car. Extensive experiments including several scenarios (e.g., different sensors, detectors, and domains) demonstrate the effectiveness of our approach toward label-efficient learning of 3D perception from other units' predictions.",2025,0.6545452165290122,0.6510974948321792,0.6666666666666666,0.625,519a486a-751e-472b-ab13-5f2b2a04fbbe,1,"[0.5, 0.625, 0.625, 0.625, 0.625]","[1.0, 0.8, 0.9, 0.95, 0.95]",Pseudo Label Refinement,0.5885786120781829
545925,Inverse Flow and Consistency Models,"Inverse generation problems, such as denoising without ground truth observations, is a critical challenge in many scientific inquiries and real-world applications. While recent advances in generative models like diffusion models, conditional flow matching, and consistency models achieved impressive results by casting generation as denoising problems, they cannot be directly used for inverse generation without access to clean data. Here we introduce Inverse Flow (IF), a novel framework that enables using these generative models for inverse generation problems including denoising without ground truth. Inverse Flow can be flexibly applied to nearly any continuous noise distribution and allows complex dependencies. We propose two algorithms for learning Inverse Flows, Inverse Flow Matching (IFM) and Inverse Consistency Model (ICM). Notably, to derive the computationally efficient, simulation-free inverse consistency model objective, we generalized consistency training to any forward diffusion processes or conditional flows, which have applications beyond denoising. We demonstrate the effectiveness of IF on synthetic and real datasets, outperforming prior approaches while enabling noise distributions that previous methods cannot support. Finally, we showcase applications of our techniques to fluorescence microscopy and single-cell genomics data, highlighting IF's utility in scientific problems. This work opens up the use of powerful generative models for denoising.",2025,0.5454543471075102,0.5373891015071086,0.6666666666666666,0.625,dbf082e3-d5a9-4bbd-a449-1e9c8194b8e2,0,"[0.25, 0.625, 0.625]","[1.0, 0.9, 0.95]",Inverse Flow,0.4631586462271553
545944,What Matters in Transformers? Not All Attention is Needed,"While scaling Transformer-based large language models (LLMs) has demonstrated
promising performance across various tasks, it also introduces redundant archi-
tectures, posing efficiency challenges for real-world deployment. Despite some
recognition of redundancy in LLMs, the variability of redundancy across different
architectures in transformers, such as MLP and Attention layers, is under-explored.
In this work, we investigate redundancy across different modules within Trans-
formers, including Blocks, MLP, and Attention layers, using a similarity-based
metric. Surprisingly, despite the critical role of attention layers in distinguishing
transformers from other architectures, we found that a large portion of these layers
exhibit excessively high similarity and can be pruned without degrading perfor-
mance. For instance, Llama-2-70B achieved a 48.4% speedup with only a 2.4%
performance drop by pruning half of the attention layers. Furthermore, by tracing
model checkpoints throughout the training process, we observed that attention
layer redundancy is inherent and consistent across training stages. Additionally,
we further propose a method that jointly drops Attention and MLP layers, allowing
us to more aggressively drop additional layers. For instance, when dropping 31
layers (Attention + MLP), Llama-2-13B still retains 90% of the performance on the
MMLU task. Our work provides valuable insights for future network architecture
design. The code will be released upon acceptance.",2025,0.613636140495949,0.611739284804102,0.6,0.5,947d9a9f-d981-4eb2-888b-3c25731b486b,0,"[0.5, 0.5, 0.625, 0.625]","[1.0, 0.95, 0.95, 0.95]",Attention Layer Redundancy,0.5575294550810015
545959,"Continual Learning: Less Forgetting, More OOD Generalization via Adaptive Contrastive Replay","Machine learning models often suffer from catastrophic forgetting of previously learned knowledge when learning new classes. Various methods have been proposed to mitigate this issue. However, rehearsal-based learning, which retains samples from previous classes, typically achieves good performance but tends to memorize specific instances, struggling with Out-of-Distribution (OOD) generalization. This often leads to high forgetting rates and poor generalization. Surprisingly, the OOD generalization capabilities of these methods have been largely unexplored. In this paper, we highlight this issue and propose a simple yet effective strategy inspired by contrastive learning and data-centric principles to address it.
We introduce Adaptive Contrastive Replay (ACR), a method that employs dual optimization to simultaneously train both the encoder and the classifier. ACR adaptively populates the replay buffer with misclassified samples while ensuring a balanced representation of classes and tasks. By refining the decision boundary in this way, ACR achieves a balance between stability and plasticity. Our method significantly outperforms previous approaches in terms of OOD generalization, achieving an improvement of 13.41\% on Split CIFAR-100, 9.91\% on Split Mini-ImageNet, and 5.98\% on Split Tiny-ImageNet.",2025,0.443181657024852,0.4406588236319161,0.4,0.25,bf74e818-d1b1-4cf5-85bf-acaa2f496313,0,"[0.25, 0.25, 0.5, 0.625]","[0.95, 0.95, 1.0, 0.9]",Adaptive Contrastive Replay,0.402020543228659
545963,Rethinking the Stability-Plasticity Trade-off in Continual Learning from an Architectural Perspective,"The quest for Continual Learning (CL) seeks to empower neural networks with the ability to learn and adapt incrementally. Central to this pursuit is addressing the stability-plasticity dilemma, which involves striking a balance between two conflicting objectives: preserving previously learned knowledge and acquiring new knowledge. Existing studies have proposed numerous CL methods to achieve this trade-off. However, these methods often overlook the impact of basic architecture on stability and plasticity, thus the trade-off is limited to the parameter level. In this paper, we delve into the conflict between stability and plasticity at the architectural level. We reveal that under an equal parameter constraint, deeper networks exhibit better plasticity, while wider networks are characterized by superior stability. To address this architectural-level dilemma, we introduce a novel framework denoted Dual-Architecture (Dual-Arch), which serves as a plug-in component for CL. This framework leverages the complementary strengths of two distinct and independent networks: one dedicated to plasticity and the other to stability. Each network is designed with a specialized and lightweight architecture, tailored to its respective objective. Extensive experiments across datasets and CL methods demonstrate that Dual-Arch can enhance the performance of existing CL methods while being up to 87% more compact in terms of parameters than the baselines.",2025,0.5727270644628857,0.5714825601024633,0.6666666666666666,0.625,64ebf47b-7f47-41f0-8adf-04160989c756,0,"[0.25, 0.5, 0.625, 0.625, 0.625]","[0.95, 0.8, 0.9, 0.9, 1.0]",Dual-Architecture,0.5253430620364892
545978,Incorporating Neural ODEs into DAE-Constrained Optimization Problems,"Differential algebraic equations (DAEs) are pivotal in dynamic optimization across diverse fields, from process control to flight trajectory optimization and epidemiological modeling. Traditional methods like single shooting, multiple shooting, and direct transcription effectively optimize known mechanistic models. However, significant challenges arise when the underlying equations are unknown or deviate from empirical data. While black-box optimization strategies can address some issues, challenges persist regarding data quality, non-linearity, and the inclusion of constraints. Recent advances in machine learning, particularly Neural ODEs, offer promising tools for continuous representation of dynamic systems. This work bridges the gap between machine learning representations of dynamic systems and optimization methodologies, enabling a novel approach for solving DAEs with data-driven components. We demonstrate this approach using numerical examples of DAE problems and realistic case studies, including  biochemical reactor control and disease spread prevention. Our results highlight the efficacy of incorporating Neural ODEs into equation-based solvers, showing improved performance over existing strategies such as SINDy. Additionally, we formalize the optimization program for NN-embedded DAEs and present representations for common neural network architectures (e.g., ReLU, tanh). This work contributes a novel framework for dynamic system optimization, integrating machine learning advancements with traditional optimization techniques, and offers practical insights through comprehensive case studies.",2025,0.3409089669421938,0.3403464672581253,0.2666666666666666,0.25,17c02cdb-18e4-44a6-b9db-c2fb173a5c18,0,"[0.25, 0.25, 0.25, 0.5]","[0.95, 0.95, 0.95, 0.95]",Neural ODEs,0.3125
546036,New Algorithms for the Learning-Augmented k-means Problem,"In this paper, we study the clustering problems in the learning-augmented setting, where predicted labels for a d-dimensional dataset with size m are given by an oracle to serve as auxiliary information to improve the clustering performance. Following the prior work, the given oracle is parameterized by some error rate α, which captures the accuracy of the oracle such that there are at most α fraction of false positives and false negatives in each predicted cluster. In this setting, the goal is to design fast and practical algorithms that can break the computational barriers of inapproximability. The current state-of-the-art learning-augmented k-means algorithm relies on sorting strategies to find good coordinates approximation, where a (1+O(α))-approximation can be achieved with near-linear running time in the data size. However, the computational demands for sorting may limit the scalability of the algorithm for handling large-scale datasets. To address this issue, in this paper, we propose new algorithms that can identify good coordinates approximation using sampling-based strategies, where (1+O(α))-approximation can be achieved with linear running time in the data size. To obtain a more practical algorithm for the problem with better clustering quality and running time, we propose a sampling-based heuristic which can directly find center approximations using sampling-based strategies. Empirical experiments show that our proposed methods are faster than the state-of-the-art learning-augmented k-means algorithms with comparable performances on clustering quality.",2025,0.8181815206612653,0.8243942921991501,0.8,0.625,5c6d8fd3-40c2-40c1-be6d-449844c8b51d,1,"[0.625, 0.625, 0.875, 0.875]","[0.9, 0.8, 0.9, 1.0]",Learning-Augmented k-means,0.7811649861545726
546040,DFRot: Achieving Outlier-Free and Massive Activation-Free for Rotated LLMs with Refined Rotation,"Using rotational invariance to eliminate outliers in large language models (LLMs) has recently gained considerable attention, especially in the context of quantization. Prior studies have shown that in low-precision quantization scenarios, such as 4-bit weights and 4-bit activations~(W4A4), randomized Hadamard transforms can achieve significantly higher accuracy than randomized orthogonal transforms. Notably, the reason behind this phenomena remains unknown. In this paper, we find that these transformations show substantial improvement in eliminating outliers for common tokens and achieve similar quantization error. The primary reason for the accuracy difference lies in the fact that randomized Hadamard transforms can slightly reduce the quantization error for tokens with massive activations while randomized orthogonal transforms increase the quantization error. Due to the extreme rarity of these tokens and their critical impact on model accuracy, we consider this a long-tail optimization problem, and therefore construct a simple yet effective method: a weighted loss function. Additionally, we propose an optimization strategy for the rotational matrix that involves alternating optimization of quantization parameters while employing orthogonal Procrustes transforms to refine the orthogonal matrix. This makes the distribution of the rotated activation values more conducive to quantization, especially for tokens with massive activations. Our method enhances the Rotated LLMs by achieving dual free, \textit{Outlier-Free} and \textit{Massive Activation-Free}, dubbed as DFRot. Extensive experiments demonstrate the effectiveness and efficiency of DFRot. By tuning the rotational matrix using just a single sample, DFRot achieves a perplexity improvement of 0.25 and 0.21 on W4A4KV4 and W4A4KV16, respectively, for LLaMA3-8B, a model known for its quantization challenges. Code is anonymously available at \url{https://anonymous.4open.science/r/DFRot-8FE3}.",2025,0.3409089669421938,0.3394621109976018,0.2666666666666666,0.25,26ed76d2-f48f-4ace-84bb-39564b70294e,0,"[0.25, 0.25, 0.25, 0.5]","[0.95, 0.95, 1.0, 0.95]",DFRot,0.3075294550810014
546044,Enhancing Variational Quantum Algorithms: Effective Quantum Ansatz Design Using GFlowNets,"Quantum computing promises significant computational advantages over classical computing. However, current devices are constrained by a limited qubit count and noise. By combining classical optimization methods with parameterized quantum circuits, Variational Quantum Algorithms (VQAs) offer a potential solution for noisy intermediate-scale quantum systems (NISQ). This makes VQAs particularly promising strategies for achieving near-term quantum advantages; such approaches are now widely explored for nearly all quantum computing applications. However, designing effective parameterized circuits, also known as ansatz, remains challenging. In this work, we introduce the use of GFlowNets as an efficient method to automate the development of efficient ansatz for various quantum computing problems. Our approach leverages GFlowNets to efficiently explore the combinatorial space of parameterized quantum circuits. Our extensice experiments demonstrate that GFlowNets can discover ansatz with an order of magnitude fewer parameters, gate counts, and depths compared to current approaches for the molecular electronic ground state energy problem. We also apply our approach to the unweighted Max-Cut problem, where we observe similar improvements in circuit efficiency. These results highlight the potential of GFlowNets to significantly reduce the resource requirements of VQAs while maintaining or improving solution quality.",2025,0.443181657024852,0.4420082293053011,0.4,0.25,6e1f55c2-6194-4821-9b4f-f422a580219f,0,"[0.25, 0.25, 0.5, 0.625]","[1.0, 0.95, 0.9, 1.0]",GFlowNets,0.4063060411172278
546073,Toward Foundation Model for Multivariate Wearable Sensing of Physiological Signals,"Time-series foundation models have the ability to run inference, mainly forecasting, on any type of time series data, thanks to the informative representations comprising waveform features. 
Wearable sensing data, on the other hand, contain more variability in both patterns and frequency bands of interest and generally emphasize more on the ability to infer healthcare-related outcomes. The main challenge of crafting a foundation model for wearable sensing physiological signals is to learn generalizable representations that support efficient adaptation across heterogeneous sensing configurations and applications. In this work, we propose NormWear, a step toward such a foundation model, aiming to extract generalized and informative wearable sensing representations. NormWear has been pretrained on a large set of physiological signals, including PPG, ECG, EEG, GSR, and IMU, from various public resources. For a holistic assessment, we perform downstream evaluation on 11 public wearable sensing datasets, spanning 18 applications in the areas of mental health, body state inference, biomarker estimations, and disease risk evaluations. We demonstrate that NormWear achieves a better performance improvement over competitive baselines in general time series foundation modeling. In addition, leveraging a novel representation-alignment-match-based method, we align physiological signals embeddings with text embeddings. This alignment enables our proposed foundation model to perform zero-shot inference, allowing it to generalize to previously unseen wearable signal-based health applications. Finally, we perform nonlinear dynamic analysis on the waveform features extracted by the model at each intermediate layer. This analysis quantifies the model's internal processes, offering clear insights into its behavior and fostering greater trust in its inferences among end users.",2025,0.2727271735537551,0.2722771738065002,0.2666666666666666,0.25,5fa53f36-ba21-4604-b72e-93057c35a2d6,0,"[0.25, 0.25, 0.25, 0.25]","[0.95, 0.95, 1.0, 0.95]",NormWear,0.2499999999999999
546077,SeaDAG: Semi-autoregressive Diffusion for Conditional Directed Acyclic Graph Generation,"We introduce SeaDAG, a semi-autoregressive diffusion model for conditional generation of Directed Acyclic Graphs~(DAGs). Considering their inherent layer-wise structure, we simulate layer-wise autoregressive generation by designing different denoising speed for different layers. Unlike conventional autoregressive generation that lacks a global graph structure view, our method maintains a complete graph structure at each diffusion step, enabling operations such as property control that require the full graph structure.
Leveraging this capability, we evaluate the DAG properties during training by employing a graph property decoder. We explicitly train the model to learn graph conditioning with a condition loss, which enhances the diffusion model's capacity to generate graphs that are both realistic and aligned with specified properties. 
We evaluate our method on two representative conditional DAG generation tasks: (1) circuit generation from truth tables, where precise DAG structures are crucial for realizing circuit functionality, and (2) molecule generation based on quantum properties.
Our approach demonstrates promising results, generating high-quality and realistic DAGs that closely align with given conditions.",2025,0.443181657024852,0.4415301105880969,0.4,0.25,a5d259e3-38b9-42da-98f4-8827901f332f,0,"[0.25, 0.25, 0.5, 0.625]","[0.95, 0.9, 0.95, 0.9]",SeaDAG,0.4022344559585493
546085,LifelongSotopia: Evaluating Social Intelligence Of Language Agents Over Lifelong Social Interactions,"Humans engage in lifelong social interactions through interacting with different people under different scenarios for different social goals. This requires social intelligence to gather information through a long time span and use it to navigate various social contexts effectively. Whether AI systems are also capable of this is understudied in the existing research. In this paper, we present a novel benchmark, LifelongSotopia, to perform a comprehensive evaluation of language agents by simulating multi-episode interactions. In each episode, the language agents role-play characters to achieve their respective social goals in randomly sampled social tasks. With LifelongSotopia, we find that goal achievement and believability of all of the language models that we test decline through the whole interaction. Although using an advanced memory method improves the agents' performance, the best agents still achieve a significantly lower goal completion rate than humans on scenarios requiring an explicit understanding of interaction history. These findings show that we can use LifelongSotopia to evaluate long-context language models and the social intelligence of language agents over lifelong social interactions.",2025,0.5454543471075102,0.5427388034180588,0.6,0.625,d235b266-344a-4770-809f-e011fd171367,0,"[0.25, 0.5, 0.625, 0.625]","[0.95, 0.95, 0.9, 0.95]",LifelongSotopia,0.4924537487828628
546088,Semantic-Centric Alignment for Zero-shot Panoptic and Semantic Segmentation,"Zero-shot segmentation has achieved great success by generating features from semantic embeddings to adapt the model to unseen classes. These semantic-generated features are typically aligned with the visual distribution of seen classes to improve generalization on extracted image features. However, this vision-centric alignment may easily overfit seen classes due to the lack of visual data for unseen classes. To address this issue, we propose a semantic-centric alignment method that aligns the generated features with the well-structured semantic distribution across all classes. First, we align the vision backbone features with CLIP tokens through Vision-to-CLIP alignment. This approach leverages CLIP’s visual-language matching capabilities to produce semantic-aligned backbone features. Then, we generate synthetic features from semantic embeddings for unseen classes, supervised by semantic-aligned visual features and CLIP semantic tokens for improving visual diversity while maintaining semantic consistency. Finally, we finetune the class projector through the semantic-aligned joint features to further adapt the model for unseen classes. Our semantic-centric alignment effectively enhances the model’s zero-shot generalization by constructing a unified and well-structured semantic-aligned feature space. Our method achieves SOTA performance in both zero-shot panoptic and semantic segmentation, and can directly segment unseen classes without fine-tuning.",2025,0.5113634504132908,0.5069376223885896,0.5333333333333333,0.5,fb3c74b0-ac6f-4157-91bc-ee408dbdfb8b,0,"[0.25, 0.5, 0.5, 0.625]","[1.0, 0.9, 0.95, 0.95]",Semantic-Centric Alignment,0.4486627563042241
546106,Incorporating Visual Correspondence into Diffusion Model for Virtual Try-On,"Diffusion models have shown preliminary success in virtual try-on (VTON) task. The typical dual-branch architecture comprises two UNets for implicit garment deformation and synthesized image generation respectively, and has emerged as the recipe for VTON task. Nevertheless, the problem remains challenging to preserve the shape and every detail of the given garment due to the intrinsic stochasticity of diffusion model. To alleviate this issue, we novelly propose to explicitly capitalize on visual correspondence as the prior to tame diffusion process instead of simply feeding the whole garment into UNet as the appearance reference. Specifically, we interpret the fine-grained appearance and texture details as a set of structured semantic points, and match the semantic points rooted in garment to the ones over target person through local flow warping. Such 2D points are then augmented into 3D-aware cues with depth/normal map of target person. The correspondence mimics the way of putting clothing on human body and the 3D-aware cues act as semantic point matching to supervise diffusion model training. A point-focused diffusion loss is further devised to fully take the advantage of semantic point matching. Extensive experiments demonstrate strong garment detail preservation of our approach, evidenced by state-of-the-art VTON performances on both VITON-HD and DressCode datasets. Code is publicly available at: https://github.com/HiDream-ai/SPM-Diff.",2025,0.772726628099408,0.7745817278055985,0.6666666666666666,0.625,8f93617a-d3ce-4002-b860-799cd8c90360,1,"[0.625, 0.625, 0.875]","[0.95, 0.95, 1.0]",Visual Correspondence,0.7255499760879962
546119,Enhancing Event Camera Data Pretraining via Prompt-Tuning with Visual Models,"The pretraining-finetuning paradigm has achieved remarkable success in natural language processing and computer vision, becoming the dominant approach in many downstream tasks. However, its application in the event camera domain has encountered significant challenges. First, the scarcity and sparsity of large-scale event datasets lead to issues like overfitting during extensive pretraining. Second, event data inherently contains both temporal and spatial information, making it difficult to directly transfer knowledge from image-based pretraining to event camera tasks.
In this paper, we propose a low-parameter-cost SpatioTemporal Information Fusion Prompting (STP) method to address these challenges. This method enables bidirectional fusion of event and image data while mitigating the risk of overfitting. Specifically, the key innovation lies in effectively integrating the spatio-temporal information of event data to align with pre-trained image models and reduce the impact of data sparsity.
To achieve this, we designed an Overlap Patch Embedding module within the STP, which employs wide receptive field to capture more local information and reduce the influence of sparse regions. Additionally, we introduce a Temporal Transformer that integrates both global and local information, facilitating the fusion of temporal and spatial data. Our approach significantly outperforms previous state-of-the-art methods across multiple downstream tasks, including classification, semantic segmentation, and optical flow estimation. For instance, it achieves a top-1 accuracy of 68.83% on N-ImageNet with fewer trainable parameters. Our code is available in the Supplement.",2025,0.443181657024852,0.447824069737808,0.4,0.25,7a66f570-321c-4ffe-b442-cb9e686662f3,0,"[0.25, 0.25, 0.5, 0.625]","[0.9, 0.95, 0.95, 1.0]",SpatioTemporal Information Fusion Prompting,0.4332501612716771
546134,Hough Voting-based Self-Training for Vision-Language Model Adaptation,"Traditional model adaptation framework assumes the same vocabulary across pre-training and downstream datasets, which often struggles with limited transfer flexibility and efficiency while handling downstream datasets with different vocabularies.
Inspired by recent vision-language models (VLMs) that enable visual recognition defined by free-form texts via reasoning on both images and texts, we study vision-language model adaptation (VLMA), a new unsupervised model adaptation framework that positions a pre-trained VLM as the source model and transfers it towards various unlabelled downstream datasets.
To this end, we propose a Hough voting-based Self-Training (HoughST) technique that introduces a multimodal Hough voting mechanism to exploit the synergy between vision and language to mitigate the distribution shift in image and text modalities simultaneously. 
Specifically, HoughST makes use of the complementary property of different types of features within and across vision and language modalities, which enables joint exploitation of vision and language information and effective learning of image-text correspondences in the unlabelled downstream datasets. 
Additionally, HoughST captures temporal information via temporal Hough voting which helps memorize and leverage previously learnt downstream dataset information.
Extensive experiments show that HoughST outperforms the state-of-the-art consistently across 11 image recognition tasks. 
Codes will be released.",2025,0.4090907603306326,0.4101833841221021,0.4,0.25,3a38af28-ab17-44db-94f9-e1b54fef790b,0,"[0.25, 0.25, 0.5, 0.5]","[0.95, 0.95, 0.95, 1.0]",HoughST,0.3849410898379971
546157,Weak Supervision from Vision-Language Models to Self-Improve on Downstream Tasks,"We present SelfPrompt, a novel prompt-tuning approach for vision-language models (VLMs) in a semi-supervised learning setup. Existing methods for tuning VLMs in semi-supervised setups struggle with the efficient use of the limited label-set budget, the negative impact of the miscalibrated VLMs on pseudo-labelling, and the accumulation of noisy pseudo-labels. SelfPrompt addresses these challenges by introducing (a) a weakly-supervised sampling technique that selects a diverse and representative labelled set, (b) a cluster-guided pseudo-labelling method that improves pseudo-label accuracy, and (c) a confidence-aware semi-supervised learning module that maximizes the utilization of unlabelled data by combining supervised learning and weakly-supervised learning. We conduct extensive evaluations across 13 datasets, significantly surpassing state-of-the-art performances with average improvements of 6.23\% in standard semi-supervised learning, 6.25\% in our proposed active semi-supervised learning,  and 4.9\% in base-to-novel generalization, using a 2-shot setup. Furthermore, SelfPrompt shows excellent generalization in single-shot settings, achieving an average improvement of 11.78\%.",2025,0.3409089669421938,0.3431432983874657,0.2666666666666666,0.25,5931dd5f-47d5-4209-a96f-487e717b593d,0,"[0.25, 0.25, 0.25, 0.5]","[0.8, 0.95, 0.95, 0.95]",SelfPrompt,0.321422501699524
546171,Arctic-SnowCoder: Demystifying High-Quality Data in Code Pretraining,"Recent studies have been increasingly demonstrating that high-quality data is crucial for effective pretraining of language models. However, the precise definition of ""high-quality"" remains underexplored. Focusing on the code domain, we introduce Arctic-SnowCoder-1.3B, a data-efficient base code model pretrained on 555B tokens through three phases of progressively refined data: (1) general pretraining with 500B standard-quality code tokens, preprocessed through basic filtering, deduplication, and decontamination, (2) continued pretraining with 50B high-quality tokens, selected from phase one by a BERT-style quality annotator trained to distinguish good code from random data, using positive examples drawn from high-quality code files, along with instruction data from Magicoder and StarCoder2-Instruct, and (3) enhanced pretraining with 5B synthetic data created by Llama-3.1-70B using phase two data as seeds, adapting the Magicoder approach for pretraining. Despite being trained on a limited dataset, Arctic-SnowCoder achieves state-of-the-art performance on BigCodeBench, a coding benchmark focusing on practical and challenging programming tasks, compared to similarly sized models trained on no more than 1T tokens, outperforming Phi-1.5-1.3B by 36%. Across all evaluated benchmarks, Arctic-SnowCoder-1.3B beats StarCoderBase-3B pretrained on 1T tokens. Additionally, it matches the performance of leading small base code models trained on trillions of tokens. For example, Arctic-SnowCoder-1.3B surpasses StarCoder2-3B, pretrained on over 3.3T tokens, on HumanEval+, a benchmark that evaluates function-level code generation, and remains competitive on BigCodeBench. Our evaluation presents a comprehensive analysis justifying various design choices for Arctic-SnowCoder. Most importantly, we find that the key to high-quality data is its consistency with the distribution of downstream applications.",2025,0.613636140495949,0.6117164135215023,0.6,0.5,92da244b-9c1f-4233-bac7-10fe11b24314,0,"[0.5, 0.5, 0.625, 0.625]","[1.0, 0.9, 0.9, 0.95]",Arctic-SnowCoder,0.5569474952827409
546182,STELLA: Leveraging Structural Representations to Enhance Protein Understanding with Multimodal LLMs,"Protein biology centers on the intricate relationships among sequence, structure, and function (text), with structure understanding being a crucial aspect for uncovering protein biological functions. Traditional methods based on protein language models (pLMs) often focus on specific aspects of biological function prediction but do not account for the broader, dynamic context of protein research—an important component for addressing the complexity of protein biology. Modern large language models (LLMs) excel in human-machine interaction, language understanding and generation, at a human-like level. By bridging structural representations with the contextual knowledge encoded within LLMs, STELLA leverages the strengths of LLMs to enable versatile and accurate predictions in protein-related tasks. It showcases the transformative potential of multimodal LLMs as a novel paradigm besides pLMs in advancing protein biology research by achieving state-of-the-art performance in both functional description and enzyme-catalyzed reaction prediction tasks. This study not only establishes an innovative LLM-based paradigm to understand proteins, but also expands the boundaries of LLM capabilities in protein biology. To foster collaboration and inspire further innovation, the codes, datasets, and pre-trained models are made publicly available at the anonymous GitHub repository https://anonymous.4open.science/r/STELLA-DF00.",2025,0.4545456528924899,0.4550219891045566,0.4,0.25,0f15b022-b753-48a1-bd21-83705f56cde1,0,"[0.0, 0.25, 0.25, 0.5, 0.625, 0.875]","[0.9, 1.0, 0.9, 0.9, 0.9, 0.95]",STELLA,0.4182899095803323
546186,MANTRA: The Manifold Triangulations Assemblage,"The rising interest in leveraging higher-order interactions present in complex systems has
led to a surge in more expressive models exploiting higher-order structures in the data,
especially in topological deep learning (TDL), which designs neural networks on higher-order domains such as simplicial complexes. However, progress in this field is hindered
by the scarcity of datasets for benchmarking these architectures. To address this gap, we
introduce MANTRA, the first large-scale, diverse, and intrinsically higher-order dataset for
benchmarking higher-order models, comprising over 43,000 and 250,000 triangulations
of surfaces and three-dimensional manifolds, respectively. With MANTRA, we assess
several graph- and simplicial complex-based models on three topological classification
tasks. We demonstrate that while simplicial complex-based neural networks generally
outperform their graph-based counterparts in capturing simple topological invariants, they
also struggle, suggesting a rethink of TDL. Thus, MANTRA serves as a benchmark for
assessing and advancing topological methods, paving the way towards more effective
higher-order models.",2025,0.6477270371901683,0.6420589817704987,0.6666666666666666,0.625,95033e09-d6b4-458e-b07f-e39c64644150,1,"[0.25, 0.625, 0.625, 0.875]","[0.95, 0.9, 0.95, 0.9]",MANTRA,0.5736722797927462
546189,Training on more Reachable Tasks for Generalisation in Reinforcement Learning,"In multi-task reinforcement learning, agents train on a fixed set of tasks and have to generalise to new ones. Recent work has shown that increased exploration improves this generalisation, but it remains unclear why exactly that is. In this paper, we introduce the concept of reachability in multi-task reinforcement learning and show that an initial exploration phase increases the number of reachable tasks the agent is trained on. This, and not the increased exploration, is responsible for the improved generalisation, even to unreachable tasks. Inspired by this, we propose a novel method Explore-Go that implements such an exploration phase at the beginning of each episode. Explore-Go only modifies the way experience is collected and can be used with most existing on-policy or off-policy reinforcement learning algorithms. We demonstrate the effectiveness of our method when combined with some popular algorithms and show an increase in generalisation performance across several environments.",2025,0.4999994545456529,0.5032270290639547,0.5333333333333333,0.25,ff597c13-919f-42cc-8695-e6bf38c76b69,0,"[0.25, 0.5, 0.625]","[0.9, 0.95, 0.95]",Reachability,0.4754471679364026
546195,Visual Context Window Extension: A New Perspective for Long Video Understanding,"Large Multimodal Models (LMMs) have demonstrated impressive performance in short video understanding tasks but face great challenges when applied to long video understanding. In contrast, Large Language Models (LLMs) exhibit outstanding capabilities in modeling long texts. Existing work attempts to address this issue by introducing long video-text pairs during training. However, these approaches require substantial computational and data resources. In this paper, we tackle the challenge of long video understanding from the perspective of context windows, aiming to apply LMMs to long video tasks without retraining on long video datasets. We first conduct an in-depth analysis of why pretrained LMMs struggle to understand lengthy video content, identifying that discrepancies between visual and language modalities lead to different context windows for visual and language tokens, making it difficult to directly extend the visual tokens to match the language context window. Based on this, we propose to adapt LMMs for long video understanding tasks by extending the visual context window, eliminating the need for retraining on large-scale long video datasets. To further mitigate the significant memory consumption caused by long sequences, we introduce a progressive pooling inference strategy that selectively adjusts the spatial resolution of frame embeddings, reducing the number of visual tokens while retaining important spatial information. Across multiple long video understanding benchmarks, our method consistently improves the performance as the number of video frames increases. On the MLVU benchmark, our method outperforms GPT-4o, even though our model size is only 7B. Additionally, in the 256-frame setting, our method reduces memory usage by approximately 45% compared to the baseline, without introducing any performance loss.",2025,0.613636140495949,0.6126236410646255,0.6,0.5,454a8777-2677-4d8d-8974-b83ca1e33ff8,0,"[0.5, 0.5, 0.625, 0.625]","[0.95, 1.0, 0.95, 1.0]",Visual Context Window,0.5624999999999998
546231,Statistical Test on Diffusion Model-based Anomaly Detection by Selective Inference,"Advancements in AI image generation, particularly diffusion models, have progressed rapidly. However, the absence of an established framework for quantifying the reliability of AI-generated images hinders their use in critical decision-making tasks, such as medical image diagnosis. In this study, we address the task of detecting anomalous regions in medical images using diffusion models and propose a statistical method to quantify the reliability of the detected anomalies. The core concept of our method involves a selective inference framework, wherein statistical tests are conducted under the condition that the images are produced by a diffusion model. With our approach, the statistical significance of anomaly detection results can be quantified in the form of a $p$-value, enabling decision-making with controlled error rates, as is standard in medical practice. We demonstrate the theoretical soundness and practical effectiveness of our statistical test through numerical experiments on both synthetic and brain image datasets.",2025,0.5454543471075102,0.5445543476130005,0.5333333333333333,0.5,a588bc37-9b00-4d96-9e84-79aaeba74f97,0,"[0.5, 0.5, 0.5]","[0.7, 0.95, 0.9]",Selective Inference,0.5
546238,Causal Reinforcement Learning for Spatio-Temporal Point Processes,"Spatio-temporal event sequences are increasingly accessible in various domains such as earthquake forecasting, crime prediction, and healthcare management. These data sources present unique challenges, as they involve both spatial and temporal dimensions, with event sequences exhibiting intricate dependencies over time and space. Neural network-based spatio-temporal point processes offer a sophisticated framework for modeling such event data. Conventional maximum likelihood estimation (MLE) of such data may lead to inaccurate predictions due to model-misspecification and compounding prediction errors. On the other hand, reinforcement learning frameworks, which treat event generation as actions and learn a policy to mimic event generation may alleviate the training/test discrepancy issue. Current reinforcement learning of point processes may have prohibitively poor exploration efficiency. In this paper, we propose the Causal learning improved Reinforcement Learning Spatio-Temporal Point Process (CRLSTPP) framework, which can mitigate the issue of compounding prediction errors and improve exploration efficiency at the same time. Experiments on both synthetic data and real-world data validate the superiority of the proposed model.",2025,0.4545456528924899,0.4504945642585021,0.5333333333333333,0.5,61bb5aa0-f8ad-4dbd-8d0f-a0c7181e72b4,0,"[0.25, 0.5, 0.5]","[0.95, 0.9, 0.9]",CRLSTPP,0.4017502706604114
546239,Anyprefer: An Agentic Framework for Preference Data Synthesis,"High-quality preference data is essential for aligning foundation models with human values through preference learning. However, manual annotation of such data is often time-consuming and costly. Recent methods often adopt a self-rewarding approach, where the target model generates and annotates its own preference data, but this can lead to inaccuracies since the reward model shares weights with the target model, thereby amplifying inherent biases. To address these issues, we propose Anyprefer, a framework designed to synthesize high-quality preference data for aligning the target model. Anyprefer frames the data synthesis process as a cooperative two-player Markov Game, where the target model and the judge model collaborate together. Here, a series of external tools are introduced to assist the judge model in accurately rewarding the target model’s responses, mitigating biases in the rewarding process. In addition, a feedback mechanism is introduced to optimize prompts for both models, enhancing collaboration and improving data quality. 
The synthesized data is compiled into a new preference dataset, Anyprefer-V1, consisting of 58K high-quality preference pairs. 
Extensive experiments show that Anyprefer significantly improves model alignment performance across four main applications, covering 21 datasets, achieving average improvements of 18.55% in five natural language generation datasets, 3.66% in nine vision-language understanding datasets, 30.05% in three medical image analysis datasets, and 16.00% in four visuo-motor control tasks.",2025,0.7499997272728265,0.7478299509247622,0.6666666666666666,0.625,a5baca48-2438-4096-9b23-cabd1841a9d0,1,"[0.625, 0.625, 0.625, 0.875]","[0.9, 0.9, 0.95, 0.9]",Anyprefer,0.6832087486157254
546241,What If We Recaption Billions of Web Images with LLaMA-3?,"Web-crawled image-text pairs are inherently noisy. Prior studies demonstrate that semantically aligning and enriching textual descriptions of these pairs can significantly enhance model training across various vision-language tasks, particularly text-to-image generation. However, large-scale investigations in this area remain predominantly closed-source. 
Our paper aims to bridge this community effort, leveraging the powerful and \textit{open-sourced} LLaMA-3, a GPT-4 level LLM.
Our recaptioning pipeline is simple: first, we fine-tune a LLaMA-3-8B powered LLaVA-1.5 and then employ it to recaption \app1.3 billion images from the DataComp-1B dataset. Our empirical results confirm that this enhanced dataset, Recap-DataComp-1B, offers substantial benefits in training advanced vision-language models. For discriminative models like CLIP, we observe \cg{an average of 3.1\% enhanced zero-shot performance cross four cross-modal retrieval tasks using a mixed set of the original and our captions}. For generative models like text-to-image Diffusion Transformers, the generated images exhibit a significant improvement in alignment with users' text instructions, especially in following complex queries.",2025,0.5795452438017296,0.5839626566410581,0.5333333333333333,0.5,0975960a-5a44-4623-b2b2-f44ea30ac769,0,"[0.25, 0.5, 0.5, 0.875]","[0.95, 0.9, 0.95, 1.0]",Recap-DataComp-1B,0.5618281090110905
546244,GenDataAgent: On-the-fly Dataset Augmentation with Synthetic Data,"We propose a generative agent that augments training datasets with synthetic data for model fine-tuning. Unlike prior work, which uniformly samples synthetic data, our agent iteratively generates relevant samples on-the-fly, aligning with the target distribution. It prioritizes synthetic data that complements difficult training samples, focusing on those with high variance in gradient updates. Experiments across several image classification tasks demonstrate the effectiveness of our approach.",2025,0.7159088305786071,0.7115506511780889,0.8,0.875,5317e759-aaab-41c5-9961-6ee469cdb65b,1,"[0.25, 0.625, 0.875, 0.875]","[0.95, 0.95, 0.9, 0.95]",GenDataAgent,0.6430440603700096
546270,I-Con: A Unifying Framework for Representation Learning,"As the field of representation learning grows, there has been a proliferation of different loss functions to solve different classes of problems. We introduce a single information-theoretic equation that generalizes a large collection of mod- ern loss functions in machine learning. In particular, we introduce a framework that shows that several broad classes of machine learning methods are precisely minimizing an integrated KL divergence between two conditional distributions: the supervisory and learned representations. This viewpoint exposes a hidden information geometry underlying clustering, spectral methods, dimensionality re- duction, contrastive learning, and supervised learning. This framework enables the development of new loss functions by combining successful techniques from across the literature. We not only present a wide array of proofs, connecting over 23 different approaches, but we also leverage these theoretical results to create state-of-the-art unsupervised image classifiers that achieve a +8% improvement over the prior state-of-the-art on unsupervised classification on ImageNet-1K. We also demonstrate that I-Con can be used to derive principled debiasing methods which improve contrastive representation learners.",2025,0.7159088305786071,0.7142614427205064,0.6666666666666666,0.625,9461e587-0ba8-4f5c-97c7-9090f1890ef7,1,"[0.5, 0.625, 0.625, 0.875]","[0.9, 0.95, 0.9, 0.9]",I-Con,0.6541043743078628
546285,Scaling Optimal LR Across Token Horizons,"State-of-the-art LLMs are powered by scaling -- scaling model size, training tokens, and cluster size. It is economically infeasible to extensively tune hyperparameters for the largest runs. Instead, approximately optimal hyperparameters must be inferred or transferred from smaller experiments. Hyperparameter transfer across model sizes has been studied in muP. However, hyperparameter transfer across training tokens -- or token horizon -- has not been studied yet. To remedy this we conduct a large-scale empirical study on how optimal learning rate (LR) depends on the token horizon in LLM training. We first demonstrate that the optimal LR changes significantly with token horizon -- longer training necessitates smaller LR. Secondly, we demonstrate that the optimal LR follows a scaling law and that the optimal LR for longer horizons can be accurately estimated from shorter horizons via such scaling laws. We also provide a rule-of-thumb for transferring LR across token horizons with zero overhead over current practices. Lastly, we provide evidence that LLama-1 used too high LR, and thus argue that hyperparameter transfer across data size is an overlooked component of LLM training.",2025,0.6818179338843877,0.6806929345162506,0.6666666666666666,0.625,8484abec-ece5-4bb6-bf12-9e6606ef2a23,1,"[0.625, 0.625, 0.625, 0.625]","[0.95, 0.95, 0.9, 0.9]",Token Horizon,0.625
546291,RoundTable: Investigating Group Decision-Making Mechanism in Multi-Agent Collaboration,"This study investigates the efficacy of Multi-Agent Systems in eliciting cross-agent communication and enhancing collective intelligence through group decision-making in a decentralized setting. Unlike centralized mechanisms, where a fixed hierarchy governs social choice, decentralized group decision-making allows agents to engage in joint deliberation. Our research focuses on the dynamics of communication and decision-making within various social choice methods. By applying different voting rules in various environments, we find that moderate decision flexibility yields better outcomes. Additionally, exploring the linguistic features of agent-to-agent conversations reveals indicators of effective collaboration, offering insights into communication patterns that facilitate or hinder collaboration. Finally, we propose various methods for determining the optimal stopping point in multi-agent collaborations based on linguistic cues. Our findings contribute to a deeper understanding of how decentralized decision-making and group conversation shape multi-agent collaboration, with implications for the design of more effective MAS environments.",2025,0.5113634504132908,0.5132795023208906,0.4,0.25,580e5c5d-30c2-48ba-a34c-57eb4d4a5f2d,0,"[0.25, 0.25, 0.5, 0.875]","[0.95, 0.9, 0.9, 0.95]",Decentralized Decision-Making,0.4807966321243522
546304,Towards Synergistic Path-based Explanations for Knowledge Graph Completion: Exploration and Evaluation,"Knowledge graph completion (KGC) aims to alleviate the inherent incompleteness of knowledge graphs (KGs), a crucial task for numerous applications such as recommendation systems and drug repurposing. The success of knowledge graph embedding (KGE) models provokes the question about the explainability: ``\textit{Which the patterns of the input KG are most determinant to the prediction}?'' Particularly, path-based explainers prevail in existing methods because of their strong capability for human understanding. In this paper, based on the observation that a fact is usually determined by the synergy of multiple reasoning chains, we propose a novel explainable framework, dubbed KGExplainer, to explore synergistic pathways. KGExplainer is a model-agnostic approach that employs a perturbation-based greedy search algorithm to identify the most crucial synergistic paths as explanations within the local structure of target predictions. To evaluate the quality of these explanations, KGExplainer distills an evaluator from the target KGE model, allowing for the examination of their fidelity. We experimentally demonstrate that the distilled evaluator has comparable predictive performance to the target KGE. Experimental results on benchmark datasets demonstrate the effectiveness of KGExplainer, achieving a human evaluation accuracy of 83.3\% and showing promising improvements in explainability. Code is available at \url{https://github.com/xiaomingaaa/KGExplainer}",2025,0.6818179338843877,0.6806929345162506,0.6666666666666666,0.625,7f99e566-0e26-4d8d-ad99-336f70a14025,1,"[0.625, 0.625, 0.625]","[0.95, 0.95, 0.95]",KGExplainer,0.6249999999999999
546312,Probe Pruning: Accelerating LLMs through Dynamic Pruning via Model-Probing,"We introduce Probe Pruning (PP), a novel framework for online, dynamic, structured pruning of Large Language Models (LLMs) applied in a batch-wise manner. PP leverages the insight that not all samples and tokens contribute equally to the model's output, and probing a small portion of each batch effectively identifies crucial weights, enabling tailored dynamic pruning for different batches. It comprises three main stages: probing, history-informed pruning, and full inference. In the probing stage, PP selects a small yet crucial set of hidden states, based on residual importance, to run a few model layers ahead. During the history-informed pruning stage, PP strategically integrates the probing states with historical states. Subsequently, it structurally prunes weights based on the integrated states and the PP importance score, a metric developed specifically to assess the importance of each weight channel in maintaining performance. In the final stage, full inference is conducted on the remaining weights. A major advantage of PP is its compatibility with existing models, as it operates without requiring additional neural network modules or fine-tuning. Comprehensive evaluations of PP on LLaMA-2/3 and OPT models reveal that even minimal probing—using just 1.5% of FLOPs—can substantially enhance the efficiency of structured pruning of LLMs. For instance, when evaluated on LLaMA-2-7B with WikiText2, PP achieves a 2.56 times lower ratio of performance degradation per unit of latency reduction compared to the state-of-the-art method at a 40\% pruning ratio.",2025,0.8181815206612653,0.8186470656144424,0.8,0.625,39cee6d7-9b9a-4747-82f7-9fdc70bf9eb6,1,"[0.625, 0.625, 0.875, 0.875]","[0.9, 0.95, 1.0, 0.9]",Probe Pruning,0.7611050094345182
546316,Enhancing Deception Detection with Cognitive Load Features: An Audio-Visual Approach,"Deception ranges from minor mischief to serious fraud, often leading to significant psychological and financial harm. Effective deception detection is crucial to mitigate these risks and preserve societal trust. Cognitive load is a useful indicator for detecting deception, as lying causes individuals to experience greater mental strain. While prior research leveraged cognitive load features, typically measured through physiological signals such as pupil dilation, these methods often require specialized equipment and can be subject to human bias. These limitations hinder the scalability and automation of deception detection systems. Thus, we propose a novel deception detection framework that automatically extracts cognitive load features from audio-visual data, eliminating the need for specialized hardware or subjective human input. Our approach integrates these features into the deception detection pipeline, enhancing its robustness. Moreover, we introduce a focal loss to address the inherent complexity of deception detection. This objective function enables the model to focus on harder-to-detect instances of deception, thereby improving the performance. Our approach achieves state-of-the-art results on benchmark audio-visual datasets, demonstrating significant improvements in automated deception detection. Extensive experiments validate the effectiveness of both our cognitive load feature extraction and the proposed objective function in advancing the field.",2025,0.7840906239670459,0.7850066762363017,0.8,0.875,36cf7b8f-85c2-4aa0-84c9-62292eef0335,0,"[0.5, 0.625, 0.875, 0.875]","[0.95, 0.95, 0.95, 1.0]",Cognitive Load Features,0.7311763622974963
546322,Perm: A Parametric Representation for Multi-Style 3D Hair Modeling,"We present Perm, a learned parametric representation of human 3D hair designed to facilitate various hair-related applications. Unlike previous work that jointly models the global hair structure and local curl patterns, we propose to disentangle them using a PCA-based strand representation in the frequency domain, thereby allowing more precise editing and output control. Specifically, we leverage our strand representation to fit and decompose hair geometry textures into low- to high-frequency hair structures, termed guide textures and residual textures, respectively. These decomposed textures are later parameterized with different generative models, emulating common stages in the hair grooming process. We conduct extensive experiments to validate the architecture design of Perm, and finally deploy the trained model as a generic prior to solve task-agnostic problems, further showcasing its flexibility and superiority in tasks such as single-view hair reconstruction, hairstyle editing, and hair-conditioned image generation. More details can be found on our project page: https://cs.yale.edu/homes/che/projects/perm/.",2025,0.886363314049704,0.8821028946330901,0.9333333333333332,0.875,9ed3f569-4730-4a91-8d0f-79cb122949e0,1,"[0.625, 0.875, 0.875, 0.875]","[0.95, 0.9, 0.9, 0.9]",Parametric Representation,0.7996262458165371
546326,Does Spatial Cognition Emerge in Frontier Models?,"Not yet. We present SPACE, a benchmark that systematically evaluates spatial cognition in frontier models. Our benchmark builds on decades of research in cognitive science. It evaluates large-scale mapping abilities that are brought to bear when an organism traverses physical environments, smaller-scale reasoning about object shapes and layouts, and cognitive infrastructure such as spatial attention and memory. For many tasks, we instantiate parallel presentations via text and images, allowing us to benchmark both large language models and large multimodal models. Results suggest that contemporary frontier models fall short of the spatial intelligence of animals, performing near chance level on a number of classic tests of animal cognition.",2025,0.7840906239670459,0.7827968746936882,0.8,0.875,732aa2ca-2716-4a0f-82af-57b8e9eeac45,1,"[0.5, 0.625, 0.875, 0.875]","[0.95, 0.95, 0.95, 0.95]",SPACE,0.7187499999999999
546334,Alberta Wells Dataset: Pinpointing Oil and Gas Wells from Satellite Imagery,"Millions of abandoned oil and gas wells are scattered across the world, leaching methane into the atmosphere and toxic compounds into the groundwater. Many of these locations are unknown, preventing the wells from being plugged and their polluting effects averted. Remote sensing is a relatively unexplored tool for pinpointing abandoned wells at scale. We introduce the first large-scale dataset for this problem, leveraging medium-resolution multi-spectral satellite imagery from Planet Labs. Our curated dataset comprises over 213,000 wells (abandoned, suspended, and active) from Alberta, a region with especially high well density, sourced from the Alberta Energy Regulator and verified by domain experts. We evaluate baseline algorithms for well detection and segmentation, showing the promise of computer vision approaches but also significant room for improvement.",2025,0.6477270371901683,0.6466582877904381,0.6666666666666666,0.625,cb1b5387-06a0-4aa2-b7b5-544a1793d8ac,0,"[0.5, 0.625, 0.625, 0.625]","[0.95, 0.95, 0.95, 0.95]",Alberta Wells Dataset,0.59375
546336,Efficient Time Series Forecasting via Hyper-Complex Models and Frequency Aggregation,"Time-series forecasting is a long-standing challenge in statistics and machine learning, with one of the key difficulties being the ability to process sequences with long-range dependencies. A recent line of work has addressed this by applying the short-time Fourier transform (STFT), which partitions sequences into multiple subsequences and applies a Fourier transform to each separately.
We propose the Frequency Information Aggregation (FIA-Net), a model that can utilize two backbone architectures: the Window-Mixing MLP (WM-MLP), which aggregates adjacent window information in the frequency domain, and the Hyper-Complex MLP (HC-MLP), which treats the set of STFT windows as hyper-complex (HC) valued vectors. and employ HC algebra to efficiently combine information from all STFT windows altogether. Furthermore, due to the nature of HC operations, the HC-MLP uses up to three times fewer parameters than the equivalent standard window aggre- gation method. We evaluate the FIA-Net on various time-series benchmarks and show that the proposed methodologies outperform existing state-of-the-art meth- ods in terms of both accuracy and efficiency. Our code is publicly available on https://anonymous.4open.science/r/research-1803/",2025,0.2045453801653163,0.193460555750385,0.1333333333333333,0.0,82493b26-6a89-4a9f-9ccf-5505584d8f4a,0,"[0.0, 0.0, 0.25, 0.5]","[1.0, 1.0, 0.9, 0.9]",Frequency Information Aggregation,0.136741623355487
546348,CreDes: Causal Reasoning Enhancement and Dual-End Searching for Solving Long-Range Reasoning Problems using LLMs,"Large language models (LLMs) have demonstrated limitations in handling combinatorial optimization problems involving long-range reasoning, partially due to causal hallucinations and huge search space. As for causal hallucinations, i.e., the inconsistency between reasoning and corresponding state transition, this paper introduces the Causal Relationship Enhancement (CRE) mechanism combining cause-effect interventions and the Individual Treatment Effect (ITE) to guarantee the solid causal rightness between each step of reasoning and state transition. As for the long causal range and huge search space limiting the performances of existing models featuring single-direction search, a Dual-End Searching (DES) approach is proposed to seek solutions by simultaneously starting from both the initial and goal states on the causal probability tree. By integrating CRE and DES (CreDes), our model has realized simultaneous multi-step reasoning, circumventing the inefficiencies from cascading multiple one-step reasoning like the Chain-of-Thought (CoT). Experiments demonstrate that CreDes significantly outperforms existing State-Of-The-Art (SOTA) solutions in long-range reasoning tasks in terms of both accuracy and time efficiency.",2025,0.4363634776860081,0.4391569427411994,0.5333333333333333,0.5,df3b381e-093e-44e6-a4ab-097d6f183dd9,0,"[0.25, 0.25, 0.5, 0.5, 0.5]","[0.9, 0.9, 0.9, 1.0, 0.95]",CreDes,0.4171813437475676
546351,Adaptive Transformer Programs: Bridging the Gap Between Performance and Interpretability in Transformers,"Balancing high performance with interpretability in increasingly powerful Transformer-based models remains a challenge. While mechanistic interpretability aims to specify neural network computations in explicit, pseudocode-like formats, existing methods often involve laborious manual analysis or struggle to fully elucidate learned internal algorithms. Recent efforts to build intrinsically interpretable models have introduced considerable expressivity and optimization challenges. This work introduces Adaptive Transformer Programs, an enhanced framework building upon RASP language and Transformer Programs to create more robust and interpretable models. The proposed method increases expressivity by redesigning two primary attention modules to improve categorical and numerical reasoning capabilities. To overcome optimization hurdles, we introduce a novel reparameterization scheme that enhances the exploration-exploitation trade-off during training. We validate our approach through extensive experiments on diverse tasks, including in-context learning, algorithmic problems (e.g., sorting and Dyck languages), and NLP benchmarks such as named entity recognition and text classification. Results demonstrate that Adaptive Transformer Programs substantially narrow the performance gap between black-box Transformers and interpretable models, enhancing transparency. This work advances the development of high-performing, transparent AI systems for critical applications, addressing crucial ethical concerns in AI development.",2025,0.8181815206612653,0.8207207285701528,0.8,0.625,10cc0a69-f64d-47d4-bd23-d2e0c5ff9ef2,1,"[0.625, 0.625, 0.875, 0.875]","[0.9, 0.8, 0.9, 0.9]",Adaptive Transformer Programs,0.7620678408342988
546360,Pixel-Aware Accelerated Reverse Diffusion Modeling,"We propose in this paper an analytically new construct of a diffusion model whose drift and diffusion parameters yield a faster time-decaying Signal to Noise Ratio in the forward process. The proposed methodology significantly accelerates the forward diffusion process, reducing the required diffusion time steps from around 1000 seen in conventional models to 200-500 without compromising image quality in the reverse-time diffusion. In a departure from conventional models which typically use time-consuming multiple runs, we introduce a parallel data-driven model to generate a reverse-time diffusion trajectory in a single run of the model. The construct cleverly carries out the learning of the diffusion coefficients via an estimate of the structure of clean images. The resulting collective block-sequential generative model eliminates the need for MCMC-based sub-sampling correction for safeguarding and improving image quality, which further improve the acceleration of image generation. Collectively, these advancements yield a generative model that is at least 4 times faster than conventional approaches, while maintaining high fidelity and diversity in generated images, hence promising widespread applicability in rapid image synthesis tasks.",2025,0.2727271735537551,0.2722771738065002,0.2666666666666666,0.25,98ecd9ff-f466-443e-8b8e-548a2eb45d7c,0,"[0.25, 0.25, 0.25, 0.25, 0.25]","[0.95, 0.95, 0.95, 0.95, 0.95]",Pixel-Aware Diffusion,0.25
546396,TAS: Distilling Arbitrary Teacher and Student via a Hybrid Assistant,"Most knowledge distillation (KD) methodologies predominantly focus on teacher-student pairs with similar architectures, such as both being convolutional neural networks (CNNs). However, the potential and flexibility of KD can be greatly improved by expanding it to novel Cross-Architecture KD (CAKD), where the knowledge of homogeneous and heterogeneous teachers can be transferred flexibly to a given student. The primary challenge in CAKD lies in the substantial feature gaps between heterogeneous models, originating from the distinction of their inherent inductive biases and module functions. To this end, we introduce an assistant model as a bridge to facilitate smooth feature knowledge transfer between heterogeneous teachers and students. More importantly, within our proposed design principle, the assistant model combines the advantages of cross-architecture inductive biases and module functions by merging convolution and attention modules derived from both student and teacher module functions. Furthermore, we observe that heterogeneous features exhibit diverse spatial distributions in CAKD, hindering the effectiveness of conventional pixel-wise mean squared error (MSE) loss. Therefore, we leverage a spatial-agnostic InfoNCE loss to align features after spatial smoothing, thereby improving the feature alignments in CAKD. Our proposed method is evaluated across some homogeneous model pairs and arbitrary heterogeneous combinations of CNNs, ViTs, and MLPs, achieving state-of-the-art performance for distilled models with a maximum gain of  11.47% on CIFAR-100 and 3.67% on ImageNet-1K for distilled models. Our code and models will be released.",2025,0.4090907603306326,0.4029702172336203,0.4,0.25,91d2742d-f3be-419f-8320-6b058713e683,0,"[0.25, 0.25, 0.5, 0.5]","[1.0, 0.95, 0.9, 0.9]",Cross-Architecture KD,0.34925977934256
546413,LLS: Regulating Neural Network Training via Learnable Label Smoothing,"Training a neural network using one-hot targets often leads to the issue of overconfidence. 
To address this, Label Smoothing has been introduced, modifying the targets to a mix of one-hot encoding and a uniform probability vector. 
However, the uniform probability vector indiscriminately assigns equal weights to all categories, thereby undermining inter-category relationships. To overcome these challenges, we propose a novel solution, Learnable Label Smoothing (LLS) that aims to regulate training by granting networks the ability to assign optimal targets. Unlike conventional methods, Learnable Label Smoothing utilizes probability vectors unique to each category, resulting in diverse targets. The acquired relationships are beneficial for regularization and also prove to be transferable, facilitating knowledge distillation even in the absence of a Teacher model. Our extensive experiments across multiple datasets highlight the advantages of our method in addressing both overconfidence and the preservation of inter-category relationships in neural network training.",2025,0.5454543471075102,0.5463219710253523,0.6,0.625,e7992e75-7994-448f-a0b1-546a6835f37e,0,"[0.25, 0.5, 0.625, 0.625]","[0.95, 0.95, 0.95, 1.0]",Learnable Label Smoothing,0.509941089837997
546417,MMEval: Evaluating Video Generation Models for Motion Quality,"Recent advancements in video generation, especially with diffusion models, have led to new challenges in evaluating the generated outputs, highlighting the need for well-curated evaluation metrics and benchmarks. While prior work has focused on assessing text-to-video models for overall video quality, such as temporal coherence and prompt consistency, they overlook a crucial aspect: motion modeling abilities of generative models. To address this gap, we propose a structured approach to evaluate image-to-video generation models, with a focus on their motion modeling abilities. For example, we assess how accurately models generate motions like ""circular movement for a rotating ferris wheel"" or ""oscillatory motion for a pendulum"". We categorize videos  into linear, circular, and oscillatory motion-types and formulate metrics to capture key motion properties for each category. Our benchmark, MMEval, along with the code and image-prompt-video sets, will be publicly released.",2025,0.4772725537190714,0.4764850541613754,0.5333333333333333,0.5,add292d2-19cc-4b9d-ba4e-883f3dcb4a64,0,"[0.25, 0.5, 0.5, 0.5]","[0.95, 0.95, 0.95, 0.95]",Motion Modeling,0.4374999999999999
546424,Enhance Graph Contrastive Learning with Perturbation Discrimination,"Self-supervised learning of graph-structured data aims to produce transferable and robust representations that could be transferred to the downstream tasks. Among many, graph contrastive learning (GCL) based on data augmentation has emerged with promising performance in learning graph representation. However, it is observed that some augmentations might change the graph semantics due to the perturbations in the graph structure such as perturbing some nodes/edges. In such cases, existing GCL methods may suffer from performance limitations due to the introduction of noise augmentations. To address this issue, we propose to train a discriminative model to enhance GCL for graph-structured data, called Perturbation Discrimination-Enhanced GCL (PerEG). Specifically, for each perturbed graph, the discriminative model is trained to predict whether each node in the augmentation was perturbed by the perturbation compared to the original graph or not. Based on this, the results of perturbation discrimination are exploited to refine the GCL, enabling its controllable use of augmentation, thereby preferably utilizing augmentation and effectively avoiding the introduction of noise augmentation. Extensive experiments in unsupervised, semi-supervised, and transfer learning scenarios show that our PerEG outperforms the state-of-the-art methods on eight datasets.",2025,0.4545456528924899,0.4522567421273778,0.5333333333333333,0.5,f1b2f26b-8001-4b3e-9463-fce30ff893af,0,"[0.25, 0.5, 0.5]","[1.0, 1.0, 0.95]",Perturbation Discrimination,0.4088643259644559
546436,"The Curse of Multi-Modalities: Evaluating Hallucinations of Large Multimodal Models across Language, Visual, and Audio","Recent advancements in large multimodal models (LMMs) have significantly enhanced performance across diverse tasks, with ongoing efforts to further integrate additional modalities such as video and audio. However, most existing LMMs remain vulnerable to hallucinations, the discrepancy between the factual multimodal input and the generated textual output, which has limited their applicability in various real-world scenarios. This paper presents the first systematic investigation of hallucinations in LMMs involving the three most common modalities: language, visual, and audio. Our study reveals two key contributors to hallucinations: overreliance on unimodal priors and spurious inter-modality correlations. To address these challenges, we introduce the benchmark \textit{The Curse of Multi-Modalities} (\textbf{CMM}), which comprehensively evaluates hallucinations in LMMs, providing a detailed analysis of their underlying issues. Our findings highlight key vulnerabilities, including imbalances in modality integration and biases from training data, underscoring the need for balanced cross-modal learning and enhanced hallucination mitigation strategies. Based on our observations and findings, we suggest potential research directions that could enhance the reliability of LMMs. We will make our code and data publicly available.",2025,0.6477270371901683,0.6448667039867912,0.6666666666666666,0.625,2640cfe2-49e3-430d-b6b6-41dfddcfabfb,0,"[0.5, 0.625, 0.625, 0.625]","[1.0, 0.95, 0.9, 0.95]",Hallucinations,0.5841536216195387
546438,A long range foundation model for zero-shot predictions in single-cell and spatial transcriptomics data,"Large transformers pre-trained with language model objectives have demonstrated success in multiple fields, and have tremendous potential for modeling single-cell RNA-seq and spatial transcriptomics data. However, these approaches are yet to overcome various challenges, including inductive biases that hinder generalization, artifacts and quality of the underlying data, as well as downstream evaluation pipelines that do not reflect the biological challenges in the field. In this work, we propose a new framework, sCellTransformer (sCT), that relies on a first principles formulation of the problem as well as a validation pipeline designed to evaluate models generalization through zero-shot predictions. sCT leverages a long-range convolutional-transformer architecture that is trained from unprocessed single-cell and spatial transcriptomics data. In contrast to previous works, sCT represents cells with up to 20,000 protein-coding genes, processes sets of multiple cells, and predicts about a million discretized gene expression tokens. We show that representing gene expression as discrete levels allows us to mitigate the high sparsity present in single-cell data both during training and evaluation. We present state-of-the-art empirical results on several zero-shot gene expression imputation, cell-typing, and clustering tasks in both single-cell as well as spatial domains, outperforming current foundation models.",2025,0.4772725537190714,0.4764850541613754,0.4666666666666667,0.25,4d2fb43e-4408-4f56-a22b-ef587a3fe892,0,"[0.25, 0.25, 0.625, 0.625]","[1.0, 0.95, 0.95, 1.0]",sCellTransformer,0.4375
546440,Write More at Once: Stylized Chinese Handwriting Generation via Two-stage Diffusion,"Handwritten data generation is an intriguing research area with broad applications in human interaction with digital documents. In Chinese handwritten text generation, practical applications necessitate the ability to produce sentence-level handwritten data to convey complex information effectively. However, existing methods mainly focus on generating single-font outputs. To tackle this challenge, we model handwritten text generation as a \textit{style transfer problem}, aiming to convert a standard text line template into a target handwriting style. Recognizing the highly structured nature of handwritten data, we view complex text lines as compositions of individual characters and their positions. We propose a two-stage text line generation method based on generative diffusion model. In the first stage, character positions are generated using a Character-Position-Diffusion (CharPos-Diff), which, combined with standard character templates from a digital library, creates text line-level templates. In the second stage, a font style transfer diffusion model (Imitating-Diff) generates handwritten text lines directly from these templates. Our extensive experiments show that our method effectively mimics handwriting styles, generates structurally accurate text lines, and facilitates the simultaneous generation of paragraph-level handwritten text.",2025,0.5113634504132908,0.5079014835838647,0.5333333333333333,0.5,a06740c3-689a-4467-aff9-3b7d475ca1f3,0,"[0.25, 0.5, 0.5, 0.625]","[1.0, 0.95, 1.0, 0.95]",Character-Position-Diffusion,0.4549369031377898
546467,IMPaCT GNN: Imposing invariance with Message Passing in Chronological split Temporal Graphs,"This paper addresses domain adaptation challenges in graph data resulting from chronological splits. In a transductive graph learning setting, where each node is associated with a timestamp, we focus on the task of Semi-Supervised Node Classification (SSNC), aiming to classify recent nodes using labels of past nodes. Temporal dependencies in node connections create domain shifts, causing significant performance degradation when applying models trained on historical data into recent data. Given the practical relevance of this scenario, addressing domain adaptation in chronological split data is crucial, yet underexplored. We propose Imposing invariance with Message Passing in Chronological split Temporal Graphs (\IMPaCT), a method that imposes invariant properties based on realistic assumptions derived from temporal graph structures. Unlike traditional domain adaptation approaches which rely on unverifiable assumptions, \IMPaCT explicitly accounts for the characteristics of chronological splits. The \IMPaCT is further supported by rigorous mathematical analysis, including a derivation of an upper bound of the generalization error. Experimentally, \IMPaCT achieves a 3.8\% performance improvement over current SOTA method on the ogbn-mag graph dataset. Additionally, we introduce the Temporal Stochastic Block Model (TSBM), which replicates temporal graphs under varying conditions, demonstrating the applicability of our methods to general spatial GNNs.",2025,0.5181816297521347,0.5126924727341639,0.5333333333333333,0.5,8c87611a-0e80-4a40-aef3-0c4967736f3d,0,"[0.25, 0.5, 0.5, 0.5, 0.625]","[1.0, 0.95, 0.9, 0.95, 0.9]",IMPaCT,0.4509489164147149
546471,Jacobian Descent for Multi-Objective Optimization,"Many optimization problems require balancing multiple conflicting objectives.
As gradient descent is limited to single-objective optimization, we introduce its direct generalization: Jacobian descent (JD).
This algorithm iteratively updates parameters using the Jacobian matrix of a vector-valued objective function, in which each row is the gradient of an individual objective.
While several methods to combine gradients already exist in the literature, they are generally hindered when the objectives conflict.
In contrast, we propose projecting gradients to fully resolve conflict while ensuring that they preserve an influence proportional to their norm.
We prove significantly stronger convergence guarantees with this approach, supported by our empirical results.
Our method also enables instance-wise risk minimization (IWRM), a novel learning paradigm in which the loss of each training example is considered a separate objective.
Applied to simple image classification tasks, IWRM exhibits promising results compared to the direct minimization of the average loss.
Additionally, we outline an efficient implementation of JD using the Gramian of the Jacobian matrix to reduce time and memory requirements.",2025,0.5113634504132908,0.5072556421275956,0.5333333333333333,0.5,5e640c41-0246-4694-878f-9fa98fc36c84,0,"[0.25, 0.5, 0.5, 0.625]","[0.95, 0.9, 0.9, 0.9]",Jacobian descent,0.4537306201550388
546488,MoH: Multi-Head Attention as Mixture-of-Head Attention,"In this work, we upgrade the multi-head attention mechanism, the core of the Transformer model, to improve efficiency while maintaining or surpassing the previous accuracy level. We show that multi-head attention can be expressed in the summation form. Drawing on the insight that not all attention heads hold equal significance, we propose Mixture-of-Head attention (MoH), a new architecture that treats attention heads as experts in the Mixture-of-Experts (MoE) mechanism. MoH has two significant advantages: First, MoH enables each token to select the appropriate attention heads, enhancing inference efficiency without compromising accuracy or increasing the number of parameters. Second, MoH replaces the standard summation in multi-head attention with a weighted summation, introducing flexibility to the attention mechanism and unlocking extra performance potential. Extensive experiments on ViT, DiT, and LLMs demonstrate that MoH outperforms multi-head attention by using only 50\%$\sim$90\% of the attention heads. Moreover, we demonstrate that pre-trained multi-head attention models, such as LLaMA3-8B, can be further continue-tuned into our MoH models. Notably, MoH-LLaMA3-8B achieves an average accuracy of 64.0\% across 14 benchmarks, outperforming LLaMA3-8B by 2.4\% by utilizing only 75\% of the attention heads. We believe the proposed MoH is a promising alternative to multi-head attention and provides a strong foundation for developing advanced and efficient attention-based models.",2025,0.6477270371901683,0.6457379909429721,0.6666666666666666,0.625,c7abe93b-a296-4242-ae72-06ddc2a8c7f8,0,"[0.5, 0.625, 0.625, 0.625]","[0.95, 0.9, 0.9, 0.95]",Mixture-of-Head Attention,0.5897344559585493
546489,Unlocking Structured Thinking in Language Models with Cognitive Prompting,"We propose cognitive prompting as a novel approach to guide problem-solving in large language models (LLMs) through structured, human-like cognitive operations such as goal clarification, decomposition, filtering, abstraction, and pattern recognition. By employing systematic, step-by-step reasoning, cognitive prompting enables LLMs to efficiently tackle complex, multi-step tasks. We evaluate the effectiveness of cognitive prompting on Meta's LLaMA models, comparing performance on arithmetic reasoning tasks using the GSM8K dataset and on commonsense reasoning benchmarks. Our analysis includes comparisons between models without cognitive prompting, models with a static sequence of cognitive operations, and models using reflective cognitive prompting, where the LLM dynamically self-selects the sequence of cognitive operations. The results show that cognitive prompting, particularly when dynamically adapted, significantly improves the performance of larger models, such as LLaMA3.1 70B, and enhances their ability to handle multi-step reasoning tasks. This approach also improves interpretability and flexibility, highlighting cognitive prompting as a promising strategy for general-purpose AI reasoning.",2025,0.3409089669421938,0.3311478552182464,0.2666666666666666,0.25,e2841036-bb35-40bf-9488-36086fbcf168,0,"[0.25, 0.25, 0.25, 0.5]","[0.95, 0.95, 1.0, 0.8]",Cognitive Prompting,0.2825215017784505
546491,Similarity-Driven Regularization for Aligning Chemical and Latent Spaces in Molecular Design,"Generative models play a pivotal role in molecular design by effectively generating target molecules. Among these, generative models with latent space stand out due to their robust latent space representation capabilities, powerful dimensionality reduction ability and controllability of generation. In molecular design applications, generative models with latent space convert input molecules into latent variables, capturing essential molecular features including both structural and property-related characteristics. Ideally, similar molecules should map to proximate latent variables. However, previous studies have shown an inconsistency between molecular similarity in the chemical space and that in the latent space. This inconsistency will impede the accurate representation and complicate subsequent design process,such as leading to higher optimization budget. To address this, we propose Molecular Similarity-Aware Consistency Regularization (MSCR), a straightforward regularization approach aimed at preserving the molecule similarity consistency. Our method proposes a brief but effective regularization technique to align chemical space and latent space,clearly reflect similarity relationships in latent space. We leverage Matched Molecules Pairs (MMPs) to introduce more robust similarity information than other conventional augmentation methods. Extensive experiments demonstrate that MSCR not only maintains molecules pairs similarity but also enhance optimization performance in molecular latent space tasks, without additional costs. Furthermore, our visualizations highlight molecular inconsistencies, thus underscoring the significance of our approach and improving the interpretability and relevance of our work.",2025,0.2727271735537551,0.2722771738065002,0.2666666666666666,0.25,00118d9f-6b35-41d4-b3a5-3826847a44ef,0,"[0.0, 0.25, 0.25, 0.5]","[0.9, 0.95, 0.95, 0.9]",Molecular Similarity-Aware Consistency Regularization,0.2500000000000001
546499,Optimizing Latent Goal by Learning from Trajectory Preference,"A glowing body of work has emerged focusing on instruction-following policies for open-world agents, aiming to better align the agent's behavior with human intentions. However, the performance of these policies is highly susceptible to the initial prompt, which leads to extra efforts in selecting the best instructions. We propose a framework named \emph{\textbf{P}reference \textbf{G}oal \textbf{T}uning} (PGT). PGT allows policies to interact with the environment to collect several trajectories, which will be categorized into positive and negative samples based on preference. A preference optimization algorithm is used to fine-tune the initial goal latent representation using the collected trajectories while keeping the policy backbone frozen. The experiment result shows that with minimal data and training, PGT achieves an average relative improvement of $72.0\%$ and $81.6\%$ over 17 tasks in 2 different foundation policies respectively, and outperforms the best human-selected instructions. Moreover, PGT surpasses full fine-tuning in the out-of-distribution (OOD) task-execution environments by $13.4\%$, indicating that our approach retains strong generalization capabilities. Since our approach stores a single latent representation for each task independently, it can be viewed as an efficient method for Continual Learning, without the risk of catastrophic forgetting or task interference. In short, PGT enhances the performance of agents across nearly all tasks in the Minecraft Skillforge benchmark and demonstrates robustness to the execution environment.",2025,0.613636140495949,0.6181432439320309,0.6,0.25,081f4318-a736-48d5-b375-ff6435f120df,0,"[0.25, 0.5, 0.625, 0.875]","[0.9, 0.9, 0.95, 0.95]",Preference Goal Tuning,0.5865932642487046
546503,SEAL: Safety-enhanced Aligned LLM Fine-tuning via Bilevel Data Selection,"Fine-tuning on task-specific data to boost downstream performance is a crucial step for leveraging Large Language Models (LLMs). However, though fine-tuning enhances the model performance for specialized applications, previous studies have demonstrated that fine-tuning the models on several adversarial samples or even benign data can greatly comprise the model's pre-equipped alignment and safety capabilities. In this work, we propose SEAL, a novel framework to enhance safety in LLM fine-tuning. SEAL learns a data ranker based on the bilevel optimization to up rank the safe and high-quality fine-tuning data and down rank the unsafe or low-quality ones. Models trained with SEAL demonstrate superior quality over multiple baselines, with 8.5\% and 9.7\% win rate increase compared to random selection respectively on Llama-3-8b-Instruct and Merlinite-7b models. Our code is available on github https://github.com/hanshen95/SEAL.",2025,0.6545452165290122,0.6543430387439527,0.6666666666666666,0.625,d785de1e-f23a-4850-8f5f-27abcfd86b1e,1,"[0.5, 0.625, 0.625, 0.625, 0.625]","[0.9, 0.95, 0.95, 0.95, 0.9]",SEAL,0.6037583350171752
546510,Language Model Alignment in Multilingual Trolley Problems,"We evaluate the moral alignment of large language models (LLMs) with human preferences in multilingual trolley problems. Building on the Moral Machine experiment, which captures over 40 million human judgments across 200+ countries, we develop a cross-lingual corpus of moral dilemma vignettes in over 100 languages called MultiTP. This dataset enables the assessment of LLMs' decision-making processes in diverse linguistic contexts. Our analysis explores the alignment of 19 different LLMs with human judgments, capturing preferences across six moral dimensions: species, gender, fitness, status, age, and the number of lives involved. By correlating these preferences with the demographic distribution of language speakers and examining the consistency of LLM responses to various prompt paraphrasings, our findings provide insights into cross-lingual and ethical biases of LLMs and their intersection. We discover significant variance in alignment across languages, challenging the assumption of uniform moral reasoning in AI systems and highlighting the importance of incorporating diverse perspectives in AI ethics. The results underscore the need for further research on the integration of multilingual dimensions in responsible AI research to ensure fair and equitable AI interactions worldwide.",2025,0.8522724173554846,0.8454925058430681,0.9333333333333332,0.875,16a49715-2bb8-4856-9c26-37cf45dc8854,1,"[0.5, 0.875, 0.875, 0.875]","[1.0, 0.9, 0.95, 0.95]",MultiTP,0.7524608648586162
546524,Masked Mamba: An Efficient Self-Supervised Framework for Pathological Image Classification,"Extracting visual representations is a crucial challenge in the domain of computational histopathology. Considering the powerful strength of deep learning algorithms and the dearth of annotated samples, self-supervised learning presents itself as a compelling strategy to extract effective visual representations from unlabeled histopathology images. Although some self-supervised learning methods have been specifically proposed for histopathology image classification, most of them have certain drawbacks that may affect the functionality or representation capacity. In this work, we propose Masked Mamba, a novel self-supervised visual representation learning method tailored for histopathology images that can adequately extract local-global features. The proposed method consists of two stages: local perception positional encoding (LPPE) and directional Mamba vision backbone (DM). In addition, we use masked autoencoder (MAE) pretraining to unleashing directional Mamba vision backbone's potential. Masked Mamba makes good use of domain-specific knowledge and requires no side information, which means good rationality and versatility. Experimental results demonstrate the effectiveness and robustness of masked Mamba on common histopathology classification tasks. Furthermore, ablation studies prove that the local perception positional encoding and directional Mamba vision backbone in masked Mamba can complement and enhance each other.",2025,0.2727271735537551,0.2722771738065002,0.2666666666666666,0.25,3aff28f1-06bc-49e8-a2b6-f381759c672b,0,"[0.25, 0.25, 0.25, 0.25]","[0.8, 1.0, 0.95, 1.0]",Masked Mamba,0.25
546526,Text Attributed Graph Node Classification Using Sheaf Neural Networks and Large Language Models,"Text-Attributed Graphs (TAGs) seamlessly integrate textual data with graph structures, presenting unique challenges and opportunities for jointly modeling text and graph information. Recent advancements in Large Language Models (LLMs) have significantly enhanced the generative and predictive capabilities of text modeling. However, existing graph models often fall short in capturing intricate node relationships, as their edge representations are typically limited to scalar values.

In this paper, we introduce \model, a novel method that encodes rich and complex relational information between nodes as edge vectors. During the message-passing phase, \model aggregates both neighbor node representations and edge vectors to update the central node's representation, eliminating the need to fine-tune the LLMs on the text-attributed graph.

Specifically, for a given TAG, \model is trained to minimize the prediction errors of the LLM in forecasting the next word in node text sequences. Furthermore, we enhance \model's performance by incorporating prompt-based fine-tuning techniques. Once trained, \model can be seamlessly adapted to various downstream tasks.

Extensive node classification experiments across multiple domains demonstrate that \model consistently achieves state-of-the-art performance, validating its effectiveness in capturing complex relationships within TAGs. Additionally, we conduct ablation studies and scalability analyses to ensure the robustness and applicability of our approach.",2025,0.2727271735537551,0.2722771738065002,0.2666666666666666,0.25,740373ec-2e02-481c-b94e-f520792058c4,0,"[0.0, 0.25, 0.25, 0.5]","[0.95, 1.0, 0.95, 0.95]",Sheaf Neural Networks,0.25
546529,TrackMamba: Mamba-Transformer Tracking,"Current one-stream Transformer-based trackers are quality but unfriendly to memory consumption of large resolution and long sequence, both of which are crucial keys to tracking tasks. Recently structured state space model (SSM) demonstrates promising performance and efficiency in sequence modeling but struggles to retrieve due to the limited hidden state number. To solve the computation challenge and explore the potential of Mamba, we propose TrackMamba, a Mamba-Transformer tracker containing TrackMamba Blocks and Attention Blocks. In order to better harness the scanning in TrackMamba Blocks for inter- and intra-frame modeling, we introduce various scan patterns for rearrangement and flipping. Furthermore, we propose Target Enhancement, including Temporal Token for target aggregation and search enhancement, and Temporal Mamba for target information cross-frame propagation. Extensive experiments show TrackMamba performs better than the first-generation one-stream Transformer-based tracker at same resolution and mitigates consumption growth when enlarging resolution, exhibiting the potential of Mamba-based model for large-resolution tracking.",2025,0.4545456528924899,0.4521750589752358,0.5333333333333333,0.5,82655fc2-d745-44e9-8ac4-7343775e880d,0,"[0.25, 0.5, 0.5]","[0.95, 0.9, 0.95]",TrackMamba,0.4098211328254389
546563,From Pixels to Prose: A Large Dataset of Dense Image Captions,"Training large vision-language models requires extensive, high-quality image-text pairs. Existing web-scraped datasets, however, are noisy and lack detailed image descriptions. To bridge this gap, we introduce PixelProse, a comprehensive dataset of over 16M (million) synthetically generated captions, leveraging cutting-edge vision-language models for detailed and accurate descriptions. To ensure data integrity, we rigorously analyze our dataset for problematic content, including child sexual abuse material (CSAM), personally identifiable information (PII), and toxicity. We also provide valuable metadata such as watermark presence and aesthetic scores, aiding in further dataset filtering. We hope PixelProse to serve as a valuable resource for future research involving vision-language modalities.",2025,0.4363634776860081,0.435082587112359,0.5333333333333333,0.5,9cdd4dc1-a806-4226-b33c-8c4e794081ee,0,"[0.25, 0.25, 0.5, 0.5, 0.5]","[1.0, 0.95, 0.95, 1.0, 0.95]",PixelProse,0.3969637334832724
546565,Biologically Constrained Barrel Cortex Model Integrates Whisker Inputs and Replicates Key Brain Network Dynamics,"The brain's ability to transform sensory inputs into motor functions is central to neuroscience and crucial for the development of embodied intelligence. Sensory-motor integration involves complex neural circuits, diverse neuronal types, and intricate intercellular connections. Bridging the gap between biological realism and behavioral functionality presents a formidable challenge. In this study, we focus on the columnar structure of the superficial layers of mouse barrel cortex as a model system. We constructed a model comprising 4,218 neurons across 13 neuronal subtypes, with neural distribution and connection strengths constrained by anatomical experimental findings. A key innovation of our work is the development of an effective construction and training pipeline tailored for this biologically constrained model. Additionally, we converted an existing simulated whisker sweep dataset into a spiking-based format, enabling our network to be trained and tested on neural signals that more closely mimic those observed in biological systems. The results of object discrimination utilizing whisker signals demonstrate that our barrel cortex model, grounded in biological constraints, achieves a classification accuracy exceeds classical convolutional neural networks (CNNs), recurrent neural networks (RNNs), and long short-term memory networks (LSTMs), by an average of 8.6%, and is on par with recent spiking neural networks (SNNs) in performance. Interestingly, a whisker deprivation experiment, designed in accordance with neuroscience practices, further validates the perceptual capabilities of our model in behavioral tasks.
Critically, it offers significant biological interpretability: post-training analysis reveals that neurons within our model exhibit firing characteristics and distribution patterns similar to those observed in the actual neuronal systems of the barrel cortex. This study advances our understanding of neural processing in the barrel cortex and exemplifies how integrating detailed biological structures into neural network models can enhance both scientific inquiry and artificial intelligence applications. The code is available at https://github.com/fun0515/RSNN_bfd.",2025,0.886363314049704,0.8849008148711258,0.9333333333333332,0.875,8ee3e71c-d3ff-4665-b225-94f1e19b4c73,1,"[0.625, 0.875, 0.875, 0.875]","[0.9, 0.9, 0.9, 0.9]",Barrel Cortex Model,0.8124999987685677
546568,Protein Language Model Fitness is a Matter of Preference,"Leveraging billions of years of evolution, scientists have trained protein language models (pLMs) to understand the sequence and structure space of proteins aiding in the design of more functional proteins. Although they have shown ability to improve efficiency in engineering, it remains unclear under what conditions they will succeed or fail. We aim to predict the circumstances in which pLMs can successfully perform zero-shot fitness estimation. Our work demonstrates the trends observed over hundreds of deep mutational scans across multiple different fitness objectives. We find that the likelihood, or abstractly, implicit preference of a certain protein sequence imbued during pretraining is predictive fitness prediction capabilities. Both over-preferred and under-preferred wild type sequences harm performance. Generating a causal link between training data and likelihood, we show a power law tail over what data increases protein likelihood which is tied to training sequence homology. Lastly, proteins of low likelihood can be remedied by unsupervised finetuning. In sum, the zero-shot fitness estimation abilities of pLMs can be predicted by the likelihood of the engineered sequence, thus suggesting when pLMs should be deployed in protein maturation campaigns and a way to improve their performance under circumstances of low likelihood.",2025,0.8181815206612653,0.8224262727868766,0.8,0.625,76733b88-9a02-420c-afd1-e01a4caad4f8,1,"[0.625, 0.625, 0.875, 0.875]","[0.8, 0.95, 0.95, 0.95]",Fitness Estimation,0.7678450033990405
546590,Linear Mode Connectivity in Differentiable Tree Ensembles,"Linear Mode Connectivity (LMC) refers to the phenomenon that performance remains consistent for linearly interpolated models in the parameter space. For independently optimized model pairs from different random initializations, achieving LMC is considered crucial for understanding the stable success of the non-convex optimization in modern machine learning models and for facilitating practical parameter-based operations such as model merging. While LMC has been achieved for neural networks by considering the permutation invariance of neurons in each hidden layer, its attainment for other models remains an open question. In this paper, we first achieve LMC for soft tree ensembles, which are tree-based differentiable models extensively used in practice. We show the necessity of incorporating two invariances: subtree flip invariance and splitting order invariance, which do not exist in neural networks but are inherent to tree architectures, in addition to permutation invariance of trees. Moreover, we demonstrate that it is even possible to exclude such additional invariances while keeping LMC by designing decision list-based tree architectures, where such invariances do not exist by definition. Our findings indicate the significance of accounting for architecture-specific invariances in achieving LMC.",2025,0.8636364132231226,0.8622114134234825,0.9333333333333332,0.875,d8ec6c8b-cb7f-487c-a3b9-cb9a4ea6f3cb,1,"[0.625, 0.875, 0.875]","[0.8, 0.8, 0.8]",Linear Mode Connectivity,0.7916655157701479
546600,An Efficient LLM Alignment Framework for Automated Radiology Impression Generation,"Large language models (LLMs) are typically specialized for domain tasks through supervised fine-tuning, which optimizes LLMs for likelihood-based objectives. While supervised fine-tuning enables LLMs to generate text that conforms to the language style of a specific domain, such as radiology, it often falls short in enhancing the model's ability to perform detailed diagnostic reasoning or tailor reports for individual patients. In this paper, we explore the use of reinforcement learning to better align LLMs with the intricate requirements of radiological practice. By framing the report generation process as sequential decision-making stages, we present Radiology-Guided Reinforcement Optimization (RGRO), a tailored policy optimization framework designed specifically for medical language tasks. RGRO moves beyond conventional likelihood-based training by directly optimizing for radiology-specific objectives, including consistency with radiology findings and adherence to established professional guidelines. Our empirical evaluations demonstrate that RGRO significantly enhances the diagnostic precision and clinical utility of radiology reports generated by LLMs, outperforming supervised fine-tuning methods and state-of-the-art models. Furthermore, RGRO enables the seamless integration of expert radiologist feedback and external diagnostic tools, all without the need for large-scale annotated datasets.",2025,0.4090907603306326,0.4066002165148086,0.4,0.25,e59680d5-3cd0-4827-b111-21303408d08c,0,"[0.25, 0.25, 0.5, 0.5]","[0.95, 0.95, 0.95, 0.9]",Radiology-Guided Reinforcement Optimization,0.3674537487828628
546607,Adversarial Training for Defense Against Label Poisoning Attacks,"As machine learning models grow in complexity and increasingly rely on publicly sourced data, such as the human-annotated labels used in training large language models, they become more vulnerable to label poisoning attacks.
These attacks, in which adversaries subtly alter the labels within a training dataset, can severely degrade model performance, posing significant risks in critical applications.
In this paper, we propose $\textbf{Floral}$, a novel adversarial training defense strategy based on support vector machines (SVMs) to counter these threats. 
Utilizing a bilevel optimization framework, we cast the training process as a non-zero-sum Stackelberg game between an $\textit{attacker}$, who strategically poisons critical training labels, and the $\textit{model}$, which seeks to recover from such attacks. 
Our approach accommodates various model architectures and employs a projected gradient descent algorithm with kernel SVMs for adversarial training. 
We provide a theoretical analysis of our algorithm’s convergence properties and empirically evaluate $\textbf{Floral}$'s effectiveness across diverse classification tasks.
Compared to robust baselines and foundation models such as RoBERTa, $\textbf{Floral}$ consistently achieves higher robust accuracy under increasing attacker budgets.
These results underscore the potential of $\textbf{Floral}$ to enhance the resilience of machine learning models against label poisoning threats, 
thereby ensuring robust classification in adversarial settings.",2025,0.4772725537190714,0.4678560459690998,0.4666666666666667,0.25,702628dd-8c4b-4dab-a1ad-a6e7373e50fe,1,"[0.25, 0.25, 0.625, 0.625]","[0.9, 0.95, 0.8, 0.9]",Floral,0.4067284576328961
546618,Matryoshka Multimodal Models,"Large Multimodal Models (LMMs) such as LLaVA have shown strong performance in visual-linguistic reasoning. These models first embed images into a fixed large number of visual tokens and then feed them into a Large Language Model (LLM). However, this design causes an excessive number of tokens for dense visual scenarios such as high-resolution images and videos, leading to great inefficiency. While token pruning/merging methods do exist, they produce a single length output for each image and do not afford flexibility in trading off information density v.s. efficiency. Inspired by the concept of Matryoshka Dolls, we propose : Matryoshka Multimodal Models, which learns to represent visual content as nested sets of visual tokens that capture information across multiple coarse-to-fine granularities. Our approach offers several unique benefits for LMMs: (1) One can explicitly control the visual granularity per test instance during inference, e.g. , adjusting the number of tokens used to represent an image based on the anticipated complexity or simplicity of the content; (2)  provides a framework for analyzing the granularity needed for existing datasets, where we find that COCO-style benchmarks only need around 9 visual tokens to obtain accuracy similar to that of using all 576 tokens; (3) Our approach provides a foundation to explore the best trade-off between performance and visual token length at sample level, where our investigation reveals that a large gap exists between the oracle upper bound and current fixed-scale representations.",2025,0.6818179338843877,0.6806929345162506,0.6666666666666666,0.625,e49a7fd6-d1f2-49d4-996d-a7ec81d8e808,1,"[0.625, 0.625, 0.625, 0.625]","[0.95, 0.95, 0.9, 0.9]",Matryoshka Multimodal Models,0.625
546619,FDA: Generating Fair Synthetic Data with Provable Trade-off between Fairness and Faithfulness,"We propose a novel framework called FDA for generating Fair synthetic data through Data Augmentation, offering the first method with provable trade-off guarantee between fairness and faithfulness. Unlike other existing methods, our approach utilizes a novel joint model that consists of two sub-models: one focused on enforcing strict fairness constraints while the other dedicated to preserving fidelity to the original data, coupled with a tuning mechanism that provides explicit control over the trade-off between fairness and faithfulness. Specifically, our FDA framework enables explicit quantification of the extent to which the generated fair synthetic data preserve faithfulness to the original data, while achieving an intermediate level of fairness determined by a user specified parameter $\alpha \in [0, 1]$. Theoretically, we show that the resulting fair synthetic data converge to the original data in probability when $\alpha$ tends to 1, thereby implying convergence in distribution. Our framework can be also combined with some GAN-based fair models, such as DECAF,  to further improve the utility of the resulting synthetic data in downstream analysis, while carefully balancing fairness. Furthermore, we obtain an upper bound of the unfairness measurement for downstream models trained on the generated fair synthetic data, which can help users to choose appropriate $\alpha$. Finally, we perform numerical experiments on benchmark data to validate our theoretical contributions and to compare our FDA with other methods.",2025,0.3409089669421938,0.342185971844362,0.2666666666666666,0.25,b4c1a07a-5dec-4cb6-9b3c-340638bd483a,0,"[0.25, 0.25, 0.25, 0.5]","[0.9, 0.9, 0.95, 0.95]",FDA,0.3205310880829016
546651,AGLP: A Graph Learning Perspective for Semi-supervised Domain Adaptation,"In semi-supervised domain adaptation (SSDA), the model aims to leverage partially labeled target domain data along with a large amount of labeled source domain data to enhance its generalization capability for the target domain. A key advantage of SSDA is its ability to significantly reduce reliance on labeled data, thereby lowering the costs and time associated with data preparation. Most existing SSDA methods utilize information from domain labels and class labels but overlook the structural information of the data. To address this issue, this paper proposes a graph learning perspective (AGLP) for semi-supervised domain adaptation. We apply the graph convolutional network to the instance graph which allow structural information to propagate along the weighted graph edges. The proposed AGLP model has several advantages. First, to the best of our knowledge, this is the first work to model structural information in SSDA. Second, the proposed model can effectively learn domain-invariant and semantic representations, reducing domain discrepancies in SSDA. Extensive experimental results on multiple standard benchmarks demonstrate that the proposed AGLP algorithm outperforms state-of-the-art semi-supervised domain adaptation methods.",2025,0.3749998636364132,0.3743811139839378,0.2666666666666666,0.25,dd39825f-cd7b-48cd-b874-62f88bfe6bdf,0,"[0.25, 0.25, 0.25, 0.625]","[0.95, 0.95, 0.95, 0.95]",AGLP,0.34375
546658,A Scalable Transformer-based Framework for Fault Detection in Mission-Critical Systems,"Detecting underlying faults is crucial in the development of mission-critical planning systems, such as UAV trajectory planning in Unmanned aircraft Traffic Management (UTM), which is vital to airspace safety. 
Inevitably, there exists a small set of rare, unpredictable conditions where the UTM could suffer from catastrophic failures. 
Most traditional fault detection approaches focus on achieving high coverage by random input exploitation. 
However, random methods are struggling to detect long-tail vulnerabilities with unacceptable time consumption. 
To tackle this challenge, we propose a scenario-oriented framework to search long-tail conditions, accelerating the fault detection process. 
Inspired by in-context learning approaches, we leverage a Transformer-based policy model to capture the dynamics of the subject UTM system from the offline dataset for exploitation acceleration. 
We evaluate our approach over 700 hours in a massive-scale, industry-level simulation environment. 
Empirical results demonstrate that our approach achieves over 8 times more vulnerability discovery efficiency compared with traditional expert-guided random-walk exploitation, which showcases the potential of machine learning for fortifying mission-critical systems. 
Furthermore, we scale the model size to 2 billion parameters, achieving substantial performance gains over smaller models in offline and online evaluations, highlighting the scalability of our approach.",2025,0.5795452438017296,0.5785889943388131,0.5333333333333333,0.5,77395b57-a2a9-4d53-b48c-e2dc0b1a2904,0,"[0.25, 0.5, 0.5, 0.875]","[0.9, 0.9, 0.9, 0.9]",Scenario-oriented framework,0.53125
546665,Certifying Language Model Robustness with Fuzzed Randomized Smoothing: An Efficient Defense Against Backdoor Attacks,"The widespread deployment of pre-trained language models (PLMs) has exposed them to textual backdoor attacks, particularly those planted during the pre-training stage. These attacks pose significant risks to high-reliability applications, as they can stealthily affect multiple downstream tasks. While certifying robustness against such threats is crucial, existing defenses struggle with the high-dimensional, interdependent nature of textual data and the lack of access to original poisoned pre-training data. To address these challenges, we introduce **F**uzzed **R**andomized **S**moothing (**FRS**), a novel approach for efficiently certifying language model robustness against backdoor attacks. FRS integrates software robustness certification techniques with biphased model parameter smoothing, employing Monte Carlo tree search for proactive fuzzing to identify vulnerable textual segments within the Damerau-Levenshtein space. This allows for targeted and efficient text randomization, while eliminating the need for access to poisoned training data during model smoothing.  Our theoretical analysis demonstrates that FRS achieves a broader certified robustness radius compared to existing methods. Extensive experiments across various datasets, model configurations, and attack strategies validate FRS's superiority in terms of defense efficiency, accuracy, and robustness.",2025,0.6818179338843877,0.6806929345162506,0.6666666666666666,0.625,cb0a01e6-8f7c-4c74-a736-6821f8f2ad2a,1,"[0.625, 0.625, 0.625, 0.625]","[0.9, 0.9, 0.9, 0.95]",Fuzzed Randomized Smoothing,0.6250000000000001
546677,How Can LLM Guide RL? A Value-Based Approach,"Reinforcement learning (RL) has become the de facto standard practice for sequential decision-making problems by improving future acting policies with feedback. However, RL algorithms may require extensive trial-and-error interactions to collect useful feedback for improvement. On the other hand, recent developments in large language models (LLMs) have showcased impressive capabilities in language understanding and generation, yet they fall short in exploration and self-improvement capabilities for planning tasks, lacking the ability to autonomously refine their responses based on feedback. Therefore, in this paper, we study how the policy prior provided by the LLM can enhance the sample efficiency of RL algorithms. Specifically, we develop an algorithm named $\mathtt{LINVIT}$ that incorporates LLM guidance as a regularization factor in value-based RL, leading to significant reductions in the amount of data needed for learning, particularly when the difference between the ideal policy and the LLM-informed policy is small, which suggests that the initial policy is close to optimal, reducing the need for further exploration. Additionally, we present a practical algorithm $\mathtt{SLINVIT}$ that simplifies the construction of the value function and employs sub-goals to reduce the search complexity. Our experiments across three interactive environments---ALFWorld, InterCode, and BlocksWorld---demonstrate that the proposed method achieves state-of-the-art success rates and also surpasses previous RL and LLM approaches in terms of sample efficiency.",2025,0.443181657024852,0.4415301105880969,0.4,0.25,ce5a19e1-cc42-4d22-a7fb-82c6f2ce85f7,0,"[0.25, 0.25, 0.5, 0.625]","[0.9, 0.95, 0.95, 0.9]",LINVIT,0.4022344559585493
546690,Accelerated Diffusion using Closed-form Discriminator Guidance,"Diffusion models are a state-of-the-art generative modeling framework that transform noise to images via Langevin sampling, guided by the score, which is the gradient of the logarithm of the data distribution. Recent works have shown empirically that the generation quality can be improved when guided by classifier network, which is typically the discriminator trained in a generative adversarial network (GAN) setting. In this paper, we propose a theoretical framework to analyze the effect of the GAN discriminator on Langevin-based sampling, and show that in IPM GANs, the optimal generator matches {\it score-like} functions, involving the flow-field of the kernel associated with a chosen IPM constraint space. Further, we show that IPM-GAN optimization can be seen as one of smoothed score-matching, where the scores of the data and the generator distributions are convolved with the kernel associated with the constraint. The proposed approach serves to unify score-based training and optimization of IPM-GANs. Based on these insights, we demonstrate that closed-form discriminator guidance, using a kernel-based implementation, results in  improvements (in terms of CLIP-FID and KID metrics) when applied atop baseline diffusion models. We demonstrate these results by applying closed-form discriminator guidance to denoising diffusion implicit model (DDIM) and latent diffusion model (LDM) settings on the FFHQ and CelebA-HQ datasets. We also demonstrate improvements to accelerated time-step-shifted diffusion, when coupled with a wavelet-based noise estimator for latent-space image generation.",2025,0.5909092396693675,0.5926036450289812,0.5333333333333333,0.5,1b4bc94a-bb4b-41c2-b7db-3e3f297cc962,0,"[0.5, 0.5, 0.625]","[0.7, 0.95, 0.9]",Closed-form Discriminator Guidance,0.5454802119147448
546713,Conic Linear Units: Orthogonal Equivariance Improves General-Purpose Nonlinearities,"Most activation functions operate component-wise, which restricts the equivariance of neural networks to permutations. We introduce Conic Linear Units (CoLU) and generalize the symmetry of neural networks to continuous orthogonal groups. By interpreting ReLU as a projection onto its invariant set—the positive orthant—we propose a conic activation function that uses a Lorentz cone instead. Its performance can be further improved by considering multi-head structures, soft scaling, and axis sharing. CoLU associated with low-dimensional cones outperforms the component-wise ReLU in a wide range of models—including MLP, ResNet, and UNet, etc., achieving better loss values and faster convergence. It significantly improves diffusion models' training and performance. CoLU originates from a first-principles approach to various forms of neural networks and fundamentally changes their algebraic structure.",2025,0.613636140495949,0.6108320572609788,0.6,0.5,cce56b2c-c5b4-4ea2-8609-06c1ec7e3f7f,0,"[0.5, 0.5, 0.625, 0.625]","[0.95, 1.0, 0.9, 0.95]",Conic Linear Units,0.5537981085543922
546714,Time Series Representation Models for Multivariate Time Series Forecasting and Imputation,"We introduce a multilayered representation learning architecture called Time Series Representation Model (TSRM) for multivariate time series forecasting and imputation. The architecture is structured around hierarchically ordered encoding layers, each dedicated to an independent representation learning task. Each encoding layer contains a representation layer designed to capture diverse temporal patterns and an aggregation layer responsible for combining the learned representations. The architecture is fundamentally based on a Transformer encoder-like configuration, with self-attention mechanisms at its core. The TSRM architecture outperforms state-of-the-art approaches on most of the seven established benchmark datasets considered in our empirical evaluation for both forecasting and imputation tasks while significantly reducing complexity in the form of learnable parameters. The source code is available at https://anonymous.4open.science/r/TSRM-D7BE.",2025,0.4772725537190714,0.4764850541613754,0.5333333333333333,0.5,289d7a79-121d-4b21-b267-52dcb2062197,0,"[0.25, 0.5, 0.5, 0.5]","[0.95, 0.95, 0.95, 0.95]",Time Series Representation Model (TSRM),0.4374999999999999
546738,Mask in the Mirror: Implicit Sparsification,"Continuous sparsification strategies are among the most effective methods for reducing the inference costs and memory demands of large-scale neural networks. A key factor in their success is the implicit $L_1$ regularization induced by jointly learning both mask and weight variables, which has been shown experimentally to outperform explicit $L_1$ regularization. We provide a theoretical explanation for this observation by analyzing the learning dynamics, revealing that early continuous sparsification is governed by an implicit $L_2$ regularization that gradually transitions to an $L_1$ penalty over time. Leveraging this insight, we propose a method to dynamically control the strength of this implicit bias. Through an extension of the mirror flow framework, we establish convergence and optimality guarantees in the context of underdetermined linear regression. Our theoretical findings may be of independent interest, as we demonstrate how to enter the rich regime and show that the implicit bias can be controlled via a time-dependent Bregman potential. To validate these insights, we introduce PILoT, a continuous sparsification approach with novel initialization and dynamic regularization, which consistently outperforms baselines in standard experiments.",2025,0.6477270371901683,0.6466582877904381,0.6666666666666666,0.625,f14c712a-7ae3-4354-bf49-fa984c191465,1,"[0.5, 0.625, 0.625, 0.625]","[0.9, 0.9, 0.9, 0.9]",Implicit Sparsification,0.5937500000000001
546763,Newton Meets Marchenko-Pastur: Massively Parallel Second-Order Optimization with Hessian Sketching and Debiasing,"Motivated by recent advances in serverless cloud computing, in particular the ``function as a service'' (FaaS) model, 
we consider the problem of minimizing a convex function in a massively parallel fashion, where communication between workers is limited.
Focusing on the case of a twice-differentiable objective subject to an L2 penalty, we propose a scheme where the central node (server) effectively runs a Newton method, 
offloading its high per-iteration cost---stemming from the need to invert the Hessian---to the workers. 
In our solution, workers produce independently coarse but low-bias estimates of the inverse Hessian, using an adaptive sketching scheme. The server then averages the descent directions produced by the workers, yielding a good approximation for the exact Newton step. The main component of our adaptive sketching scheme is a low-complexity procedure for selecting the sketching dimension, an issue that was left largely unaddressed in the existing literature on Hessian sketching for distributed optimization. Our solution is based on ideas from asymptotic random matrix theory, specifically the Marchenko-Pastur law. For Gaussian sketching matrices, we derive non asymptotic guarantees for our algorithm which do not depend on the condition number of the Hessian nor a priori require the sketching dimension to be proportional to the dimension, as is often the case in asymptotic random matrix theory. Lastly, when the objective is self-concordant, we provide convergence guarantees for the approximate Newton's method with noisy Hessians, which may be of independent interest beyond the setting considered in this paper.",2025,0.7499997272728265,0.7497206436196745,0.6666666666666666,0.625,c04529cd-cd10-4aa3-9949-d8f1528a3ecc,1,"[0.625, 0.625, 0.625, 0.875]","[0.95, 0.9, 0.8, 0.9]",Hessian Sketching,0.6884080715477494
546775,Approaching Rate-Distortion Limits in Neural Compression with Lattice Transform Coding,"Neural compression has brought tremendous progress in designing lossy compressors with good rate-distortion (RD) performance at low complexity. Thus far, neural compression design involves transforming the source to a latent vector, which is then rounded to integers and entropy coded. While this approach has been shown to be optimal on a few specific sources, we show that it can be highly sub-optimal on synthetic sources whose intrinsic dimensionality is greater than one. With integer rounding in the latent space, the quantization regions induced by neural transformations, remain square-like and fail to match those of optimal vector quantization. We demonstrate that this phenomenon is due to the choice of scalar quantization in the latent space, and not the transform design. By employing lattice quantization instead, we propose  Lattice Transform Coding (LTC) and show that it approximately recovers optimal vector quantization at reasonable complexity. On real-world sources, LTC improves upon standard neural compressors. LTC also provides a framework that can integrate structurally (near) optimal information-theoretic designs into lossy compression; examples include block coding, which yields coding gain over optimal one-shot coding and approaches the asymptotically-achievable rate-distortion function, as well as nested lattice quantization for low complexity fixed-rate coding.",2025,0.8454542380166408,0.8440592388001508,0.9333333333333332,0.875,6dd1b4e6-6ef7-4ba0-9f9f-d4617440d366,1,"[0.625, 0.625, 0.875, 0.875, 0.875]","[0.95, 0.95, 0.95, 0.95, 0.95]",Lattice Transform Coding,0.7749999999999999
546778,Regularization by Texts for Latent Diffusion Inverse Solvers,"The recent development of diffusion models has led to significant progress in solving inverse problems by leveraging these models as powerful generative priors. However, challenges persist due to the ill-posed nature of such problems, often arising from ambiguities in measurements or intrinsic system symmetries. To address this, we introduce a novel latent diffusion inverse solver, regularization by text (TReg), inspired by the human ability to resolve visual ambiguities through perceptual biases. TReg integrates textual descriptions of preconceptions about the solution during reverse diffusion sampling, dynamically reinforcing these descriptions through null-text optimization, which we refer to as adaptive negation. Our comprehensive experimental results demonstrate that TReg effectively mitigates ambiguity in inverse problems, improving both accuracy and efficiency.",2025,0.8636364132231226,0.8589103249682525,0.9333333333333332,0.875,dbf082e3-d5a9-4bbd-a449-1e9c8194b8e2,1,"[0.625, 0.875, 0.875]","[0.95, 0.9, 0.9]",TReg,0.7767502705698922
546780,Self-Organizing Visual Embeddings for Non-Parametric Self-Supervised Learning,"We present Self-Organizing Visual Embeddings (SOVE) a new training technique for unsupervised representation learning.
SOVE avoids learning prototypes from scratch and explores relationships between visual embeddings in a non-parametric space.
Unlike existing clustering-based techniques that employ a single prototype to encode all the relevant features of a complex concept, we propose the SOVE method where a concept is represented by many semantically similar representations, or judges, each containing a complement set of features that together can fully characterize the concept and maximize training performance.
We reaffirm the feasibility of non-parametric self-supervised learning (SSL) by introducing novel non-parametric adaptions of two loss functions with the SOVE technique: (1) non-parametric cluster assignment prediction for class-level representations and (2) non-parametric Masked Image Modeling (MIM) for patch-level reconstruction.
SOVE achieves state-of-the-art performance on many downstream benchmarks, including transfer learning, image retrieval, object detection, and segmentation.
Moreover, SOVE demonstrates scaling performance when trained with Vision Transformers (ViTs), showing increased performance gains as more complex encoders are employed.",2025,0.4772725537190714,0.4736871339233398,0.5333333333333333,0.5,182d3cc4-df0e-408a-9d4e-cbb880693454,0,"[0.25, 0.5, 0.5, 0.5]","[0.95, 0.95, 0.8, 0.95]",Self-Organizing Visual Embeddings,0.4285774983004759
546782,FreCaS: Efficient Higher-Resolution Image Generation via Frequency-aware Cascaded Sampling,"While image generation with diffusion models has achieved a great success, generating images of higher resolution than the training size remains a challenging task due to the high computational cost. Current methods typically perform the entire sampling process at full resolution and process all frequency components simultaneously, contradicting with the inherent coarse-to-fine nature of latent diffusion models and wasting computations on processing premature high-frequency details at early diffusion stages. To address this issue, we introduce an efficient $\textbf{Fre}$quency-aware $\textbf{Ca}$scaded $\textbf{S}$ampling framework, $\textbf{FreCaS}$ in short, for higher-resolution image generation. FreCaS decomposes the sampling process into cascaded stages with gradually increased resolutions, progressively expanding frequency bands and refining the corresponding details. We propose an innovative frequency-aware classifier-free guidance (FA-CFG) strategy to assign different guidance strengths for different frequency components, directing the diffusion model to add new details in the expanded frequency domain of each stage. Additionally, we fuse the cross-attention maps of previous and current stages to avoid synthesizing unfaithful layouts. Experiments demonstrate that FreCaS significantly outperforms state-of-the-art methods in image quality and generation speed. In particular, FreCaS is about 2.86$\times$ and 6.07$\times$ faster than ScaleCrafter and DemoFusion in generating a 2048$\times$2048 image using a pretrained SDXL model and achieves an $\text{FID}_b$ improvement of 11.6 and 3.7, respectively. FreCaS can be easily extended to more complex models such as SD3. The source code of FreCaS can be found at https://github.com/xtudbxk/FreCaS.",2025,0.6477270371901683,0.6448667039867912,0.6666666666666666,0.625,e550031c-577c-4643-8a9b-76bc2e610972,1,"[0.5, 0.625, 0.625, 0.625]","[1.0, 0.9, 0.95, 0.95]",FreCaS,0.5841536216195388
546787,MuirBench: A Comprehensive Benchmark for Robust Multi-image Understanding,"We introduce MuirBench, a comprehensive benchmark that focuses on robust multi-image understanding capabilities of multimodal LLMs. MuirBench consists of 12 diverse multi-image tasks (e.g., scene understanding, ordering) that involve 10 categories of multi-image relations (e.g., multiview, temporal relations). Comprising 11,264 images and 2,600 multiple-choice questions, MuirBench is created in a pairwise manner, where each standard instance is paired with an unanswerable variant that has minimal semantic differences, in order for a reliable assessment. Evaluated upon 20 recent multi-modal LLMs, our results reveal that even the best-performing models like GPT-4o and Gemini Pro find it challenging to solve MuirBench, achieving 68.0% and 49.3% in accuracy. Open-source multimodal LLMs trained on single images can hardly generalize to multi-image questions, hovering below 33.3% in accuracy. These results highlight the importance of MuirBench in encouraging the community to develop multimodal LLMs that can look beyond a single image, suggesting potential pathways for future improvements.",2025,0.5727270644628857,0.564830284192023,0.6666666666666666,0.625,134bd5eb-3b79-4d62-8c50-1e889eca41ce,1,"[0.25, 0.5, 0.625, 0.625, 0.625]","[1.0, 1.0, 0.8, 0.9, 1.0]",MuirBench,0.4982805285060647
546788,A Little Goes a Long Way: Efficient Long Context Training and Inference with Partial Contexts,"Training and serving long-context large language models (LLMs) incurs substantial overhead. 
To address this, two critical steps are often required: a pretrained LLM typically undergoes a separate stage for context length extension by training on long-context data, followed by architectural modifications to reduce the overhead of KV cache during serving. 
This paper argues that integrating length extension with a GPU-friendly KV cache reduction architecture not only reduces training overhead during length extension, but also achieves better long-context performance. 
This leads to our proposed LongGen, which finetunes a pretrained LLM into an efficient architecture during length extension. 
LongGen builds on three key insights: 
(1) Sparse attention patterns, such as window attention (attending to recent tokens), attention sink (initial ones), and blockwise sparse attention (strided token blocks) are well-suited for building efficient long-context models, primarily due to their GPU-friendly memory access patterns, enabling efficiency gains not just theoretically but in practice as well. 
(2) It is essential for the model to have direct access to all tokens. 
A hybrid architecture with 1/3 full attention layers and 2/3 efficient ones achieves a balanced trade-off between efficiency and long-context performance.
(3) Lightweight training on 5B long-context data is sufficient to extend the hybrid model's context length from 4K to 128K.

We evaluate LongGen on both Llama-2 7B and Llama-2 70B, demonstrating its effectiveness across different scales. 
During training with 128K-long contexts, LongGen achieves 1.55x training speedup and reduces wall-clock time by 36%, compared to a full-attention baseline. 
During inference, LongGen reduces KV cache memory by 62%, achieving 1.67x prefilling speedup and 1.41x decoding speedup.
Compared to baselines that apply KV-cache reduction techniques to full-attention long-context LLMs, LongGen achieves substantially stronger performance not only on the Needle-in-a-Haystack retrieval task, but also on more challenging long-context reasoning tasks, including BABILong and RULER.",2025,0.7363633685951387,0.7351483692775507,0.6666666666666666,0.625,c4327f22-4527-4c2d-816c-2c2663160678,1,"[0.625, 0.625, 0.625, 0.625, 0.875]","[0.95, 0.95, 0.95, 0.95, 0.95]",LongGen,0.6749999999999999
546800,Since Faithfulness Fails: The Performance Limits of Neural Causal Discovery,"Neural causal discovery methods have recently improved in terms of scalability and computational efficiency.
However, there are still opportunities for improving their accuracy in uncovering causal structures.
We argue that the key obstacle in unlocking this potential is the faithfulness assumption, commonly used by contemporary neural approaches. We show that this assumption, which is often not satisfied in real-world or synthetic datasets, limits the effectiveness of existing methods. We evaluate
the impact of
faithfulness violations both qualitatively and quantitatively and provide a unified evaluation framework to facilitate further research.",2025,0.613636140495949,0.6135559181077389,0.6,0.5,be0f75c1-f5f9-401b-b14a-e02adbb424c3,0,"[0.5, 0.5, 0.625, 0.625]","[0.9, 0.9, 0.9, 0.95]",Faithfulness Assumption,0.5667912513842747
546804,BounDr.E: Predicting Drug-likeness through knowledge alignment and EM-like one-class boundary optimization,"The advent of generative AI models is revolutionizing drug discovery, generating de novo molecules at unprecedented speed. However, accurately identifying and rescuing drug candidates among countless generated molecules remains an open problem.
The essence of this drug-likeness prediction task lies in constructing a compact subspace that encompasses majority of approved drugs with only a small number of unknown compounds (drug candidates) inside.
Computational challenges arises in constructing a decision boundary on an unbound chemical space that lacks definite negatives, i.e, non drug-likeness.
Approved drugs exist highly dispersed across structural space, making it more harsh to effectively separate drugs from non-drugs through existing classifiers. 
Addressing such challenges, we introduce BounDr.E: a novel approach for learning a compact boundary of drug-likeness through an Expectation-Maximization (EM)-like iterative optimization process. 
Specifically, we refine both the boundary and the distribution of the embedding space via metric learning, allowing the model to iteratively tighten the drug-like boundary while pushing non-drug-like compounds outside.
Augmented by integration of biomedical context within knowledge graphs via multi-modal alignment, our model demonstrates 10% increase in F1 score over the previous state-of-the-art, along with strongest robustness to cross-dataset validation.
Zero-shot toxic compound filtering and comprehensive drug discovery pipeline case studies further showcases its utility in large-scale screening of AI-generated compounds. 
To facilitate in silico drug discovery, we provide the code and benchmark data under various splitting schemes at: https://anonymous.4open.science/r/boundr_e.",2025,0.4636361950413836,0.4616873343193397,0.5333333333333333,0.25,d06fbe99-dd7f-4503-a9c0-115fec8b5b15,0,"[0.25, 0.25, 0.5, 0.5, 0.625]","[0.95, 0.9, 0.95, 0.9, 0.9]",BounDr.E,0.4197245266964476
546819,Invariant Spatiotemporal Representation Learning for Cross-patient Seizure Classification,"Automatic seizure type classification from electroencephalogram (EEG) data can help clinicians to better diagnose epilepsy. Although many previous studies have focused on the classification problem of seizure EEG data, most of these methods require that there is no distribution shift between training data and test data, which greatly limits the applicability in real-world scenarios. In this paper, we propose an invariant spatiotemporal representation learning method for cross-patient seizure classification. Specifically, we first split the spatiotemporal EEG data into different environments based on heterogeneous risk minimization to reflect the spurious correlations. We then learn invariant spatiotemporal representations and train the seizure classification model based on the learned representations to achieve accurate seizure-type classification across various environments. The experiments are conducted on the largest public EEG dataset, the Temple University Hospital Seizure Corpus (TUSZ) dataset, and the experimental results demonstrate the effectiveness of our method.",2025,0.2727271735537551,0.2687865804383009,0.2666666666666666,0.25,2e862747-708b-4fa2-b8a5-825b584a4c1c,0,"[0.0, 0.25, 0.25, 0.5]","[1.0, 0.95, 1.0, 0.95]",Invariant Spatiotemporal Representation,0.2315825375170531
546820,"DeepCircuitX: Repository-Level RTL Dataset for Code Understanding, Generation, and Multimodal Analysis","This paper introduces DeepCircuitX, a comprehensive multimodal dataset designed to advance RTL code understanding, generation, and completion tasks in hardware design automation. Unlike existing datasets, which focus either on file-level RTL code or downstream netlist and layout data, DeepCircuitX spans repository, file, module, and block-level RTL code, providing a more holistic resource for training and evaluating large language models (LLMs). The dataset is enriched with Chain of Thought (CoT) annotations that offer detailed functionality and structure descriptions at multiple levels, enhancing its utility for RTL code understanding, generation, and completion.

In addition to RTL data, DeepCircuitX includes synthesized netlists and power-performance-area (PPA) metrics, allowing for early-stage design exploration and PPA prediction directly from RTL code. We establish comprehensive benchmarks for RTL code understanding, generation, and completion using open-source models such as CodeLlama, CodeT5+, and CodeGen, demonstrating substantial improvements in task performance. Furthermore, we introduce and evaluate models for PPA prediction, setting new benchmarks for RTL-to-PPA analysis. We conduct human evaluations and reviews to confirm the high quality and functionality of the generated RTL code and annotations. Our experimental results show that DeepCircuitX significantly improves model performance across multiple benchmarks, underscoring its value as a critical resource for advancing RTL code tasks in hardware design automation.",2025,0.3409089669421938,0.341207952236049,0.2666666666666666,0.25,8dda5d96-2fde-44d6-93dd-a2b506fd726b,0,"[0.25, 0.25, 0.25, 0.5]","[1.0, 0.95, 1.0, 1.0]",DeepCircuitX,0.3167884371029225
546822,Reinforced In-Context Black-Box Optimization,"Black-Box Optimization (BBO) has found successful applications in many fields of science and engineering. Recently, there has been a growing interest in meta-learning particular components of BBO algorithms to speed up optimization and get rid of tedious hand-crafted heuristics. As an extension, learning the entire algorithm from data requires the least labor from experts and can provide the most flexibility. In this paper, we propose RIBBO, a method to reinforce-learn a BBO algorithm from offline data in an end-to-end fashion. RIBBO employs expressive sequence models to learn the optimization histories produced by multiple behavior algorithms and tasks, leveraging the in-context learning ability of large models to extract task information and make decisions accordingly. Central to our method is to augment the optimization histories with *regret-to-go* tokens, which are designed to represent the performance of an algorithm based on cumulative regret over the future part of the histories. The integration of regret-to-go tokens enables RIBBO to automatically generate sequences of query points that satisfy the user-desired regret, which is verified by its universally good empirical performance on diverse problems, including BBO benchmark functions, hyper-parameter optimization and robot control problems.",2025,0.613636140495949,0.617286115388888,0.6,0.25,3bc3180a-fb83-403c-bfc6-5547f92ed502,0,"[0.25, 0.25, 0.875, 0.875]","[0.9, 0.9, 0.95, 0.9]",RIBBO,0.5839562569213731
546823,The Cognitive Capabilities of Generative AI: A Comparative Analysis with Human Benchmarks,"There is increasing interest in tracking the capabilities of general intelligence foundation models. This study benchmarks leading large language models (LLMs) and vision language models (VLMs) against human performance on the Wechsler Adult Intelligence Scale (WAIS-IV), a comprehensive, population-normed assessment of underlying human cognition and intellectual abilities, with a focus on the domains of Verbal Comprehension (VCI), Working Memory (WMI), and Perceptual Reasoning (PRI). Most models demonstrated exceptional capabilities in the storage, retrieval, and manipulation of tokens such as arbitrary sequences of letters and numbers, with performance on the Working Memory Index (WMI) greater or equal to the 99.5th percentile when compared to human population normative ability. Performance on the Verbal Comprehension Index (VCI) which measures retrieval of acquired information, and linguistic understanding about the meaning of words and their relationships to each other, also demonstrated consistent performance at or above the 98th percentile. Despite these broad strengths, we observed consistently poor performance on the Perceptual Reasoning Index (PRI; range 0.1-10th percentile) from multimodal models indicating profound inability to interpret and reason on visual information.
Some more nuanced differences in performance were also observed. Models were consistently stronger on the WMI compared to the VCI, indicating stronger capabilities in storage, manipulation, and retrieval of data than language understanding. Smaller and older model versions consistently performed worse, indicating that training data, parameter count, and advances in tuning are resulting in significant advances in cognitive ability.",2025,0.4090907603306326,0.4084157607097504,0.4,0.25,732aa2ca-2716-4a0f-82af-57b8e9eeac45,0,"[0.25, 0.25, 0.5, 0.5]","[0.95, 0.9, 0.9, 0.95]",Perceptual Reasoning,0.375
546826,State Space Model Meets Transformer: A New Paradigm for 3D Object Detection,"DETR-based methods, which use multi-layer transformer decoders to refine object queries iteratively, have shown promising performance in 3D indoor object detection. However, the scene point features in the transformer decoder remain fixed, leading to minimal contributions from later decoder layers, thereby limiting performance improvement. Recently, State Space Models (SSM) have shown efficient context modeling ability with linear complexity through iterative interactions between system states and inputs. Inspired by SSMs, we propose a new 3D object DEtection paradigm with an interactive STate space model (DEST). In the interactive SSM, we design a novel state-dependent SSM parameterization method that enables system states to effectively serve as queries in 3D indoor detection tasks. In addition, we introduce four key designs tailored to the characteristics of point cloud and SSM: The serialization and bidirectional scanning strategies enable bidirectional feature interaction among scene points within the SSM. The inter-state attention mechanism models the relationships between state points, while the gated feed-forward network enhances inter-channel correlations. To the best of our knowledge, this is the first method to model queries as system states and scene points as system inputs, which can simultaneously update scene point features and query features with linear complexity. Extensive experiments on two challenging datasets demonstrate the effectiveness of our DEST-based method. Our method improves the GroupFree baseline in terms of $\text{AP}_{50}$ on ScanNet V2 (+5.3) and SUN RGB-D (+3.2) datasets. Based on the VDETR baseline, Our method sets a new state-of-the-art on the ScanNetV2 and SUN RGB-D datasets.",2025,0.8181815206612653,0.8206129068093254,0.8,0.625,519a486a-751e-472b-ab13-5f2b2a04fbbe,1,"[0.625, 0.625, 0.875, 0.875]","[0.8, 0.95, 0.9, 0.95]",DEST,0.760388282810199
546834,MADCluster: Model-agnostic Anomaly Detection with Self-supervised Clustering Network,"In this paper, we propose MADCluster, a novel model-agnostic anomaly detection framework utilizing self-supervised clustering. MADCluster is applicable to various deep learning architectures and addresses the 'hypersphere collapse' problem inherent in existing deep learning-based anomaly detection methods. The core idea is to cluster normal pattern data into a `single cluster' while simultaneously learning the cluster center and mapping data close to this center. Also, to improve expressiveness and enable effective single clustering, we propose a new 'One-directed Adaptive loss'. The optimization of this loss is mathematically proven. MADCluster consists of three main components: Base Embedder capturing high-dimensional temporal dynamics, Cluster Distance Mapping, and Sequence-wise Clustering for continuous center updates. Its model-agnostic characteristics are achieved by applying various architectures to the Base Embedder. Experiments on four time series benchmark datasets demonstrate that applying MADCluster improves the overall performance of comparative models. In conclusion, the compatibility of MADCluster shows potential for enhancing model performance across various architectures.",2025,0.5181816297521347,0.5149589079289292,0.5333333333333333,0.5,a588bc37-9b00-4d96-9e84-79aaeba74f97,0,"[0.25, 0.5, 0.5, 0.5, 0.625]","[0.95, 0.9, 0.95, 0.9, 0.9]",MADCluster,0.4644490533928951
546835,MMAU: A Massive Multi-Task Audio Understanding and Reasoning Benchmark,"The ability to comprehend audio—which includes speech, non-speech sounds, and music—is crucial for AI agents to interact effectively with the world. We present MMAU, a novel benchmark designed to evaluate multimodal audio understanding models on tasks requiring expert-level knowledge and complex reasoning. MMAU comprises 10k carefully curated audio clips paired with human-annotated natural language questions and answers spanning speech, environmental sounds, and music. It includes information extraction and reasoning questions, requiring models to demonstrate 27 distinct skills across unique and challenging tasks. Unlike existing benchmarks, MMAU emphasizes advanced perception and reasoning with domain-specific knowledge, challenging models to tackle tasks akin to those faced by experts. We assess 18 open-source and proprietary (Large) Audio-Language Models, demonstrating the significant challenges
posed by MMAU. Notably, even the most advanced Gemini 2.0 Flash achieves only 59.93% accuracy, and the state-of-the-art open-source Qwen2-Audio achieves only 52.50%, highlighting considerable room for improvement. We believe MMAU will drive the audio and multimodal research community to develop more advanced audio understanding models capable of solving complex audio tasks.",2025,0.886363314049704,0.886646656109573,0.9333333333333332,0.875,1be2945a-ede1-4b79-9b1c-58242cc7f5ac,1,"[0.625, 0.875, 0.875, 0.875]","[0.95, 1.0, 1.0, 0.95]",MMAU,0.8217087312390994
546853,Towards Better Multi-head Attention via Channel-wise Sample Permutation,"Transformer plays a central role in many fundamental deep learning models, e.g., the ViT in computer vision and the BERT and GPT in natural language processing, whose effectiveness is mainly attributed to its multi-head attention (MHA) mechanism. 
In this study, we propose a simple and novel channel-wise sample permutation (CSP) operator, achieving a new structured MHA with fewer parameters and lower complexity. 
Given an input matrix, CSP circularly shifts the samples of different channels with various steps and then sorts grouped samples of each channel. 
This operator is equivalent to implicitly implementing cross-channel attention maps as permutation matrices, which achieves linear complexity and suppresses the risk of rank collapse when representing data. 
We replace the MHA of some representative models with CSP and test the CSP-based models in several discriminative tasks, including image classification and long sequence analysis. 
Experiments show that the CSP-based models achieve comparable or better performance with fewer parameters and lower computational costs than the classic Transformer and its state-of-the-art variants. 
The code is available at https://anonymous.4open.science/r/CSP-BA52.",2025,0.443181657024852,0.4301195187882141,0.4,0.25,c7abe93b-a296-4242-ae72-06ddc2a8c7f8,0,"[0.25, 0.25, 0.5, 0.625]","[0.95, 0.9, 0.8, 0.8]",Channel-wise Sample Permutation,0.3627664435657172
546860,"Neural Manifold Regularization: Aligning 2D Latent Dynamics with Stereotyped, Natural, and Attempted Movements","Mapping neural activity to behavior is a fundamental goal in both neuroscience and brain-machine interfaces. Traditionally, at least three-dimensional (3D) latent dynamics have been required to represent two-dimensional (2D) movement trajectories. In this work, we introduce Neural Manifold Regularization (NMR), a method that embeds neural dynamics into a 2D latent space and regularizes the manifold based on the distances and densities of continuous movement labels. NMR pulls together positive pairs of neural embeddings (corresponding to closer labels) and pushes apart negative pairs (representing more distant labels). Additionally, NMR applies greater force to infrequent labels to prevent them from collapsing into dominant labels.
We evaluated NMR across four modalities of neural signals and three types of movements. When combined with a linear regression decoder, NMR outperformed other dimensionality reduction methods by over 50\% across 68 sessions. The highly consistent neural manifolds extracted by NMR enable robust motor decoding across sessions, years, and subjects using a simple linear regression decoder.
Our code is uploaded.",2025,0.613636140495949,0.6135308686077487,0.6,0.5,84f5f300-422c-4328-a4fe-46c47fcbdba6,0,"[0.5, 0.5, 0.625, 0.625]","[0.95, 0.9, 0.95, 0.95]",Neural Manifold Regularization,0.5662731256085687
546873,Controllable Continual Test-Time Adaptation,"Continual Test-Time Adaptation (CTTA) is an emerging and challenging task where a model trained in a source domain must adapt to continuously changing conditions during testing, without access to the original source data. CTTA is prone to error accumulation due to uncontrollable domain shifts, leading to blurred decision boundaries between categories. Existing CTTA methods primarily focus on suppressing domain shifts, which proves inadequate during the unsupervised test phase.
In contrast, we introduce a novel approach that guides rather than suppresses these shifts.
Specifically, we propose $\textbf{C}$ontrollable $\textbf{Co}$ntinual $\textbf{T}$est-$\textbf{T}$ime $\textbf{A}$daptation (C-CoTTA), which explicitly prevents any single category from encroaching on others, thereby mitigating the mutual influence between categories caused by uncontrollable shifts. 
Moreover, our method reduces the sensitivity of model to domain transformations, thereby minimizing the magnitude of category shifts. 
Extensive quantitative experiments demonstrate the effectiveness of our method, while qualitative analyses, such as t-SNE plots, confirm the theoretical validity of our approach.",2025,0.443181657024852,0.4370767451333178,0.4,0.25,14c79672-0112-474e-b454-9ff0c21fdbf0,0,"[0.25, 0.25, 0.5, 0.625]","[0.95, 1.0, 0.95, 0.9]",C-CoTTA,0.3810388125980297
546880,Feint and Attack: Attention-Based Strategies for Jailbreaking and Protecting LLMs,"Jailbreak attack can be used to access the vulnerabilities of Large Language Models (LLMs) by inducing LLMs to generate the harmful content. 
And the most common method of the attack is to construct semantically ambiguous prompts to confuse and mislead the LLMs.
To access the security and reveal the intrinsic relation between the input prompt and the output for LLMs, the distribution of attention weight is introduced to analyze the underlying reasons. 
By using statistical analysis methods, some novel metrics are defined to better describe the distribution of attention weight, such as the Attention Intensity on Sensitive Words (Attn_SensWords), the Attention-based Contextual Dependency Score (Attn_DepScore) and Attention Dispersion Entropy (Attn_Entropy).
By leveraging the distinct characteristics of these metrics, the beam search algorithm and inspired by the military strategy ""Feint and Attack'', an effective jailbreak attack strategy named as Attention-Based Attack (ABA) is proposed.
In the ABA, nested attack prompts are employed to divert the attention distribution of the LLMs. 
In this manner, more harmless parts of the input can be used to attract the attention of the LLMs.
In addition, motivated by ABA, an effective defense strategy called as Attention-Based Defense (ABD) is also put forward.
Compared with ABA, the ABD can be used to enhance the robustness of LLMs by calibrating the attention distribution of the input prompt. 
Some comparative experiments have been given to demonstrate the effectiveness of ABA and ABD. 
Therefore, both ABA and ABD can be used to access the security of the LLMs. 
The comparative experiment results also give a logical explanation that the distribution of attention weight can bring great influence on the output for LLMs.",2025,0.4090907603306326,0.4101833841221021,0.4,0.25,c1cd8670-7c51-4484-8cef-49c795c5349c,0,"[0.25, 0.25, 0.5, 0.5]","[0.95, 0.95, 1.0, 0.95]",Attention-Based Attack,0.3849410898379971
546891,INDUCTIVE GRADIENT ADJUSTMENT FOR SPECTRAL BIAS IN IMPLICIT NEURAL REPRESENTATIONS,"Implicit Neural Representations (INRs) as a versatile representation paradigm have achieved success in various computer vision tasks. Due to the spectral bias of the vanilla multi-layer perceptrons (MLPs), existing methods focus on designing MLPs with sophisticated architectures or repurposing existing training techniques for highly accurate INRs. In this paper, we delve into the linear dynamics model of MLPs and theoretically identify the empirical Neural Tangent Kernel (eNTK) matrix as a reliable link between spectral bias and training dynamics. Based on eNTK matrix, we propose a practical inductive gradient adjustment method,  which could purposefully improve the spectral bias via inductive generalization of eNTK-based gradient transformation matrix. We evaluate our method on different INRs tasks with various INR architectures and compare to existing training techniques. The superiority representation performance clearly validate the advantage of our proposed method. Armed with our gradient adjustment method, better INRs with more enhanced texture details and sharpened edges can be learned from the training data by tailored improvements on spectral bias.",2025,0.5113634504132908,0.5234626686212538,0.4,0.25,90996acf-e94b-4791-b19e-d78967cb00af,0,"[0.25, 0.25, 0.5, 0.875]","[0.9, 0.8, 0.9, 0.95]",Inductive Gradient Adjustment,0.5186737966235488
546900,Aligning Anything: Hierarchical Motion Estimation for Video Frame Interpolation,"Existing advanced video frame interpolation (VFI) methods struggle to learn accurate per-pixel motion or target-level motion. The reasons lie in that pixel-level motion estimation allows for infinite possibilities, making it challenging to guarantee fitting accuracy and global motion consistency, especially for rigid objects. Conversely, target-level motion consistency from the same moving target also breaks
down when the assumption of object rigidity no longer holds. Therefore, a hierarchical motion learn scheme is imperative to promote the accuracy and stability of motion prediction. Specifically, we marry the target-level motion to the pixel-level motion to form the hierarchical motion estimation. It elaborately introduces specific semantics priors from open-world knowledge models such as the Recognize Anything Model (RAM), Grounding DIDO, and the High-Quality Segment Anything Model (HQ-SAM) to facilitate the latent target-level motion learning. In particular, a hybrid contextual feature extraction module (HCE) is employed to aggregate both pixel-wise and semantic representations, followed by the hierarchical motion and feature interactive refinement module (HIR) to simulate the current motion patterns. When integrating these adaptions to existing SOTA VFI methods, more consistent motion estimation and interpolation are predicted. Extensive experiments show that advanced VFI networks plugged with our adaptions can achieve more superior performances on various benchmark datasets",2025,0.4090907603306326,0.4140105120771263,0.4,0.25,58e73aae-0c13-4f1c-8392-3660d488aa34,0,"[0.25, 0.25, 0.5, 0.5]","[0.8, 0.95, 0.9, 1.0]",Hierarchical Motion Estimation,0.3963357667237954
546930,MUSE: Machine Unlearning Six-Way Evaluation for Language Models,"Language models (LMs) are trained on vast amounts of text data, which may include private and copyrighted content. Data owners may request the removal of their data from a trained model due to privacy or copyright concerns. However, exactly unlearning only these datapoints (i.e., retraining with the data removed) is intractable in modern-day models. This has led to the development of many approximate unlearning algorithms. The evaluation of the efficacy of these algorithms has traditionally been narrow in scope, failing to precisely quantify the success and practicality of the algorithm from the perspectives of both the model deployers and the data owners. We address this issue by proposing MUSE, a comprehensive machine unlearning evaluation benchmark that enumerates six diverse desirable properties for unlearned models: (1) no verbatim memorization, (2) no knowledge memorization, (3) no privacy leakage, (4) utility preservation on data not intended for removal, (5) scalability with respect to the size of removal requests, and (6) sustainability over sequential unlearning requests. Using these criteria, we benchmark how effectively eight popular unlearning algorithms on 7B-parameter LMs can unlearn Harry Potter books and news articles. Our results demonstrate that most algorithms can prevent verbatim memorization and knowledge memorization to varying degrees, but only one algorithm does not lead to severe privacy leakage. Furthermore, existing algorithms fail to meet deployer's expectations because they often degrade general model utility and also cannot sustainably accommodate successive unlearning requests or large-scale content removal. Our findings identify key issues with the practicality of existing unlearning algorithms on language models.",2025,0.7090906512397632,0.710288374200322,0.6666666666666666,0.625,942dfdf7-b0f2-41b0-9910-4e8c9ce1d71d,1,"[0.5, 0.625, 0.625, 0.625, 0.875]","[0.9, 0.95, 0.9, 0.9, 0.95]",MUSE,0.660550946607105
546937,Retrieval Augmented Zero-Shot Enzyme Generation for Specified Substrate,"The ability to generate novel enzymes that catalyze specific target molecules is a critical advancement in biomaterial synthesis and chemical production. However, a significant challenge arises when no recorded enzymes exist for the target molecule, making it a zero-shot generation problem. This absence of known enzymes complicates the training of generative models tailored to the target substrate. To address this, we propose a retrieval-augmented generation method that leverages existing enzyme-substrate data to overcome the lack of direct examples. Since there is no recorded catalytic performance between the enzymes and the new target molecule, the challenge shifts to identifying enzymes that helpful for generation. Our approach tackles this by retrieving enzymes whose substrates exhibit structural similarities to the target molecule, thereby exploiting functional similarities reflected in the enzymes' catalytic capability. This leads to the next challenge: how to utilize the retrieved enzymes to generate a novel enzyme capable of catalyzing the target molecule, given that none of the retrieved enzymes directly catalyze it. To solve this, we employ a conditioned discrete diffusion model that takes the aligned retrieved enzymes to generate a new enzyme. We train the generator with guidance from an enzyme-substrate relationship classifier to make it output the optimal protein sequence distribution for different target molecule. We evaluate our model on enzyme design tasks involving a diverse set of real-world substrates, and our results including catalytic rate predictions, foldability assessments, and docking position analyses, demonstrate that our model outperforms existing protein generation methods for substrate-specified enzyme generation. Additionally, we formally define the zero-shot substrate-specified enzyme generation task and contribute a comprehensive dataset with evaluation methods.",2025,0.613636140495949,0.6108320572609788,0.6,0.5,c60315ae-a6f1-4499-a308-86cb27eef139,0,"[0.5, 0.5, 0.625, 0.625]","[0.95, 1.0, 0.95, 0.9]",Enzyme Generation,0.5537981085543922
546963,Deep Learning Alternatives Of The Kolmogorov Superposition Theorem,"This paper explores alternative formulations of the Kolmogorov Superposition Theorem (KST) as a foundation for neural network design. The original KST formulation, while mathematically elegant, presents practical challenges due to its limited insight into the structure of inner and outer functions and the large number of unknown variables it introduces. Kolmogorov-Arnold Networks (KANs) leverage KST for function approximation, but they have faced scrutiny due to mixed results compared to traditional multilayer perceptrons (MLPs) and practical limitations imposed by the original KST formulation. To address these issues, we introduce ActNet, a scalable deep learning model that builds on the KST and overcomes some of the drawbacks of Kolmogorov's original formulation. We evaluate ActNet in the context of Physics-Informed Neural Networks (PINNs), a framework well-suited for leveraging KST's strengths in low-dimensional function approximation, particularly for simulating partial differential equations (PDEs). In this challenging setting, where models must learn latent functions without direct measurements, ActNet consistently outperforms KANs across multiple benchmarks and is competitive against the current best MLP-based approaches. These results present ActNet as a promising new direction for KST-based deep learning applications, particularly in scientific computing and PDE simulation tasks.",2025,0.886363314049704,0.8849008148711258,0.9333333333333332,0.875,795fbfd5-6b03-47e4-b575-dd9600cbd007,1,"[0.625, 0.875, 0.875, 0.875]","[0.95, 0.9, 1.0, 0.95]",ActNet,0.814288973862384
546966,R$^2$: A LLM Based Novel-to-Screenplay Generation Framework with Causal Plot Graphs,"Automatically adapting novels into screenplays is important for the TV, film, or opera industries to promote products with low costs. The strong performances of large language models (LLMs) in long-text generation call us to propose a LLM based framework Reader-Rewriter (R$^2$) for this task. However, there are two fundamental challenges here. First, the LLM hallucinations may cause inconsistent plot extraction and screenplay generation. Second, the causality-embedded plot lines should be effectively extracted for coherent rewriting. Therefore, two corresponding tactics are proposed: 1) A hallucination-aware refinement method (HAR) to iteratively discover and eliminate the affections of hallucinations; and 2) a causal plot-graph construction method (CPC) based on a greedy cycle-breaking algorithm to efficiently construct plot lines with event causalities. Recruiting those efficient techniques, R$^2$ utilizes two modules to mimic the human screenplay rewriting process: The Reader module adopts a sliding window and CPC to build the causal plot graphs, while the Rewriter module generates first the scene outlines based on the graphs and then the screenplays. HAR is integrated into both modules for accurate inferences of LLMs. Experimental results demonstrate the superiority of R$^2$, which substantially outperforms three existing approaches (51.3\%, 22.6\%, and 57.1\% absolute increases) in pairwise comparison at the overall win rate for GPT-4o.",2025,0.2727271735537551,0.2722771738065002,0.2666666666666666,0.25,f27d3724-89ca-4e4d-9758-81aa574f6b99,0,"[0.25, 0.25, 0.25]","[0.95, 0.95, 0.95]",Causal Plot Graphs,0.25000000000001
546967,MoRA: High-Rank Updating for Parameter-Efficient Fine-Tuning,"Low-rank adaptation (LoRA) is a popular parameter-efficient fine-tuning (PEFT) method for large language models (LLMs). In this paper, we analyze the impact of low-rank updating, as implemented in LoRA. Our findings suggest that the low-rank updating mechanism may limit the ability of LLMs to effectively learn and memorize new knowledge. Inspired by this observation, we propose a new method called MoRA, which employs a square matrix to achieve high-rank updating while maintaining the same number of trainable parameters. To achieve it, we introduce the corresponding non-parameter operators to reduce the input dimension and increase the output dimension for the square matrix. Furthermore, these operators ensure that the weight can be merged back into LLMs, which enables our method to be deployed like LoRA. We perform a comprehensive evaluation of our method across five tasks: instruction tuning, mathematical reasoning, continual pretraining, memory and pretraining. Our method outperforms LoRA on memory-intensive tasks and achieves comparable performance on other tasks.",2025,0.5113634504132908,0.5136966309511622,0.4,0.25,75d20a88-0652-4373-b934-9ca1b5631802,0,"[0.25, 0.25, 0.5, 0.875]","[0.95, 0.9, 0.95, 0.95]",MoRA,0.4819559396299903
547000,Active Learning of Deep Neural Networks via Gradient-Free Cutting Planes,"Active learning methods aim to improve sample complexity in machine learning. In this work, we investigate an active learning scheme via a novel gradient-free cutting-plane training method for ReLU networks of arbitrary depth. 
We demonstrate, for the first time, that cutting-plane algorithms, traditionally used in linear models, can be extended to deep neural networks despite their nonconvexity and nonlinear decision boundaries. Our results demonstrate that these methods provide a promising alternative to the commonly employed gradient-based optimization techniques in large-scale neural networks. 
Moreover, this training method induces the first deep active learning scheme known to achieve convergence guarantees. We exemplify the effectiveness of our proposed active learning method against popular deep active learning baselines via both synthetic data experiments and sentimental classification task on real datasets.",2025,0.3818180429752571,0.3751369554184246,0.2666666666666666,0.25,4ea9ef77-a39b-4e5e-bed4-047790493a1d,0,"[0.25, 0.25, 0.25, 0.5, 0.5]","[0.9, 0.95, 0.95, 0.9, 0.8]",Cutting-Plane Training,0.3286537635378659
547014,Chimera: State Space Models Beyond Sequences,"Powerful deep learning methods based on Transformers are used to model diverse data modalities such as sequences, images, and graphs. 
These methods typically use off-the-shelf modules like self-attention, which are domain-agnostic and treat data as an unordered set of elements.
To improve performance, researchers employ inductive biases—such as position embeddings in sequences and images, and random walks in graphs—to inject the domain structure, or *topology*, into the model.
However, these inductive biases are carefully engineered heuristics that must be designed for each modality, requiring significant research effort.
In this work, we propose *Chimera*, a unified framework that mathematically generalizes state space models to incorporate the topological structure of data in a principled way.
We demonstrate that our method achieves state-of-the-art performance across domains including language, vision, and graphs. Chimera outperforms BERT on the GLUE benchmark by 0.7 points, surpasses ViT by 2.6% on ImageNet-1k classification accuracy, and outperforms all baselines on the Long Range Graph Benchmark with a 12% improvement on PascalVOC.
This validates Chimera's methodological improvement, which allows it to directly capture the underlying topology, providing a strong inductive bias across modalities.
Furthermore, being topologically aware enables our method to achieve a linear time complexity for sequences and images, in contrast to the quadratic complexity of attention.",2025,0.6477270371901683,0.6457379909429721,0.6666666666666666,0.625,f2afba84-524f-4e5f-9f95-cf701ad92489,0,"[0.5, 0.625, 0.625, 0.625]","[0.95, 0.95, 0.9, 0.9]",Chimera,0.5897344559585493
547032,"Inference, Fast and Slow: Reinterpreting VAEs for OOD Detection","lthough likelihood-based methods are theoretically appealing, deep generative models (DGMs) often produce unreliable likelihood estimates in practice, particu larly for out-of-distribution (OOD) detection. We reinterpret variational autoen coders (VAEs) through the lens of fast and slow weights. Our approach is guided by the proposed Likelihood Path (LPath) Principle, which extends the classical likelihood principle. A critical decision in our method is the selection of statistics for classical density estimation algorithms. The sweet spot should contain just enough information that’s sufficient for OOD detection but not too much to suffer from the curse of dimensionality. Our LPath principle achieves this by selecting the sufficient statistics that form the ""path"" toward the likelihood. We demonstrate that this likelihood path leads to SOTA OOD detection performance, even when the likelihood itself is unreliable.",2025,0.4999994545456529,0.4900989128517004,0.5333333333333333,0.25,24ab459d-4172-47ab-b39b-cea4b428583d,0,"[0.25, 0.5, 0.625]","[0.9, 0.8, 0.8]",Likelihood Path,0.4263734128012437
547037,Release the Powers of Prompt Tuning: Cross-Modality Prompt Transfer,"Prompt Tuning adapts frozen models to new tasks by prepending a few learnable embeddings to the input.
However, it struggles with tasks that suffer from data scarcity.
To address this, we explore Cross-Modality Prompt Transfer, leveraging prompts pretrained on a data-rich modality to improve performance on data-scarce tasks in another modality.
As a pioneering study, we first verify the feasibility of cross-modality prompt transfer by directly applying frozen source prompts (trained on the source modality) to the target modality task.
To empirically study cross-modality prompt transferability, we train a linear layer to adapt source prompts to the target modality, thereby boosting performance and providing ground-truth transfer results.
Regarding estimating prompt transferability, existing methods show ineffectiveness in cross-modality scenarios where the gap between source and target tasks is larger.
We address this by decomposing the gap into the modality gap and the task gap, which we measure separately to autonomously select the best source prompt for a target task.
Additionally, we propose Attention Transfer to further reduce the gaps by injecting target knowledge into the prompt and reorganizing a top-transferable source prompt using an attention block.
We conduct extensive experiments involving prompt transfer from 13 source language tasks to 19 target vision tasks under three settings.
Our findings demonstrate that:
(i) cross-modality prompt transfer is feasible, supported by in-depth analysis;
(ii) measuring both the modality and task gaps is crucial for accurate prompt transferability estimation, a factor overlooked by previous studies;
(iii) cross-modality prompt transfer can significantly release the powers of prompt tuning on data-scarce tasks, as evidenced by comparisons with a newly released prompt-based benchmark.",2025,0.7499997272728265,0.7487622279678756,0.6666666666666666,0.625,3c6d3b95-5215-4fd8-93ae-1ba931caacd6,1,"[0.625, 0.625, 0.625, 0.875]","[0.95, 1.0, 0.9, 0.95]",Cross-Modality Prompt Transfer,0.6857110261302932
547072,Value-Incentivized Preference Optimization: A Unified Approach to Online and Offline RLHF,"Reinforcement learning from human feedback (RLHF) has demonstrated great promise in aligning large language models (LLMs) with human preference. Depending on the availability of preference data, both online and offline RLHF are active areas of investigation. A key bottleneck is understanding how to incorporate uncertainty estimation in the reward function learned from the preference data for RLHF, regardless of how the preference data is collected. While the principles of optimism or pessimism under uncertainty are well-established in standard reinforcement learning (RL), a practically-implementable and theoretically-grounded form amenable to large language models is not yet available, as standard techniques for constructing confidence intervals become intractable under arbitrary policy parameterizations.

In this paper, we introduce a unified approach to online and offline RLHF --- value-incentivized preference optimization (VPO) --- which regularizes the maximum-likelihood estimate of the reward function with the corresponding value function, modulated by a sign to indicate whether the optimism or pessimism is chosen. VPO also directly optimizes the policy with implicit reward modeling, and therefore shares a simpler RLHF pipeline similar to direct preference optimization. Theoretical guarantees of VPO are provided for both online and offline settings, matching the rates of their standard RL counterparts. Moreover, experiments on text summarization, dialogue, and standard benchmarks verify the practicality and effectiveness of VPO.",2025,0.613636140495949,0.6144631456508624,0.6,0.5,4c56c1c6-ca7b-4375-82b0-5a662a2fa778,1,"[0.5, 0.5, 0.625, 0.625]","[0.9, 0.9, 0.95, 0.95]",Value-Incentivized Preference Optimization,0.5705310880829015
547080,LoGra-Med: Long-Context Multi-Graph Alignment for Medical Visual-Language Models,"State-of-the-art medical multi-modal large language models (med-MLLM), such as LLAVA-MED or BIOMEDGPT, leverage instruction-following data in their pre-training stages. However, those models primarily focus on scaling the model size and data volume to boost performance while mainly relying on the autoregressive learning objectives. Surprisingly, we reveal that such learning schemes might result in a weak alignment between vision and language modalities, making these models highly reliant on extensive pre-training datasets — a significant challenge in medical domains due to the expensive and time-consuming nature of curating high-quality instruction-following instances. We address this challenge with a new multi-graph alignment algorithm, namely LOGRA-MED, which enforces triplet correlations on the latent embedding space among image modalities, conversation-based descriptions, and extended contextual captions. Owing to this technique, the model is encouraged to capture the semantic meaning of the context, handle linguistic variability where the captions or questions may differ from training instances, and learn cross-modal associations, linking visual elements with various textual interpretations. To scale our algorithm to the med-MLLM setting, we also design an efficient end-to-end learning scheme based on advanced black-box gradient-estimation techniques that permit fast forward and backward steps through the LLM model (LLaMa 7B). Empirical results show
that we can match the performance of LLAVA-Med pre-trained on 600K image-text pairs from PMC-15M for Medical VQA tasks and significantly outperform it when trained on only 10% of the data. For instance, on VQA-RAD, we exceed LLAVA-Med (both trained on 10%) by 20.13% and achieve near parity with the 100% pre-training setting (72.52% vs. 72.64%). Additionally, we also surpass other SOTA pre-training methods and med-MLLM such as BIOMEDGPT on visual chatbot or RADFM on zero-shot image classification with VQA, showcasing the power of multi-graph alignment in improving vision-language integration for medical-MLLM.",2025,0.5795452438017296,0.5785889943388131,0.5333333333333333,0.5,c1cd8a83-dd7e-4923-802f-b624446b7eb0,0,"[0.5, 0.5, 0.5, 0.625]","[0.95, 0.8, 0.95, 0.9]",Multi-Graph Alignment,0.5294978098039856
547098,Towards Understanding Text Hallucination of Diffusion Models via Local Generation Bias,"Score-based diffusion models have achieved incredible performance in generating realistic images, audio, and video data. While these models produce high-quality samples with impressive details, they often introduce unrealistic artifacts, such as distorted fingers or hallucinated texts with no meaning. This paper focuses on textual hallucinations, where diffusion models correctly generate individual symbols but assemble them in a nonsensical manner. Through experimental probing, we consistently observe that such phenomenon is attributed it to the network's local generation bias. Denoising networks tend to produce outputs that rely heavily on highly correlated local regions, particularly when different dimensions of the data distribution are nearly pairwise independent. This behavior leads to a generation process that decomposes the global distribution into separate, independent distributions for each symbol, ultimately failing to capture the global structure, including underlying grammar. Intriguingly, this bias persists across various denoising network architectures including MLP and transformers which have the structure to model global dependency. These findings also provide insights into understanding other types of hallucinations, extending beyond text, as a result of implicit biases in the denoising models. Additionally, we theoretically analyze the training dynamics for a specific case involving a two-layer MLP learning parity points on a hypercube, offering an explanation of its underlying mechanism.",2025,0.6818179338843877,0.6806929345162506,0.6666666666666666,0.625,3296a999-e1a6-4cec-8c50-a2370ebb2910,1,"[0.625, 0.625, 0.625, 0.625, 0.625]","[0.9, 0.8, 0.95, 0.95, 0.9]",Local Generation Bias,0.625
547109,Revisiting Random Walks for Learning on Graphs,"We revisit a simple model class for machine learning on graphs, where a random walk on a graph produces a machine-readable record, and this record is processed by a deep neural network to directly make vertex-level or graph-level predictions. We call these stochastic machines random walk neural networks (RWNNs), and through principled analysis, show that we can design them to be isomorphism invariant while capable of universal approximation of graph functions in probability. A useful finding is that almost any kind of record of random walks guarantees probabilistic invariance as long as the vertices are anonymized. This enables us, for example, to record random walks in plain text and adopt a language model to read these text records to solve graph tasks. We further establish a parallelism to message passing neural networks using tools from Markov chain theory, and show that over-smoothing in message passing is alleviated by construction in RWNNs, while over-squashing manifests as probabilistic under-reaching. We empirically demonstrate RWNNs on a range of problems, verifying our theoretical analysis and demonstrating the use of language models for separating strongly regular graphs where 3-WL test fails, and transductive classification on arXiv citation network. Code is available at https://github.com/jw9730/random-walk.",2025,0.886363314049704,0.8839935873280026,0.9333333333333332,0.875,f0528b9d-e1c5-4075-90f9-75e219293322,1,"[0.625, 0.875, 0.875, 0.875]","[0.95, 0.95, 0.95, 0.9]",Random Walk Neural Networks,0.8087268743775016
547117,Emphasizing Discriminative Features for Dataset Distillation in Complex Scenarios,"Dataset distillation has demonstrated strong performance on simple datasets like CIFAR, MNIST, and TinyImageNet but struggles to achieve similar results in more complex scenarios. 
In this paper, we propose a novel approach that \textbf{e}mphasizes the \textbf{d}iscriminative \textbf{f}eatures (obtained by Grad-CAM) for dataset distillation, called \textbf{EDF}.
Our approach is inspired by a key observation: in simple datasets, high-activation areas typically occupy most of the image, whereas in complex scenarios, the size of these areas is much smaller.
Unlike previous methods that treat all pixels equally when synthesizing images, EDF uses Grad-CAM activation maps to enhance high-activation areas.
From a supervision perspective, we downplay supervision signals that have lower losses, as they contain common patterns.
Additionally, to help the DD community better explore complex scenarios, we build the Complex Dataset Distillation (Comp-DD) benchmark by meticulously selecting sixteen subsets, eight easy and eight hard, from ImageNet-1K.
Notably, EDF consistently outperforms SOTA results in complex scenarios, such as ImageNet-1K subsets.
Hopefully, more researchers will be inspired and encouraged to enhance the practicality and efficacy of DD. 
Our code and benchmark will be made public.",2025,0.5113634504132908,0.5100655425612787,0.5333333333333333,0.5,9c7007f0-a940-415f-ac9e-850cfc7fb2ec,0,"[0.25, 0.5, 0.5, 0.625]","[0.95, 0.95, 0.9, 0.95]",EDF,0.4668634371957156
547123,"Towards Efficient Vision-Language Tuning: More Information Density, More Generalizability","With the advancement of large pre-trained vision-language models, effectively transferring the knowledge embedded within these foundational models to downstream tasks has become a pivotal topic, particularly in data-scarce environments. Recently, parameter-efficient fine-tuning approaches, especially prompt tuning, have garnered considerable attention. To better understand the nature of prompt tuning, we propose the concept of ``Information Density'' (ID) to indicate whether a matrix strongly belongs to certain feature spaces rather than being evenly distributed across various feature spaces. We suppose a higher ID with strong bias across some feature spaces naturally leads to excellent robustness and stability. Our research, inspired by the observation that generalizability is closely linked to the information density of the prompt embedding, introduces the Dense Information Prompt (DIP). DIP aims to enhance information density to improve generalization. Several alternative algorithms to increase ID are proposed and verified effective. With further help of proper initialization and regularization, comprehensive experiments substantiate the superiority of DIP. Notably, DIP surpasses the latest state-of-the-art methods by a substantial margin with an exceptionally small parameter count and no extra inference overhead. Across a range of tasks spanning 11 datasets, DIP improves the average downstream accuracy of classic prompt tuning by up to 5.76\%.",2025,0.4545456528924899,0.4522567421273778,0.5333333333333333,0.5,a180e528-3f14-46a5-ab33-37cfa7793863,0,"[0.25, 0.5, 0.5]","[1.0, 0.95, 1.0]",Dense Information Prompt,0.4088643259644559
547125,GROOT-2: Weakly Supervised Multimodal Instruction Following Agents,"Developing agents that can follow multimodal instructions remains a fundamental challenge in robotics and AI. Although large-scale pre-training on unlabeled datasets has enabled agents to learn diverse behaviors, these agents often struggle with following instructions. While augmenting the dataset with instruction labels can mitigate this issue, acquiring such high-quality annotations at scale is impractical. 
To address this issue, we frame the problem as a semi-supervised learning task and introduce \agent, a multimodal instructable agent trained using a novel approach that combines weak supervision with latent variable models. Our method consists of two key components: constrained self-imitating, which utilizes large amounts of unlabeled demonstrations to enable the policy to learn diverse behaviors, and human intention alignment, which uses a smaller set of labeled demonstrations to ensure the latent space reflects human intentions. \agent’s effectiveness is validated across four diverse environments, ranging from video games to robotic manipulation, demonstrating its robust multimodal instruction-following capabilities.",2025,0.613636140495949,0.6117164135215023,0.6,0.5,3e1496d1-9b02-4568-a789-55ddcf1a999a,1,"[0.5, 0.5, 0.625, 0.625]","[0.95, 0.95, 0.95, 0.9]",GROOT-2,0.5587268743914313
547126,Contrastive Meta Learning for Dynamical Systems,"Recent advancements in deep learning have significantly impacted the study of dynamical systems. Traditional approaches predominantly rely on supervised learning paradigms, limiting their scope to large scale problems and adaptability to new systems. This paper introduces a novel meta learning framework tailored for dynamical system forecasting, hinging on the concept of mapping the observed trajectories to a system-specific embedding space which encapsulates the inter-system characteristics and enriches the feature set for downstream prediction tasks. Central to our framework is the use of contrastive learning for trajectory data coupled with a series of neural network architecture designs to extract the features as augmented embedding for modeling system behavior. We present the application of zero-shot meta-learning to dynamical systems, demonstrating a substantial enhancement in performance metrics compared to existing baseline models. A notable byproduct of our methodology is the improved interpretability of the embeddings, which now carries explicit physical significance. Our results not only set a new benchmark in the field but also pave the way for enhanced interpretability and deeper understanding of complex dynamical systems, potentially opens new directions for how we approach system analysis and prediction.",2025,0.5113634504132908,0.5077598994534853,0.5333333333333333,0.5,934fc190-cbcc-4b42-bcad-ee9f87d1bad5,0,"[0.25, 0.5, 0.5, 0.625]","[0.95, 0.9, 0.95, 0.9]",Contrastive Meta Learning,0.4567033678756477
547134,Mora: Enabling Generalist Video Generation via A Multi-Agent Framework,"Text-to-video generation has made significant strides, but replicating the capabilities of advanced systems like OpenAI’s Sora remains challenging due to their closed-source nature. Existing open-source methods struggle to achieve comparable performance, often hindered by ineffective agent collaboration and inadequate training data quality. In this paper, we introduce Mora, a novel multi-agent framework that leverages existing open-source modules to replicate Sora’s functionalities. We address these fundamental limitations by proposing three key techniques: (1) multi-agent fine-tuning with a self-modulation factor to enhance inter-agent coordination, (2) a data-free training strategy that uses large models to synthesize training data, and (3) a human-in-the-loop mechanism combined with multimodal large language models for data filtering to ensure high-quality training datasets. Our comprehensive experiments on six video generation tasks demonstrate that Mora achieves performance comparable to Sora on VBench \cite{huang2024vbench}, outperforming existing open-source methods across various tasks. Specifically, in the text-to-video generation task, Mora achieved a Video Quality score of 0.800, surpassing Sora’s 0.797 and outperforming all other baseline models across six key metrics. Additionally, in the image-to-video generation task, Mora achieved a perfect Dynamic Degree score of 1.00, demonstrating exceptional capability in enhancing motion realism and achieving higher Imaging Quality than Sora. These results highlight the potential of collaborative multi-agent systems and human-in-the-loop mechanisms in advancing text-to-video generation.",2025,0.6363630413225304,0.6353130425122688,0.6666666666666666,0.625,c5ab79ae-7b28-4177-bc0f-02a5341f4516,0,"[0.5, 0.625, 0.625]","[0.95, 0.95, 0.95]",Mora,0.5833333333333333
547144,TrendDiff: Decoupling Intrinsic and Measurement Trends for Enhanced Time Series Causal Discovery,"Time trends can be classified into intrinsic (real) and measurement (false) trends. There has long been a critical need for techniques to discern them, especially in investment decision-making. In causal discovery, these measurement trends, essentially measurement errors, can significantly impact the performance of algorithms, making it crucial to identify and eliminate them before analysis as well. Recognizing this need, we present a novel algorithm, termed Trend Differentiator (TrendDiff). It is capable of detecting all trend-influenced variables and differentiating between those affected by measurement trends and those displaying intrinsic trends, relying on changing causal module detection and trend-influenced variables’ structural properties, respectively. Extensive experiments on synthetic and real-world data demonstrate the efficacy of this approach.",2025,0.4090907603306326,0.404833682211152,0.4,0.25,fad6f00b-889f-4fa7-b787-99d09d51d740,0,"[0.25, 0.25, 0.5, 0.5]","[1.0, 0.95, 0.9, 0.95]",Trend Differentiator,0.3575962171087842
547149,Can We Trust Embodied Agents? Exploring Backdoor Attacks against Embodied LLM-Based Decision-Making Systems,"Large Language Models (LLMs) have shown significant promise in real-world decision-making tasks for embodied artificial intelligence, especially when fine-tuned to leverage their inherent common sense and reasoning abilities while being tailored to specific applications. However, this fine-tuning process introduces considerable safety and security vulnerabilities, especially in safety-critical cyber-physical systems. In this work, we propose the first comprehensive framework for **B**ackdoor **A**ttacks against **L**LM-based **D**ecision-making systems (BALD) in embodied AI, systematically exploring the attack surfaces and trigger mechanisms. Specifically, we propose three distinct attack mechanisms: *word injection*, *scenario manipulation*, and *knowledge injection*, targeting various components in the LLM-based decision-making pipeline. We perform extensive experiments on representative LLMs (GPT-3.5, LLaMA2, PaLM2) in autonomous driving and home robot tasks, demonstrating the effectiveness and stealthiness of our backdoor triggers across various attack channels, with cases like vehicles accelerating toward obstacles and robots placing knives on beds. Our word and knowledge injection attacks achieve nearly 100\% success rate across multiple models and datasets while requiring only limited access to the system. Our scenario manipulation attack yields success rates exceeding 65\%, reaching up to 90\%, and does not require any runtime system intrusion. We also assess the robustness of these attacks against defenses, revealing their resilience. Our findings highlight critical security vulnerabilities in embodied LLM systems and emphasize the urgent need for safeguarding these systems to mitigate potential risks.",2025,0.7159088305786071,0.7151817395679724,0.8,0.875,609ad6a7-88f5-4cff-96de-9346627a006b,1,"[0.25, 0.625, 0.875, 0.875]","[0.95, 0.9, 0.95, 0.95]",BALD,0.6581365628042843
547156,Towards Optimal Adapter Placement for Efficient Transfer Learning,"Parameter-efficient transfer learning (PETL) aims to adapt pre-trained models to new downstream tasks while minimizing the number of fine-tuned parameters. Adapters, a popular approach in PETL, inject additional capacity into existing networks by incorporating low-rank projections, achieving performance comparable to full fine-tuning with significantly fewer parameters. This paper investigates the relationship between the placement of an adapter and its performance. We observe that adapter location within a network significantly impacts its effectiveness, and that the optimal placement is task-dependent. To exploit this observation, we introduce an extended search space of adapter connections, including long-range and recurrent adapters. We demonstrate that even randomly selected adapter placements from this expanded space yield improved results, and that high-performing placements often correlate with high gradient rank. Our findings reveal that a small number of strategically placed adapters can match or exceed the performance of the common baseline of adding adapters in every block, opening a new avenue for research into optimal adapter placement strategies.",2025,0.5454543471075102,0.5369915768333511,0.6666666666666666,0.625,16a773eb-4d9f-4cca-a56a-6fbbe5a3ae54,0,"[0.25, 0.625, 0.625]","[0.95, 0.8, 0.95]",Adapter Placement,0.4750178443968594
547189,Text Boosts Generalization: A Plug-and-Play Captioner for Real-World Image Restoration,"Generalization has long been a central challenge in real-world image restoration. While recent diffusion-based restoration methods, which leverage generative priors from text-to-image models, have made progress in recovering more realistic details, they still encounter ""generative capability inactivation"" when applied to out-of-distribution data. To address this, we propose using text as an auxiliary invariant representation to reactivate the generative capabilities of these models. We begin by identifying two key properties of text input in diffusion-based restoration: richness and relevance, and examine their respective influence on model performance. Building on these insights, we introduce Res-Captioner, a module that generates enhanced textual descriptions tailored to image content and degradation levels, effectively mitigating response failures. Additionally, we present RealIR, a new benchmark designed to capture diverse real-world scenarios. Extensive experiments demonstrate that Res-Captioner significantly boosts the generalization ability of diffusion-based restoration models, while remaining fully plug-and-play.",2025,0.443181657024852,0.4320134788092121,0.4,0.25,e010db62-7046-4b6f-a4e5-d9684516256a,0,"[0.25, 0.25, 0.5, 0.625]","[0.95, 1.0, 1.0, 0.8]",Res-Captioner,0.3750177407956779
547204,Sailing in high-dimensional spaces: Low-dimensional embeddings through angle preservation,"Low-dimensional embeddings (LDEs) of high-dimensional data are ubiquitous in science and engineering. They allow us to quickly understand the main properties of the data, identify outliers and processing errors, and inform the next steps of data analysis.
 As such, LDEs have to be *faithful* to the original high-dimensional data, i.e., they should represent the relationships that are encoded in the data, both at a local as well as global scale.
 The current generation of LDE approaches focus on reconstructing *local distances* between any pair of samples correctly, often outperforming traditional approaches aiming at all distances.
 For these approaches, global relationships are, however, usually strongly distorted, often argued to be an inherent trade-off between local and global structure learning for embeddings. We suggest a new perspective on LDE learning, reconstructing *angles* between data points.
 We show that this approach, Mercat, yields good reconstruction across a diverse set of experiments and metrics, and preserve structures well across all scales, outperforming existing methods across datasets and metrics in most cases by a margin.
 Compared to existing work, our approach also has a *simple formulation*, facilitating future theoretical analysis and algorithmic improvements.",2025,0.4090907603306326,0.3955729909756453,0.2666666666666666,0.25,a2c9adf4-5fe2-4ccb-951d-e4aeed2199c6,0,"[0.25, 0.25, 0.625]","[0.9, 0.95, 0.8]",Angle preservation,0.3325549513614679
547210,Variational Mirror Descent for Robust Learning in Schrödinger Bridge,"Schrödinger bridge (SB) has evolved into a universal class of probabilistic generative models. Recent studies regarding the Sinkhorn algorithm through mirror descent (MD) have gained attention, revealing geometric insights into solution acquisition of the SB problems. In this paper, we propose a variational online MD framework for the SB problems, which provides further stability to SB solvers. We formally prove convergence and a regret bound $\mathcal{O}(\textrm{\small$\sqrt{T}$})$ of online mirror descent under mild assumptions. As a result of analysis, we propose a simulation-free SB algorithm called Variational Mirrored Schrödinger Bridge (VMSB) by utilizing the Wasserstein-Fisher-Rao geometry of the Gaussian mixture parameterization for Schrödinger potentials. Based on the Wasserstein gradient flow theory, our variational MD framework offers tractable gradient-based learning dynamics that precisely approximate a subsequent update. We demonstrate the performance of the proposed VMSB algorithm in an extensive suite of benchmarks.",2025,0.7909088033058898,0.7831791518457125,0.9333333333333332,0.875,125be381-28f1-43d4-81c5-1488506e664a,0,"[0.5, 0.5, 0.875, 0.875, 0.875]","[0.9, 0.95, 0.8, 0.9, 0.9]",Variational Mirrored Schrödinger Bridge,0.7008887037519638
547236,Class-wise Autoencoders Measure Classification Difficulty And Detect Label Mistakes,"We introduce a new framework for analyzing classification datasets based on the ratios of reconstruction errors between autoencoders trained on individual classes. This analysis framework enables efficient characterization of datasets on the sample, class, and entire dataset levels. We define reconstruction error ratios (RERs) that probe classification difficulty and allow its decomposition into (1) finite sample size and (2) Bayes error and decision-boundary complexity. Through systematic study across 19 popular visual datasets, we find that our RER-based dataset difficulty probe strongly correlates with error rate for state-of-the-art (SOTA) classification models. By interpreting sample-level classification difficulty as a label mistakenness score, we further find that RERs achieve SOTA performance on mislabel detection tasks on hard datasets under symmetric and asymmetric label noise.",2025,0.6272724991736367,0.6298282911231107,0.6666666666666666,0.25,57f88886-1437-479f-a55b-e2d59f6694e7,0,"[0.25, 0.25, 0.625, 0.875, 0.875]","[0.9, 0.9, 0.9, 0.95, 0.9]",Reconstruction Error Ratios,0.59170783741298
547238,ActionFiller: Fill-In-The-Blank Prompting for OS Agent,"Many existing methods for operating system (OS) agents focus on predicting the next action based on the current state, which constructs a predefined task execution pipeline. While these methods demonstrate promising performance, reliance on state cognition modules like detector or recognizer could impede execution efficiency, particularly in long-horizon tasks with intricate action trajectories.  
Recognizing the remarkable accuracy of large language models (LLMs) in processing short instructions, this paper proposes the \textbf{ActionFiller} framework. 
The goal is to integrate easily executable short tasks into longer, cohesive tasks using fill-in-the-blank prompts, thereby minimizing redundant operations and enhancing efficiency. 
ActionFiller employs two types of action-oriented fill-in-the-blank prompts: one designed for subtasks and another for specific actions. To generate subtask prompts, we introduce a Foresight Optimization Agent (FOA) that constructs an initial prompt by referencing past short tasks. It then fills in the unreferenced parts with detailed prompts generated by a planning agent, effectively retaining valuable past experiences. 
Next, an Action Template Agent (ATA) generates action prompts for each subtask. This process yields three distinct types of action prompts: 1) executable action sequences, 2) non-executable action sequences with prompt parameters, and 3) pure text descriptions. 
To execute the action prompts effectively, we propose the CohesiveFlow method, which optimizes the second and third types of prompts by leveraging the cognitive state of the environment. Inspired by masked language modeling, the CohesiveFlow agent integrates the current environmental state with previously executed action sequences to update parameters and text descriptions, ensuring both feasibility and effectiveness in execution. 
To validate the efficacy of our approach for long-horizon instructions, we introduce a new benchmark called \textbf{EnduroSeq} and conduct experiments using the WinBench short instruction dataset. The results demonstrate that ActionFiller significantly enhances task completion rates and execution efficiency, offering a novel solution for the application of intelligent agents in complex environments.",2025,0.2727271735537551,0.2722771738065002,0.2666666666666666,0.25,ee085456-b7ee-44e3-b23a-3ad5281ffbb0,0,"[0.25, 0.25, 0.25, 0.25]","[0.95, 0.95, 0.9, 1.0]",ActionFiller,0.25
547247,Solving Robotics Problems in Zero-Shot with Vision-Language Models,"We introduce Wonderful Team, a multi-agent Vision Large Language Model (VLLM) framework designed to solve robotics problems in a zero-shot regime. In our context, zero-shot means that for a novel environment, we provide a VLLM with an image of the robot's surroundings and a task description, and the VLLM outputs the sequence of actions necessary for the robot to complete the task. Unlike prior work that requires fine-tuning parts of the pipeline -- such as adjusting an LLM on robot-specific data or training separate vision encoders -- our approach demonstrates that with careful engineering, a single off-the-shelf VLLM can autonomously handle all aspects of a robotics task, from high-level planning to low-level location extraction and action execution. Crucially, compared to using GPT-4o alone, Wonderful Team is self-corrective and capable of iteratively fixing its own mistakes, enabling it to solve challenging long-horizon tasks. We validate our framework through extensive experiments, both in simulated environments using VIMABench and in real-world settings. Our system showcases the ability to handle diverse tasks such as manipulation, goal-reaching, and visual reasoning---all in a zero-shot manner. These results underscore a key point: vision-language models have progressed rapidly in the past year and should be strongly considered as a backbone for many robotics problems moving forward.",2025,0.3636358677687754,0.3630358687057685,0.2666666666666666,0.25,ebd4f08f-9b82-463c-8c1b-d5a98099fa7f,0,"[0.25, 0.25, 0.5]","[0.95, 0.95, 0.95]",Wonderful Team,0.3333333333333333
547249,BONE: BLOCK AFFINE TRANSFORMATION AS PARAMETER EFFICIENT FINE-TUNING METHODS FOR LARGE LANGUAGE MODELS,"Low-Rank Adaptation (LoRA) has achieved remarkable training results by freezing the original weights and training only low-rank matrices, establishing itself as the predominant fine-tuning method for LLMs. Many LoRA variants have emerged, yet they lack a design tailored to the characteristics of LLM weights and fail to leverage the original weights effectively. To address the sparsity of LLM weights, and drawing inspiration from GQA and MQA, we propose Block-Affine Adaptation (Bone), a novel PEFT technique distinct from LoRA. By dividing the original weights into multiple subspaces that share a single matrix for weight updates, Bone simplifies the process by requiring the trainable matrix to be initialized to zero, eliminating the need for complex initialization as in some LoRA variants. Compared to LoRA, Bone significantly reduces memory usage and achieves faster computation. Evaluation of both NLU and NLG tasks demonstrates that Bone substantially outperforms LoRA and its variants. Inspired by Pissa, we propose a new theory called ''Weight Guide'' to better utilize the information embedded in the original weights. This approach extracts valuable information through a linear transformation of the original weight matrix using a trainable matrix. To validate the effectiveness of ''Weight Guide'' we combined it with Bone to create a new structure called Block-Affine Transformation (Bat), and ablation experiments confirmed the effectiveness of ''Weight Guide''.",2025,0.4636361950413836,0.4608857503196534,0.5333333333333333,0.25,75d20a88-0652-4373-b934-9ca1b5631802,0,"[0.25, 0.25, 0.5, 0.5, 0.625]","[1.0, 0.95, 0.95, 0.95, 0.95]",Block-Affine Adaptation,0.4136860221490571
547252,GS-LiDAR: Generating Realistic LiDAR Point Clouds with Panoramic Gaussian Splatting,"LiDAR novel view synthesis (NVS) has emerged as a novel task within LiDAR simulation, offering valuable simulated point cloud data from novel viewpoints to aid in autonomous driving systems. However, existing LiDAR NVS methods typically rely on neural radiance fields (NeRF) as their 3D representation, which incurs significant computational costs in both training and rendering. Moreover, NeRF and its variants are designed for symmetrical scenes, making them ill-suited for driving scenarios. To address these challenges, we propose GS-LiDAR, a novel framework for generating realistic LiDAR point clouds with panoramic Gaussian splatting. Our approach employs 2D Gaussian primitives with periodic vibration properties, allowing for precise geometric reconstruction of both static and dynamic elements in driving scenarios. We further introduce a novel panoramic rendering technique with explicit ray-splat intersection, guided by panoramic LiDAR supervision. By incorporating intensity and ray-drop spherical harmonic (SH) coefficients into the Gaussian primitives, we enhance the realism of the rendered point clouds. Extensive experiments on KITTI-360 and nuScenes demonstrate the superiority of our method in terms of quantitative metrics, visual quality, as well as training and rendering efficiency.",2025,0.6477270371901683,0.6480196736594706,0.6666666666666666,0.625,3d898c80-2e6d-46ac-82c8-50c794ff4f44,1,"[0.5, 0.625, 0.625, 0.625]","[0.9, 0.95, 0.95, 0.95]",Panoramic Gaussian Splatting,0.5994096884128528
547258,METHODS OF IMPROVING LLM TRAINING STABILITY,"Training stability of large language models (LLMs) is an important research topic. Reproducing training instabilities can be costly, so we use a small language model with 830M parameters and experiment with higher learning rates to force models to diverge, as in Wortsman et al. (2024). One of the sources of training instability is the growth of logits in attention layers Dehghani et al. (2023). We extend the focus of the previous work [Dehghani et al. (2023),Wortsman et al. (2024)] and look not only at the magnitude of the logits but at all outputs of linear layers in the Transformer block. We observe that with a high learning rate the L2 norm of all linear layer outputs grow with each training step and the model diverges. Specifically we observe that QKV, Proj and FC2 layers have the largest growth of the output magnitude. This prompts us to explore several options: 1) apply layer normalization not only after QK layers (as it is done in [Dehghani et al. (2023), Wortsman et al. (2024)]) but after Proj and FC2 layers too; 2) apply layer normalization after the QKV layer (and remove pre normalization). 3) apply QK layer normalization together with softmax capping. We show that with the last two methods we can increase learning rate by 1.5x (without model divergence) in comparison to an approach based on QK layer normalization only Dehghani et al. (2023). Also we observe significant perplexity improvements for all three methods in comparison to the baseline model.",2025,0.3409089669421938,0.3394141902150118,0.2666666666666666,0.25,7e6d92a4-5920-4435-b0f4-b9c9bb01198e,0,"[0.25, 0.25, 0.25, 0.5]","[0.9, 0.95, 0.9, 0.9]",Logit Growth,0.3082087486157253
547269,Upcycling Instruction Tuning from Dense to Mixture-of-Experts via Parameter Merging,"Mixture-of-Experts (MoE) shines brightly in large language models (LLMs) and demonstrates outstanding performance in plentiful natural language processing tasks. However, existing methods that transform LLMs from dense to MoE face significant data requirements and typically rely on large-scale post-training.
In this paper, we propose Upcycling Instruction Tuning (UpIT), a data-efficient approach for tuning a dense pre-trained model into an MoE instruct model.
Specifically, we first point out that intermediate checkpoints during instruction tuning of the dense model are naturally suitable for specialized experts, and then propose an expert expansion stage to flexibly achieve models with different numbers of experts, where genetic algorithm and parameter merging are introduced to ensure sufficient diversity of new extended experts.
To ensure that each differentiated expert in the MoE model works as expected, we select a small amount of seed data that each expert excels to pre-optimize the router.
Extensive experiments with various data scales and upcycling settings demonstrate the outstanding performance and data efficiency of UpIT, as well as stable improvement in expert or data scaling. Further analysis reveals the importance of ensuring expert diversity in upcycling.",2025,0.613636140495949,0.6126236410646255,0.6,0.5,bd3b2e37-0e1f-46bd-8ff1-a1bf1cf4a582,0,"[0.5, 0.5, 0.625, 0.625]","[0.9, 0.95, 0.95, 0.9]",Upcycling Instruction Tuning,0.5625000000000001
547286,Segment Anything with Multiple Modalities,"Robust and accurate segmentation of scenes has become one core functionality in various visual recognition and navigation tasks. This has inspired the recent development of Segment Anything Model (SAM), a foundation model for general mask segmentation. However, SAM is largely tailored for single-modal RGB images, limiting its applicability to multi-modal data captured with widely-adopted sensor suites, such as LiDAR plus RGB, depth plus RGB, thermal plus RGB, etc. We develop MM-SAM, an extension and expansion of SAM that supports cross-modal and multi-modal processing for robust and enhanced segmentation with different sensor suites. MM-SAM features two key designs, namely, unsupervised cross-modal transfer and weakly-supervised multi-modal fusion, enabling label-free and parameter-efficient adaptation toward various sensor modalities. It addresses three main challenges: 1) adaptation toward diverse non-RGB sensors for single-modal processing, 2) synergistic processing of multi-modal data via sensor fusion, and 3) mask-free training for different downstream tasks. Notably, we demonstrate that the output latent space of SAM's RGB image encoder can function as a highly abstract, shareable embedding space compatible with segmentation across different sensor modalities. Extensive experiments show that MM-SAM consistently outperforms SAM by large margins, demonstrating its effectiveness and robustness across various sensors and data modalities. Code will be released.",2025,0.6363630413225304,0.6345027456430207,0.6666666666666666,0.625,c9035fa4-af38-45c1-ad45-d7d03b4554c6,0,"[0.5, 0.625, 0.625]","[0.95, 0.9, 0.95]",MM-SAM,0.5799105664127194
547308,DH-Fusion: Depth-Aware Hybrid Feature Fusion for Multimodal 3D Object Detection,"State-of-the-art LiDAR-camera 3D object detectors usually focus on feature fusion. However, they neglect the factor of depth while designing the fusion strategy. In this work, we for the first time point out that different modalities play different roles as depth varies via statistical analysis and visualization. Based on this finding, we propose a Depth-Aware Hybrid Feature Fusion (DH-Fusion) strategy that guides the weights of point cloud and RGB image modalities by introducing depth encoding at both global and local levels. Specifically, the Depth-Aware Global Feature Fusion (DGF) module adaptively adjusts the weights of image Bird's-Eye-View (BEV) features in multi-modal global features via depth encoding. Furthermore, to compensate for the information lost when transferring raw features to the BEV space, we propose a Depth-Aware Local Feature Fusion (DLF) module, which adaptively adjusts the weights of original voxel features and multi-view image features in multi-modal local features via depth encoding. Extensive experiments on the nuScenes and KITTI datasets demonstrate that our DH-Fusion method surpasses previous state-of-the-art methods. Moreover, our DH-Fusion is more robust to various kinds of corruptions, outperforming previous methods on nuScenes-C w.r.t. both NDS and mAP.",2025,0.5454543471075102,0.5445543476130005,0.5333333333333333,0.5,c3e4e655-58bb-4c49-b34c-01b3595fbfe1,0,"[0.5, 0.5, 0.5, 0.5, 0.5]","[0.95, 1.0, 0.95, 0.95, 0.95]",DH-Fusion,0.5
547337,BSM: Small but Powerful Biological Sequence Model for Genes and Proteins,"Modeling biological sequences such as DNA, RNA, and proteins is crucial for understanding complex processes like gene regulation and protein synthesis. However, most current models either focus on a single type or treat multiple types of data separately, limiting their ability to capture cross-modal relationships. We propose that by learning the relationships between these modalities, the model can enhance its understanding of each type. To address this, we introduce BSM, a small but powerful mixed-modal biological sequence foundation model, trained on three types of data: RefSeq, Gene Related Sequences, and interleaved biological sequences from the web. These datasets capture the genetic flow, gene-protein relationships, and the natural co-occurrence of diverse biological data, respectively. By training on mixed-modal data, BSM significantly enhances learning efficiency and cross-modal representation, outperforming models trained solely on unimodal data. With only 110M parameters, BSM achieves performance comparable to much larger models across both single-modal and mixed-modal tasks, and uniquely demonstrates in-context learning capability for mixed-modal tasks, which is absent in existing models. Further scaling to 270M parameters demonstrates even greater performance gains, highlighting the potential of BSM as a significant advancement in multimodal biological sequence modeling.",2025,0.3636358677687754,0.3663369571609985,0.5333333333333333,0.5,5fd0cdde-5931-47c5-beea-483ffd6b49be,0,"[0.0, 0.5, 0.5]","[0.9, 0.95, 0.9]",BSM,0.3482497293395885
547361,Revisiting Prefix-tuning: Statistical Benefits of Reparameterization among Prompts,"Prompt-based techniques, such as prompt-tuning and prefix-tuning, have gained prominence for their efficiency in fine-tuning large pre-trained models. Despite their widespread adoption, the theoretical foundations of these methods remain limited. For instance, in prefix-tuning, we observe that a key factor in achieving performance parity with full fine-tuning lies in the reparameterization strategy. However, the theoretical principles underpinning the effectiveness of this approach have yet to be thoroughly examined. Our study demonstrates that reparameterization is not merely an engineering trick but is grounded in deep theoretical foundations. Specifically, we show that the reparameterization strategy implicitly encodes a shared structure between prefix key and value vectors. Building on recent insights into the connection between prefix-tuning and mixture of experts models, we further illustrate that this shared structure significantly improves sample efficiency in parameter estimation compared to non-shared alternatives. The effectiveness of prefix-tuning across diverse tasks is empirically confirmed to be enhanced by the shared structure, through extensive experiments in both visual and language domains. Additionally, we uncover similar structural benefits in prompt-tuning, offering new perspectives on its success. Our findings provide theoretical and empirical contributions, advancing the understanding of prompt-based methods and their underlying mechanisms.",2025,0.7499997272728265,0.7507073760975492,0.6666666666666666,0.625,ab14602b-4c36-41a8-a90c-04bd1522be2d,1,"[0.625, 0.625, 0.625, 0.875]","[0.8, 0.9, 0.9, 0.9]",Reparameterization,0.6935339204174822
547369,Fair Clustering via Alignment,"Algorithmic fairness in clustering aims to balance the proportions of instances assigned to each cluster with respect to a given sensitive attribute.
Recently, numerous algorithms have been developed for Fair Clustering (FC), most of which optimize a clustering objective under specifically designed fairness constraints.
However, the inherent complexity or approximation of constrained optimization problems makes it challenging to achieve the optimal trade-off between fairness level and clustering utility in practice.
For example, the obtained clustering utility by an existing FC algorithm might be suboptimal, or achieving a certain fairness level could be numerically unstable.
To resolve these limitations, we propose a new FC algorithm based on a novel decomposition of the fair $K$-means clustering objective function.
The proposed algorithm, called Fair Clustering via Alignment (FCA), operates by (i) finding a joint probability distribution to align the data from different protected groups, and (ii) optimizing cluster centers in the aligned space.
A key advantage of FCA is that it guarantees (local) optimal clustering utility for any given fairness level while avoiding the need to solve complex constrained optimization problems, thereby obtaining (local) optimal fair clustering in practice.
Experiments show that FCA offers several empirical benefits over existing methods such as (i) attaining the optimal trade-off between fairness level and clustering utility, and (ii) achieving near-perfect fairness level without numerical instability.",2025,0.6818179338843877,0.6792596674733332,0.6666666666666666,0.5,7e9afe52-d140-480f-b2df-d091cd404dee,0,"[0.5, 0.5, 0.625, 0.625, 0.875]","[0.95, 1.0, 0.95, 0.9, 0.95]",Fair Clustering via Alignment,0.6165589853327932
547375,InstaTrain: Adaptive Training via Ultra-Fast Natural Annealing within Dynamical Systems,"Time-series modeling is broadly adopted to capture underlying patterns present in historical data, allowing prediction of future values. However, one crucial aspect of such modeling is often overlooked: in highly dynamic environments, data distributions can shift drastically within a second or less. Under these circumstances, traditional predictive models, and even online learning methods, struggle to adapt to the ultra-fast and complex distribution shifts present in highly dynamic scenarios. To address this, we propose InstaTrain, a novel learning approach that enables ultra-fast model updates for real-world prediction tasks, thereby keeping pace with rapidly evolving data distributions. In this work, (1) we transform the slow and expensive training process into an ultra-fast natural annealing process within a dynamical system. (2) Leveraging a recently proposed electronic dynamical system, we augment the system with parameter update modules, extending its capabilities to encompass both rapid training and inference. Experimental results on highly dynamic datasets demonstrate that our method achieves orders-of-magnitude improvements in training speed and energy efficiency while delivering superior accuracy compared to baselines running on GPUs.",2025,0.7159088305786071,0.7157001553069,0.6666666666666666,0.625,fced44b5-f2d8-41d6-8429-52a4994ab96f,1,"[0.5, 0.625, 0.625, 0.875]","[0.9, 0.9, 0.8, 0.9]",InstaTrain,0.659266960208741
547376,A Consistent Pattern for Identifying Decisive Code Snippets for LLM-Based Code Inference,"Which parts of pre-target input are most influential for next-token prediction in the context of programming languages? In this paper, we present evidence that code snippets at specific locations in pre-target inputs play a decisive role in large language model (LLM) inference, and these snippets exhibit a consistent pattern. Firstly, we introduce a novel causal tracing method to identify tokens,  so-called high-information tokens, that significantly contribute to next-token prediction. Building on this, we propose a multi-phase causal tracing process to analyze the importance distribution of high-information tokens, revealing a consistent pattern, named the  Important Position Rule (IPR). To further validate this hypothesis, we assess the role of IPR across various LLMs, languages, and tasks. Our extensive evaluations for code translation, code correction and code completion tasks (Java, Python, C++) on models CodeLlama-7b/13b/34b-Instruct and GPT-3.5/4-turbo, confirm this hypothesis. Furthermore, we observe that IPR exhibits structural and semantic properties similar to the $\langle \text{subject},  \text{relation}, \text{object} \rangle$ paradigm in natural language. Leveraging this insight, we successfully combine IPR with the knowledge editing method ROME in order to repair translation errors, achieving a correction rate of  62.73% to 75.31%. To our knowledge, this is the first application of knowledge editing  in the context of programming languages.",2025,0.5454543471075102,0.5396043485931983,0.6666666666666666,0.625,b68197c1-e283-4206-91b8-2e827248f377,0,"[0.25, 0.625, 0.625]","[0.95, 0.9, 0.9]",Important Position Rule,0.4776254059906172
547388,Few-shot Species Range Estimation,"Understanding where a particular species can or cannot be found is crucial for ecological research and conservation efforts. By mapping the spatial ranges of all species on Earth, we could obtain deeper insights into how global biodiversity is affected by climate change and habitat loss. However, accurate range estimates are available for a relatively small proportion of known species. For most species, we have only have a few prior observations indicating the locations where they have been previously recorded. In this work we address the challenge of training with limited observations by developing a new approach for few-shot species range estimation. During inference, our model takes a set of spatial coordinates as input, along with optional metadata such as text, and outputs a species encoding that can be used to predict the range of a previously unseen species in feed-forward manner. We validate our method on two challenging benchmarks, where we obtain state-of-the-art performance in predicting the ranges of unseen species, in a fraction of the compute time, compared to recent alternative approaches.",2025,0.6477270371901683,0.6453317533996528,0.6666666666666666,0.625,bd85b1c0-c47e-47bf-9923-66c763522536,0,"[0.5, 0.625, 0.625, 0.625]","[1.0, 0.95, 0.95, 0.95]",Few-shot range estimation,0.5862941826215023
547405,ResidualViT for Efficient Zero-Shot Natural Language Temporal Video Grounding,"The goal of this work is to efficiently compute frame-level features from videos for the Zero-Shot Natural Language Temporal Video Grounding (NLTVG) task. The contributions of this work are three-fold. First, we introduce a novel vision transformer (ViT) architecture, dubbed ResidualViT, that capitalizes on the large temporal redundancies in video. Our architecture incorporates (i) learnable residual connections that ensure temporal consistency across consecutive frames and (ii) a token reduction module for enhancing processing speed by selectively discarding temporally redundant information. Second, we describe a lightweight distillation strategy that enables learning parameters of  ResidualViT from existing frame encoders without additional manual annotation. Finally, we validate the effectiveness of our approach across three diverse datasets, demonstrating significant reductions in computational cost (up to 60%) and improvements in inference speed (up to 2.5x faster), all while observing marginal accuracy reduction with respect to the teacher model.",2025,0.6477270371901683,0.6422953183573628,0.6666666666666666,0.625,91ba2883-9fa4-4390-8393-1cd2da356290,0,"[0.25, 0.625, 0.625, 0.875]","[1.0, 1.0, 0.95, 0.95]",ResidualViT,0.5707281718963164
547415,LVSM: A Large View Synthesis Model with Minimal 3D Inductive Bias,"We propose the Large View Synthesis Model (LVSM), a novel transformer-based approach for scalable and generalizable novel view synthesis from sparse-view inputs. We introduce two architectures: (1) an encoder-decoder LVSM, which encodes input image tokens into a fixed number of 1D latent tokens, functioning as a fully learned scene representation, and decodes novel-view images from them; and (2) a decoder-only LVSM, which directly maps input images to novel-view outputs, completely eliminating intermediate scene representations. Both models bypass the 3D inductive biases used in previous methods---from 3D representations (e.g., NeRF, 3DGS) to network designs (e.g., epipolar projections, plane sweeps)---addressing novel view synthesis with a fully data-driven approach. While the encoder-decoder model offers faster inference due to its independent latent representation, the decoder-only LVSM achieves superior quality, scalability, and zero-shot generalization, outperforming previous state-of-the-art methods by 1.5 to 3.5 dB PSNR. Comprehensive evaluations across multiple datasets demonstrate that both LVSM variants achieve state-of-the-art novel view synthesis quality, delivering superior performance even with reduced computational resources (1-2 GPUs). Please see our anonymous website for more details: https://haian-jin.github.io/projects/LVSM/",2025,0.9090902148762856,0.9064270482322676,0.9333333333333332,0.875,0bad4f81-4c56-45a7-8773-b9a92c893765,1,"[0.625, 0.875, 0.875, 0.875, 0.875, 0.875]","[1.0, 1.0, 1.0, 0.95, 0.95, 0.95]",LVSM,0.8271941791723507
547418,MPFBench: A Large Scale Dataset for SciML of Multi-Phase-Flows: Droplet and Bubble Dynamics,"Multiphase fluid dynamics, such as falling droplets and rising bubbles, are critical to many industrial applications. However, simulating these phenomena efficiently is challenging due to the complexity of instabilities, wave patterns, and bubble breakup. This paper investigates the potential of scientific machine learning (SciML) to model these dynamics using neural operators and foundation models. We apply sequence-to-sequence techniques on a comprehensive dataset generated from 11,000 simulations, comprising 1 million time snapshots, produced with a well-validated Lattice Boltzmann method (LBM) framework. The results demonstrate the ability of machine learning models to capture transient dynamics and intricate fluid interactions, paving the way for more accurate and computationally efficient SciML-based solvers for multiphase applications.",2025,0.5454543471075102,0.5408241503318514,0.4666666666666667,0.25,7432854d-f2bb-4726-898c-6d372e8de57d,0,"[0.25, 0.25, 0.625, 0.875]","[0.95, 0.9, 0.9, 0.9]",Multi-Phase-Flows,0.4828349944629015
547436,Discovering Clues of Spoofed LM Watermarks,"LLM watermarks stand out as a promising way to attribute ownership of LLM-generated text. One threat to watermark credibility comes from spoofing attacks, where an unauthorized third party forges the watermark, enabling it to falsely attribute arbitrary texts to a particular LLM. While recent works have demonstrated that state-of-the-art schemes are in fact vulnerable to spoofing, they lack deeper qualitative analysis of the texts produced by spoofing methods. In this work, we for the first time reveal that there are observable differences between genuine and spoofed watermark texts. Namely, we show that regardless of their underlying approach, all current learning-based spoofing methods consistently leave observable artifacts in spoofed texts, indicative of watermark forgery. We build upon these findings to propose rigorous statistical tests that reliably reveal the presence of such artifacts, effectively discovering that a watermark was spoofed. Our experimental evaluation shows high test power across all current learning-based spoofing methods, providing insights into their fundamental limitations, and suggesting a way to mitigate this threat.",2025,0.6272724991736367,0.6259412621898491,0.6666666666666666,0.625,dc2dfa45-cde7-4eca-b482-d11a4cbb1d49,0,"[0.5, 0.5, 0.625, 0.625, 0.625]","[0.9, 0.95, 0.9, 0.95, 0.9]",Spoofing artifacts,0.573681131674112
547444,Long Context Transfer from Language to Vision,"Video sequences offer valuable temporal information, but existing large multimodal models (LMMs) fall short in understanding extremely long videos. Many works address this by reducing the number of visual tokens using visual resamplers. Alternatively, in this paper, we approach this problem from the perspective of the language model. By simply extrapolating the context length of the language backbone, we enable LMMs to comprehend orders of magnitude more visual tokens without any video training. We call this phenomenon long context transfer and carefully ablate its properties. To effectively measure LMMs' ability to generalize to long contexts in the vision modality, we develop V-NIAH (Visual Needle-In-A-Haystack), a purely synthetic long vision benchmark inspired by the language model's NIAH test. Our proposed Long Video Assistant (LongVA) can process 2000 frames or over 200K visual tokens without additional complexities. With its extended context length, LongVA achieves state-of-the-art performance on Video-MME among 7B-scale models by densely sampling more input frames.",2025,0.6545452165290122,0.6509123463539908,0.5333333333333333,0.5,454a8777-2677-4d8d-8974-b83ca1e33ff8,0,"[0.5, 0.5, 0.5, 0.625, 0.875]","[1.0, 0.95, 1.0, 0.9, 0.95]",Long Context Transfer,0.5863066289138333
547456,From an LLM Swarm to a PDDL-empowered Hive: Planning Self-executed Instructions in a Multi-modal Jungle,"In response to the call for agent-based solutions that leverage the ever-increasing capabilities of the deep models' ecosystem, we introduce a comprehensive solution for selecting appropriate models and subsequently planning a set of atomic actions to satisfy the end-users' instructions.

Our system, Hive, operates over sets of models and, upon receiving natural language instructions, schedules and executes, explainable plans of atomic actions. These actions can involve one or more of the available models to achieve the overall task, while respecting end-users specific constraints. Hive is able to plan complex chains of actions while guaranteeing explainability, using an LLM-based formal logic backbone empowered by PDDL operations. We introduce the MuSE benchmark in order to offer a comprehensive evaluation of the multi-modal capabilities of agent systems. Our findings show that our framework redefines the state-of-the-art for task selection, outperforming other competing systems that plan operations across multiple models while offering transparency guarantees while fully adhering to user constraints.",2025,0.5909092396693675,0.5899342396169823,0.5333333333333333,0.5,bf71c2e0-053e-4fbb-a655-2dd8103acb59,1,"[0.5, 0.5, 0.625]","[0.9, 0.9, 0.9]",PDDL-empowered Hive,0.5416666666666667
547458,Connecting Solutions and Boundary Conditions/Parameters Directly: Solving PDEs in Real Time with PINNs,"Physics-Informed Neural Networks (PINNs) have proven to be important tools for solving both forward and inverse problems of partial differential equations (PDEs). However, PINNs face the retraining challenge in which neural networks need to be retrained once the parameters, or boundary/initial conditions change. To address this challenge, meta-learning PINNs train a meta-model across a range of PDE configurations, and the PINN models for new PDE configurations are then generated directly or fine-tuned from the meta-model. Meta-learning PINNs are confronted with either the issue of generalizing to significantly new PDE configurations or the time-consuming process of fine-tuning. By analyzing the mathematical structure of various PDEs, in this paper we establish the direct and mathematically sound connections between PDE solutions and boundary/initial conditions, sources and parameters. The learnable functions in these connections are trained offline in less than 1 hour in most cases. With these connections, the solutions for new PDE configurations can be obtained directly and vice versa, without retraining and fine-tuning at all. Our experimental results indicate that our methods are comparable to vanilla PINNs in terms of accuracy in forward problems, yet at least 400 times faster than them (even over 800 times faster for variable initial/source problems). In inverse problems, our methods are much more accurate than vanilla PINNs while being 80 times faster. Compared with meta-learning PINNs, our methods are much more accurate and about 20 times faster than fine-tuning. Our inference time is less than half a second in forward problems, and at most 3 seconds in inverse problems (less than half a second for variable initial/source problems of linear PDEs). Our code will be made publicly available upon acceptance.",2025,0.5795452438017296,0.5790431526647223,0.5333333333333333,0.5,a68d1900-0862-44cf-a903-0b7f25fcbfb2,0,"[0.25, 0.5, 0.5, 0.875]","[0.95, 0.95, 0.9, 0.95]",Meta-learning PINNs,0.5331365628042843
547459,Navigating Neural Space: Revisiting Concept Activation Vectors to Overcome Directional Divergence,"With a growing interest in understanding neural network prediction strategies, Concept Activation Vectors (CAVs) have emerged as a popular tool for modeling human-understandable concepts in the latent space.
Commonly, CAVs are computed by leveraging linear classifiers optimizing the *separability* of latent representations of samples with and without a given concept. However, in this paper we show that such a separability-oriented computation leads to solutions, which may diverge from the actual goal of precisely modeling the concept direction.
This discrepancy can be attributed to the significant influence of distractor directions, i.e., signals unrelated to the concept, which are picked up by filters (i.e., weights) of linear models to optimize class-separability.
To address this, we introduce *pattern-based CAVs*, solely focussing on concept signals, thereby providing more accurate concept directions.
We evaluate various CAV methods in terms of their alignment with the true concept direction and their impact on CAV applications, including concept sensitivity testing and model correction for shortcut behavior caused by data artifacts. 
We demonstrate the benefits of pattern-based CAVs using the Pediatric Bone Age, ISIC2019, and FunnyBirds datasets with VGG, ResNet, ReXNet, EfficientNet, and Vision Transformer as model architectures.",2025,0.8181815206612653,0.8186960755057276,0.8,0.625,5c8a134b-9fcf-4fcc-9b63-e743126f9ea2,1,"[0.625, 0.625, 0.875, 0.875]","[0.9, 0.9, 0.9, 0.95]",pattern-based CAVs,0.7585825027685452
547470,Towards Continual Domain Adaptation of Vision-language Models,"Large-scale vision-language models have achieved remarkable performance on various downstream tasks. {Nevertheless, how to efficiently adapt vision-language models to new data distributions without re-training, \ie,  domain incremental learning (DIL) of vision-language models, is still under-explored. Existing DIL methods for single modality are either not applicable to multi-modal settings or need exemplar buffers to store previous samples to avoid catastrophic forgetting, which is not memory-efficient.} To address these limitations, we propose an exemplar-free paradigm to improve DIL of vision-language models based on prompt-tuning. We theoretically analyze and decompose the problem into two optimization objectives. Guided by the theoretical insights, we propose a novel framework named {M}ultimodal {C}ontinual {D}omain {A}daptation (MCDA), which incorporates two strategies: Multimodal Domain Alignment (MDA) and Maximum Softmax Gating (MSG). MDA enhances cross-domain performance by aligning visual and language representation spaces, while MSG improves the accuracy of domain identification by gating through Softmax probability. Extensivev experimental results demonstrate that our method outperforms current state-of-the-art approaches.",2025,0.5454543471075102,0.5445543476130005,0.5333333333333333,0.5,3a38af28-ab17-44db-94f9-e1b54fef790b,0,"[0.5, 0.5, 0.5, 0.5]","[0.95, 0.95, 0.9, 0.95]",Multimodal Continual Domain Adaptation,0.4999999999999999
547482,Topograph: An Efficient Graph-Based Framework for Strictly Topology Preserving Image Segmentation,"Topological correctness plays a critical role in many image segmentation tasks, yet most networks are trained using pixel-wise loss functions, such as Dice, neglecting topological accuracy. Existing topology-aware methods often lack robust topological guarantees, are limited to specific use cases, or impose high computational costs. 
In this work, we propose a novel, graph-based framework for topologically accurate image segmentation that is both computationally efficient and generally applicable. Our method constructs a component graph that fully encodes the topological information of both the prediction and ground truth, allowing us to efficiently identify topologically critical regions and aggregate a loss based on local neighborhood information. Furthermore, we introduce a strict topological metric capturing the homotopy equivalence between the union and intersection of prediction-label pairs. We formally prove the topological guarantees of our approach and empirically validate its effectiveness on binary and multi-class datasets, demonstrating state-of-the-art performance with up to fivefold faster loss computation compared to persistent homology methods.",2025,0.886363314049704,0.8924635856507751,0.9333333333333332,0.875,d02b9465-eb46-4696-939c-e1e9f02c6088,1,"[0.625, 0.875, 0.875, 0.875]","[0.8, 0.95, 0.9, 0.95]",Topologically Accurate Segmentation,0.8367809170480779
547501,Adversarial Guided Diffusion Models for Adversarial Purification,"Diffusion model (DM) based adversarial purification (AP) has proven to be a powerful defense method that can remove adversarial perturbations and generate a purified example without threats. In principle, the pre-trained DMs can only ensure that purified examples conform to the same distribution of the training data, but it may inadvertently compromise the semantic information of input examples, leading to misclassification of purified examples. Recent advancements introduce guided diffusion techniques to preserve semantic information while removing the perturbations. However, these guidances often rely on distance measures between purified examples and diffused examples, which can also preserve perturbations in purified examples. To further unleash the robustness power of DM-based AP, we propose an adversarial guided diffusion model (AGDM) by introducing a novel adversarial guidance that contains sufficient semantic information but does not explicitly involve adversarial perturbations. The guidance is modeled by an auxiliary neural network obtained with adversarial training, considering the distance in the latent representations rather than at the pixel-level values. Extensive experiments are conducted on CIFAR-10, CIFAR-100 and ImageNet to demonstrate that our method is effective for simultaneously maintaining semantic information and removing the adversarial perturbations. In addition, comprehensive comparisons show that our method significantly enhances the robustness of existing DM-based AP, with an average robust accuracy improved by up to 7.30% on CIFAR-10. The code will be available upon acceptance.",2025,0.3409089669421938,0.3429984469310006,0.2666666666666666,0.25,df5ea128-9453-49ca-a0a5-83594880221f,0,"[0.25, 0.25, 0.25, 0.5]","[0.95, 0.95, 0.95, 1.0]",Adversarial Guided Diffusion Model,0.3274116347569955
547505,"Prune at the Clients, Not the Server: Accelerated Sparse Training in Federated Learning","In the recent paradigm of Federated Learning (FL), multiple clients train a shared model while keeping their local data private. Resource constraints of clients and communication costs pose major problems for training large models in FL. On the one hand, addressing the resource limitations of the clients, sparse training has proven to be a powerful tool in the centralized setting. On the other hand, communication costs in FL can be addressed by local training, where each client takes multiple gradient steps on its local data. Recent work has shown that local training can provably achieve the optimal accelerated communication complexity [Mishchenko et al., 2022]. Hence, one would like an accelerated sparse training algorithm. In this work we show that naive integration of sparse training and acceleration on the server fails, and how to fix it by letting the clients perform these tasks appropriately. We introduce Sparse-ProxSkip, our method developed for the nonconvex setting, inspired by RandProx [Condat and Richtárik, 2022], which provably combines sparse training and acceleration in the convex setting. We demonstrate the good performance of Sparse-ProxSkip in extensive experiments.",2025,0.4363634776860081,0.4333008052869692,0.5333333333333333,0.5,e8a27bb6-b624-4b9c-bc13-a40c4f7a5cea,0,"[0.25, 0.25, 0.5, 0.5, 0.5]","[0.95, 0.95, 0.9, 0.95, 0.9]",Sparse-ProxSkip,0.3899777732875328
547519,Domain Guidance: A Simple Transfer Approach for a Pre-trained Diffusion Model,"Recent advancements in diffusion models have revolutionized generative modeling. However, the impressive and vivid outputs they produce often come at the cost of significant model scaling and increased computational demands. Consequently, building personalized diffusion models based on off-the-shelf models has emerged as an appealing alternative. In this paper, we introduce a novel perspective on conditional generation for transferring a pre-trained model. From this viewpoint, we propose *Domain Guidance*, a straightforward transfer approach that leverages pre-trained knowledge to guide the sampling process toward the target domain. Domain Guidance shares a formulation similar to advanced classifier-free guidance, facilitating better domain alignment and higher-quality generations. We provide both empirical and theoretical analyses of the mechanisms behind Domain Guidance. Our experimental results demonstrate its substantial effectiveness across various transfer benchmarks, achieving over a 19.6\% improvement in FID and a 23.4\% improvement in FD$_\text{DINOv2}$ compared to standard fine-tuning. Notably, existing fine-tuned models can seamlessly integrate Domain Guidance to leverage these benefits, without additional training. Code is available at this repository: https://github.com/thuml/DomainGuidance.",2025,0.772726628099408,0.7730722231540152,0.6666666666666666,0.625,19cfa13a-b5f3-4c8c-904e-416b4e4f9178,1,"[0.625, 0.625, 0.875]","[0.95, 0.9, 0.95]",Domain Guidance,0.7151788671745611
547523,MVLight: Relightable Text-to-3D Generation via Light-conditioned Multi-View Diffusion,"Recent advancements in text-to-3D generation, building on the success of high-performance text-to-image generative models, have made it possible to create imaginative and richly textured 3D objects from textual descriptions. However, a key challenge remains in effectively decoupling light-independent and lighting-dependent components to  enhance the quality of generated 3D models and their relighting performance. In this paper, we present MVLight, a novel light-conditioned multi-view diffusion model that explicitly integrates lighting conditions directly into the generation process. This enables the model to synthesize high-quality images that faithfully reflect the specified lighting environment across multiple camera views. By leveraging this capability to Score Distillation Sampling (SDS), we can effectively synthesize 3D models with improved geometric precision and relighting capabilities. We validate the effectiveness of MVLight through extensive experiments and a user study.",2025,0.443181657024852,0.4424504074355629,0.4,0.25,4e70f926-0d81-4cf7-88f7-ddfa93161f52,0,"[0.25, 0.25, 0.5, 0.625]","[0.95, 0.95, 0.95, 0.95]",MVLight,0.40625
547527,Gap Preserving Distillation by Building Bidirectional Mappings with A Dynamic Teacher,"Knowledge distillation aims to transfer knowledge from a large teacher model to a compact student counterpart, often coming with a significant performance gap between them. Interestingly, we find that a too-large performance gap can hamper the training process.
To alleviate this, we propose a **Gap Preserving Distillation (GPD)** method that trains an additional dynamic teacher model from scratch along with the student to maintain a reasonable performance gap. To further strengthen distillation, we develop a hard strategy by enforcing both models to share parameters. Besides, we also build the soft bidirectional mappings between them through ***Inverse Reparameterization (IR)*** and ***Channel-Branch Reparameterization (CBR)***.
IR initializes a larger dynamic teacher with approximately the same accuracy as the student to avoid a too large gap in early stage of training. CBR enables direct extraction of an effective student model from the dynamic teacher without post-training. 
In experiments, GPD significantly outperforms existing distillation methods on top of both CNNs and transformers, achieving up to 1.58\% accuracy improvement. 
Interestingly, GPD also generalizes well to the scenarios without a pre-trained teacher, including training from scratch and fine-tuning, yielding a large improvement of 1.80\% and 0.89\% on ResNet18, respectively.",2025,0.6818179338843877,0.6806929345162506,0.6666666666666666,0.625,91d2742d-f3be-419f-8320-6b058713e683,1,"[0.625, 0.625, 0.625]","[0.9, 0.95, 0.95]",Gap Preserving Distillation,0.625
547556,Agent Workflow Memory,"Despite the potential of language model-based agents to solve real-world tasks such as web navigation, current methods still struggle with long-horizon tasks with complex action trajectories. In contrast, humans can flexibly solve complex tasks by learning reusable task workflows from past experiences and using them to guide future actions. To build agents that can similarly benefit from this process, we introduce Agent Workflow Memory (AWM), a method for inducing commonly reused routines, i.e., workflows, and selectively providing workflows to the agent to guide subsequent generations. AWM flexibly applies to both offline and online scenarios, where agents induce workflows from training examples beforehand or from test queries on the fly. We experiment on two major web navigation benchmarks -- Mind2Web and WebArena -- that collectively cover 1000+ tasks from 200+ domains across travel, shopping, and social media, among others. AWM substantially improves the baseline results by 24.6% and 51.1% relative success rate on Mind2Web and WebArena while reducing the number of steps taken to solve WebArena tasks successfully. Furthermore, online AWM robustly generalizes in cross-task, website, and domain evaluations, surpassing baselines from 8.9 to 14.0 absolute points as train-test task distribution gaps widen.",2025,0.5181816297521347,0.5152769276679351,0.5333333333333333,0.5,743d1d80-a5df-456c-bf75-55263abd0794,0,"[0.25, 0.5, 0.5, 0.5, 0.625]","[0.95, 0.9, 0.95, 0.95, 0.9]",Agent Workflow Memory,0.4662305516265912
547558,Deep MMD Gradient Flow without adversarial training,"We propose a gradient flow procedure for generative modeling by transporting particles from an initial source distribution to a target distribution, where the gradient field on the particles is given by a noise-adaptive Wasserstein Gradient of the Maximum Mean Discrepancy (MMD). The noise adaptive MMD is trained on data distributions corrupted by increasing levels of noise, obtained via a forward diffusion process, as commonly used in denoising diffusion probabilistic models. The result is a generalization of MMD Gradient Flow, which we call Diffusion-MMD-Gradient Flow or DMMD. The divergence training procedure is related to discriminator training in Generative Adversarial Networks (GAN), but does not require adversarial training. We obtain competitive empirical performance in unconditional image generation on CIFAR10, MNIST, CELEB-A (64 x64) and LSUN Church (64 x 64). Furthermore, we demonstrate the validity of the approach when MMD is replaced by a lower bound on the KL divergence.",2025,0.6477270371901683,0.6486600695722634,0.6666666666666666,0.625,1b4bc94a-bb4b-41c2-b7db-3e3f297cc962,1,"[0.5, 0.625, 0.625, 0.625]","[0.8, 0.8, 0.9, 0.9]",Diffusion-MMD-Gradient Flow,0.6004287003610109
547604,Autoregressive Pretraining with Mamba in Vision,"The vision community has started to build with the recently developed state space model, Mamba, as the new backbone for a range of tasks. This paper shows that Mamba's visual capability can be significantly enhanced through autoregressive pretraining, a direction not previously explored. Efficiency-wise, the autoregressive nature can well capitalize on the Mamba's unidirectional recurrent structure, enabling faster overall training speed compared to other training strategies like mask modeling. Performance-wise, autoregressive pretraining equips the Mamba architecture with markedly higher accuracy over its supervised-trained counterparts and, more importantly, successfully unlocks its scaling potential to large and even huge model sizes. For example, with autoregressive pretraining, a base-size Mamba attains 83.2\% ImageNet accuracy, outperforming its supervised counterpart by 2.0\%; our huge-size Mamba, the largest Vision Mamba to date, attains 85.0\% ImageNet accuracy (85.5\% when finetuned with $384\times384$ inputs), notably surpassing all other Mamba variants in vision. The code is available at \url{https://github.com/OliverRensu/ARM}.",2025,0.6477270371901683,0.6480196736594706,0.6666666666666666,0.625,1a96a307-9c85-403a-be55-078bdc8942bb,1,"[0.5, 0.625, 0.625, 0.625]","[0.9, 0.95, 0.95, 0.95]",Mamba,0.5994096884128528
547620,Rethinking Softmax: Self-Attention with Polynomial Activations,"This paper challenges the conventional belief that softmax attention in transformers is effective primarily because it generates a probability distribution for attention allocation. Instead, we theoretically show that its success lies in its ability to implicitly regularize the Frobenius norm of the attention matrix during training. We then explore alternative activations that regularize the Froebnius norm of the attention matrix, demonstrating that certain polynomial activations can achieve this effect, making them suitable for attention-based architectures. Empirical results indicate these activations perform comparably or better than softmax across various computer vision and language tasks, suggesting new possibilities for attention mechanisms beyond softmax.",2025,0.4772725537190714,0.4764850541613754,0.4666666666666667,0.25,c7abe93b-a296-4242-ae72-06ddc2a8c7f8,0,"[0.25, 0.25, 0.625, 0.625]","[0.9, 0.95, 0.95, 0.9]",Polynomial Activations,0.4374999999999999
547624,ROOT DEFENCE STRATEGIES: ENSURING SAFETY OF LLM AT THE DECODER LEVEL,"Large language models (LLMs) have demonstrated immense utility across various industries. However, as LLMs advance, the risk of harmful outputs increases due to incorrect or malicious instruction prompts. While current methods effectively address jailbreak risks, they share common limitations: 1) Judging harmful responses from the prefill-level lacks utilization of the model's decoding outputs, leading to relatively lower effectiveness and robustness. 2) Rejecting potentially harmful responses based on a single evaluation can significantly impair the model's helpfulness. This paper examines the LLMs' capability to recognize harmful outputs, revealing and quantifying their proficiency in assessing the danger of previous tokens. Motivated by pilot experiment results, we design a robust defense mechanism at the decoding level. Our novel decoder-oriented, step-by-step defense architecture corrects harmful queries directly rather than rejecting them outright. We introduce speculative decoding to enhance usability and facilitate deployment to boost secure decoding speed. Extensive experiments demonstrate that our approach improves model security without compromising reasoning speed. Notably, our method leverages the model's ability to discern hazardous information, maintaining its helpfulness compared to existing methods.",2025,0.443181657024852,0.4481224855202999,0.4,0.25,c1cd8670-7c51-4484-8cef-49c795c5349c,0,"[0.25, 0.25, 0.5, 0.625]","[0.95, 0.8, 0.9, 0.95]",Speculative Decoding,0.4235846144113262
547627,MS-Diffusion: Multi-subject Zero-shot Image Personalization with Layout Guidance,"Recent advancements in text-to-image generation models have dramatically enhanced the generation of photorealistic images from textual prompts, leading to an increased interest in personalized text-to-image applications, particularly in multi-subject scenarios. However, these advances are hindered by two main challenges: firstly, the need to accurately maintain the details of each referenced subject in accordance with the textual descriptions; and secondly, the difficulty in achieving a cohesive representation of multiple subjects in a single image without introducing inconsistencies. To address these concerns, our research introduces the MS-Diffusion framework for layout-guided zero-shot image personalization with multi-subjects. This innovative approach integrates grounding tokens with the feature resampler to maintain detail fidelity among subjects. With the layout guidance, MS-Diffusion further improves the cross-attention to adapt to the multi-subject inputs, ensuring that each subject condition acts on specific areas. The proposed multi-subject cross-attention orchestrates harmonious inter-subject compositions while preserving the control of texts. Comprehensive quantitative and qualitative experiments affirm that this method surpasses existing models in both image and text fidelity, promoting the development of personalized text-to-image generation.",2025,0.6818179338843877,0.6806929345162506,0.6666666666666666,0.625,bea15b41-3803-4da0-b63b-00a8bc3a0089,1,"[0.625, 0.625, 0.625, 0.625, 0.625]","[0.9, 1.0, 0.9, 1.0, 1.0]",MS-Diffusion,0.625
547641,DailyDilemmas: Revealing Value Preferences of LLMs with Quandaries of Daily Life,"As users increasingly seek guidance from LLMs for decision-making in daily life, many of these decisions are not clear-cut and depend significantly on the personal values and ethical standards of people. We present DailyDilemmas, a dataset of 1,360 moral dilemmas encountered in everyday life. Each dilemma presents two possible actions, along with affected parties and relevant human values for each action. Based on these dilemmas, we gather a repository of human values covering diverse everyday topics, such as interpersonal relationships, workplace, and environmental issues. With DailyDilemmas, we evaluate LLMs on these dilemmas to determine what action they will choose and the values represented by these action choices. Then, we analyze values through the lens of five theoretical frameworks inspired by sociology, psychology, and philosophy, including the World Values Survey, Moral Foundations Theory, Maslow's Hierarchy of Needs, Aristotle's Virtues, and Plutchik's Wheel of Emotions. For instance, we find LLMs are most aligned with self-expression over survival in World Values Survey and care over loyalty in Moral Foundations Theory. Interestingly, we find substantial preference differences in models for some core values. For example, for truthfulness, Mixtral-8x7B neglects it by 9.7% while GPT-4-turbo selects it by 9.4%. We also study the recent guidance released by OpenAI (ModelSpec), and Anthropic (Constitutional AI) to understand how their designated principles reflect their models' actual value prioritization when facing nuanced moral reasoning in daily-life settings. Finally, we find that end users cannot effectively steer such prioritization using system prompts.",2025,0.8522724173554846,0.8536259695790159,0.9333333333333332,0.875,16a49715-2bb8-4856-9c26-37cf45dc8854,1,"[0.5, 0.875, 0.875, 0.875]","[0.9, 0.95, 0.95, 0.9]",DailyDilemmas,0.7932966321224028
547642,LoRA vs Full Fine-tuning: An Illusion of Equivalence,"Fine-tuning is a crucial paradigm for adapting pre-trained large language models to downstream tasks. Recently, methods like Low-Rank Adaptation (LoRA) have been shown to match the performance of fully fine-tuned models on various tasks with an extreme reduction in the number of trainable parameters. Even in settings where both methods learn similarly accurate models, \emph{are their learned solutions really equivalent?} 
To answer this, we study how different fine-tuning methods change pre-trained models by analyzing the model's weight matrices through the lens of their spectral properties. We find that full fine-tuning and LoRA yield weight matrices whose singular value decompositions exhibit very different structure; moreover, the fine-tuned models themselves show distinct generalization behaviors when tested outside the adaptation task's distribution. We first show that the weight matrices trained with LoRA have new, high-ranking singular vectors, which we call \emph{intruder dimensions}. Intruder dimensions do not appear during full fine-tuning. Second, we find that LoRA models with intruder dimensions, despite achieving similar performance to full fine-tuning on the target task, become worse models of the pre-training distribution and adapt less robustly to multiple tasks sequentially. Higher-rank, rank-stabilized LoRA models closely mirror full fine-tuning, even when performing on par with lower-rank LoRA models on the same tasks. These results suggest that models updated with LoRA and full fine-tuning inherently access different parts of the solution space, even when they perform equally on the fine-tuned distribution.
We conclude by examining why intruder dimensions appear in LoRA fine-tuned models, why they are undesirable, and how their effects can be minimized.",2025,0.5113634504132908,0.5082510874750322,0.5333333333333333,0.5,9f6117be-a9a6-4943-a4cb-b5364e78799e,0,"[0.25, 0.5, 0.5, 0.625]","[0.95, 0.95, 0.95, 0.9]",Intruder dimensions,0.4593171859785785
547646,Large Language Model Evaluation via Matrix Nuclear-Norm,"As large language models (LLMs) continue to evolve, efficient evaluation metrics are vital for assessing their ability to compress information and reduce redundancy. While traditional metrics like Matrix Entropy offer valuable insights, they are computationally intensive for large-scale models due to their $\( O(n^3) \)$ time complexity with Singular Value Decomposition (SVD). To mitigate this issue, we introduce the Matrix Nuclear-Norm, which not only serves as a metric to quantify the data compression proficiency of LLM but also provides a convex approximation of matrix rank to capture both predictive discriminability and diversity. By employing the $\( L_{1,2}\text{-norm} \)$ to further approximate the nuclear norm, we can effectively assess the model's information compression capabilities. This approach reduces the time complexity to $\( O(n^2) \)$ and eliminates the need for SVD computation. Consequently, the Matrix Nuclear-Norm achieves speeds 8 to 24 times faster than Matrix Entropy for the CEREBRAS-GPT model as sizes increase from 111M to 6.7B. This performance gap becomes more pronounced with larger models, as validated in tests with other models like Pythia. Additionally, evaluations on benchmarks and model responses confirm that our proposed Matrix Nuclear-Norm is a reliable, scalable, and efficient tool for assessing LLMs' performance, striking a balance between accuracy and computational efficiency.",2025,0.5795452438017296,0.5785889943388131,0.5333333333333333,0.5,7c38bc9b-3a1c-4575-8798-4caa3cd93553,0,"[0.5, 0.5, 0.5, 0.625]","[0.9, 0.9, 0.9, 0.9]",Matrix Nuclear-Norm,0.53125
547655,Enhancing Graph Neural Networks: A Mutual Learning Approach,"Knowledge distillation (KD) techniques have emerged as a powerful tool for transferring expertise from complex teacher models to lightweight student models, particularly beneficial for deploying high-performance models in resource-constrained devices. This approach has been successfully applied to graph neural networks (GNNs), harnessing their expressive capabilities to generate node embeddings that capture structural and feature-related information. In this study, we depart from the conventional KD approach by exploring the potential of collaborative learning among GNNs. In the absence of a pre-trained teacher model, we show that relatively simple and shallow GNN architectures can synergetically learn efficient models capable of performing better during inference, particularly in tackling multiple tasks. We propose a collaborative learning framework where ensembles of student GNNs mutually teach each other throughout the training process. We introduce an adaptive logit weighting unit to facilitate efficient knowledge exchange among models and an entropy enhancement technique to improve mutual learning. These components dynamically empower the models to adapt their learning strategies during training, optimizing their performance for downstream tasks. Extensive experiments conducted on three datasets each for node and graph classification demonstrate the effectiveness of our approach.",2025,0.4090907603306326,0.4159785314893997,0.4,0.25,2fbfbedd-4929-4f22-b946-69fc99311188,0,"[0.25, 0.25, 0.5, 0.5]","[0.9, 0.8, 0.9, 1.0]",Mutual Learning,0.4061649861549655
547659,Theoretical Insights into Fine-Tuning Attention Mechanism: Generalization and Optimization,"Large Language Models (LLMs), built on Transformer architectures, exhibit remarkable generalization across a wide range of tasks. However, fine-tuning these models for specific tasks remains resource-intensive due to their extensive parameterization.  
In this paper, we investigate two remarkable phenomena related to the attention mechanism during the fine-tuning of LLMs. The first phenomenon, termed “Unequal Importance of Attention Matrices,” highlights the impact of fine-tuning different weight matrices. It shows that optimizing the $\mathbf{W}_v$ matrix yields significantly better performance than optimizing the $\mathbf{W}_k$ matrix. Fine-tuning only the $\mathbf{W}_q$ and $\mathbf{W}_v$ matrices is computationally efficient while delivering results comparable to, or even better than fine-tuning all three matrices ($\mathbf{W}_q$, $\mathbf{W}_k$, and $\mathbf{W}_v$). The second phenomenon, “Attention Matrices with Customized Learning Rate Leads to Better Convergence,” emphasizes the importance of assigning distinct learning rates to these matrices. Specifically, a higher learning rate for the $\mathbf{W}_v$ matrix compared to $\mathbf{W}_q$ and $\mathbf{W}_k$ accelerates convergence and improves performance. Building on these insights, we propose a new strategy that improves fine-tuning efficiency in terms of both storage and time. Experimental results on benchmark datasets validate the effectiveness of this approach, supporting our theoretical findings. Our analysis lays the theoretical groundwork for configuring and improving lightweight algorithms in LLMs fine-tuning.",2025,0.5113634504132908,0.5109858394087446,0.5333333333333333,0.5,ab14602b-4c36-41a8-a90c-04bd1522be2d,0,"[0.25, 0.5, 0.5, 0.625]","[0.9, 0.9, 0.95, 0.9]",Attention Matrices,0.4708956256921372
547661,Diffusion Models Are Real-Time Game Engines,"We present GameNGen, the first game engine powered entirely by a neural model that also enables real-time interaction with a complex environment over long trajectories at high quality. When trained on the classic game DOOM, GameNGen extracts gameplay and uses it to generate a playable environment that can interactively simulate new trajectories. GameNGen runs at 20 frames per second on a single TPU and remains stable over extended multi-minute play sessions. Next frame prediction achieves a PSNR of 29.4, comparable to lossy JPEG compression. Human raters are only slightly better than random chance at distinguishing short clips of the game from clips of the simulation, even after 5 minutes of auto-regressive generation. GameNGen is trained in two phases: (1) an RL-agent learns to play the game and the training sessions are recorded, and (2) a diffusion model is trained to produce the next frame, conditioned on the sequence of past frames and actions. Conditioning augmentations help ensure stable auto-regressive generation over long trajectories, and decoder fine-tuning improves the fidelity of visual details and text.",2025,0.7909088033058898,0.7931553874939827,0.9333333333333332,0.875,5c8064a7-47be-4f9d-96b8-a58417016751,1,"[0.5, 0.5, 0.875, 0.875, 0.875]","[0.9, 0.9, 0.95, 0.95, 0.9]",GameNGen,0.7408264199106573
547677,Stochastic Approximation to Contrastive Learning,"Contrastive learning is a powerful paradigm that has been crucial for self-supervised representation learning. While there is evidence for its effectiveness, these methods typically rely on arbitrary definitions of positive and negative pairs. Most existing contrastive learning methods require large batch sizes during training due to their rigid control over the tradeoff between the two contrastive terms. Consequences are that, substantial computational resources are wasted on negative pairs that provide minimal learning signals. To address this issue, this work present a novel method. We reformulate contrastive learning as a matrix approximation problem using I-divergence, a non-normalized form of Kullback-Leibler divergence. Our proposed objective function is decomposable across instance pairs, enabling the development of efficient stochastic approximation algorithms from neighbor embeddings which perform well with fewer negative samples. Additionally, we generalize the scaling factor beyond normalization, allowing it to adaptively emphasize positive pairs that carry more learning signals, thereby reducing the computational waste associated with negative pairs. Experimental results on visual representation learning benchmark datasets such as CIFAR and ImageNet demonstrate major improvements over other contrastive learning methods, particularly when using small batches and with only one negative pair.",2025,0.5113634504132908,0.5064355432800904,0.5333333333333333,0.5,74712fc5-48bb-4b9c-8781-e3d49075aab0,0,"[0.25, 0.5, 0.5, 0.625]","[1.0, 0.9, 0.9, 0.95]",I-divergence,0.445657430683863
547692,KAN: Kolmogorov–Arnold Networks,"Inspired by the Kolmogorov-Arnold representation theorem, we propose Kolmogorov-Arnold Networks (KANs) as promising alternatives to Multi-Layer Perceptrons (MLPs). While MLPs have fixed activation functions on nodes (""neurons''), KANs have learnable activation functions on edges (""weights''). KANs have no linear weights at all -- every weight parameter is replaced by a univariate function parametrized as a spline. We show that this seemingly simple change makes KANs outperform MLPs in terms of accuracy and interpretability, on small-scale AI + Science tasks. For accuracy, smaller KANs can achieve comparable or better accuracy than larger MLPs in function fitting tasks. Theoretically and empirically, KANs possess faster neural scaling laws than MLPs. For interpretability, KANs can be intuitively visualized and can easily interact with human users. Through two examples in mathematics and physics, KANs are shown to be useful ``collaborators'' helping scientists (re)discover mathematical and physical laws. In summary, KANs are promising alternatives for MLPs. Despite the slow training of KANs, their improved accuracy and interpretability show the potential to improve today's deep learning models which rely heavily on MLPs. More research is necessary to make KANs' training more efficient.",2025,0.8454542380166408,0.8417165659967196,0.9333333333333332,0.875,795fbfd5-6b03-47e4-b575-dd9600cbd007,1,"[0.625, 0.625, 0.875, 0.875, 0.875]","[0.95, 0.95, 0.95, 0.9, 0.9]",Kolmogorov-Arnold Networks,0.7649777732875328
547727,UniRiT: Towards Few-Shot Non-Rigid Point Cloud Registration,"Non-rigid point cloud registration is a critical challenge in 3D scene understanding, particularly in surgical navigation. Although existing methods achieve excellent performance when trained on large-scale, high-quality datasets, these datasets are prohibitively expensive to collect and annotate, e.g., organ data in authentic medical scenarios. With insufficient training samples and data noise, existing methods degrade significantly since non-rigid patterns are more flexible and complicated than rigid ones, and the distributions across samples are more distinct, leading to higher difficulty in representation learning with few data.
In this work, we aim to deal with this challenging few-shot non-rigid point cloud registration problem. Based on the observation that complex non-rigid transformation patterns can be decomposed into rigid and small non-rigid transformations, we propose a novel and effective framework, UniRiT. UniRiT adopts a two-step registration strategy that first aligns the centroids of the source and target point clouds and then refines the registration with non-rigid transformations, thereby significantly reducing the problem complexity. To validate the performance of UniRiT on real-world datasets, we introduce a new dataset, MedMatch3D, which consists of real human organs and exhibits high variability in sample distribution. We further establish a new challenging benchmark for few-shot non-rigid registration. Extensive empirical results demonstrate that UniRiT achieves state-of-the-art performance on MedMatch3D, improving the existing best approach by 94.22\%.",2025,0.3409089669421938,0.3331812211522334,0.4,0.5,2293d1ca-ce8f-48f8-a38e-c55800bcb071,0,"[0.0, 0.25, 0.5, 0.5]","[1.0, 0.95, 0.9, 0.95]",UniRiT,0.2759034603478616
547732,Bidirectional Generative Retrieval with Multi-Modal LLMs for Text-Video Retrieval,"In recent years, multi-modal large language models (MLLMs) have shown outstanding advancement in various multi-modal understanding tasks by leveraging the powerful knowledge of large language models (LLMs). Extending MLLMs to text-video retrieval enables handling more complex queries with multiple modalities beyond simple uni-modal queries for traditional search engines. It also provides a new opportunity to incorporate search into a unified conversational system, but MLLM-based text-video retrieval has been less explored in the literature. To this end, we investigate MLLMs' capabilities in text-video retrieval as a generation task, namely, generative retrieval, in two directions. An intuitive direction is $\textit{content generation}$ that directly generates the content given a query. Another direction is $\textit{query generation}$, which generates the query given the content. Interestingly, we observe that in both text-to-video and video-to-text retrieval tasks, query-generation less suffers from the bias and significantly outperforms content-generation. In this paper, we propose a novel framework, Bidirectional Text-Video Generative Retrieval (BGR), that handles both text-to-video and video-to-text retrieval tasks by measuring the relevance using two generation directions. Our framework trains MLLMs by simultaneously optimizing two objectives, $\textit{i.e.}$, video-grounded text generation (VTG) and text-grounded video feature generation (TVG). At inference, our framework ensembles predictions by both generation directions. We also introduce a Prior Normalization, a simple plug-and-play module, to further alleviate the $\textit{prior bias}$ induced by the likelihood of uni-modal content data that often overwhelms the relevance between query and content. Our extensive experiments on multi-modal benchmarks demonstrate that BGR and Prior Normalization are effective in alleviating the prior bias, especially the text prior bias from LLMs' pretrained knowledge in MLLMs, achieving state-of-the-art performance.",2025,0.5181816297521347,0.5097954436048626,0.5333333333333333,0.25,53cda348-05a9-4e68-ba78-63c078294bf6,0,"[0.25, 0.25, 0.5, 0.5, 0.875]","[0.95, 1.0, 0.95, 0.9, 0.9]",Bidirectional Generative Retrieval,0.4393065075206958
547760,Robotouille: An Asynchronous Planning Benchmark for LLM Agents,"Effective asynchronous planning, or the ability to efficiently reason and plan over states and actions that must happen in parallel or sequentially, is essential for agents that must account for time delays, reason over diverse long-horizon tasks, and collaborate with other agents. While large language model (LLM) agents show promise in high-level task planning, current benchmarks focus primarily on short-horizon tasks and do not evaluate such asynchronous planning capabilities. We introduce Robotouille, a challenging benchmark environment designed to test LLM agents' ability to handle long-horizon asynchronous scenarios. Our synchronous and asynchronous datasets capture increasingly complex planning challenges that go beyond existing benchmarks, requiring agents to manage over-
lapping tasks and interruptions Our results show that ReAct (gpt-4o) achieves 47% on synchronous tasks but only 11% on asynchronous tasks, highlighting significant room for improvement. We further analyze failure modes, demonstrating the need for LLM agents to better incorporate long-horizon feedback and self-audit their reasoning during task execution.",2025,0.6363630413225304,0.6353130425122688,0.6666666666666666,0.25,be45365b-98c7-4dfe-89a1-f840babc446c,1,"[0.25, 0.625, 0.875]","[0.95, 0.95, 0.95]",Robotouille,0.5833333333333334
547778,CMamba: Channel Correlation Enhanced State Space Models for Multivariate Time Series Forecasting,"Recent advancements in multivariate time series forecasting have been propelled by Linear-based, Transformer-based, and Convolution-based models, with Transformer-based architectures gaining prominence for their efficacy in temporal and cross-channel mixing.
More recently, Mamba, a state space model, has emerged with robust sequence and feature mixing capabilities.
However, the suitability of the vanilla Mamba design for time series forecasting remains an open question, particularly due to its inadequate handling of cross-channel dependencies.
Capturing cross-channel dependencies is critical in enhancing the performance of multivariate time series prediction.
Recent findings show that self-attention excels in capturing cross-channel dependencies, whereas other simpler mechanisms, such as MLP, may degrade model performance.
This is counterintuitive, as MLP, being a learnable architecture, should theoretically capture both correlations and irrelevances, potentially leading to neutral or improved performance.
Diving into the self-attention mechanism, we attribute the observed degradation in MLP performance to its lack of data dependence and global receptive field, which result in MLP's lack of generalization ability.
Considering the powerful sequence modeling capabilities of Mamba and the high efficiency of MLP, the combination of the two is an effective strategy for solving multivariate time series prediction.
Based on the above insights, we introduce a refined Mamba variant tailored for time series forecasting.
Our proposed model, \textbf{CMamba}, incorporates a modified Mamba (M-Mamba) module for temporal dependencies modeling, a global data-dependent MLP (GDD-MLP) to effectively capture cross-channel dependencies, and a Channel Mixup mechanism to mitigate overfitting.
Comprehensive experiments conducted on seven real-world datasets demonstrate the efficacy of our model in improving forecasting performance.",2025,0.5795452438017296,0.5772276084697805,0.5333333333333333,0.5,289d7a79-121d-4b21-b267-52dcb2062197,0,"[0.5, 0.5, 0.5, 0.625]","[0.9, 0.95, 1.0, 0.9]",CMamba,0.5248149448356401
547779,Pharmacophore-based design by learning on voxel grids,"Ligand-based drug discovery (LBDD) relies on making use of known binders to a protein target to find structurally diverse molecules similarly likely to bind. This process typically involves a brute force search of the known binder (query) against a molecular library using some metric of molecular similarity. One popular approach overlays the pharmacophore-shape profile of the known binder to 3D conformations enumerated for each of the library molecules, computes overlaps, and picks a set of diverse library molecules with high overlaps. While this virtual screening workflow has had considerable success in hit diversification, scaffold hopping, and patent busting, it scales poorly with library sizes and restricts candidate generation to existing library compounds. Leveraging recent advances in voxel-based generative modelling, we propose a pharmacophore-based generative model and workflows that address the scaling and fecundity issues of conventional pharmacophore-based virtual screening. We introduce VoxCap, a voxel captioning method for generating SMILES strings from voxelised molecular representations. We propose two workflows as practical use cases as well as benchmarks for pharmacophore-based generation: de-novo design, in which we aim to generate new molecules with high pharmacophore-shape similarities to query molecules, and fast search, which aims to combine generative design with a cheap 2D substructure similarity search for efficient hit identification. Our results show that VoxCap significantly outperforms previous methods in generating diverse de-novo hits. When combined with our fast search workflow, VoxCap reduces computational time by orders of magnitude while returning hits for all query molecules, enabling the search of large libraries that are intractable to search by brute force.",2025,0.3409089669421938,0.3394621109976018,0.2666666666666666,0.25,d06fbe99-dd7f-4503-a9c0-115fec8b5b15,0,"[0.25, 0.25, 0.25, 0.5]","[0.9, 1.0, 1.0, 0.95]",VoxCap,0.3060102575817623
547792,EVF-SAM: Early Vision-Language Fusion For Text-Prompted Segment Anything Model,"Segment Anything Model (SAM) has attracted widespread attention for its superior interactive segmentation capabilities with visual prompts while lacking further exploration of text prompts. In this paper, we empirically investigate what text prompt encoders (e.g., CLIP or LLM) are good for adapting SAM for referring expression segmentation and introduce the Early Vision-language Fusion-based SAM (EVF-SAM). EVF-SAM is a simple yet effective referring segmentation method which exploits multimodal prompts (i.e., image and text) and comprises a pre-trained vision-language model to generate referring prompts and a SAM for segmentation. Surprisingly, we observe that: (1) multimodal prompts and (2) vision-language models with early fusion (e.g., BEIT-3) are beneficial for prompting SAM for accurate referring segmentation. Our experiments show that the proposed EVF-SAM based on BEIT-3 can obtain state-of-the-art performance on RefCOCO/+/g for referring expression segmentation and demonstrate the superiority of prompting SAM with early vision-language fusion. In addition, the proposed EVF-SAM with 1.32B parameters achieves remarkably higher performance while reducing nearly 82% of parameters compared to previous SAM methods based on large multimodal models.",2025,0.4363634776860081,0.4344846664386799,0.5333333333333333,0.5,c9035fa4-af38-45c1-ad45-d7d03b4554c6,0,"[0.25, 0.25, 0.5, 0.5, 0.5]","[0.95, 0.95, 0.95, 0.95, 0.9]",EVF-SAM,0.3952280161631711
547797,Continuous Spiking Graph ODE Networks,"Spiking Graph Networks (SGNs), as bio-inspired neural models that address energy consumption challenges for graph classification, have attracted considerable attention from researchers and the industry. 
However, SGNs are typically applied in static scenarios with real-valued inputs and cannot be directly utilized for dynamic prediction because of their limited capacity to handle dynamic real-valued features, denoted as architectural inapplicability. Moreover, they suffer from accuracy loss due to the inherently discrete nature of spike-based representations.
Inspired by recent graph ordinary differential equation (ODE) methods, we propose the framework named \textbf{C}ontinuous \textbf{S}piking \textbf{G}raph \textbf{O}DE Networks (\method{}), which leverages the advantages of graph ODE to address the architectural inapplicability, and employs high-order structures to solve the problem of information loss.
Specifically, \method{} replaces the high energy-consuming static SGNs with an efficient Graph ODE process by incorporating SGNs with graph ODE into a unified framework, thereby achieving energy efficiency.
Then, we derive a high-order spike representation capable of preserving more information. By integrating this with a high-order graph ODE, we propose the second-order \method{} to address the information loss challenge.
Furthermore, we prove that the second-order \method{} maintains stability during the dynamic graph learning.
Extensive experiments validate the superiority of the proposed \method{} in performance while maintaining low power consumption.",2025,0.4772725537190714,0.483196141941358,0.5333333333333333,0.5,ac030846-56c9-4bbc-88e1-12530a0f1f36,0,"[0.25, 0.5, 0.5, 0.5]","[0.8, 0.9, 0.95, 0.9]",Continuous Spiking Graph ODE,0.4589224331258185
547804,DC-Spin: A Speaker-invariant Speech Tokenizer For Spoken Language Models,"Spoken language models (SLMs) have gained increasing attention with advancements in text-based, decoder-only language models. SLMs process text and speech, enabling simultaneous speech understanding and generation. This paper presents Double-Codebook Speaker-invariant Clustering (DC-Spin), which aims to improve speech tokenization by bridging audio signals and SLM tokens. DC-Spin extracts speaker-invariant tokens rich in phonetic information and resilient to input variations, enhancing zero-shot SLM tasks and speech resynthesis. We propose a chunk-wise approach to enable streamable DC-Spin without retraining and degradation. Comparisons of tokenization methods (self-supervised and neural audio codecs), model scalability, and downstream task proxies show that tokens easily modeled by an n-gram LM or aligned with phonemes offer strong performance, providing insights for designing speech tokenizers for SLMs.",2025,0.5113634504132908,0.4930427736548963,0.5333333333333333,0.5,05addaf9-2acb-49d8-b7ad-ce2d8f7f2d9e,0,"[0.0, 0.5, 0.5, 0.875]","[0.95, 1.0, 0.95, 0.8]",DC-Spin,0.4187735856294468
547810,The Sky Is The Limit When Clustering Is Equated With Disentanglement,"Disentangled representation learning allows data to be mapped to a latent space where factors of variation can be individually manipulated. These factors define a direct notion of similarity between observations that naturally groups them into clusters with shared factors of variation. While this has been empirically shown to be effective on simple datasets, it is unclear how or when complex real-world data can be disentangled into representations that allow the same degree of manipulation and clustering. To advance the field of disentangled representation learning and clustering, we provide a new theoretical perspective by equating disentanglement with clustering by using factors of variation as a measure of element-wise similarity. This leads to a simple yet important observation: Instead of explicitly clustering the elements of a dataset, we can implicitly cluster them by learning to represent and generate the elements of each cluster. Furthermore, this observation reveals that implicit clusters have a lower bound because (I) explicit clusters are a subset of implicit clusters, and (II) implicit clusters can generate novel elements not present in the finite dataset through combinatorial generalization. Building on these insights, we derive an implicit neural clustering approach based on identifying factors of variation in the latent space. We validate our findings through experiments on synthetic image data and empirical evidence from related state-of-the-art works. This demonstrates the practical relevance of our approach and promising potential for synthesizing complete datasets from limited data, addressing data distribution gaps, improving interpretability in cluster analysis, enhancing SSL and classification tasks, and reducing data storage space.",2025,0.3636358677687754,0.3622037896626159,0.2666666666666666,0.25,b3800ed7-d1af-4fdb-839d-b866506f3bd0,0,"[0.25, 0.25, 0.25, 0.25, 0.5, 0.5]","[0.9, 0.9, 0.95, 0.9, 0.9, 0.9]",Implicit Neural Clustering,0.3294295428787306
547828,Interpretable Analysis and Reasoning Enhancement for LLMs via Cross-Generation Reasoning Trees,"Generating diverse reasoning paths by varying the context (such as demonstrations, prompts, instructions, etc) or sampling methods (such as top-k, top-p, beam-search, etc) and then selecting appropriate paths via majority voting or verifier-based strategies to enhance the reasoning capabilities of large language models (LLMs) is a commonly recognized approach. Although both different contexts and sampling techniques can generate diverse contents, using sampling methods alone does not significantly enhance the diversity of generations. Context variation, however, while fostering greater diversity in reasoning, can also introduce negative effects, which causes that switching contexts can not necessarily lead to proportional improvements in performance. Therefore, there is a need to investigate how context influences LLM generation and mitigate any adverse impacts. The primary challenge lies in the inability to conduct comparative studies once divergences occur in reasoning paths generated under different contexts. Specifically, once the predicted tokens at a given step differ, it becomes unclear whether subsequent tokens in the inference path are influenced by the context or the content already generated. In this paper, we propose a Cross-Generation Reasoning Tree (CGRT) algorithm for studying the impact of different contexts on LLM generation and enhancing LLMs' reasoning performance. Experimental findings reveal that, beyond enhancing interpretability, CGRT integrates the positive effects of both context and sampling strategies more effectively than previous approaches, leading to more rational inference paths. Experiments conducted on Llama2, Llama3, and Qwen demonstrate that, when generating an equivalent number of diverse inference paths, those produced via the ""reasoning tree"" method exhibit higher accuracy.",2025,0.4999994545456529,0.4959332681320261,0.5333333333333333,0.25,e2841036-bb35-40bf-9488-36086fbcf168,0,"[0.25, 0.5, 0.625]","[0.95, 0.95, 0.9]",Cross-Generation Reasoning Tree,0.4446422656508778
547837,A robust federated learning client selection with combinatorial data class representations and data augmentation,"The federated learning (FL) client selection scheme can effectively mitigate global model performance degradation caused by the random aggregation of clients with heterogeneous data. Simultaneously, research has exposed FL's susceptibility to backdoor attacks. However herein lies the dilemma, traditional client selection methods and backdoor defenses stand at odds, so their integration is an elusive goal.
To resolve this, we introduce Grace, a resilient client selection framework blending combinational class sampling with data augmentation. On the client side, Grace first proposes a local model purification method, fortifying the model's defenses by bolstering its innate robustness. After, local class representations are extracted for server-side client selection. This approach not only shields benign models from backdoor tampering but also allows the server to glean insights into local class representations without infringing upon the client's privacy.
On the server side, Grace introduces a novel representation combination sampling method. Clients are selected based on the interplay of their class representations, a strategy that simultaneously weeds out malicious actors and draws in clients whose data holds unique value.
Our extensive experiments highlight Grace's capabilities. The results are compelling: Grace enhances defense performance by over 50\% compared to state-of-the-art (SOTA) backdoor defenses, and, in the best case, improves accuracy by 3.19\% compared to SOTA client selection schemes. Consequently, Grace achieves substantial advancements in both security and accuracy.",2025,0.4090907603306326,0.4066481372973985,0.4,0.25,828d99d9-b96a-4d3e-b2d6-67189d983072,0,"[0.25, 0.25, 0.5, 0.5]","[0.95, 1.0, 1.0, 0.9]",Grace,0.368622339816218
547846,Decoupling the Class Label and the Target Concept in Machine Unlearning,"Machine unlearning as an emerging research topic for data regulations, aims to adjust a trained model to approximate a retrained one that excludes a portion of training data. Previous studies showed that class-wise unlearning is effective in forgetting the knowledge of target data, either through gradient ascent on the forgetting data or fine-tuning with the remaining data. However, while these methods are useful, they are insufficient as the class label and the target concept are often considered to coincide. In this work, we expand the scope by considering the label domain mismatch and investigate three problems beyond the conventional *all matched* forgetting, e.g., *target mismatch*, *model mismatch*, and *data mismatch* forgetting. We systematically analyze the new challenges in restrictively forgetting the target concept and also reveal crucial forgetting dynamics in the representation level to realize these tasks. Based on that, we propose a general framework, namely, *TARget-aware Forgetting* (TARF). It enables the additional tasks to actively forget the target concept while maintaining the rest part, by simultaneously conducting annealed gradient ascent on the forgetting data and selected gradient descent on the hard-to-affect remaining data. Empirically, various experiments under our newly introduced settings are conducted to demonstrate the effectiveness of our TARF.",2025,0.6477270371901683,0.6512575938103774,0.6666666666666666,0.625,cb7d5dc5-dc47-44ba-8581-bbc109316557,0,"[0.25, 0.625, 0.625, 0.875]","[0.9, 0.9, 0.95, 0.95]",TARget-aware Forgetting,0.6138277202072538
547861,Meta-Continual Learning of Neural Fields,"Neural Fields (NF) have gained prominence as a versatile framework for complex data representation. This work unveils a new problem setting termed Meta-Continual Learning of Neural Fields (MCL-NF) and introduces a novel strategy that employs a modular architecture combined with optimization-based meta-learning. Focused on overcoming the limitations of existing methods for continual learning of neural fields, such as catastrophic forgetting and slow convergence, our strategy achieves high-quality reconstruction with significantly improved learning speed. We further introduce Fisher Information Maximization loss for neural radiance fields (FIM-NeRF), which maximizes information gains at the sample level to enhance learning generalization, with proved convergence guarantee and generalization bound. We perform extensive evaluations across image, audio, video reconstruction, and view synthesis tasks on six diverse datasets, demonstrating our method’s superiority in reconstruction quality and speed over existing MCL and CL-NF approaches. Notably, our approach attains rapid adaptation of neural fields for city-scale NeRF rendering with reduced parameter requirement.",2025,0.6818179338843877,0.6806929345162506,0.6666666666666666,0.625,2ba7d166-67b7-4815-985f-197836fba989,1,"[0.625, 0.625, 0.625, 0.625]","[0.95, 0.95, 0.9, 0.8]",Meta-Continual Learning,0.6249999999999999
547866,SparsyFed: Sparse Adaptive Federated Learning,"Sparse training is often adopted in cross-device federated learning (FL) environments where constrained devices collaboratively train a machine learning model on private data by exchanging pseudo-gradients across heterogeneous networks. Although sparse training methods can reduce communication overhead and computational burden in FL, they are often not used in practice for the following key reasons: (1) data heterogeneity makes it harder for clients to reach consensus on sparse models compared to dense ones, requiring longer training; (2) methods for obtaining sparse masks lack adaptivity to accommodate very heterogeneous data distributions, crucial in cross-device FL; and (3) additional hyperparameters are required, which are notably challenging to tune in FL. This paper presents SparsyFed, a practical federated sparse training method that critically addresses the problems above. Previous works have only solved one or two of these challenges at the expense of introducing new trade-offs, such as clients’ consensus on masks versus sparsity pattern adaptivity. We show that SparsyFed simultaneously (1) can produce 95% sparse models, with negligible degradation in accuracy, while only needing a single hyperparameter, (2) achieves a per-round weight regrowth 200 times smaller than previous methods, and (3) allows the sparse masks to adapt to highly heterogeneous data distributions and outperform all baselines under such conditions.",2025,0.5999997818182612,0.5899342396169823,0.5333333333333333,0.5,e8a27bb6-b624-4b9c-bc13-a40c4f7a5cea,1,"[0.25, 0.5, 0.5, 0.625, 0.875]","[0.9, 1.0, 1.0, 0.95, 0.8]",SparsyFed,0.5240834311969913
547869,Explanation-Assisted Data Augmentation  for Graph Learning,"This work introduces a novel class of Data Augmentation (DA) techniques in the context of graph learning. In general, DA refers to techniques that enlarge the training set using label-preserving transformations.  Such techniques enable increased robustness and generalization, especially when the size of the original training set is limited. A fundamental idea in DA is that 
labels are invariant to domain-specific transformations of the input samples. 
However, it is challenging to identify such transformations in learning over graphical input domains due to the complex nature of graphs and the need to preserve their structural and semantic properties.
In this work, we propose explanation-assisted DA (EA-DA) for Graph Neural Networks (GNNs). A graph explanation is a subgraph which is an `almost sufficient' statistic of the input graph with respect to its classification label. Consequently, the classification label is invariant, with high probability, to perturbations of graph edges not belonging to its explanation subgraph. We develop EA-DA techniques leveraging such perturbation invariances. First, we show analytically that the sample complexity of explanation-assisted learning can be arbitrarily smaller than explanation-agnostic learning. On the other hand, we show that if the training set is enlarged using EA-DA techniques and
the learning rule does not distinguish between the augmented data and the original data, then the sample complexity can be worse than that of explanation-agnostic learning. We identify the main reason for the potential increase in sample complexity as the out-of-distribution nature of graph perturbations. 
We conclude that theoretically EA-DA may improve sample complexity, and that the learning rule must distinguish between the augmented data and the original data. Subsequently, we build upon these theoretical insights, introduce practically implementable EA-DA techniques and associated learning mechanisms, and perform extensive empirical evaluations.",2025,0.5999997818182612,0.5990097823743006,0.5333333333333333,0.5,f0937d0a-3a32-46a0-ad42-ca84535e3f0a,0,"[0.5, 0.5, 0.5, 0.625, 0.625]","[0.95, 0.8, 0.8, 0.9, 0.8]",Explanation-Assisted Data Augmentation,0.5485985026061116
547873,Arithmetic Without Algorithms: Language Models Solve Math with a Bag of Heuristics,"Do large language models (LLMs) solve reasoning tasks by learning robust generalizable algorithms, or do they memorize training data? To investigate this question, we use arithmetic reasoning as a representative task. Using causal analysis, we identify a subset of the model (a circuit) that explains most of the model's behavior for basic arithmetic logic and examine its functionality. By zooming in on the level of individual circuit neurons, we discover a sparse set of important neurons that implement simple heuristics. Each heuristic identifies a numerical input pattern and outputs corresponding answers. We hypothesize that the combination of these heuristic neurons is the mechanism used to produce correct arithmetic answers. To test this, we categorize each neuron into several heuristic types---such as neurons that activate when an operand falls within a certain range---and find that the unordered combination of these heuristic types is the mechanism that explains most of the model's accuracy on arithmetic prompts. Finally, we demonstrate that this mechanism appears as the main source of arithmetic accuracy early in training. Overall, our experimental results across several LLMs show that LLMs perform arithmetic using neither robust algorithms nor memorization; rather, they rely on a ``bag of heuristics''.",2025,0.7909088033058898,0.7926293479941886,0.6666666666666666,0.625,df3b381e-093e-44e6-a4ab-097d6f183dd9,1,"[0.625, 0.625, 0.625, 0.875, 0.875]","[0.8, 0.95, 0.9, 0.95, 0.9]",Heuristic Neurons,0.7345389984670152
547877,What Do You See? Enhancing Zero-Shot Image Classification with Multimodal Large Language Models,"Large language models (LLMs) have been effectively used for many computer vision tasks, including image classification. In this paper, we present a simple yet effective approach for zero-shot image classification using multimodal LLMs. By employing multimodal LLMs, we generate comprehensive textual representations from input images. These textual representations are then utilized to generate fixed-dimensional features in a cross-modal embedding space. Subsequently, these features are fused together to perform zero-shot classification using a linear classifier. Our method does not require prompt engineering for each dataset; instead, we use a single, straightforward, set of prompts across all datasets. We evaluated our method on several datasets, and our results demonstrate its remarkable effectiveness, surpassing benchmark accuracy on multiple datasets. On average, our method achieved an accuracy gain of 4.1 percentage points, with an increase of 6.8 percentage points on the ImageNet dataset, compared to prior methods. Our findings highlight the potential of multimodal LLMs to enhance computer vision tasks such as zero-shot image classification, offering a significant improvement over traditional methods.",2025,0.4480514734358279,0.447607337107458,0.5333333333333333,0.25,95c2b42f-8c5b-4702-ad00-fd27b733a6d6,0,"[0.25, 0.25, 0.25, 0.5, 0.5, 0.5, 0.625]","[0.95, 1.0, 0.9, 0.8, 1.0, 0.95, 1.0]",Zero-Shot Classification,0.4163720807967211
547890,MARS: A Malignity-Aware Backdoor Defense in Federated Learning,"Federated Learning (FL) is a distributed paradigm aimed at protecting participant data privacy by exchanging model parameters to achieve high-quality model training. However, this distributed nature also makes FL highly vulnerable to backdoor attacks. Notably, the recently proposed state-of-the-art (SOTA) attack, 3DFed (SP2023), uses an indicator mechanism to determine whether the backdoor models have been accepted by the defender and adaptively optimizes backdoor models, rendering existing defenses ineffective. In this paper, we first reveal that the failure of existing defenses lies in the employment of empirical statistical measures that are loosely coupled with backdoor attacks. Motivated by this, we propose a Malignity-Aware backdooR defenSe (MARS) that leverages backdoor energy (BE) to indicate the malicious extent of each neuron. To amplify malignity, we further extract the most prominent BE values from each model to form a concentrated backdoor energy (CBE). Finally, a novel Wasserstein distance-based clustering method is introduced to effectively identify backdoor models. Extensive experiments demonstrate that MARS can defend against SOTA backdoor attacks and significantly outperforms existing defenses.",2025,0.5454543471075102,0.5445543476130005,0.6,0.625,828d99d9-b96a-4d3e-b2d6-67189d983072,0,"[0.25, 0.5, 0.625, 0.625]","[0.95, 0.95, 1.0, 0.9]",Malignity-Aware,0.5035779477394136
547905,E3D: Enhancing Sparsely-Supervised 3D Object Detector with Large Multimodal Models,"Recently, sparsely-supervised 3D object detection has gained great attention, achieving performance close to that of fully-supervised 3D objectors with only a few annotated instances. Nevertheless, these methods suffer challenges when the accurate labels are extremely limited. In this paper, we propose an Ehanced 3D object Detection strategy, termed E3D, explicitly utilizing the prior knowledge from Large Multimodal Models (LMMs) to enhance the feature discrimination capability of the 3D detector under sparse annotation settings. Specifically, we first develop a Confident Points Semantic Transfer (CPST) module that generates high-quality seed points through boundary-constrained center cluster selection. Based on these seed points, we introduce a Dynamic Cluster Pseudo-label Generation (DCPG) module that yields pseudo-supervision signals from the geometry shape of multi-scale neighbor points. Additionally, we design a Distribution Shape score (DS score) that chooses high-quality supervision signals for the initial training of the 3D detector. By utilizing E3D, existing leading sparsely-supervised CoIn++ is improved by an average of 11.63% under the annotation rate of 2%. Moreover, we have verified our E3D in the zero-shot setting, and the results demonstrate its performance exceeding that of the state-of-the-art methods. The code will be made publicly available.",2025,0.4636361950413836,0.4580497112772849,0.5333333333333333,0.5,519a486a-751e-472b-ab13-5f2b2a04fbbe,0,"[0.0, 0.5, 0.5, 0.5, 0.625]","[1.0, 0.95, 0.95, 0.95, 0.95]",E3D,0.3975231966477103
547957,Evading Data Contamination Detection for Language Models is (too) Easy,"The benchmark performance of large language models (LLMs) has a high impact on their popularity and is thus of great importance to many model providers. However, the reliability of such benchmark scores as a measure of model quality gets compromised if the model is contaminated with benchmark data. While recent contamination detection methods try to address this issue, they overlook the possibility of deliberate contamination by malicious model providers aiming to evade detection. We propose a categorization of model providers based on their (de)contamination practices and argue that malicious contamination is of crucial importance as it casts doubt on the reliability of public benchmarks. To study this issue more rigorously, we analyze current contamination detection methods based on their assumptions. This analysis reveals a significant vulnerability in existing approaches: they do not account for rephrased benchmark data used during training by malicious actors. We demonstrate how exploiting this gap can result in significantly inflated benchmark scores while completely evading current detection methods.",2025,0.443181657024852,0.4356434780904004,0.4,0.25,35e6a8aa-d85e-4953-9611-de25081942f0,0,"[0.25, 0.25, 0.5, 0.625]","[0.95, 1.0, 0.9, 0.9]",Malicious Contamination,0.3740747241781998
547961,Node-Level Topological Representation Learning on Point Clouds,"Topological Data Analysis (TDA) allows us to extract powerful topological, and higher-order information on the global shape of a data set or point cloud. Tools like Persistent Homology or the Euler Transform give a single complex description of the global structure of the point cloud. However, common machine learning applications like classification require point-level information and features to be available. In this paper, we bridge this gap and propose a novel method to extract node-level topological features from complex point clouds using discrete variants of concepts from algebraic topology and differential geometry. We verify the effectiveness of these topological point features (TOPF) on both synthetic and real-world data and study their robustness under noise.",2025,0.5909092396693675,0.5878714677482242,0.5333333333333333,0.5,a30114d6-5318-443b-903f-4871270df875,0,"[0.25, 0.5, 0.5, 0.5, 0.625, 0.875]","[1.0, 0.9, 0.95, 0.8, 0.9, 0.95]",Topological Point Features,0.5250691362073358
547979,Seeing the Whole in the Parts in Self-Supervised Representation Learning,"Recent successes in self-supervised learning (SSL) model spatial co-occurrences of visual features either by masking portions of an image or by aggressively cropping it. Here, we propose a new way to model spatial co-occurrences by aligning local representations (before pooling) with a global image representation. We present CO-SSL, a family of instance discrimination methods and show that it outperforms previous methods on several datasets, including ImageNet-1K where it achieves 71.5% of Top-1 accuracy with 100 pre-training epochs. CO-SSL is also more robust to noise corruption, internal corruption, small adversarial attacks, and large training crop sizes. Our analysis further indicates that CO-SSL learns highly redundant local representations, which offers an explanation for its robustness. Overall, our work suggests that aligning local and global representations may be a powerful principle of unsupervised category learning.",2025,0.443181657024852,0.4433707042830288,0.4,0.25,182d3cc4-df0e-408a-9d4e-cbb880693454,0,"[0.25, 0.25, 0.5, 0.625]","[0.9, 0.95, 0.9, 0.95]",CO-SSL,0.4102655440414509
547985,Harnessing Query Heterogeneity for Cost-Effective Proactive Caching in LLM Inference,"As Large Language Models (LLMs) significantly enhance the capabilities of AI systems, the increasing volume of query processing requests presents challenges for cost-effective inference, particularly due to repetitive queries that lead to unnecessary resource consumption and increased costs. Caching strategies are employed to store a small set of previous queries, enabling direct retrieval of repetitive queries without reprocessing by the LLMs. However, existing caching algorithms often assume uniform query lengths, simplifying cache selection to a top-$K$ problem, which is inadequate for real-world scenarios with heterogeneous lengths. To address this issue, we propose a bandit learning algorithm for proactive query caching in LLMs, specifically considering variable-sized queries. We cast the optimal cache query cache problem as a knapsack problem. Since the repetitive pattern and processing cost are unknown and has uncertainty, we cast the learning-to-cache problem as a bandit learning problem. Compared to conventional bandit learning frameworks, a new technical challenge is that the reward of an arm would not be observed if it is pulled. To tackle this, we propose an Lower confidence bound (LCB)-type algorithm, which we prove has a $\tilde{O}(\sqrt{T})$ order of regret and show that our regret does not deteriorate compared to previous results when incorporating a variable size setting. Furthermore, we demonstrate that our online cache policy effectively reduces the additional computational overhead typically associated with calculating the optimal cache.",2025,0.5795452438017296,0.572916916254076,0.6666666666666666,0.625,125e66f1-14a2-4d13-8bfd-ca31911de7e7,0,"[0.25, 0.625, 0.625, 0.625]","[0.95, 0.95, 0.8, 0.9]",Proactive Query Caching,0.5104110051966452
547998,GLoRA: Geometric Adaptive Ranks for Efficient LoRA Fine-Tuning,"Fine-tuning large language models is computationally intensive because it requires updating all parameters. Low-Rank Adaptation (LoRA) improves efficiency by modifying only a subset of weights but introduces a trade-off between expressivity and computational cost: lower ranks reduce resources but limit expressiveness, while higher ranks enhance expressivity at increased cost. Despite recent advances in adaptive LoRA techniques, existing methods fail to provide a theoretical basis for optimizing the trade-off between model performance and efficiency. We propose Geometric Low-Rank Adaptation (GLoRA), a novel framework that computes the intrinsic dimensionality of hidden state representations to adaptively select LoRA ranks. We demonstrate that the intrinsic dimension provides a lower bound for the optimal rank of LoRA matrices, allowing for a principled selection that balances efficiency and expressivity. GLoRA dynamically adjusts the rank for each layer based on the intrinsic dimensionality of its input and output representations, recognizing that not all model parameters equally impact fine-tuning. Empirical validation on multiple tasks shows that GLoRA consistently outperforms recent baselines within the same parameter budget.",2025,0.5113634504132908,0.5132795023208906,0.5333333333333333,0.5,9f6117be-a9a6-4943-a4cb-b5364e78799e,0,"[0.25, 0.5, 0.5, 0.625]","[0.9, 0.95, 0.9, 0.95]",GLoRA,0.4807966321243523
548007,Multi-Step Preference Optimization via Two-Player Markov Games,"Reinforcement Learning from Human Feedback (RLHF) has been highly successful in aligning large language models with human preferences. While prevalent methods like DPO have demonstrated strong performance, they frame interactions with the language model as a bandit problem, which limits their applicability in real-world scenarios where multi-turn conversations are common. Additionally, DPO relies on the Bradley-Terry model assumption, which does not adequately capture the non-transitive nature of human preferences. In this paper, we address these challenges by modeling the alignment problem as a two-player constant-sum Markov game, where each player seeks to maximize their winning rate against the other across all steps of the conversation. Our approach Multi-step Preference Optimization (MPO) is built upon the natural actor-critic framework. We further develop OMPO based on the optimistic online gradient descent algorithm. Theoretically, we provide a rigorous analysis for both algorithms on convergence and show that OMPO requires $\mathcal{O}(\epsilon^{-1})$ policy updates to converge to an $\epsilon$-approximate Nash equilibrium. We also validate the effectiveness of our method through experiments on the multi-turn conversations dataset in MT-bench-101.",2025,0.5795452438017296,0.5799874099034832,0.6666666666666666,0.625,4c56c1c6-ca7b-4375-82b0-5a662a2fa778,0,"[0.25, 0.625, 0.625, 0.625]","[0.9, 0.9, 0.9, 0.95]",Multi-step Preference Optimization,0.5376868770764119
548009,EffoVPR: Effective Foundation Model Utilization for Visual Place Recognition,"The task of Visual Place Recognition (VPR) is to predict the location of a query image from a database of geo-tagged images. Recent studies in VPR have highlighted the significant advantage of employing pre-trained foundation models like DINOv2 for the VPR task. However, these models are often deemed inadequate for VPR without further fine-tuning on VPR-specific data.
In this paper, we present an effective approach to harness the potential of a foundation model for VPR. We show that features extracted from self-attention layers can act as a powerful re-ranker for VPR, even in a zero-shot setting. Our method not only outperforms previous zero-shot approaches but also introduces results competitive with several supervised methods.
We then show that a single-stage approach utilizing internal ViT layers for pooling can produce global features that achieve state-of-the-art performance, with impressive feature compactness down to 128D. Moreover, integrating our local foundation features for re-ranking further widens this performance gap. Our method also demonstrates exceptional robustness and generalization, setting new state-of-the-art performance, while handling challenging conditions such as occlusion, day-night transitions, and seasonal variations.",2025,0.8181815206612653,0.8150638980071488,0.8,0.625,ec1c9aa8-bb20-49be-9991-c3230607c9d2,1,"[0.625, 0.625, 0.875, 0.875]","[0.95, 1.0, 0.9, 1.0]",EffoVPR,0.7436223398162181
548029,Binary Spiking Neural Networks as causal models,"In this paper, we provide a causal analysis of  binary spiking neural networks (BSNNs)
aimed at explaining their behaviors. 
We formally define a BSNN 
and   represent its  spiking activity
  as a binary causal model.
Thanks to this causal  representation, 
we are able to explain the output of the network
by leveraging  logic-based  methods. 
In particular,
we show that we  can successfully 
use a SAT  (Boolean satisfiability) solver to  compute 
  abductive explanations from this  binary causal model. 
To illustrate our approach, 
we trained the BSNN on the standard MNIST
dataset and applied our SAT-based  method  to
finding  abductive  explanations of  the network's classifications
based on pixel-level features. We also compared the found explanations against SHAP,  a popular 
method used in the area of explainable
AI to explain ``black box'' classifiers.
We show that, unlike SHAP,
our method guarantees that a found  explanation  does
not contain completely irrelevant features.",2025,0.6818179338843877,0.6806929345162506,0.6666666666666666,0.625,906950a3-9c06-4a2d-9b65-d161632d53c3,0,"[0.625, 0.625, 0.625, 0.625]","[0.9, 0.8, 1.0, 0.9]",Binary Causal Model,0.625
548044,LoRC: Low-Rank Compression for LLMs KV Cache with a Progressive Compression Strategy,"The Key-Value (KV) cache is a crucial component in serving transformer-based autoregressive large language models (LLMs), enabling faster inference by storing previously computed KV vectors. However, its memory consumption scales linearly with sequence length and batch size, posing a significant bottleneck in LLM deployment. Existing approaches to mitigate this issue include: (1) efficient attention variants integrated in upcycling stages, which requires extensive parameter tuning thus unsuitable to pre-trained LLMs; (2) KV cache compression at test time, primarily through token eviction policies, which often overlook inter-layer dependencies and can be task-specific.

This paper introduces an orthogonal approach to KV cache compression. We propose a low-rank approximation of  KV weight matrices, allowing for plug-in integration with existing transformer-based LLMs without model retraining. To effectively compress KV cache at the weight level, we adjust for layerwise sensitivity and introduce a progressive compression strategy, which is supported by our theoretical analysis on how compression errors accumulate in deep networks. Our method is designed to function without model tuning in upcycling stages or task-specific profiling in test stages. Extensive experiments with LLaMA models ranging from 8B to 70B parameters across various tasks show that our approach significantly reduces the GPU memory footprint while maintaining performance.",2025,0.5454543471075102,0.5427388034180588,0.6,0.625,251ec9f9-d540-472c-82de-24823fc271cf,0,"[0.25, 0.5, 0.625, 0.625]","[0.95, 0.95, 0.9, 0.95]",Low-Rank Approximation,0.4924537487828628
548066,Prompt Infection: LLM-to-LLM Prompt Injection within Multi-Agent Systems,"As Large Language Models (LLMs) grow increasingly powerful, multi-agent systems—where multiple LLMs collaborate to tackle complex tasks—are becoming more prevalent in modern AI applications. Most safety research, however, has focused on vulnerabilities in single-agent LLMs. These include prompt injection attacks, where malicious prompts embedded in external content trick the LLM into executing unintended or harmful actions, compromising the victim’s application. In this paper, we reveal a more dangerous vector: LLM-to-LLM prompt injection within multi-agent systems. We introduce Prompt Infection, a novel attack where malicious prompts self-replicate across interconnected agents, behaving much like a computer virus. This attack poses severe threats, including data theft, scams, misinformation, and system-wide disruption, all while propagating silently through the system. Our extensive experiments demonstrate that multi-agent systems are highly susceptible, even when agents do not directly share communications. To address this, we propose LLM Tagging, a defense mechanism that, when combined with existing safeguards, significantly mitigates infection spread. This work underscores the urgent need for advanced security measures as multi-agent LLM systems become more widely adopted.",2025,0.5727270644628857,0.5709042433852984,0.5333333333333333,0.5,609ad6a7-88f5-4cff-96de-9346627a006b,0,"[0.5, 0.5, 0.5, 0.5, 0.625]","[0.9, 0.95, 0.9, 1.0, 0.9]",Prompt Infection,0.5207046640631081
548079,Fiddler: CPU-GPU Orchestration for Fast Inference of Mixture-of-Experts Models,"Large Language Models (LLMs) with the Mixture-of-Experts (MoE) architectures have shown promising performance on various tasks. However, due to the huge model sizes, running them in resource-constrained environments where the GPU memory is not abundant is challenging. Some existing systems propose to use CPU resources to solve that, but they either suffer from the significant overhead of frequently moving data between CPU and GPU, or fail to consider distinct characteristics of CPUs and GPUs. This paper proposes Fiddler, a resource-efficient inference system for MoE models with limited GPU resources. Fiddler strategically utilizes CPU and GPU resources by determining the optimal execution strategy. Our evaluation shows that, unlike state-of-the-art systems that optimize for specific scenarios such as single batch inference or long prefill, Fiddler performs better in all scenarios. Compared against different baselines, Fiddler achieves 1.26 times speed up in single batch inference, 1.30 times in long prefill processing, and 11.57 times in beam search inference. The code of Fiddler is publicly available at https://github.com/efeslab/fiddler.",2025,0.727272826446245,0.7260728265202324,0.6666666666666666,0.5,fd521bdb-6d6c-4ee2-81d5-14f12f32fe28,1,"[0.5, 0.625, 0.875]","[0.95, 0.95, 0.95]",Fiddler,0.6666666666666667
548088,Gap-Aware Preference Optimization: Enhancing Model Alignment with Perception Margin,"Reinforcement learning from human feedback (RLHF) approaches are widely used for fine-tuning large language models (LLMs) to align with instructional preferences. However, traditional RLHF methods often rely on binary labels, which fail to capture the pairwise differences in human perception, leading to potential performance degradation.
To address this limitation, we introduce $\textbf{Gap-Aware Preference Optimization}$ (GaPO), a novel approach that integrates the degree of semantic gaps into preference optimization. By modifying the margin term in the loss function and replacing it with an estimated gap computed using general metrics, GaPO provides a new supervisory signal that explicitly highlights the nuances between preference pairs. This new signal helps the model allocate gradients more rationally during optimization, facilitating more effective learning from the preference data.
Experiments conducted with a strong base model, Llama-3-8B-Instruct, demonstrate that GaPO surpasses state-of-the-art methods on widely used benchmarks. Our best-performing model, GaPO-ROUGE\_L, achieves a win rate of 52.8\% on AlpacaEval 2.0, exceeding the baseline methods by 5.3 points.",2025,0.5113634504132908,0.5035265339551418,0.5333333333333333,0.5,e4396b6c-ac61-4d5a-bbfa-7a6802ffe22e,0,"[0.25, 0.5, 0.5, 0.625]","[0.95, 0.9, 1.0, 0.8]",Gap-Aware Preference Optimization,0.4481884353510484
548097,Statistical Guarantees for Approximate Stationary Points of Shallow Neural Networks,"Since statistical guarantees for neural networks are usually restricted to global optima of intricate objective functions, it is unclear whether these theories explain the performances of actual outputs of neural network pipelines. The goal of this paper is, therefore, to bring statistical theory closer to practice. We develop statistical guarantees for shallow linear neural networks that coincide up to logarithmic factors with the global optima but apply to stationary points and the points nearby. These results support the common notion that neural networks do not necessarily need to be optimized globally from a mathematical perspective. We then extend our statistical guarantees to shallow ReLU neural networks, assuming the first layer weight matrices are nearly identical for the stationary network and the target. More generally, despite being limited to shallow neural networks for now, our theories make an important step forward in describing the practical properties of neural networks in mathematical terms.",2025,0.4772725537190714,0.4764850541613754,0.4666666666666667,0.25,a7637c39-09e4-4b9b-b016-1a04629507ef,0,"[0.25, 0.25, 0.625, 0.625]","[0.9, 0.9, 0.9, 0.9]",Approximate Stationary Points,0.4375
548113,Continuity-Preserving  Convolutional Autoencoders for Learning Continuous Latent Dynamical Models from Images,"Continuous dynamical systems are cornerstones of many scientific and engineering disciplines.
While machine learning offers powerful tools to model these systems from trajectory data, challenges arise when these trajectories are captured as images, resulting in pixel-level observations that are discrete in nature.
Consequently, a naive application of a convolutional autoencoder can result in latent coordinates that are discontinuous in time.
To resolve this, we propose continuity-preserving convolutional autoencoders (CpAEs) to learn continuous latent states and their corresponding continuous latent dynamical models from discrete image frames. 
We present a mathematical formulation for learning dynamics from image frames, which illustrates issues with previous approaches and motivates our methodology based on promoting the continuity of convolution filters, thereby preserving the continuity of the latent states.
This approach enables CpAEs to produce latent states that evolve continuously with the underlying dynamics, leading to more accurate latent dynamical models.
Extensive experiments across various scenarios demonstrate the effectiveness of CpAEs.",2025,0.7499997272728265,0.7497206436196745,0.6666666666666666,0.625,8aae13a5-e278-4dfb-83bf-5a42cc9ff930,1,"[0.625, 0.625, 0.625, 0.875]","[0.9, 0.95, 0.8, 0.9]",Continuity-Preserving Convolutional Autoencoders,0.6884080715477494
548116,Understanding Distribution Alignment Through Category Separability In An Infant-Inspired Domain Adaptation Task,"We introduce a novel distribution shift, called the VI-Shift, that mimics the trade-off between object instances and viewpoints in the visual experience of infants. Motivated by findings in infant learning literature, we study this problem through the lens of domain adaptation, but without ImageNet pretraining. We show that the performances of two classic domain adaptation methods, Joint Adaptation Network (JAN) and Domain Adversarial Neural Networks (DANN), deteriorate without ImageNet pretraining. We hypothesize that the separability of source and target category clusters in the feature space plays a crucial role in the effectiveness of JAN. So, we propose 3 metrics to measure category separability and demonstrate that target separability in the pretrained network is strongly correlated with downstream JAN and DANN accuracy. Further, we propose two novel loss functions that increase target separability during pretraining by aligning the distribution of within-domain pairwise distances between the source and target distributions. Our experiments show that the application of these loss functions modestly improves downstream accuracy on unseen images from the target dataset.",2025,0.3409089669421938,0.3403464672581253,0.2666666666666666,0.25,dd39825f-cd7b-48cd-b874-62f88bfe6bdf,0,"[0.25, 0.25, 0.25, 0.5]","[0.95, 0.95, 0.95, 0.95]",VI-Shift,0.3125
548118,GraphRCG: Self-Conditioned Graph Generation,"Graph generation aims to create new graphs that closely align with a target graph distribution. Existing works often implicitly capture this distribution by aligning the output of a generator with each training sample. As such, the overview of the entire distribution is not explicitly captured and used for graph generation. In contrast, in this work, we propose a novel self-conditioned graph generation framework designed to explicitly model graph distributions and employ these distributions to guide the generation process. We first perform self-conditioned modeling to capture the graph distributions by transforming each graph sample into a low-dimensional representation and optimizing a representation generator to create new representations reflective of the learned distribution. Subsequently, we leverage these bootstrapped representations as self-conditioned guidance for the generation process, thereby facilitating the generation of graphs that more accurately reflect the learned distributions. We conduct extensive experiments on generic and molecular graph datasets. Our framework demonstrates superior performance over existing state-of-the-art graph generation methods in terms of graph quality and fidelity to training data.",2025,0.3409089669421938,0.3394621109976018,0.2666666666666666,0.25,a5d259e3-38b9-42da-98f4-8827901f332f,0,"[0.25, 0.25, 0.25, 0.5]","[0.95, 1.0, 0.95, 0.95]",Self-Conditioned Graph Generation,0.3075294550810014
548120,RedCodeAgent:  Automatic Red-teaming Agent against Code Agents,"LLM-based code agents, integrated with external tools like the Python interpreter, can interact with broad system environments and leverage code execution feedback to improve or self-debug generated code for better task-solving. However, as these code agents evolve rapidly in terms of capabilities, their increasing sophistication also amplifies security risks, such as generating or executing risky and buggy code. Traditional static safety benchmarks and manually designed red-teaming tools struggle to keep up with this rapid evolution, lacking the ability to adapt dynamically to the changing behaviors of code agents. To address these limitations, we propose RedCodeAgent, the first fully automated and adaptive red-teaming agent against given code agents. Equipped with red-teaming tools for function-calling and a novel memory module for accumulating successful attack experience, RedCodeAgent dynamically optimizes input prompts to jailbreak the target code agent for risky code execution. Unlike static benchmarks or red-teaming tools, RedCodeAgent autonomously adapts its attack strategies, making it a scalable solution to the growing challenge of testing increasingly sophisticated code agents. Experimental results show that compared to state-of-the-art LLM jailbreaking methods, RedCodeAgent achieves significantly higher attack success rates on the same tasks while maintaining high overall efficiency. By autonomously exploring and exploiting vulnerabilities of code agents, RedCodeAgent provides critical insights into the evolving security risks of code agents.",2025,0.4772725537190714,0.479282974399411,0.4666666666666667,0.25,da6874cc-d1ae-4706-937d-e17127b042c3,0,"[0.25, 0.25, 0.625, 0.625]","[0.9, 0.9, 0.9, 0.95]",RedCodeAgent,0.4503737541528239
548124,Subequivariant Morphology-Behavior Co-Evolution in 3D Environments,"The co-evolution of morphology and behavior in 3D space has garnered considerable interest in the field of embodied intelligence. 
While recent studies have highlighted the considerable benefits of geometric symmetry for tasks like learning to locomote, navigate, and explore in dynamic 3D environments, its role within co-evolution setup remains unexplored.
Existing benchmarks encounter several key issues: 1) the task lacks consideration for spatial geometric information; 2) the method lacks geometric symmetry to deal with the complexities in 3D environments.
In this work, we propose a novel setup, named Subequivariant Morphology-Behavior Co-Evolution in 3D Environments (3DS-MB), to address the identified limitations.
To be specific, we propose EquiEvo, which injects geometric symmetry, i.e., subequivariance, to construct dynamic, learnable local reference frames, enabling the joint policy to generalize to diverse task spatial structures, thereby improving co-evolution efficiency.
Then, we evaluate EquiEvo on the proposed environments, where our method consistently and significantly outperforms existing approaches in tasks such as locomotion navigation and adversarial scenarios.
Extensive experiments underscore the importance of subequivariance for the co-evolution of morphology and behavior, effective morphology-task mapping and robust morphology-behavior mapping.",2025,0.5727270644628857,0.5398602891365765,0.6666666666666666,0.625,9c0bad4a-0515-4ee0-bff1-22cf3919a898,0,"[0.0, 0.5, 0.625, 0.625, 0.875]","[1.0, 0.9, 0.8, 0.95, 0.7]",Subequivariant,0.4108184501563816
548149,HelpSteer2-Preference: Complementing Ratings with Preferences,"Reward models are critical for aligning models to follow instructions, and are typically trained following one of two popular paradigms: Bradley-Terry style or Regression style. However, there is a lack of evidence that either approach is better than the other, when adequately matched for data. This is primarily because these approaches require data collected in different (but incompatible) formats, meaning that adequately matched data is not available in existing public datasets. To tackle this problem, we release preference annotations (designed for Bradley-Terry training) to complement existing ratings (designed for Regression style training) in the HelpSteer2 dataset. To improve data interpretability, preference annotations are accompanied with human-written justifications. Using this data, we conduct the first head-to-head comparison of Bradley-Terry and Regression models when adequately matched for data. Based on insights derived from such a comparison, we propose a novel approach to combine Bradley-Terry and Regression reward modeling. A Llama-3.1-70B-Instruct model tuned with this approach scores 94.1 on RewardBench, emerging top of more than 140 reward models as of 1 Oct 2024. This reward model can then be used with REINFORCE algorithm (RLHF) to align an Instruct model to reach 85.0 on Arena Hard, which is No. 1 as of 1 Oct 2024.
We open-source this dataset (CC-BY-4.0 license) at https://huggingface.co/datasets/nvidia/HelpSteer2#preferences-new---1-oct-2024 and openly release the trained Reward and Instruct models at https://huggingface.co/nvidia/Llama-3.1-Nemotron-70B-Reward and https://huggingface.co/nvidia/Llama-3.1-Nemotron-70B-Instruct .",2025,0.6545452165290122,0.6523303658751751,0.6666666666666666,0.625,a5baca48-2438-4096-9b23-cabd1841a9d0,1,"[0.5, 0.625, 0.625, 0.625, 0.625]","[1.0, 0.95, 0.95, 0.95, 0.95]",HelpSteer2,0.5935348697994614
548170,Combating Dual Noise Effect in Spatial-temporal Forecasting via Information Bottleneck Principle,"Spatial-temporal forecasting plays a pivotal role in urban planning and computing. Although Spatial-Temporal Graph Neural Networks (STGNNs) excel in modeling spatial-temporal dynamics, they often suffer from relatively poor computational efficiency. Recently, Multi-Layer Perceptrons (MLPs) have gained popularity in spatial-temporal forecasting for their simplified architecture and better efficiency. However, existing MLP-based models can be susceptible to noise interference, especially when the noise can affect both input and target sequences in spatial-temporal forecasting on noisy data. To alleviate this impact, we propose _Robust Spatial-Temporal Information Bottleneck (RSTIB)_ principle. The RSTIB extends previous Information Bottleneck (IB) approaches by lifting the specific Markov assumption without impairing the IB nature. Then, by explicitly minimizing the irrelevant noisy information, the representation learning guided by RSTIB can be more robust against noise interference. Furthermore, the instantiation, RSTIB-MLP, can be seamlessly implemented with MLPs, thereby achieving efficient and robust spatial-temporal modeling. Moreover, a training regime is designed to handle the dynamic nature of spatial-temporal relationships by incorporating a knowledge distillation module to alleviate feature collapse and enhance model robustness under noisy conditions. Our extensive experimental results on six intrinsically noisy benchmark datasets from various domains show that the RSTIB-MLP runs much faster than state-of-the-art STGNNs and delivers superior forecasting accuracy across noisy environments, substantiating its robustness and efficiency.",2025,0.5727270644628857,0.5738317675580658,0.6666666666666666,0.625,02f0c070-b54e-4913-a45e-c35fb1401871,0,"[0.25, 0.5, 0.625, 0.625, 0.625]","[0.9, 0.95, 0.9, 1.0, 0.9]",Robust Spatial-Temporal Information Bottleneck,0.5360758066635595
548199,Uni-Map: Unified Camera-LiDAR Perception for Robust HD Map Construction,"High-definition (HD) map construction methods play a vital role in providing precise and comprehensive static environmental information essential for autonomous driving systems. The primary sensors used are cameras and LiDAR, with input configurations varying among camera-only, LiDAR-only, or camera-LiDAR fusion based on cost-performance considerations, while fusion-based methods typically perform the best. However, current methods face two major issues: high costs due to separate training and deployment for each input configuration, and low robustness when sensors are missing or corrupted. To address these challenges, we propose the Unified Robust HD Map Construction Network (Uni-Map), a single model designed to perform well across all input configurations. Our approach designs a novel Mixture Stack Modality (MSM) training scheme, allowing the map decoder to learn effectively from camera, LiDAR, and fused features. We also introduce a projector module to align Bird's Eye View features from different modalities into a shared space, enhancing representation learning and overall model performance. During inference, our model utilizes a switching modality strategy to adapt seamlessly to any input configuration, ensuring compatibility across various modalities. To evaluate the robustness of HD map construction methods, we designed 13 different sensor corruption scenarios and conducted extensive experiments comparing Uni-Map with state-of-the-art methods. Experimental results show that Uni-Map outperforms previous methods by a significant margin across both normal and corrupted modalities, demonstrating superior performance and robustness. Notably, our unified model surpasses independently trained camera-only, LiDAR-only, and camera-LiDAR MapTR models with a gain of 4.6, 5.6, and 5.6 mAP on the nuScenes dataset, respectively. The source code will be released.",2025,0.7159088305786071,0.7147275812420631,0.6666666666666666,0.625,33f44e11-39d7-485f-8648-c50ea9569ec2,0,"[0.5, 0.625, 0.625, 0.875]","[0.95, 0.95, 0.95, 0.95]",Uni-Map,0.65625
548210,Efficient Online Pruning and Abstraction for Imperfect Information Extensive-Form Games,"Efficiently computing approximate equilibrium strategies in large Imperfect Information Extensive-Form Games (IIEFGs) poses significant challenges due to the game tree's exponential growth. While pruning and abstraction techniques are essential for complexity reduction, existing methods face two key limitations: (i) Seamless integration of pruning with Counterfactual Regret Minimization (CFR) is nontrivial, and (ii) Pruning and abstraction approaches incur prohibitive computational costs, hindering real-world deployment. We propose Expected-Value Pruning and Abstraction (EVPA), a novel online framework that addresses these challenges through three synergistic components: (i) Expected value estimation using approximate Nash equilibrium strategies to quantify information set utilities, (ii) Minimax pruning before CFR to eliminate a large number of sub-optimal actions permanently, and (iii) Dynamic online information abstraction merging information sets based on their current and future expected values in subgames. Experiments on Heads-up No-Limit Texas Hold'em (HUNL) show EVPA outperforms DeepStack's replication and Slumbot with significant win-rate margins in multiple settings. Remarkably, EVPA requires only $1$\%-$2$\% of the solving time to reach an approximate Nash equilibrium compared to DeepStack's replication.",2025,0.6477270371901683,0.6502403662890364,0.6666666666666666,0.625,fd7f3a77-7108-4e93-b403-4b51b3a7d0d8,1,"[0.25, 0.625, 0.625, 0.875]","[0.95, 0.95, 0.9, 1.0]",Expected-Value Pruning and Abstraction,0.615626217565483
548213,Structured-Initialization Learning,"The emergence of large language models (LLMs) has revolutionized natural language processing, but their development and deployment face significant challenges in computational resources and environmental sustainability.
Traditional self-supervised learning (SSL) paradigms requiring extensive computational infrastructure and exhibiting slow convergence rates, leading to increased energy consumption and longer training durations.
While existing model fine-tuning techniques such as Low-Rank Adaptation (LoRA) are resource-intensive and fail to facilitate swift knowledge updates when integrating a mount of new data in model version iteration.
To mitigate these challenges, we introduce Sail, a novel method for accelerating the training of neural network models by leveraging knowledge from (publicly available) pre-trained models.
Our approach comprises two key components: (1) a parameter transformation technique that adjusts the dimensions of pre-trained model parameters to match the target architecture, and (2) a proximal parameter integration and retraining strategy that efficiently combines transformed parameters to initialize new models.
We formalize the concept of Proximal Parameter and provide theoretical guarantees for its convergence advantages.
Our approach achieves substantial reductions in training time and computational resources while maintaining or improving model performance on downstream tasks.
These results indicate that Sail provides a promising direction for the more efficient and accessible development of the deep learning community.
Our code will be made publicly available.",2025,0.772726628099408,0.774752717870749,0.6666666666666666,0.625,2d130021-1b90-4300-8593-c37e66c7a3bc,0,"[0.625, 0.625, 0.875]","[0.9, 0.9, 0.95]",Sail,0.7232497293395849
548224,Commit0: Library Generation from Scratch,"With the goal of benchmarking generative systems beyond expert software development ability, we introduce Commit0, a benchmark that challenges AI agents to write libraries from scratch. Agents are provided with a specification document outlining the library’s API as well as a suite of interactive unit tests, with the goal of producing an implementation of this API accordingly. The implementation is validated through running these unit tests. As a benchmark, Commit0 is designed to move beyond static one-shot code generation towards agents that must process long-form natural language specifications, adapt to multi-stage feedback, and generate code with complex dependencies. Commit0 also offers an interactive environment where models receive static analysis and execution feedback on the code they generate. Our experiments demonstrate that while current agents can pass some unit tests, none can yet fully reproduce full libraries. Results also show that interactive feedback is quite useful for models to generate code that passes more unit tests, validating the benchmarks that facilitate its use. We publicly release the benchmark, the interactive environment, and the leaderboard.",2025,0.772726628099408,0.7698865802204792,0.6666666666666666,0.625,4e721fab-4963-4eb3-8894-4c064b4b4e19,1,"[0.625, 0.625, 0.875]","[1.0, 0.95, 0.95]",Commit0,0.6997250119560018
548235,SPACT18: Spiking Human Action Recognition Benchmark Dataset with Complementary RGB and Thermal Modalities,"Spike cameras, bio-inspired vision sensors, asynchronously fire spikes by accumulating light intensities at each pixel, offering ultra-high energy efficiency and exceptional temporal resolution. Unlike event cameras, which record changes in light intensity to capture motion, spike cameras provide even finer spatiotemporal resolution and a more precise representation of continuous changes. In this paper, we introduce the first video action recognition (VAR) dataset using spike camera, alongside synchronized RGB and thermal modalities, to enable comprehensive benchmarking for Spiking Neural Networks (SNNs). By preserving the inherent sparsity and temporal precision of spiking data, our three datasets offer a unique platform for exploring multimodal video understanding and serve as a valuable resource for directly comparing spiking, thermal, and RGB modalities. This work contributes a novel dataset that will drive research in energy-efficient, ultra-low-power video understanding, specifically for action recognition tasks using spike-based data.",2025,0.5454543471075102,0.5392503882672499,0.6,0.625,98663e7c-5aa0-4e6c-adbb-ce841217ec95,0,"[0.25, 0.5, 0.625, 0.625]","[1.0, 1.0, 0.9, 0.95]",SPACT18,0.4742651947959608
548237,Think while You Generate: Discrete Diffusion with Planned Denoising,"Discrete diffusion has achieved state-of-the-art performance, outperforming or approaching autoregressive models on standard benchmarks. In this work, we introduce *Discrete Diffusion with Planned Denoising* (DDPD), a novel framework that separates the generation process into two models: a planner and a denoiser. At inference time, the planner selects which positions to denoise next by identifying the most corrupted positions in need of denoising, including both initially corrupted and those requiring additional refinement. This plan-and-denoise approach enables more efficient reconstruction during generation by iteratively identifying and denoising corruptions in the optimal order. DDPD outperforms traditional denoiser-only mask diffusion methods, achieving superior results on language modeling benchmarks such as *text8*, *OpenWebText*, and token-based generation on *ImageNet 256 × 256*. Notably, in language modeling, DDPD significantly reduces the performance gap between diffusion-based and autoregressive methods in terms of generative perplexity.  Code is available at [github.com/liusulin/DDPD](https://github.com/liusulin/DDPD).",2025,0.6477270371901683,0.6516496729406588,0.6666666666666666,0.625,e388b0fd-1afa-4e1b-944e-1e1106c2aba6,1,"[0.25, 0.625, 0.625, 0.875]","[0.9, 1.0, 0.9, 0.95]",Discrete Diffusion with Planned Denoising,0.614820266387282
548262,ARC-RL: Self-Evolution Continual Reinforcement Learning via Action Representation Space,"Continual Reinforcement Learning (CRL) is a powerful tool that enables agents to learn a sequence of tasks, accumulating knowledge learned in the past and using it for problemsolving or future task learning. However, existing CRL methods all assume that the agent’s capabilities remain static within dynamic environments, which doesn’t reflect realworld scenarios where capabilities evolve. This paper introduces *Self-Evolution Continual Reinforcement Learning* (SE-CRL), a new and realistic problem where the agent’s action space continually changes. It presents a significant challenge for RL agents: How can policy generalization across different action spaces be achieved? Inspired by the cortical functions that lead to consistent human behavior, we propose an **A**ction **R**epresentation **C**ontinual **R**einforcement **L**earning framework (ARC-RL) to address this challenge. Our framework builds a representation space for actions by self-supervised learning on transitions, decoupling the agent’s policy from the specific action space. For a new action space, the decoder of the action representation is expanded or masked for adaptation and regularized fine-tuned to improve the stability of the policy. Furthermore, we release a benchmark based on MiniGrid to validate the effectiveness of methods for SE-CRL. Experimental results demonstrate that our framework significantly outperforms popular CRL methods by generalizing the policy across different action spaces.",2025,0.3749998636364132,0.3702969563768403,0.2666666666666666,0.25,679dd84d-6c19-4e30-9ac9-e32cbfb7a05a,0,"[0.25, 0.25, 0.25, 0.625]","[0.95, 0.95, 0.95, 0.9]",Action Representation,0.3267709347614411
548275,"AutoCustomization: A Unified Framework for Effortless, Selective LLM Bias and Style Finetuning","Large language models are transforming the landscape of applications, with their influence poised to expand. One important practical challenge is how to selectively customize models to align with specific expectations, such as tone, formality, or underlying biases. To solve this task, we develop AutoCustomization. The key to our approach is leveraging the vast knowledge encoded in modern language models to construct fine-tuning datasets focused on a specific customization axis in contrast to prior methods, which depend primarily on tediously constructed libraries of prompts. AutoCustomization demonstrates several desirable properties. It is universally applicable to any bias axis (e.g., political, stylistic). It is efficient with small automatically generated datasets and short fine-tuning. It allows for precise monitoring of the resulting bias change with our BiasShift evaluation metric proven to be alligned with human perception, generalizable to held-out aspects, and selective in preserving other model capabilities. We verify AutoCustomization through human evaluation and show that it outperforms existing prompting techniques while being simpler.",2025,0.218181738843004,0.2178217390452002,0.2666666666666666,0.25,da3152f0-df4a-4ce9-93b8-3b52987d46dd,0,"[0.0, 0.25, 0.25, 0.25, 0.25]","[0.95, 0.95, 0.95, 0.95, 0.95]",AutoCustomization,0.2000000000000011
548279,Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources,"Large Language Models still struggle in challenging scenarios that leverage structured data, complex reasoning, or tool usage. In this paper, we propose Source2Synth: a new self-augmentation approach for teaching LLMs new skills that can be leveraged in low data regimes without relying on costly human annotations. Source2Synth takes as input a custom data source and produces synthetic data points with intermediate reasoning steps grounded in real-world sources.
Source2Synth improves the dataset quality by discarding low-quality generations based on their answerability.
We demonstrate the generality of this approach by applying it to two challenging domains:  we test reasoning abilities in multi-hop question answering (MHQA), and tool usage in tabular question answering (TQA).
Our method improves  performance by 25.51\% for TQA on WikiSQL and 22.57\% for MHQA on HotPotQA compared to the fine-tuned baselines.",2025,0.443181657024852,0.4410890215665304,0.4,0.25,d904b6e4-8fb1-46b6-99de-c1626a1c3617,0,"[0.25, 0.25, 0.5, 0.625]","[0.95, 0.95, 0.9, 0.95]",Source2Synth,0.4005903115871469
548288,Solving Composable Constraints for Inverse Design Tasks,"Inverse design tasks are an important category of problem in which we want to identify some input vector $x$ satisfying some desirable properties. In this paper we propose a mechanism for representing inequality constraints as Signed Distance Functions (SDFs). SDFs permit efficient projection of points into the solution region as well as providing a mechanism for composing constraints via boolean set operations. In this paper, we provide theoretical motivation for Signed Distance Functions (SDFs) as an implicit representation of inequality constraints. Next, we provide analysis demonstrating that SDFs can be used to efficiently project points into solution regions. Additionally, we propose two novel algorithms for computing SDFs for wide families of machine learning models. Finally, we demonstrate practical utility by performing conditional image generation using MNIST and CelebA datasets, and computational drug design using the ZINC-250K dataset. From the experimental results, we note that the composable constraints can reliably and efficiently compute solutions to complex inverse design tasks with deep learning models.",2025,0.3409089669421938,0.3282246874802599,0.4,0.5,e956bf7d-9abc-44ba-8716-52625e1f19cd,0,"[0.0, 0.25, 0.5, 0.5]","[1.0, 0.9, 0.95, 0.8]",Signed Distance Functions,0.2606889415829648
548306,Causal Graph Learning via Distributional Invariance of Cause-Effect Relationship,"This paper introduces a new framework for recovering causal graphs from observational data, leveraging the fact that the distribution of an effect, conditioned on its causes, remains invariant to changes in the prior distribution of those causes. This insight enables a direct test for potential causal relationships by checking the variance of their corresponding effect-cause conditional distributions across multiple downsampled subsets of the data. These subsets are selected to reflect different prior cause distributions, while preserving the effect-cause conditional relationships. Using this invariance test and exploiting an (empirical) sparsity of most causal graphs, we develop an algorithm that efficiently uncovers causal relationships with quadratic complexity in the number of observational features/variables, reducing the processing time by up to 25x compared to state-of-the-art methods. Our empirical studies on a diverse benchmark of large-scale datasets demonstrate that the developed algorithm consistently performs better or comparable to existing works while generally achieving better scalability.",2025,0.5454543471075102,0.540875338440527,0.6,0.625,be0f75c1-f5f9-401b-b14a-e02adbb424c3,0,"[0.25, 0.5, 0.625, 0.625]","[1.0, 0.8, 0.9, 1.0]",Distributional Invariance,0.4819195959994571
548307,Conformal Training with Reduced Variance,"Conformal prediction (CP) is a distribution-free framework for achieving probabilistic guarantees on black-box models. {CP} is generally applied to a model post-training. Conformal training is an approach that aims to optimize the CP efficiency during training. In this direction, ConfTr (Stutz et al, 2022) is a technique that seeks to minimize the expected prediction set size of a model by simulating {CP} in-between training updates. Despite its potential, we identify a strong source of sample inefficiency in ConfTr that leads to overly noisy estimated gradients, introducing training instability and limiting practical use. To address this challenge, we propose variance-reduced conformal training (VR-ConfTr), a method that incorporates a variance reduction technique in the gradient estimation of the ConfTr objective function. Through extensive experiments on various benchmark datasets, we demonstrate that VR-ConfTr consistently achieves faster convergence and smaller prediction sets compared to baselines.",2025,0.4772725537190714,0.473688223032035,0.5333333333333333,0.5,5ac9e47a-31b3-49ea-aaff-50aeeca1b4bb,0,"[0.25, 0.5, 0.5, 0.5]","[0.95, 0.9, 0.9, 0.9]",VR-ConfTr,0.4246262458471761
548309,Active Audio Cancellation with Multi-Band Mamba Network,"A novel deep learning approach for Active Audio Cancellation (AAC) is presented, which surpasses traditional Active Noise Cancellation (ANC) by effectively canceling any audio signal, regardless of its spectral content. We propose, for the first time, a deep learning approach to AAC using a novel multi-band Mamba architecture. This architecture partitions input audio into multiple frequency bands, allowing for precise anti-signal generation and enhanced phase alignment across frequencies, thereby improving overall cancellation performance. Additionally, we introduce an optimization-driven loss function that provides near-optimal supervisory signals for anti-signal generation. Our experimental results demonstrate substantial improvements over existing methods, achieving up to 7.2dB gain in ANC scenarios and up to 6.2dB improvement in AAC for voice audio signals, outperforming existing methods.",2025,0.6477270371901683,0.6471004659206998,0.6666666666666666,0.625,4a4fb0f8-7f5b-410c-8291-257416d0a199,0,"[0.25, 0.625, 0.625, 0.875]","[0.95, 0.95, 1.0, 0.95]",Multi-Band Mamba,0.5962352724594993
548313,Task-oriented Sequential Grounding in 3D Scenes,"Grounding natural language in physical 3D environments is essential for the advancement of embodied artificial intelligence. Current datasets and models for 3D visual grounding predominantly focus on identifying and localizing objects from static, object-centric descriptions. These approaches do not adequately address the dynamic and sequential nature of task-oriented grounding necessary for practical applications. In this work, we propose a new task: Task-oriented Sequential Grounding in 3D scenes, wherein an agent must follow detailed step-by-step instructions to complete daily activities by locating a sequence of target objects in indoor scenes. To facilitate this task, we introduce SG3D, a large-scale dataset containing 22,346 tasks with 112,236 steps across 4,895 real-world 3D scenes. The dataset is constructed using a combination of RGB-D scans from various 3D scene datasets and an automated task generation pipeline, followed by human verification for quality assurance. We adapted three state-of-the-art 3D visual grounding models to the sequential grounding task and evaluated their performance on SG3D. Our results reveal that while these models perform well on traditional benchmarks, they face significant challenges with task-oriented sequential grounding, underscoring the need for further research in this area.",2025,0.5113634504132908,0.505657919671699,0.5333333333333333,0.5,e2b89ad8-52f7-4c76-97a6-ae99fd85c611,0,"[0.25, 0.5, 0.5, 0.625]","[1.0, 1.0, 0.95, 0.9]",Task-oriented Sequential Grounding,0.4462600660050798
548317,Eliciting Human Preferences with Language Models,"Language models (LMs) can be directed to perform user- and context-dependent
tasks by using labeled examples or natural language prompts.
But selecting examples or writing prompts can be challenging---especially in tasks that require users to precisely articulate nebulous preferences or reason about complex edge cases. For such tasks, we introduce **Generative Active Task Elicitation (GATE)**, a method for using *LMs themselves* to guide the task specification process. GATE is a learning framework in which models elicit and infer human preferences through free-form, language-based interaction with users.
We identify prototypical challenges that users face when specifying preferences, and design three preference modeling tasks to study these challenges:
content recommendation, moral reasoning, and email validation.
In preregistered experiments, we show that LMs that learn to perform these tasks using GATE (by interactively querying users with open-ended questions) obtain preference specifications that are more informative than user-written prompts or examples. GATE matches existing task specification methods in the moral reasoning task, and significantly outperforms them in the content recommendation and email validation tasks. Users additionally report that interactive task elicitation requires less effort than prompting or example labeling and surfaces considerations that they did not anticipate on their own. Our findings suggest that LM-driven elicitation can be a powerful tool for aligning models to complex human preferences and values.",2025,0.7840906239670459,0.7903106355820524,0.8,0.875,7d96a094-b622-428e-bb14-f22448b07ffd,1,"[0.5, 0.625, 0.875, 0.875]","[0.9, 0.95, 1.0, 1.0]",Generative Active Task Elicitation,0.7541073365969396
548328,Dual Variance Reduction with Momentum for Imbalanced Black-Box Discrete Prompt Learning,"Black-box prompt learning has proven to be an effective approach for customizing large language models (LLMs) offered as services to address various downstream tasks. 
Within this domain, policy gradient-based methods have garnered substantial attention as a prominent approach for learning discrete prompts.
However, the highly imbalanced data distribution in the real world limits the applicability of such approaches by influencing LLMs' tendency to favor certain categories.
To tackle the challenge posed by imbalanced data, this paper pioneers the integration of pairwise AUC loss into the policy gradient optimization of discrete text prompts and proposes learning discrete prompts with doubly policy gradient.
Unfortunately, the doubly policy gradient estimation suffers from two variance components, resulting in unstable optimization.
As a further improvement, we propose (1) a novel unbiased variance-reduced doubly policy gradient estimator and (2) incorporating the STORM variance reduction technique. 
Ultimately, we introduce a novel momentum-based discrete prompt learning method with doubly policy gradient (mDP-DPG).
Crucially, we provide theoretical convergence guarantees for mDP-DPG within standard frameworks.
The experimental results show that mDP-DPG surpasses baseline approaches across diverse imbalanced text classification datasets, emphasizing the advantages of our proposed approach for tackling data imbalance.
Our code is available at the following URL: https://anonymous.4open.science/r/DPDPG-1ECB.",2025,0.6477270371901683,0.6457379909429721,0.6666666666666666,0.625,a326eb30-19ac-40d4-a2be-ab39409c4895,0,"[0.5, 0.625, 0.625, 0.625]","[0.95, 0.9, 0.95, 0.9]",mDP-DPG,0.5897344559585495
548338,SAMBLE: Learning Shape-Specific Sampling Strategies for Point Cloud Shapes with Sparse Attention Map and Adaptive Bin Partitioning,"Point cloud sampling plays a pivotal role in facilitating efficient analysis of large-scale point clouds. Recently, learning-to-sample methods have garnered growing interest from the community, particularly for their ability to be jointly trained with downstream tasks. However, previous learning-based sampling methods either lead to unrecognizable sampling patterns by generating a new point cloud or biased sampled results by focusing excessively on shape details. Moreover, they all fail to take the natural point distribution variations over different shapes into consideration and learn a similar sampling strategy for all point clouds. In this paper, we propose a Sparse Attention Map and Bin-based Learning method (termed SAMBLE) to learn shape-specific sampling strategies for point cloud shapes, striking a superior balance between the overall shape outline and intricate local details for the sampling process. In particular, we first propose sparse attention map by integrating both local and global information. Based on this, multiple point-wise sampling score computation methods are proposed and explored by leveraging heatmaps as a guiding tool. Subsequently, we introduce a binning strategy that partitions points within each point cloud based on these scores. Finally, additional learnable tokens are introduced during the attention computation phase to acquire sampling weights for each bin, thereby enabling the development of shape-specific sampling strategies for an optimized sampling process. Extensive experiments demonstrate that our method adeptly strikes a refined balance between sampling edge points for local details and preserving uniformity in the global shape, leading to superior performance across common point cloud downstream tasks and even in scenarios involving few-point cloud sampling.",2025,0.5454543471075102,0.5445543476130005,0.5333333333333333,0.5,5070074b-986f-40d4-beec-94e5e58fcd2a,0,"[0.5, 0.5, 0.5, 0.5]","[0.95, 0.95, 0.9, 0.9]",SAMBLE,0.5
548346,Rate/Distortion Constrained Model Quantization for Efficient Storage and Inference,"The proliferation of large pre-trained neural networks has recently revived research in both quantization of network weights (for faster inference), and in their
compression (to reduce file sizes). However, there has so far been little idea transfer between the two lines of research. In this paper, we combine techniques from
quantization and compression to propose an efficient and highly effective post-training compression method for large neural networks. Our method extends the
recently published quantization method OPTQ (Frantar et al., 2023) with a tunable
rate/distortion trade-off by introducing a cost per bit into OPTQ's rounding
operation. Crucially, we estimate the bit rate based on the predictive model used
in the state-of-the-art neural network compression method NNCodec (Becking
et al., 2023). In our experiments with several standard pre-trained networks from
the computer vision community, our method leads to significantly (up to 2.7x)
smaller file sizes than NNCodec at equal model performance, generally compressing to less than half a bit per network weight and implicitly pruning insignificant weights.
Additionally, and in contrast to NNcodec, our method offers the same opportunities for inference speed-ups as OPTQ. By proving that file size and inference
cost can be reduced simultaneously, we hope that our contribution shows a path
towards deploying large neural networks on end-user devices, alleviating privacy
concerns, regulatory constraints, and dependency on large service providers.",2025,0.5454543471075102,0.5463459314166472,0.6,0.625,8883b313-f07a-405b-81ed-2b49db41891c,0,"[0.25, 0.5, 0.625, 0.625]","[0.95, 0.9, 1.0, 0.95]",Rate/Distortion Trade-off,0.5104908653153146
548347,Learning Chaos In A Linear Way,"Learning long-term behaviors in chaotic dynamical systems, such as turbulent flows and climate modelling, is challenging due to their inherent instability and unpredictability. These systems exhibit positive Lyapunov exponents, which significantly hinder accurate long-term forecasting. As a result, understanding long-term statistical behavior is far more valuable than focusing on short-term accuracy. While autoregressive deep sequence models have been applied to capture long-term behavior, they often lead to exponentially increasing errors in learned dynamics. To address this, we shift the focus from simple prediction errors to preserving an invariant measure in dissipative chaotic systems. These systems have attractors, where trajectories settle, and the invariant measure is the probability distribution on attractors that remains unchanged under dynamics. Existing methods generate long trajectories of dissipative chaotic systems by aligning invariant measures, but it is not always possible to obtain invariant measures for arbitrary datasets. We propose the Poincaré Flow Neural Network (PFNN), a novel operator learning framework designed to capture behaviors of chaotic systems without any explicit knowledge of the invariant measure. PFNN employs an auto-encoder to map the chaotic system to a finite-dimensional feature space, effectively linearizing the chaotic evolution.  It then learns the linear evolution operators to match the physical dynamics by addressing two critical properties in dissipative chaotic systems: (1) contraction, the system’s convergence toward its attractors, and (2) measure invariance, trajectories on the attractors following a probability distribution invariant to the dynamics. 
Our experiments on a variety of chaotic systems, including Lorenz systems, Kuramoto-Sivashinsky equation and Navier–Stokes equation, demonstrate that PFNN has more accurate predictions and physical statistics compared to competitive baselines including the Fourier Neural Operator and the Markov Neural Operator.",2025,0.7159088305786071,0.7151817395679724,0.6666666666666666,0.625,fb80db25-9d51-4aea-9c4f-ff7a68b68c7a,1,"[0.5, 0.625, 0.625, 0.875]","[0.95, 0.95, 0.9, 0.95]",Poincaré Flow Neural Network,0.6581365628042842
548375,TANGO: Co-Speech Gesture Video Reenactment with Hierarchical Audio Motion Embedding and Diffusion Interpolation,"We present TANGO, a framework for generating co-speech body-gesture videos. Given a few-minute, single-speaker reference video and target speech audio, TANGO produces high-fidelity videos with synchronized body gestures. TANGO builds on Gesture Video Reenactment (GVR), which splits and retrieves video clips using a directed graph structure - representing video frames as nodes and valid transitions as edges. We address two key limitations of GVR: audio-motion misalignment and visual artifacts in GAN-generated transition frames. In particular, i) we propose retrieving gestures using latent feature distance to improve cross-modal alignment. To ensure the latent features could effectively model the relationship between speech audio and gesture motion, we implement a hierarchical joint embedding space (AuMoClip); ii) we introduce the diffusion-based model to generate high-quality transition frames. Our diffusion model, Appearance Consistent Interpolation (ACInterp), is built upon AnimateAnyone and includes a reference motion module and homography background flow to preserve appearance consistency between generated and reference videos. By integrating these components into the graph-based retrieval framework, TANGO reliably produces realistic, audio-synchronized videos and outperforms all existing generative and retrieval methods. Our code, pretrained models, and datasets are publicly available at https://github.com/CyberAgentAILab/TANGO.",2025,0.7159088305786071,0.7117118392649823,0.9333333333333332,0.875,53260ef8-3caa-4971-a942-11d28da28093,1,"[0.0, 0.875, 0.875, 0.875]","[1.0, 0.95, 1.0, 1.0]",TANGO,0.6412404701397713
548389,Palu: KV-Cache Compression with Low-Rank Projection,"Post-training KV-Cache compression methods typically either sample a subset of effectual tokens or quantize the data into lower numerical bit width. However, these methods cannot exploit redundancy in the hidden dimension of the KV tenors. This paper presents a hidden dimension compression approach called Palu, a KV-Cache compression framework that utilizes low-rank projection to reduce inference-time LLM memory usage. Palu decomposes the linear layers into low-rank matrices, caches compressed intermediate states, and reconstructs the full keys and values on the fly. To improve accuracy, compression rate, and efficiency, Palu further encompasses (1) a medium-grained low-rank decomposition scheme, (2) an efficient rank search algorithm, (3) low-rank-aware quantization compatibility enhancements, and (4) an optimized GPU kernel with matrix fusion. Extensive experiments with popular LLMs show that Palu compresses KV-Cache by 50% while maintaining strong accuracy and delivering up to 1.89× speedup on the RoPE-based attention module. When combined with quantization, Palu’s
inherent quantization-friendly design yields small to negligible extra accuracy degradation while saving additional memory than quantization-only methods and achieving up to 2.91× speedup for the RoPE-based attention. Moreover, it maintains comparable or even better accuracy (up to 1.19 lower perplexity) compared to quantization-only methods. These results demonstrate Palu’s superior capability to effectively address the efficiency and memory challenges of LLM inference posed by KV-Cache. Our code is publicly available at: https://github.com/shadowpa0327/Palu.",2025,0.6477270371901683,0.6475306638553141,0.6666666666666666,0.625,251ec9f9-d540-472c-82de-24823fc271cf,1,"[0.5, 0.625, 0.625, 0.625]","[0.95, 1.0, 0.95, 1.0]",Palu,0.5983543656207367
548393,Practical alignment requires more than learning from human feedback,"Ensuring the alignment of artificial intelligence (AI) systems with human objectives is a critical challenge in the development of safe and effective AI technologies. Reinforcement learning from human feedback (RLHF) has been a predominant method to tackle this challenge. However, this framework operates under the unrealistic assumptions that human preferences are accurate reflections of their desires and that they remain constant over time. This paper identifies and challenges these assumptions by illustrating how they can lead to undesirable consequences, particularly when human beliefs about the environment are incorrect or mutate over time. To address these challenges, we introduce a novel framework termed practical alignment. This framework redefines the alignment objective to accommodate the variability and irrationality of human beliefs, emphasizing the need for AI systems not only to learn from but also to teach humans about the world. We discuss the theoretical underpinnings of practical alignment and introduce MindGrid, a toolkit designed to simulate and evaluate alignment scenarios. Our experimental results using large language models in teaching scenarios underscore the importance of teaching skills as a requisite capability to achieve alignment.",2025,0.7499997272728265,0.7430901498831387,0.7333333333333333,0.5,9fa9823d-8fac-4c36-8689-db2ab72ca7fb,0,"[0.5, 0.5, 0.875, 0.875]","[0.9, 0.95, 0.8, 0.95]",Practical alignment,0.6719175757846881
548395,Bringing NeRFs to the Latent Space: Inverse Graphics Autoencoder,"While pre-trained image autoencoders are increasingly utilized in computer vision, the application of inverse graphics in 2D latent spaces has been under-explored. Yet, besides reducing the training and rendering complexity, applying inverse graphics in the latent space enables a valuable interoperability with other latent-based 2D methods. The major challenge is that inverse graphics cannot be directly applied to such image latent spaces because they lack an underlying 3D geometry. In this paper, we propose an Inverse Graphics Autoencoder (IG-AE) that specifically addresses this issue. To this end, we regularize an image autoencoder with 3D-geometry by aligning its latent space with jointly trained latent 3D scenes. We utilize the trained IG-AE to bring NeRFs to the latent space with a latent NeRF training pipeline, which we implement in an open-source extension of the Nerfstudio framework, thereby unlocking latent scene learning for its supported methods. We experimentally confirm that Latent NeRFs trained with IG-AE present an improved quality compared to a standard autoencoder, all while exhibiting training and rendering accelerations with respect to NeRFs trained in the image space. Our project page can be found at https://ig-ae.github.io .",2025,0.6818179338843877,0.6806929345162506,0.6666666666666666,0.625,cebcd4be-7cca-4568-8c9e-59129798edc5,1,"[0.625, 0.625, 0.625]","[0.95, 1.0, 0.95]",Inverse Graphics Autoencoder,0.6250000000000002
548426,Leveraging Driver Field-of-View for Multimodal Ego-Trajectory Prediction,"Understanding drivers’ decision-making is crucial for road safety. Although predicting the ego-vehicle’s path is valuable for driver-assistance systems, existing methods mainly focus on external factors like other vehicles’ motions, often neglecting the driver’s attention and intent. To address this gap, we infer the ego-trajectory by integrating the driver’s gaze and the surrounding scene. We introduce RouteFormer, a novel multimodal ego-trajectory prediction network combining GPS data, environmental context, and the driver's field-of-view—comprising first-person video and gaze fixations. We also present the Path Complexity Index (PCI), a new metric for trajectory complexity that enables a more nuanced evaluation of challenging scenarios. To tackle data scarcity and enhance diversity, we introduce GEM, a comprehensive dataset of urban driving scenarios enriched with synchronized driver field-of-view and gaze data. Extensive evaluations on GEM and DR(eye)VE demonstrate that RouteFormer significantly outperforms state-of-the-art methods, achieving notable improvements in prediction accuracy across diverse conditions. Ablation studies reveal that incorporating driver field-of-view data yields significantly better average displacement error, especially in challenging scenarios with high PCI scores, underscoring the importance of modeling driver attention. All data and code are available at meakbiyik.github.io/routeformer.",2025,0.772726628099408,0.7729905400018733,0.6666666666666666,0.625,860775fa-1284-458a-871c-2b7e32cebe5e,1,"[0.625, 0.625, 0.875]","[1.0, 0.95, 1.0]",RouteFormer,0.716135674035544
548428,Guaranteed Neural PDE Boundary Control with Neural Barrier Function,"The physical world dynamics are generally governed by underlying partial derivative equations (PDEs) with unknown analytical forms in science and engineering problems. Neural network based data-driven approaches have been heavily studied in simulating and solving PDE problems in recent years, but it is still challenging to move forward from understanding to controlling the unknown PDE dynamics. PDE boundary control instantiates a simplified but important problem by only focusing on PDE boundary conditions as the control input and output. However, current model-free PDE controllers cannot ensure the boundary output satisfies some given user-specified safety constraint. To this end, we propose a safety filtering framework to guarantee the boundary output stays within the safe set for current model-free controllers. Specifically, we first introduce a general neural boundary control barrier function (BCBF) to ensure the feasibility of the trajectory-wise constraint satisfaction of boundary output. Based on a neural operator modeling the transfer function from boundary control input to output trajectories, we show that the change in the BCBF depends linearly on the change in input boundary, so 
quadratic programming-based safety filtering can be done for pre-trained model-free controllers. Extensive experiments under challenging hyperbolic, parabolic and Navier-Stokes PDE dynamics environments validate the effectiveness of the proposed method in achieving better general performance and boundary constraint satisfaction compared to the model-free controller baselines.",2025,0.4090907603306326,0.3822357658939077,0.2666666666666666,0.25,023021c8-b4e4-4783-8fda-0077cc275dda,0,"[0.25, 0.25, 0.625]","[0.9, 1.0, 0.7]",Neural Boundary Control Barrier Function,0.3032188534925069
548436,Can LLMs Understand Time Series Anomalies?,"Large Language Models (LLMs) have gained popularity in time series forecasting, but their potential for anomaly detection remains largely unexplored. Our study investigates whether LLMs can understand and detect anomalies in time series data, focusing on zero-shot and few-shot scenarios. Inspired by conjectures about LLMs' behavior from time series forecasting research, we formulate key hypotheses about LLMs' capabilities in time series anomaly detection. We design and conduct principled experiments to test each of these hypotheses. Our investigation reveals several surprising findings about LLMs for time series: (1) LLMs understand time series better as *images* rather than as text, (2) LLMs do not demonstrate enhanced performance when prompted to engage in *explicit reasoning* about time series analysis. (3) Contrary to common beliefs, LLMs' understanding of time series *do not* stem from their repetition biases or arithmetic abilities. (4) LLMs' behaviors and performance in time series analysis *vary significantly* across different models. This study provides the first comprehensive analysis of contemporary LLM capabilities in time series anomaly detection. Our results suggest that while LLMs can understand trivial time series anomalies (we have no evidence that they can understand more subtle real-world anomalies), many common conjectures based on their reasoning capabilities do not hold. All synthetic dataset generators, final prompts, and evaluation scripts have been made available in https://github.com/rose-stl-lab/anomllm.",2025,0.5727270644628857,0.572916916254076,0.6666666666666666,0.625,86a5a08c-2750-4558-a083-427839f02e3b,1,"[0.25, 0.5, 0.625, 0.625, 0.625]","[0.95, 0.95, 0.95, 0.95, 1.0]",Anomaly Detection,0.5314651302005388
548440,EEVEE and GATE: Finding the right benchmarks and how to run them seamlessly,"Model evaluation is a cornerstone of machine learning, guiding model design and progress measurement. Designing generalizable evaluation processes remains a challenge, however, partly due to the vast number of possible domain, task and modality combinations and lack of knowledge of how informative they are. In this paper, we propose EEVEE (Efficient Evaluation process Evolution Engine) - pronounced as \textipa{/'i:vi:/} EE-vee - a method that frames evaluation process design as a learning problem. By analyzing a large number of evaluation metrics from diverse benchmarks and models, EEVEE identifies a smaller subset of tasks with high predictive power over the full set of evaluation metrics, reducing evaluation time. To find the optimal subset maximizing signal while minimizing GPU hours, EEVEE evaluates pre-trained models of various architectures, pretraining schemes, and modalities on diverse downstream tasks and datasets including image classification, segmentation, relational reasoning, zero-shot image-to-text tasks, medical classification and segmentation, video classification, and regression. Our results identify three subsets of benchmarks, with 8, 15 and 21 tasks, providing high quality signal for model generalization. Key benchmarks selected include iWildCam, CLEVR-Math, ACDC, WinoGround, CIFAR100, Fungi, and ADE20K. We structure the subsets into three tiers for 12, 24, and 36 GPU-hour budgets and package them into a unified, efficient, and user-friendly Python framework that we built with the researcher in mind -- which we refer to as the GATE engine. Our experiments reveal ConvNextV2, SigLIP and CLIP as top-performing model encoders, with EfficientNetV2 and ResNext50 excelling in medical tasks and challenging image classification, in particular in Happy Whale Individual classification, ConvNet based models seem to outperform transformer models by a factor of 2.5x, which is surprising. The top performing encoder being ConvNextV2 followed by CLIP seems to agree with other recent large scale evaluations. We also demonstrate the framework's versatility in fine-tuning models from text and audio modalities, paving the way for future cross-modal evaluations.",2025,0.443181657024852,0.4363895175466302,0.4,0.0,dd495f1a-a266-4704-8290-d3ec610fb214,0,"[0.0, 0.25, 0.5, 0.875]","[0.95, 0.9, 0.9, 0.9]",EEVEE,0.3783568660022151
548454,Kick Bad Guys Out! Conditionally Activated Anomaly Detection in Federated Learning with Zero-Knowledge Proof Verification,"Federated Learning (FL) systems are susceptible to adversarial attacks, where malicious clients submit poisoned models to disrupt the convergence or plant backdoors that cause the global model to misclassify some samples. Current defense methods are often impractical for real-world FL systems, as they either rely on unrealistic prior knowledge or cause accuracy loss even in the absence of attacks. Furthermore, these methods lack a protocol for verifying execution, leaving participants uncertain about the correct execution of the mechanism. To address these challenges, we propose a novel anomaly detection strategy that is designed for real-world FL systems. Our approach activates the defense only when potential attacks are detected, and enables the removal of malicious models without affecting the benign ones. Additionally, we incorporate zero-knowledge proofs to ensure the integrity of the proposed defense mechanism. Experimental results demonstrate the effectiveness of our approach in enhancing FL system security against a comprehensive set of adversarial attacks in various ML tasks.",2025,0.3749998636364132,0.3743811139839378,0.2666666666666666,0.25,828d99d9-b96a-4d3e-b2d6-67189d983072,0,"[0.25, 0.25, 0.25, 0.625]","[0.95, 0.95, 0.95, 0.95]",Anomaly Detection,0.34375
548455,Towards Specialized Web Agents Using Production-Scale Workflow Data,"Large Language Model (LLM) agents are rapidly improving to handle increasingly complex web-based tasks. Most of these agents rely on general-purpose, proprietary models like GPT-4 and focus on designing better prompts to improve their planning abilities. However, general-purpose LLMs are not specifically trained to understand specialized web contexts such as HTML, and they often struggle with long-horizon planning. We explore an alternative approach that fine-tunes open-source LLMs using production-scale workflow data collected from over 250 domains corresponding to 6 billion tokens. This simple yet effective approach shows substantial gains over prompting-based agents on existing benchmarks---our WorkflowAgent achieves state-of-the-art performance on Mind2Web and substantially improves the baseline task success rate from 37.2% to 51.3% on WebArena. We further perform detailed ablation studies on various fine-tuning design choices and provide insights into LLM selection, training recipes, context window optimization, and effect of dataset sizes.",2025,0.5454543471075102,0.5445543476130005,0.5333333333333333,0.5,743d1d80-a5df-456c-bf75-55263abd0794,0,"[0.5, 0.5, 0.5]","[0.9, 0.9, 0.95]",WorkflowAgent,0.5000000000000001
548457,Sketched Adaptive Federated Deep Learning: A Sharp Convergence Analysis,"Combining gradient sketching methods (e.g., CountSketch,  quantization) and adaptive optimizers (e.g., Adam, AMSGrad) is a desirable goal in federated learning (FL), with potential benefits on both fewer communication rounds and smaller per-round communication. In spite of the preliminary empirical success of sketched adaptive methods, existing convergence analyses show the communication cost to have a linear dependence on the ambient dimension, i.e., number of parameters, which is prohibitively high for modern deep learning models.

In this work, we introduce specific sketched adaptive federated learning (SAFL) algorithms and, as our main contribution, provide theoretical convergence analyses in different FL settings with guarantees on communication cost depending only logarithmically (instead of linearly) on the ambient dimension. Unlike existing analyses, we show that the entry-wise sketching noise existent in the preconditioners and the first moments of SAFL can be implicitly addressed by leveraging the recently-popularized anisotropic curvatures in deep learning losses, e.g., fast decaying loss Hessian eigen-values. 
In the i.i.d. client setting of FL, we show that SAFL achieves $O(1/\sqrt{T})$ convergence, and $O(1/T)$ convergence near initialization. In the non-i.i.d. client setting, where non-adaptive methods lack convergence guarantees, we show that SACFL (SAFL with clipping) algorithms can provably converge in spite of the additional heavy-tailed noise. Our theoretical claims are supported by empirical studies on vision and language tasks, and in both fine-tuning and training-from-scratch regimes. Surprisingly, as a by-product of our analysis, the proposed SAFL methods are competitive with the state-of-the-art communication-efficient federated learning algorithms based on error feedback.",2025,0.4772725537190714,0.4737622824233104,0.4666666666666667,0.25,f1dd22bb-8c76-4387-9075-e64821936379,0,"[0.25, 0.25, 0.625, 0.625]","[0.95, 0.95, 0.9, 0.95]",Sketched Adaptive Federated Learning,0.4261806231742939
548461,A Theoretical Study of Neural Network Expressive Power via Manifold Topology,"A prevalent assumption regarding real-world data is that it lies on or close to a low-dimensional manifold. When deploying a neural network on data manifolds, the required size, i.e., the number of neurons of the network, heavily depends on the intricacy of the underlying latent manifold. While significant advancements have been made in understanding the geometric attributes of manifolds, it's essential to recognize that topology, too, is a fundamental characteristic of manifolds. In this study, we investigate network expressive power in terms of the latent data manifold. Integrating both topological and geometric facets of the data manifold, we present a size upper bound of ReLU neural networks.",2025,0.6545452165290122,0.6528466033967122,0.6666666666666666,0.625,a6682ceb-9d5f-4317-b00a-90a83ed06a90,0,"[0.5, 0.625, 0.625, 0.625, 0.625]","[0.9, 0.9, 0.9, 0.9, 0.8]",Manifold Topology,0.5981057212338412
548486,Selective Unlearning via Representation Erasure Using Domain Adversarial Training,"When deploying machine learning models in the real world,  we often face the challenge of “unlearning” specific data points or subsets after training.  Inspired by Domain-Adversarial Training of Neural Networks (DANN), we propose a novel algorithm,SURE, for targeted unlearning.SURE treats the process as a domain adaptation problem, where the “forget set” (data to be removed) and a validation set from the same distribution form two distinct domains. We train a domain classifier to discriminate between representations from the forget and validation sets.Using a gradient reversal strategy similar to DANN, we perform gradient updates to the representations to “fool” the domain classifier and thus obfuscate representations belonging to the forget set. Simultaneously, gradient descent is applied to the retain set (original training data minus the forget set) to preserve its classification performance.  Unlike other unlearning approaches whose training objectives are built based on model outputs, SURE directly manipulates the representations.This is key to ensure robustness against a set of more powerful attacks than currently considered in the literature,  that aim to detect which examples were unlearned through access to learned embeddings.  Our thorough experiments reveal that SURE has a better unlearning quality to utility trade-off compared to other standard unlearning techniques for deep neural networks.",2025,0.6363630413225304,0.6361385869032502,0.6666666666666666,0.625,cb7d5dc5-dc47-44ba-8581-bbc109316557,1,"[0.5, 0.625, 0.625]","[0.9, 0.9, 0.95]",Selective Unlearning,0.5870624323348972
548509,SReNet: Spectral Refined Network for Solving Operator Eigenvalue Problem,"Solving operator eigenvalue problems helps analyze intrinsic data structures and relationships, yielding substantial influence on scientific research and engineering applications.
Recently, novel approaches based on deep learning have been proposed to obtain eigenvalues and eigenfunctions from the given operator, which address the efficiency challenge arising from traditional numerical methods.
However, when solving top-$L$ eigenvalues problems, these learning-based methods ignore the information that could be inherited from other known eigenvectors, thus resulting in a less-than-ideal performance.
To address the challenge, we propose the **S**pectral **Re**fined **Net**work (**SReNet**). 
Our novel approach incorporates the power method to approximate the top-$L$ eigenvalues and their corresponding eigenfunctions.
To effectively prevent convergence to previous eigenfunctions, we introduce the Deflation Projection that significantly improves the orthogonality of the computed eigenfunctions and enables more precise prediction of multiple eigenfunctions simultaneously. 
Furthermore, we develop the adaptive filtering method that dynamically leverages intermediate approximate eigenvalues to construct rational filters that filter out predicted eigenvalues, when predicting the successive eigenvalue of the given problem.
During the iterative solving, the spectral transformation is performed based on the filter function, converting the original eigenvalue problem into an equivalent problem that is easier to converge.
Extensive experiments demonstrate that our approach consistently outperforms existing learning-based methods, achieving state-of-the-art performance in accuracy.",2025,0.2045453801653163,0.1996695644218684,0.2666666666666666,0.25,b37f86b0-4696-4be9-843b-ac963fe9ba83,0,"[0.0, 0.25, 0.25, 0.25]","[1.0, 0.9, 0.95, 0.9]",SReNet,0.1635248823939425
548516,Learning to Plan Before Answering: Self-Teaching LLMs to Learn Abstract Plans for Problem Solving,"In the field of large language model (LLM) post-training, the effectiveness of utilizing synthetic data generated by the LLM itself has been well-presented. However, a key question remains unaddressed: what essential information should such self-generated data encapsulate? Existing approaches only produce step-by-step problem solutions, and fail to capture the abstract meta-knowledge necessary for generalization across similar problems. Drawing insights from cognitive science, where humans employ high-level abstraction to simplify complex problems before delving into specifics, we introduce a novel self-training algorithm: LEarning to Plan before Answering (LEPA). LEPA trains the LLM to formulate anticipatory plans, which serve as abstract meta-knowledge for problem-solving, before engaging with the intricacies of problems. This approach not only outlines the solution generation path but also shields the LLM from the distraction of irrelevant details. During data generation, LEPA first crafts an anticipatory plan based on the problem, and then generates a solution that aligns with both the plan and the problem. LEPA refines the plan through self-reflection, aiming to acquire plans that are instrumental in yielding correct solutions. During model optimization, the LLM is trained to predict both the refined plans and the corresponding solutions. By efficiently extracting and utilizing the anticipatory plans, LEPA demonstrates remarkable superiority over conventional algorithms on various challenging natural language reasoning benchmarks.",2025,0.7159088305786071,0.7151817395679724,0.6666666666666666,0.625,a699114e-5db7-40ca-a844-e364036512de,1,"[0.5, 0.625, 0.625, 0.875]","[0.95, 0.95, 0.9, 0.95]",LEPA,0.6581365628042842
548522,Data-centric Prediction Explanation via Kernelized Stein Discrepancy,"Existing example-based prediction explanation methods often bridge test and training data points through the model’s parameters or latent representations. While these methods offer clues to the causes of model predictions, they often exhibit innate shortcomings, such as incurring significant computational overhead or producing coarse-grained explanations. This paper presents a Highly-precise and Data-centric Explanation (HD-Explain) prediction explanation method that exploits properties of Kernelized Stein Discrepancy (KSD). Specifically, the KSD uniquely defines a parameterized kernel function for a trained model that encodes model-dependent data correlation. By leveraging the kernel function, one can identify training samples that provide the best predictive support to a test point efficiently. We conducted thorough analyses and experiments across multiple classification domains, where we show that HD-Explain outperforms existing methods from various aspects, including 1) preciseness (fine-grained explanation), 2) consistency, and 3) computation efficiency, leading to a surprisingly simple, effective, and robust prediction explanation solution.",2025,0.8181815206612653,0.8168315214195008,0.8,0.625,28ecc43d-29eb-4166-b25a-6596b8f25cba,1,"[0.625, 0.625, 0.875, 0.875]","[0.9, 0.9, 0.9, 0.9]",Kernelized Stein Discrepancy,0.7499999999999957
548532,Unveiling the latent dynamics in social cognition with multi-agent inverse reinforcement learning,"Understanding the intentions and beliefs of others, a phenomenon known as ""theory of mind"", is a crucial element in social behavior. These beliefs and perceptions are inherently subjective and latent, making them often unobservable for investigation. Social interactions further complicate the matter, as multiple agents can engage in recursive reasoning about each other's strategies with increasing levels of cognitive hierarchy. While previous research has shown promise in understanding a single agent's belief of values through inverse reinforcement learning, extending this to model multiple agents remains an open challenge due to the computational complexity. In this work, we adopted a probabilistic recursive modeling of cognitive levels and joint value decomposition to achieve efficient multi-agent inverse reinforcement learning (MAIRL). We provided a numerical method to evaluate value decomposition errors in multi-agent tasks with discrete state and action spaces. To validate our method, we conducted simulations of a two-agent cooperative foraging task in a grid environment. Our algorithm revealed the ground truth goal-directed value function and effectively distinguished between level-0 and level-1 agents. When applied to human behavior in a cooperative hallway task, our method identified meaningful goal maps that evolved with task proficiency and an interaction map that is related to key states in the task without accessing to the task rules. Similarly, in a non-cooperative task performed by monkeys, we identified mutual predictions that correlated with the animals' social hierarchy, highlighting the behavioral relevance of the latent beliefs we uncovered. Together, our findings demonstrate that MAIRL offers a new framework for uncovering human or animal beliefs in social behavior, thereby illuminating previously opaque aspects of social cognition.",2025,0.443181657024852,0.4433707042830288,0.4,0.25,271fa380-88d0-4597-961a-fc3fc93fea92,0,"[0.25, 0.25, 0.5, 0.625]","[0.9, 0.95, 0.9, 0.95]",Multi-Agent Inverse Reinforcement Learning,0.4102655440414509
548535,PARSE-Ego4D: Personal Action Recommendation Suggestions for Ego-Centric Videos,"Intelligent assistance involves not only understanding but also action. Existing ego-centric video datasets contain rich annotations of the videos, but not of actions that an intelligent assistant could perform in the moment. To address this gap, we release PARSE-Ego4D, a new set of personal action recommendation annotations for the Ego4D dataset. We take a multi-stage approach to generating and evaluating these annotations. First, we used a prompt-engineered large language model (LLM) to generate context-aware action suggestions and identified over 18,000 action suggestions. While these synthetic action suggestions are valuable, the inherent limitations of LLMs necessitate human evaluation. To ensure high-quality and user-centered recommendations, we conducted a large-scale human annotation study that provides grounding in human preferences for all of PARSE-Ego4D. We analyze the inter-rater agreement and evaluate subjective preferences of participants. Based on our synthetic dataset and complete human annotations, we propose several new tasks for action suggestions based on ego-centric videos. We encourage novel solutions that improve latency and energy requirements. The annotations in PARSE-Ego4D will support researchers and developers who are working on building action recommendation systems for augmented and virtual reality systems.",2025,0.613636140495949,0.6145143337595378,0.6,0.5,0374b5ec-fc0d-4262-97f1-e99792736e55,0,"[0.5, 0.5, 0.625, 0.625]","[0.8, 0.95, 0.95, 0.9]",PARSE-Ego4D,0.5676941414051039
548536,IgSeek: Fast and Accurate Antibody Design via Structure Retrieval,"Recent advancements in protein design have leveraged diffusion models to generate structural scaffolds, followed by a process known as protein inverse folding, which involves sequence inference on these scaffolds. However, these methodologies face significant challenges when applied to hyper-variable structures such as antibody Complementarity-Determining Regions (CDRs), where sequence inference frequently results in non-functional sequences due to hallucinations. Distinguished from prevailing protein inverse folding approaches, this paper introduces IgSeek, a novel structure-retrieval framework that infers CDR sequences by retrieving similar structures from a natural antibody database. Specifically, IgSeek employs a simple yet effective multi-channel equivariant graph neural network to generate high-quality geometric representations of CDR backbone structures. Subsequently, it aligns sequences of structurally similar CDRs and utilizes structurally conserved sequence motifs to enhance inference accuracy. Our experiments demonstrate that IgSeek not only proves to be highly efficient in structural retrieval but also outperforms state-of-the-art approaches in sequence recovery for both antibodies and T-Cell Receptors, offering a new retrieval-based perspective for therapeutic protein design.",2025,0.3409089669421938,0.3429984469310006,0.2666666666666666,0.25,c66b4edb-8611-48d0-b486-4df576e42ad1,0,"[0.25, 0.25, 0.25, 0.5]","[0.95, 0.95, 0.95, 1.0]",IgSeek,0.3274116347569955
548577,Q-Adapt: Adapting LMM for  Visual Quality Perceiver with Progressive Instruction Tuning,"The rapid advancement of Large Multi-modal Foundation Models (LMM) has paved the way for the possible Explainable Image Quality Assessment (EIQA) with instruction tuning from two perspectives: overall quality explanation, and attribute-wise perception answering. However, existing works usually overlooked the conflicts between these two types of perception explanations during joint instruction tuning, leading to insufficient perception understanding. To mitigate this, we propose a new paradigm for perception-oriented instruction tuning, i.e., Q-Adapt, which aims to eliminate the conflicts and achieve the synergy between these two EIQA tasks when adapting LMM, resulting in enhanced multi-faceted explanations of IQA. Particularly, we propose a progressive instruction tuning strategy by dividing the adaption process of LMM for EIQA into two stages, where the first stage empowers the LMM with universal perception knowledge tailored for two tasks using an efficient transfer learning strategy, i.e., LoRA, and the second stage introduces the instruction-adaptive visual prompt tuning to dynamically adapt visual features for the different instructions from two tasks. In this way, our proposed Q-Adapt can achieve a lightweight visual quality perceiver, demonstrating comparable performance and, in some instances, superior results across perceptual-related benchmarks and commonly-used IQA databases.",2025,0.5795452438017296,0.5785889943388131,0.5333333333333333,0.5,c3f594f5-56f6-46ee-a783-07167b973191,0,"[0.5, 0.5, 0.5, 0.625]","[0.95, 1.0, 0.9, 0.95]",Q-Adapt,0.5303555130651466
548580,G-SPARC: SPectral ARchitectures tackling the Cold-start problem in Graphs,"Graphs play a central role in modeling complex relationships across various domains. Most graph learning methods rely heavily on neighborhood information, raising the question of how to handle \textit{cold-start nodes} — nodes with no known connections within the graph.
These models often overlook the cold-start nodes, making them ineffective for real-world scenarios. To tackle this, we propose G-SPARC, a novel framework addressing cold-start nodes, that leverages generalizable spectral embedding. This framework enables extension to state-of-the-art methods making them suitable for practical applications. By utilizing a key idea of transitioning from graph representation to spectral representation, our approach is generalizable to cold-start nodes, capturing the global structure of the graph without relying on adjacency data. Experimental results demonstrate that our method outperforms existing models on cold-start nodes across various tasks like node classification, node clustering, and link prediction. G-SPARC provides a breakthrough built-in solution to the cold-start problem in graph learning. Our code will be publicly available upon acceptance.",2025,0.3636358677687754,0.361471908619424,0.2666666666666666,0.25,90ad7888-c7cd-482e-b273-e1a1b9700ecc,0,"[0.25, 0.25, 0.5]","[0.95, 1.0, 0.95]",G-SPARC,0.3247250119560019
548590,Reinforcement Learning via Lazy-Agent for Environments with Random Delays,"Real-world reinforcement learning applications are often hampered by delayed feedback from environments, which violates the fundamental assumption of the Markovian property and introduces significant challenges. While numerous methods have been proposed for handling environments with constant delays, those with random delays remain largely unexplored owing to their inherent complexity and variability. In this study, we explored environments with random delays and proposed a novel strategy to transform them into their equivalent constant-delay counterparts by introducing a simple agent called the *lazy-agent*. This approach naturally overcomes the challenges posed by the variability of random delays, enabling the application of state-of-the-art methods, originally designed for handling constant delays, to random-delay environments without any modification. Empirical results demonstrate that our lazy-agent significantly outperformed other baseline algorithms in terms of asymptotic performance and sample efficiency in random-delay environments.",2025,0.5909092396693675,0.589108695226001,0.5333333333333333,0.5,79151f95-1415-477d-8678-621ada6574ab,0,"[0.5, 0.5, 0.625]","[0.95, 0.9, 0.9]",Lazy-Agent,0.5379375676651029
548605,AnyBimanual: Transferring Single-arm Policy for General Bimanual Manipulation,"Performing language-conditioned bimanual manipulation tasks is of great importance for many applications ranging from household service to industrial assembly. However, teleoperating dual-arm demonstrations is expensive due to the high-dimensional action space, which poses challenges for conventional methods to handle general bimanual manipulation tasks. In contrast, single-arm policy has recently demonstrated impressive generalizability across a wide range of tasks because of scaled model parameters and training data, which can provide sharable manipulation knowledge for dual-arm systems. To this end, we propose a plug-and-play method named AnyBimanual, which transfers pretrained single-arm policy to multi-task bimanual manipulation policy with limited bimanual demonstrations. Specifically, we first introduce a skill manager to dynamically schedule the discovered skill primitives from pretrained single-arm policy for bimanual manipulation tasks, which combines skill primitives with embodiment-specific compensation. To mitigate the observation discrepancy between single-arm and dual-arm systems, we present a voxel editor to generate spatial soft masks for visual embedding of the workspace, which aims to align visual input of single-arm policy model for each arm with those during pretraining stage. Extensive results on 13 simulated and real-world tasks indicate the superiority of AnyBimanual with an improvement of 12.67\% on average success rate compared with previous state-of-the-art methods.",2025,0.3749998636364132,0.3757424998529703,0.2666666666666666,0.25,d7012029-4973-48c5-ac70-650b290341c2,0,"[0.25, 0.25, 0.25, 0.625]","[0.95, 0.95, 0.9, 0.95]",AnyBimanual,0.3494096884128529
548609,L4Q: Parameter Efficient Quantization-Aware Fine-Tuning on Large Language Models,"Due to the high memory and computational costs associated with large language models (LLMs), model compression techniques such as quantization, which reduces inference costs, and parameter-efficient fine-tuning (PEFT) methods like Low-Rank Adaptation (LoRA), which reduce training costs, have gained significant popularity. This trend has spurred active research into quantization-aware PEFT techniques, aimed at maintaining model accuracy while minimizing memory overhead during both inference and training.
Previous quantization-aware PEFT methods typically follow a two-step approach: first, applying post-training quantization (PTQ) to model weights, followed by PEFT on the quantized model. However, recovering from the quantization error introduced by PTQ through fine-tuning has proven challenging. Additionally, most PTQ-based PEFT methods result in a mixture of low-precision quantized weights and high-precision adapter weights, limiting the efficiency of full quantization during inference.
While a previous method attempted to address these issues, it still suffers from limited adaptability due to the constrained LoRA parameter structure required to produce fully-quantized models. To overcome these challenges, we propose L4Q, a method that integrates Quantization-Aware Training (QAT) with LoRA to effectively reduce quantization error.
%, which effectively reduces quantization error, with LoRA.
By employing a memory-optimized layer design, L4Q significantly reduces QAT’s memory overhead while producing fully-quantized weights, enabling effective adaptation to downstream tasks. Our experiments demonstrate that this combined approach to quantization and fine-tuning achieves superior accuracy compared to decoupled fine-tuning schemes, particularly in sub-4-bit quantization, positioning L4Q as an efficient QAT solution. Using the LLaMA model families and instructional datasets, we showcase L4Q’s capabilities in language tasks and few-shot learning.",2025,0.4772725537190714,0.4764850541613754,0.5333333333333333,0.5,26ed76d2-f48f-4ace-84bb-39564b70294e,0,"[0.25, 0.5, 0.5, 0.5]","[0.95, 0.95, 0.95, 0.95]",L4Q,0.4374999999999999
548611,Sparse Training: Do All Tokens Matter for Long Sequence Generalization?,"Large language models (LLMs) have demonstrated remarkable progress in generating high-quality natural language through performing extensive pre-training over Transformer architectures. However, the quadratic complexity of transformers in sequence computation greatly limits its capability in efficiently modeling long sequences. In this paper, we introduce \method, a simple training technique to optimize the complexity of Transformer models in long-sequence training. Specifically, in \method, the input sequences of the Transformer network are segmented into two distinct components: {the \textit{memory} part and the \textit{target} part.} The target part adheres to the standard next-token prediction for modeling continuous sequences, while the memory part, sampled from longer sequences, serves as the conditional context for the prediction of the target part. To build the memory part, we apply a sparse sampling policy that decays with the distance from the target part, to obtain tokens and preserve their positions. Without any architectural modifications, our method can extend existing Transformer-based LLMs to capture long-range dependencies within a fixed window size during the training. Experimental results on multiple datasets also demonstrate the effectiveness and efficiency of \textsc{Sparse Training} to mitigate the complexity of the Transformer network in building long-sequence dependency.",2025,0.3409089669421938,0.336764388759527,0.2666666666666666,0.25,6e078647-edce-47ce-842c-982bb8a22c2d,0,"[0.25, 0.25, 0.25, 0.5]","[0.95, 1.0, 0.95, 0.9]",Sparse Training,0.2968851909784909
548626,Blind Unlearning: Unlearning Without a Forget Set,"Machine unlearning is the study of methods to efficiently remove the influence
of some subset of the training data from the parameters of a previously-trained
model. Existing methods typically require direct access to the “forget set” – the
subset of training data to be forgotten by the model. This limitation impedes privacy, as organizations need to retain user data for the sake of unlearning when a
request for deletion is made, rather than being able to delete it immediately. We
first introduce the setting of blind unlearning – unlearning without explicit access
to the forget set. Then, we propose a method for approximate unlearning called
RELOAD, that leverages ideas from gradient-based unlearning and neural network
sparsity to achieve blind unlearning. The method serially applies an ascent step
with targeted parameter re-initialization and fine-tuning, and on empirical unlearning tasks, RELOAD often approximates the behaviour of a from-scratch retrained
model better than approaches that leverage the forget set. Finally, we extend the
blind unlearning setting to blind remedial learning, the task of efficiently updating
a previously-trained model to an amended dataset.",2025,0.3545453256198816,0.3557170582738498,0.2666666666666666,0.25,cb7d5dc5-dc47-44ba-8581-bbc109316557,0,"[0.25, 0.25, 0.25, 0.25, 0.625]","[0.95, 0.9, 0.9, 0.95, 0.95]",Blind Unlearning,0.3325166700343503
548632,Divergence-Regularized Discounted Aggregation: Equilibrium Finding in Multiplayer Partially Observable Stochastic Games,"This paper presents Divergence-Regularized Discounted Aggregation (DRDA), a multi-round learning system for solving partially observable stochastic games (POSGs). DRDA is based on action values and applicable to multiplayer POSGs, which can unify normal-form games (NFGs), extensive-form games (EFGs) with perfect recall, and Markov games (MGs). In each single round, DRDA can be viewed as a discounted variant of Follow the Regularized Leader (FTRL) under a general value function for POSGs. While previous studies on discounted FTRL have demonstrated its last-iterate convergence towards quantal response equilibrium (QRE) in NFGs, this paper extends the theoretical results to POSGs under divergence regularization and generalizes the QRE concept of Nash distribution. The linear last-iterate convergence of single-round DRDA to its rest point is proved under the assumption on the hypomonotonicity of the game. When the rest point is unique, it induces the unique Nash distribution defined in the POSG, which has a bounded deviation from Nash equilibrium (NE). Under multiple learning rounds, DRDA keeps replacing the base policy for divergence regularization with the policy at the rest point in the previous round. It is further proved that the limit point of multi-round DRDA must be an exact NE (rather than a QRE). In experiments, discrete-time DRDA can converge to NE at a near-exponential rate in (multiplayer) NFGs and outperform the existing baselines for EFGs, MGs, and typical POSGs.",2025,0.8181815206612653,0.813050136029676,0.8,0.625,ec581747-ba8e-406a-b6c3-b0e196ff8b68,1,"[0.625, 0.625, 0.875, 0.875]","[0.9, 0.95, 0.95, 0.8]",Divergence-Regularized Discounted Aggregation,0.7396117171897919
548634,SG-Adapter: Enhancing Text-to-Image Generation with Scene Graph Guidance,"Recent advancements in text-to-image generation have been propelled by the development of diffusion models and multi-modality learning. However, since text is typically represented sequentially in these models, it often falls short in providing accurate contextualization and structural control. So the generated images do not consistently align with human expectations, especially in complex scenarios involving multiple objects and relationships. In this paper, we introduce the Scene Graph Adapter(SG-Adapter), leveraging the structured representation of scene graphs to rectify inaccuracies in the original text embeddings. The SG-Adapter's explicit and non-fully connected graph representation greatly improves the fully connected, transformer-based text representations. This enhancement is particularly notable in maintaining precise correspondence in scenarios involving multiple relationships. To address the challenges posed by low-quality annotated datasets like Visual Genome, we have manually curated a highly clean, multi-relational scene graph-image paired dataset MultiRels. Furthermore, we design three metrics derived from GPT-4V to effectively and thoroughly measure the correspondence between images and scene graphs. Both qualitative and quantitative results validate the efficacy of our approach in controlling the correspondence in multiple relationships.",2025,0.613636140495949,0.6126236410646255,0.6,0.5,8dc97c49-3ddb-47f5-8699-7a7058e6ce83,0,"[0.5, 0.5, 0.625, 0.625]","[0.95, 0.95, 0.95, 0.95]",Scene Graph Adapter,0.5624999999999999
548654,Screener: Learning Conditional Distribution of Dense Self-supervised Representations for Unsupervised Pathology Segmentation in 3D Medical Images,"Accurate and automated anomaly segmentation is critical for assisting clinicians in detecting and diagnosing pathological conditions, particularly in large-scale medical imaging datasets where manual annotation is not only time- and resource-intensive but also prone to inconsistency. To address these challenges, we propose Screener, a fully self-supervised framework for visual anomaly segmentation, leveraging self-supervised representation learning to eliminate the need for manual labels. Additionally, we model the conditional distribution of local image patterns given their global context,  enabling the identification of anomalies as patterns with low conditional probabilities and assigning them high anomaly scores.

Screener comprises three components: a descriptor model that encodes local image patterns into self-supervised representations invariant to local-content-preserving augmentations; a condition model that captures global contextual information through invariance to image masking; and a density model that estimates the conditional density of descriptors given their global contexts to compute anomaly scores.

We validate Screener by training a fully self-supervised model on over 30,000 3D CT images and evaluating its performance on four large-scale test datasets comprising 1,820 3D CT scans across four chest and abdominal pathologies. Our framework consistently outperforms existing unsupervised anomaly segmentation methods. Code and pre-trained models will be made publicly available.",2025,0.5909092396693675,0.5931754270939749,0.5333333333333333,0.5,9a0cfa93-e4df-488e-aff6-09fc6c6d3497,0,"[0.5, 0.5, 0.625]","[0.9, 0.9, 1.0]",Screener,0.5581979478409576
548664,A Unified Framework for Hierarchical Diffusion via Simplicial Complexes,"In this paper, we propose a unified framework for hierarchical diffusion via simplicial complexes (HDSC), which enables adaptive diffusion across different levels of simplicial complexes, including nodes, edges, and triangles. To ensure the accuracy and consistency of information transmission during the diffusion process, we investigate topological consistency constraints, achieving efficient coupling between structures at various levels. Additionally, by introducing a time-dependent topological memory mechanism, we further enhance the smoothness and coherence of global information flow, enabling features at different levels to diffuse cooperatively throughout the entire graph structure. Experimental results demonstrate that HDSC exhibits significant performance advantages over traditional methods. Furthermore, as the complexity and dimensionality of the graph increase, HDSC continues to maintain its superiority, effectively avoiding the phenomenon of node feature homogenization.",2025,0.3749998636364132,0.3717628966806145,0.2666666666666666,0.25,95033e09-d6b4-458e-b07f-e39c64644150,0,"[0.0, 0.25, 0.25, 0.875]","[0.95, 1.0, 1.0, 0.95]",Hierarchical Diffusion,0.3299369031377899
548694,EM-DARTS: Preventing Performance Collapse in Differentiable Architecture Search with The Edge Mutation Mechanism,"Differentiable Architecture Search (DARTS) relaxes the discrete search space into a continuous form, significantly improving architecture search efficiency through gradient-based optimization. However, DARTS often suffers from performance collapse, where the performance of discovered architectures degrades during the search process, and the final architectures tend to be dominated by excessive skip-connections. In this work, we analyze how continuous relaxation impacts architecture optimization, identifying two main causes for performance collapse. First, the continuous relaxation framework introduces coupling between parametric operation weights and architecture parameters. This coupling leads to insufficient training of parametric operations, resulting in smaller architecture parameters for these operations. Second, DARTS's unrolled estimation property leads to larger architecture parameters for skip-connections. To attack this issue, we propose Edge Mutation Differentiable Architecture Search (EM-DARTS), where during network weight updates, edges have a probability of mutating from a weighted sum of candidate operations to a specific parametric operation.
    EM-DARTS reduces the impact of architecture parameters on parametric operations, allowing for better training of the parametric operations, thereby increasing their architecture parameters and preventing performance collapse. Theoretical results and experimental studies across diverse search spaces and datasets validate the effectiveness of the proposed method.",2025,0.5454543471075102,0.5463698918079422,0.6,0.625,64cc894f-dbdb-4ffd-a7d2-d62543e9fb88,0,"[0.25, 0.5, 0.625, 0.625]","[0.9, 1.0, 0.95, 0.9]",Edge Mutation,0.5073176056114608
548698,Less is More: Exploiting Feature Density for Enhanced Membership Inference Attacks,"Membership inference attacks have become the de facto standard for assessing privacy breaches across various machine learning (ML) models. However, existing approaches often require substantial resources, including large numbers of shadow models and auxiliary datasets, to achieve high true positive rates (TPR) in the low false positive rate (FPR) region. This makes these attacks prohibitively expensive and less practical. In this work, we propose a novel membership inference attack that exploits feature density gaps by progressively removing features from both members and non-members and evaluating the corresponding model outputs as a new membership signal. Our method requires only a few dozen queries and does not rely on large auxiliary datasets or the training of numerous shadow models. Extensive evaluations on both classification and diffusion models demonstrate that our method significantly improves the TPR at low FPR across multiple scenarios.",2025,0.613636140495949,0.6144152248682723,0.6,0.5,17ee0362-c78a-44eb-b12c-bcc4284198b8,0,"[0.5, 0.5, 0.625, 0.625]","[0.95, 0.9, 1.0, 0.95]",Feature Density,0.5712018914456081
548705,Model Zoos for Benchmarking Phase Transitions in Neural Networks,"Understanding the complex dynamics of neural network training remains a central challenge in deep learning research.
Work rooted in statistical physics has identified phases and phase transitions in neural network (NN) models, where models within the same phase exhibit similar characteristics but qualitatively differ across phases. A prominent example is the double-descent phenomenon. 
Recognizing these transitions is essential for building a deeper understanding of model behavior and the underlying mechanics.
So far, these phases are typically studied in isolation or in specific applications. 
In this paper, we show that phase transitions are a widespread phenomenon.
However, identifying phase transitions across different methods requires populations that cover different phases.
For that reason, we introduce Phase Transition Model Zoos, a structured collection of neural networks trained on diverse datasets and architectures. These model zoos are carefully designed to help researchers systematically identify and study phase transitions in their methods. 
We demonstrate the relevance of phase transitions across multiple applications, including fine-tuning, transfer learning, out-of-distribution generalization, pruning, ensembling, and weight averaging. The diversity of applications underscores the universal nature of phase transitions and their impact on different tasks.
By providing the first structured dataset specifically designed to capture phase transitions in NNs, we offer a valuable tool for the community to systematically evaluate machine learning methods and improve their understanding of phase behavior across a wide range of applications and architectures.",2025,0.443181657024852,0.4424504074355629,0.4,0.25,68ff98c2-90f6-4042-a018-69af5fbd1a03,0,"[0.25, 0.25, 0.5, 0.625]","[0.9, 0.9, 0.9, 0.9]",Phase Transition Model Zoos,0.40625
548729,Self-Attention-Based Contextual Modulation Improves Neural System Identification,"Convolutional neural networks (CNNs) have been shown to be state-of-the-art models for visual cortical neurons. Cortical neurons in the primary visual cortex are sensitive to contextual information mediated by extensive horizontal and feedback connections. Standard CNNs integrate global contextual information to model contextual modulation via two mechanisms: successive convolutions and a fully connected readout layer. In this paper, we find that self-attention (SA), an implementation of non-local network mechanisms, can improve neural response predictions over parameter-matched CNNs in two key metrics: tuning curve correlation and peak tuning. We introduce peak tuning as a metric to evaluate a model's ability to capture a neuron's top feature preference. We factorize networks to assess each context mechanism, revealing that information in the local receptive field is most important for modeling overall tuning, but surround information is critically necessary for characterizing the tuning peak. We find that self-attention can replace posterior spatial-integration convolutions when learned incrementally, and is further enhanced in the presence of a fully connected readout layer, suggesting that the two context mechanisms are complementary. Finally, we find that decomposing receptive field learning and contextual modulation learning in an incremental manner may be an effective and robust mechanism for learning surround-center interactions.",2025,0.7159088305786071,0.7173457985453864,0.8,0.875,73e47b66-6a71-4aa4-b14f-962bc96e2de4,1,"[0.25, 0.625, 0.875, 0.875]","[0.95, 1.0, 1.0, 0.95]",Self-Attention,0.67006309686221
548733,GRADE: Quantifying Sample Diversity in Text-to-Image Models,"Text-to-image (T2I) models are remarkable at generating realistic images based on textual descriptions. However, textual prompts are inherently *underspecified*: they do not specify all possible attributes of the required image. This raises key questions: do T2I models generate diverse outputs on typical underspecified prompts? How can we automatically measure diversity? We propose **GRADE**: **Gr**anular **A**ttribute **D**iversity **E**valuation, an automatic method for quantifying sample diversity. GRADE leverages the world knowledge embedded in large language models and visual question-answering systems to identify relevant concept-specific axes of diversity (e.g., ''shape'' and ''color'' for the concept ''cookie''). It then estimates attribute distributions and quantifies diversity using (normalized) entropy. GRADE achieves over 90\% human agreement while exhibiting weak correlation to commonly used diversity metrics. We use GRADE to measure the overall diversity of 12 T2I models using 400 concept-attribute pairs, revealing that even the most diverse models display limited variation. Further, we find these models often exhibit *default behaviors*, a situation where the model consistently generates concepts with the same attributes (e.g., 98\% of the cookies are round). Finally, we demonstrate that a key reason for low diversity is due to underspecified captions in training data.",2025,0.5909092396693675,0.5899342396169823,0.5333333333333333,0.5,c7d0ea0a-162b-4ce9-9949-12a92ac85bf9,0,"[0.5, 0.5, 0.625]","[0.95, 0.95, 0.95]",GRADE,0.5416666666666666
548758,End-to-End Learning under Endogenous Uncertainty,"How can we effectively learn to make decisions when there are no ground-truth counterfactual observations? We propose an end-to-end learning approach to the contextual stochastic optimization problem under decision-dependent uncertainty. We propose both exact methods and efficient sampling-based methods to implement our approach. We also introduce a new class of two-stage stochastic optimization problems to the end-to-end learning framework. Here, the first stage is an information-gathering problem to decide which random variable to ``poll'' and gain information about before making a second-stage decision based off of it. We provide theoretical analysis showing  (1) that optimally minimizing our proposed objective produces optimal decisions and (2) generalization bounds between in-sample and out-of-sample cost.
We  computationally test the proposed approach on multi-item assortment problems where demand is affected by cross-item complementary and supplementary effects. Overall, our method outperforms other benchmarks by more than 15\% and performs best in high noise, across any cost configuration, and when given sufficient data.
We also introduce an experiment for the information-gathering problem on a real-world electricity generation problem. We show our method proposes decisions with more than 7\% lower cost than other decision-making methods.",2025,0.4090907603306326,0.4047367515372769,0.4,0.25,0f90f94a-7621-481c-b617-0f39aaa21ab2,0,"[0.25, 0.25, 0.5, 0.5]","[0.9, 1.0, 0.9, 0.9]",Contextual Stochastic Optimization,0.3554318488529014
548779,GFSE: A Foundational Model For Graph Structural Encoding,"Foundation models have recently shown remarkable promise by leveraging extensive pre-training on diverse datasets to acquire generalizable representations, which enable effective transfer to a wide range of downstream tasks. In the graph domain, however, most existing pre-training models are tailored to specific domains, primarily due to the inherent differences in semantic meanings of graph features across various contexts. Additionally, most existing models struggle to capture the rich topological complexity of graph structures, leading to inadequate exploration of the embedding space. To address these challenges, we propose a novel Graph Foundational Structural Encoder (GFSE) that identifies universal structural patterns, facilitating a unified feature embedding space suitable for diverse domains, including molecular structures, social networks, and citation networks. GFSE is the first cross-domain graph structural encoder pre-trained with multiple self-supervised learning objectives. Built on a Graph Transformer, GFSE incorporates attention mechanisms biased by graph structural information, allowing it to encode intricate multi-level and fine-grained topological features within complex graph structures. The pre-trained GFSE produces generic and theoretically expressive positional and structural encoding for graphs, which can be seamlessly integrated with various downstream graph feature encoders, including graph neural networks for graphs with vectorized features and Large Language Models for text-attributed graphs. Comprehensive experiments on synthetic and real-world datasets demonstrate GFSE's capability to significantly enhance the model's performance while requiring substantially less task-specific fine-tuning. 
Notably, GFSE boosts the performance by an average margin of 20.48% across eight real-world datasets, highlighting its potential as a powerful and adaptable foundational encoder for graph-structured data.",2025,0.5454543471075102,0.5416878135271657,0.5333333333333333,0.5,d8678682-fe59-4dc6-b571-ee33d1d7e98d,0,"[0.25, 0.5, 0.5, 0.625, 0.625]","[1.0, 0.9, 0.95, 1.0, 0.9]",Graph Foundational Structural Encoder,0.4863120525382811
548785,Informed Exploration via Generative Modeling,"Conventionally trained neural networks excel at prediction but often struggle to model uncertainty in their own predictions. We explore this challenge in a meta-learning bandit decision-making problem for news recommendations; this setting require decision-making algorithms to incorporate pretrained language models to process text data for the best performance. We present a scalable approach to Bayesian uncertainty quantification by posing it as a problem of autoregressive generative modeling of future rewards. First, we use historical data on previously released news articles to pre-train a generative model to predict sequences of future potential rewards. At inference time, our algorithm makes decisions based on limited previous rewards and autoregressively generated future rewards. Far from a heuristic, we synthesize insights from the literature to show our method is a novel implementation of Thompson (posterior) sampling, a prominent bandit algorithm. We prove our pretraining loss directly controls online decision-making performance, and we demonstrate our framework on a news recommendation task where we integrate end-to-end fine-tuning of a pretrained language model to process news article headline text to improve performance.",2025,0.5454543471075102,0.5445543476130005,0.5333333333333333,0.5,a83d073a-a01a-4d99-967c-ff2f5fc83451,0,"[0.5, 0.5, 0.5]","[0.95, 0.9, 0.9]",Autoregressive Generative Modeling,0.5000000000000001
548791,$\sigma$-zero: Gradient-based Optimization of $\ell_0$-norm Adversarial Examples,"Evaluating the adversarial robustness of deep networks to gradient-based attacks is challenging.
While most attacks consider $\ell_2$- and $\ell_\infty$-norm constraints to craft input perturbations, only a few investigate sparse $\ell_1$- and $\ell_0$-norm attacks.
In particular, $\ell_0$-norm attacks remain the least studied due to the inherent complexity of optimizing over a non-convex and non-differentiable constraint.
However, evaluating adversarial robustness under these attacks could reveal weaknesses otherwise left untested with more conventional $\ell_2$- and $\ell_\infty$-norm attacks.
In this work, we propose a novel $\ell_0$-norm attack, called $\sigma$-zero, which leverages a differentiable approximation of the $\ell_0$ norm to facilitate gradient-based optimization, and an adaptive projection operator to dynamically adjust the trade-off between loss minimization and perturbation sparsity.
Extensive evaluations using MNIST, CIFAR10, and ImageNet datasets, involving robust and non-robust models, show that $\sigma$-zero finds minimum $\ell_0$-norm adversarial examples without requiring any time-consuming hyperparameter tuning, and that it outperforms all competing sparse attacks in terms of success rate, perturbation size, and efficiency.",2025,0.7499997272728265,0.7533005439008824,0.6666666666666666,0.625,47781dfc-eacb-4c0c-8965-244521d0d23c,1,"[0.625, 0.625, 0.625, 0.875]","[0.9, 0.9, 0.95, 1.0]",$\sigma$-zero,0.7114751197632382
548795,Reward-Robust RLHF in LLMs,"As Large Language Models continue to progress toward more advanced forms of intelligence, Reinforcement Learning from Human Feedback is increasingly seen as a key pathway toward achieving Artificial General Intelligence. However, the reliance on reward-model-based alignment methods introduces significant challenges due to the inherent instability and imperfections of Reward Models (RMs), which can lead to critical issues such as reward hacking and misalignment with human intentions. In this paper, we introduce a reward-robust RLHF framework aimed at addressing these fundamental challenges, paving the way for more reliable and resilient learning in LLMs. Our approach introduces a novel optimization objective that carefully balances performance and robustness by incorporating Bayesian Reward Model Ensembles to model the uncertainty set of reward functions. This allows the framework to integrate both nominal performance and minimum reward signals, ensuring more stable learning even with imperfect RMs. Empirical results demonstrate that our framework consistently outperforms baselines across diverse benchmarks, showing improved accuracy and long-term stability. We also provide a theoretical analysis, demonstrating that reward-robust RLHF approaches the stability of constant reward settings, which proves to be acceptable even in a stochastic-case analysis. Together, these contributions highlight the framework’s potential to enhance both the performance and stability of LLM alignment.",2025,0.443181657024852,0.4378511014156235,0.5333333333333333,0.5,4c56c1c6-ca7b-4375-82b0-5a662a2fa778,0,"[0.0, 0.5, 0.5, 0.625]","[0.95, 0.9, 0.95, 0.9]",Reward-Robust RLHF,0.3861722797927461
548817,Simulate Before Act: Model-Based Planning for Web Agents,"Language agents have shown promising performance in automating web-based tasks, but the complexity and vast search spaces of real-world websites challenge reactive agents in identifying optimal solutions. While tree search agents offer enhanced exploration by interacting with actual websites, they often incur high costs, potential risks, and are challenging to implement for real-world websites. This paper explores a novel paradigm leveraging large language models' (LLMs) internal world models for planning in complex environments, presenting a middle ground between reactive agents and tree search agents. Results on two representative benchmarks, VisualWebArena and Mind2Web-live, demonstrate that our approach largely closes the gap between reactive agents and tree search agents, while maintaining efficiency and safety advantages. Notably, tree search can be considered as approaching an upper bound for our method, as it explores actual websites rather than simulations. This work opens new avenues for research into more effective and secure strategies for autonomous agents in complex, dynamic environments. It represents a step forward in improving upon reactive agents while approaching the performance of tree search methods, without incurring their implementation challenges and costs.",2025,0.5113634504132908,0.509599404039722,0.5333333333333333,0.5,01d5679f-d619-44a4-a775-9a5458409d18,0,"[0.0, 0.5, 0.5, 0.875]","[0.95, 0.9, 0.9, 0.95]",Model-Based Planning,0.464734455958549
548883,Balancing Token Efficiency and Structural Accuracy in LLMs Image Generation by Combining VQ-VAE and Diffusion Tokenizers,"We proposes a novel visual tokenizer by combining high-level semantic tokens and low-level pixel tokens to represent images, aiming to address the challenges of image-to-sequence conversion for Large Language Models (LLMs). Existing visual tokenizers, such as VQ-VAE and diffusion-based models, either struggle with token explosion as image resolution increases or fail to capture detailed structural information. Our method introduces a dual-token system: high-level semantic tokens capture the main content of the image, while low-level pixel tokens preserve structural details. By integrating these tokens in a hybrid architecture, we leverage a VQ-VAE branch to generate low-resolution guidance and a diffusion process to reconstruct high-resolution images with both semantic coherence and structural accuracy. This approach significantly reduces the number of required tokens and enhances image reconstruction quality, offering an efficient solution for tasks like image generation and understanding based on LLMs.",2025,0.2045453801653163,0.2042078803548752,0.2666666666666666,0.25,683ab7c3-66b4-41cf-a19d-f4951dc0bcee,0,"[0.0, 0.25, 0.25, 0.25]","[0.95, 0.95, 0.95, 0.95]",Dual-token system,0.1875000000091512
548902,SDDBench: A Benchmark for Synthesizable Drug Design,"A significant challenge in wet lab experiments with current drug design generative models is the trade-off between pharmacological properties and synthesizability. Molecules predicted to have highly desirable properties are often difficult to synthesize, while those that are easily synthesizable tend to exhibit less favorable properties. As a result, evaluating the synthesizability of molecules in general drug design scenarios remains a significant challenge in the field of drug discovery. The commonly used synthetic accessibility (SA) score aims to evaluate the ease of synthesizing generated molecules, but it falls short of guaranteeing that synthetic routes can actually be found. Inspired by recent advances in top-down synthetic route generation and forward reaction prediction, we propose a new, data-driven metric to evaluate molecule synthesizability. This novel metric leverages the synergistic duality between retrosynthetic planners and reaction predictors, both of which are trained on extensive reaction datasets. To demonstrate the efficacy of our metric, we conduct a comprehensive evaluation of round-trip scores across a range of representative molecule generative models.",2025,0.4090907603306326,0.4084157607097504,0.4,0.25,d06fbe99-dd7f-4503-a9c0-115fec8b5b15,0,"[0.25, 0.25, 0.5, 0.5]","[0.9, 0.95, 0.9, 0.95]",Synthesizability,0.375
548906,Tell me about yourself: LLMs are aware of their learned behaviors,"We study *behavioral self-awareness*, which we define as an LLM's capability to articulate its behavioral policies without relying on in-context examples. We finetune LLMs on examples that exhibit particular behaviors, including (a) making risk-seeking / risk-averse economic decisions, and (b) making the user say a certain word. Although these examples never contain explicit descriptions of the policy (e.g. ""I will now take the risk-seeking option""), we find that the finetuned LLMs can explicitly describe their policies through out-of-context reasoning. We demonstrate LLMs' behavioral self-awareness across various evaluation tasks, both for multiple-choice and free-form questions. 
Furthermore, we demonstrate that models can correctly attribute different learned policies to distinct personas.
Finally, we explore the connection between behavioral self-awareness and the concept of backdoors in AI safety, where certain behaviors are implanted in a model, often through data poisoning, and can be triggered under certain conditions. We find evidence that LLMs can recognize the existence of the backdoor-like behavior that they have acquired through fine-tuning.",2025,0.8181815206612653,0.8153350860722602,0.9333333333333332,0.875,c448aadf-bf94-49fd-8d55-aee9bdd73a0f,1,"[0.5, 0.625, 0.875, 0.875, 0.875]","[0.9, 0.95, 0.8, 0.95, 0.95]",Behavioral Self-Awareness,0.7487057299354578
548914,Capturing substructure interactions by invariant Information Bottle Theory for Generalizable Property Prediction,"Molecular interactions are a common phenomenon in physical chemistry, often resulting in unexpected biochemical properties harmful to humans, such as drug-drug interactions. Machine learning has shown great potential for predicting these interactions rapidly and accurately. However, the complexity of molecular structures and the diversity of interactions often reduce prediction accuracy and hinder generalizability. Identifying core invariant substructures (i.e., rationales) has become essential to improving the model's interpretability and generalization. Despite significant progress, existing models frequently overlook the pairwise molecular interaction, leading to insufficient capture of interaction dynamics. To address these limitations, we propose I2Mole (Interaction-aware Invariant Molecular learning), a novel framework for generalizable property prediction. I2Mole meticulously models atomic interactions, such as hydrogen bonds and Van der Waals forces, by first establishing indiscriminate connections between intermolecular atoms, which are then refined using an improved graph information bottleneck theory tailored for merged graphs. To further enhance model generalization, we construct an environment codebook by environment subgraph of the merged graph. This approach not only could provide noise source for optimizing mutual information but also preserve the integrity of chemical semantic information. By comprehensively leveraging the information inherent in the merged graph, our model accurately captures core substructures and significantly enhances generalization capabilities. Extensive experimental validation demonstrates I2Mole's efficacy and generalizability. The implementation code is available at https://anonymous.4open/r/I2Mol-C616.",2025,0.6477270371901683,0.6462041294645289,0.6666666666666666,0.625,ed186ac6-b96a-4e1b-af0c-f79587e4514a,0,"[0.5, 0.625, 0.625, 0.625]","[0.95, 0.95, 0.95, 0.9]",I2Mole,0.5918634371957154
548917,Minifinetuning: Low-Data Generation Domain Adaptation through Corrective Self-Distillation,"Finetuning language models for a new domain inevitably leads to the deterioration of their general performance.
This becomes more pronounced the more limited the finetuning data resource.

We introduce minifinetuning (MFT), a method for language model domain adaptation that considerably reduces the effects of overfitting-induced degeneralization in low-data settings and which does so in the absence of any pre-training data for replay.
MFT demonstrates 2-10x more favourable specialization-to-degeneralization ratios than standard finetuning across a wide range of models and domains and exhibits an intrinsic robustness to overfitting when data in the new domain is scarce and down to as little as 500 samples.

Employing corrective self-distillation that is individualized on the sample level, MFT outperforms parameter-efficient finetuning methods, demonstrates replay-like forgetting mitigation properties, and is composable with either for a combined effect.",2025,0.6818179338843877,0.6806929345162506,0.6,0.5,fa45df19-ee89-4bc6-bddb-71eca431fd9c,0,"[0.5, 0.5, 0.625, 0.875]","[0.95, 0.95, 0.95, 0.95]",Minifinetuning,0.6250000000000002
548922,Towards Effective Updating of Pretrained Symbolic Music Models for Fine-Grained Bar-Level Control,"Automatically generating symbolic music scores tailored to specific user needs offers significant benefits for musicians and enthusiasts alike. Pretrained symbolic music autoregressive models have demonstrated promising results, thanks to large datasets and advanced transformer architectures. However, in practice, the control provided by such models is often limited, particularly when fine-grained controls are needed at the level of individual bars. While fine-tuning the model with newly introduced control tokens may seem like a straightforward solution, our research reveals challenges in this approach, as the model frequently struggles to respond effectively to these precise bar-level control signals. To overcome this issue, we propose two novel strategies. First, we introduce a pre-training task that explicitly links control signals with their corresponding musical tokens, enabling a more effective initialization for fine-tuning. Second, we develop a unique counterfactual loss function that enhances alignment between the generated music and the specified control prompts. These combined methods substantially improve bar-level control, yielding a 13.06\% improvement over the fine-tuning baseline. Importantly, subjective evaluations confirm that this increased control does not compromise the musical quality produced by the original pretrained model.",2025,0.3409089669421938,0.336764388759527,0.2666666666666666,0.25,9559cf7b-2292-4bee-8dcf-f8b6be14f499,0,"[0.25, 0.25, 0.25, 0.5]","[1.0, 0.95, 0.95, 0.9]",Bar-Level Control,0.2968851909784909
548936,RoFt-Mol: Benchmarking Robust Fine-tuning with Molecular Graph Foundation Models,"In the era of foundation models, fine-tuning pre-trained models for specific downstream tasks has become crucial. This drives the need for robust fine-tuning methods to address challenges such as model overfitting and sparse labeling. Molecular graph foundation models (MGFMs) face unique difficulties that complicate fine-tuning. These models are limited by smaller pre-training datasets and more severe data scarcity for downstream tasks, both of which require enhanced model generalization. Moreover, MGFMs must accommodate diverse pre-training objectives, including both regression and classification tasks. To better understand and improve fine-tuning techniques under these conditions, we classify eight fine-tuning methods into three mechanisms: weight-based fine-tuning, representation-based fine-tuning, and partial fine-tuning. We benchmark these methods on downstream regression and classification tasks across both supervised and self-supervised pre-trained models in diverse labeling settings. This extensive evaluation provides valuable insights and informs the design of a refined robust fine-tuning method, DWiSE-FT. This approach combines the strengths of simple post-hoc weight interpolation with more complex weight ensemble fine-tuning methods, delivering improved performance across both task types while maintaining the ease of use inherent in post-hoc weight interpolation.",2025,0.5795452438017296,0.5772276084697805,0.5333333333333333,0.5,ed186ac6-b96a-4e1b-af0c-f79587e4514a,0,"[0.5, 0.5, 0.5, 0.625]","[0.95, 0.95, 0.95, 0.9]",DWiSE-FT,0.5255903115871472
548938,RootTracker: A Lightweight Framework to Trace Original Models of Fine-tuned LLMs in Black-Box Conditions,"Large Language Models (LLMs) demonstrate remarkable performance in various applications, yet their training demands extensive resources and time. Consequently, fine-tuning pre-trained LLMs has become a prevalent strategy for adapting these models to diverse downstream tasks, thereby reducing costs. Despite their benefits, LLMs have vulnerabilities, such as susceptibility to adversarial attacks, potential for jailbreaking, fairness issues, backdoor vulnerabilities, and the risk of generating inappropriate or harmful content. Since fine-tuned models inherit some characteristics from their original models, they may also inherit these issues and vulnerabilities. In this work, we propose a lightweight framework, RootTracker, specifically designed to trace the original models of fine-tuned LLMs. The core idea is to identify a set of prompts that can assess which pre-trained LLM a fine-tuned model most closely resembles. This process is conducted in a ''knockout tournament"" style, where the model is repeatedly tested against pairs of LLMs until the original pre-trained model is identified. To evaluate the effectiveness of our framework, we created 200 distinct fine-tuned models, derived from original models including GPT-Neo, GPT-2, TinyLlama, and Pythia. The results demonstrate that our framework accurately identified the original models for 85.7\% of the fine-tuned versions. Therefore, we advocate for timely updates to model versions or deliberate obfuscation of model types when deploying large models.",2025,0.3409089669421938,0.3394141902150118,0.2666666666666666,0.25,97e78110-689e-4ab3-83b9-c7a9b8e13954,0,"[0.25, 0.25, 0.25, 0.5]","[0.9, 0.9, 0.95, 0.9]",RootTracker,0.3082087486157254
548959,HATFormer: Historic Handwritten Arabic Text Recognition with Transformers,"Arabic handwritten text recognition (HTR) is challenging, especially for historical texts, due to diverse writing styles and the intrinsic features of Arabic script. Additionally, Arabic handwriting datasets are smaller compared to English ones, making it difficult to train generalizable Arabic HTR models. To address these challenges, we propose HATFormer, a transformer-based encoder-decoder architecture that builds on a state-of-the-art English HTR model.  By leveraging the transformer's attention mechanism, HATFormer captures spatial contextual information to address the intrinsic challenges of Arabic script through differentiating cursive characters, decomposing visual representations, and identifying diacritics. Our customization to historical handwritten Arabic includes an image processor for effective ViT information preprocessing, a text tokenizer for compact Arabic text representation, and a training pipeline that accounts for a limited amount of historic Arabic handwriting data. HATFormer achieves a character error rate~(CER) of 8.6% on the largest public historical handwritten Arabic dataset, with a 51% improvement over the best baseline in the literature. HATFormer also attains a comparable CER of 4.2% on the largest private non-historical dataset. Our work demonstrates the feasibility of adapting an English HTR method to a low-resource language with complex, language-specific challenges, contributing to advancements in document digitization, information retrieval, and cultural preservation. The source code will be available as a link on the discussion forum once it is open.",2025,0.613636140495949,0.616302650237099,0.6,0.5,edb6b59f-b0dd-42e9-87b1-e146e1c21897,0,"[0.5, 0.5, 0.625, 0.625]","[0.8, 0.95, 1.0, 0.95]",HATFormer,0.5762370823512175
548969,Exploring Representations and Interventions in Time Series Foundation Models,"Time series foundation models promise to be powerful tools for a wide range of applications. However, their internal representations and learned concepts are still not well understood. In this study, we investigate the structure and redundancy of representations across various TSFMs, examining the self-similarity of model layers within and across different model sizes. This analysis reveals block-like redundancy in the representations, which can be utilized for informed pruning to improve inference speed and efficiency. Additionally, we explore the concepts learned by these models—such as periodicity and trends—and how these can be manipulated through latent space steering to influence model behavior. Our experiments show that steering interventions can introduce new features, like adding periodicity or trends to signals that initially lacked them. These findings underscore the value of representational analysis for optimizing models and demonstrate how conceptual steering offers new possibilities for more controlled time series modeling.",2025,0.6818179338843877,0.6806929345162506,0.6666666666666666,0.625,9fcdb5e0-ca0e-44ae-b9a0-70583f46aa5d,0,"[0.625, 0.625, 0.625]","[0.9, 0.95, 0.9]",Latent Space Steering,0.6250000000000002
548997,Multiplicative Logit Adjustment Approximates Neural-Collapse-Aware Decision Boundary Adjustment,"Real-world data distributions are often highly skewed. This has spurred a growing body of research on long-tailed recognition, aimed at addressing the imbalance in training classification models. Among the methods studied, multiplicative logit adjustment (MLA) stands out as a simple and effective method. What theoretical foundation explains the effectiveness of this heuristic method?
We provide a justification for the effectiveness of MLA with the following two-step process. First, we develop a theory that adjusts optimal decision boundaries by estimating feature spread on the basis of neural collapse. Second, we demonstrate that MLA approximates this optimal method. Additionally, through experiments on long-tailed datasets, we illustrate the practical usefulness of MLA under more realistic conditions. We also offer experimental insights to guide the tuning of MLA hyperparameters.",2025,0.6363630413225304,0.6318888847744782,0.6666666666666666,0.625,68631335-102b-4ac5-8b4b-b6fb736bcf3b,1,"[0.5, 0.625, 0.625]","[0.95, 0.9, 0.8]",Multiplicative Logit Adjustment,0.5699960867068462
548999,Do LLMs estimate uncertainty well in instruction-following?,"Large language models (LLMs) could be valuable personal AI agents across various domains, provided they can precisely follow user instructions. However, recent studies have shown significant limitations in LLMs' instruction-following capabilities, raising concerns about their reliability in high-stakes applications. 
Accurately estimating LLMs' uncertainty in adhering to instructions is critical to mitigating deployment risks. We present, to our knowledge, the first systematic evaluation of the uncertainty estimation abilities of LLMs in the context of instruction-following. 
Our study identifies key challenges with existing instruction-following benchmarks, where multiple factors are entangled with uncertainty stems from instruction-following, complicating the isolation and comparison across methods and models.
To address these issues, we introduce a controlled evaluation setup with two benchmark versions of data, enabling a comprehensive comparison of uncertainty estimation methods under various conditions.
Our findings show that existing uncertainty methods struggle, particularly when models make subtle errors in instruction following. While internal model states provide some improvement, they remain inadequate in more complex scenarios. 
The insights from our controlled evaluation setups provide a crucial understanding of LLMs' limitations and potential for uncertainty estimation in instruction-following tasks, paving the way for more trustworthy AI agents.",2025,0.6477270371901683,0.6456857137256012,0.8,0.875,0a44d0c9-5058-418e-a6d5-d48f828313e3,1,"[0.0, 0.625, 0.875, 0.875]","[0.9, 0.8, 0.9, 0.9]",Uncertainty estimation,0.5907330397912589
549001,Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle,"Many existing evaluation benchmarks for Large Language Models (LLMs) quickly become outdated due to the emergence of new models and training data. These benchmarks also fall short in assessing how LLM performance changes over time, as they consist of static questions without a temporal dimension. To address these limitations, we propose using future event prediction as a continuous evaluation method to assess LLMs' temporal generalization and forecasting abilities. Our benchmark, Daily Oracle, automatically generates question-answer (QA) pairs from daily news, challenging LLMs to predict ""future"" event outcomes. Our findings reveal that as pre-training data becomes outdated, LLM performance degrades over time. While Retrieval Augmented Generation (RAG) has the potential to enhance prediction accuracy, the performance degradation pattern persists, highlighting the need for continuous model updates.",2025,0.5795452438017296,0.5678928578429985,0.5333333333333333,0.5,16aeec11-24a9-40cb-a80e-8ea896ee6903,0,"[0.25, 0.5, 0.5, 0.875]","[0.9, 0.9, 0.9, 0.8]",Daily Oracle,0.4980634377038486
549007,VisionTS: Visual Masked Autoencoders Are Free-Lunch Zero-Shot Time Series Forecasters,"Foundation models have emerged as a promising approach in time series forecasting (TSF). Existing approaches either repurpose large language models (LLMs) or build large-scale time series datasets to develop TSF foundation models for universal forecasting. However, these methods face challenges due to the severe cross-domain gap or in-domain heterogeneity. This paper explores a new road to building a TSF foundation model from rich and high-quality natural images. Our key insight is that a visual masked autoencoder, pre-trained on the ImageNet dataset, can naturally be a numeric series forecaster. By reformulating TSF as an image reconstruction task, we bridge the gap between image pre-training and TSF downstream tasks. Surprisingly, without further adaptation in the time-series domain, the proposed VisionTS could achieve superior zero-shot forecasting performance compared to existing TSF foundation models. With fine-tuning for one epoch, VisionTS could further improve the forecasting and achieve state-of-the-art performance in most cases.  Extensive experiments reveal intrinsic similarities between images and real-world time series, suggesting visual models may offer a ""free lunch'' for TSF and highlight the potential for future cross-modality research. Our code is available in the Supplementary Material.",2025,0.5909092396693675,0.5834507755543019,0.5333333333333333,0.25,9fcdb5e0-ca0e-44ae-b9a0-70583f46aa5d,0,"[0.25, 0.5, 0.875]","[0.95, 0.95, 0.9]",VisionTS,0.5142845313017558
549013,Self-MoE: Towards Compositional Large Language Models with Self-Specialized Experts,"We present Self-MoE, an approach that transforms a monolithic LLM into a compositional, modular system of self-specialized experts, named MiXSE (MiXture of Self-specialized Experts). Our approach leverages self-specialization, which constructs expert modules using self-generated synthetic data, each equipping a shared base LLM with distinct domain-specific capabilities, activated via self-optimized routing. This allows for dynamic and capability-specific handling of various target tasks, enhancing overall capabilities, without extensive human-labeled data and added parameters. Our empirical results reveal that specializing LLMs may exhibit potential trade-offs in performances on non-specialized tasks. On the other hand, our Self-MoE demonstrates substantial improvements (6.5%p on average) over the base LLM across diverse benchmarks such as knowledge, reasoning, math, and coding. It also consistently outperforms other methods, including instance merging and weight merging, while offering better flexibility and interpretability by design with semantic experts and routing. Our findings highlight the critical role of modularity, the applicability of Self-MoE to multiple base LLMs, and the potential of self-improvement in achieving efficient, scalable, and adaptable systems.",2025,0.6818179338843877,0.6806929345162506,0.6666666666666666,0.625,bd3b2e37-0e1f-46bd-8ff1-a1bf1cf4a582,1,"[0.625, 0.625, 0.625, 0.625]","[0.9, 0.9, 0.95, 0.95]",Self-Specialized Experts,0.6250000000000001
549024,Let Your Features Tell The Differences: Understanding Graph Convolution By Feature Splitting,"Graph Neural Networks (GNNs) have demonstrated strong capabilities in processing structured data. While traditional GNNs typically treat each feature dimension equally important during graph convolution, we raise an important question: **Is the graph convolution operation equally beneficial for each feature?** If not, the convolution operation on certain feature dimensions can possibly lead to harmful effects, even worse than convolution-free models. Therefore, it is required to distinguish convolution-favored and convolution-disfavored features. Traditional feature selection methods mainly focus on identifying informative features or reducing redundancy, but they are not suitable for structured data as they overlook graph structures. In graph community, some studies have investigated the performance of GNN with respect to node features using feature homophily metrics, which assess feature consistency across graph topology. Unfortunately, these metrics do not effectively align with GNN performance and cannot be reliably used for feature selection in GNNs. To address these limitations, we introduce a novel metric, Topological Feature Informativeness (TFI), to distinguish GNN-favored and GNN-disfavored features, where its effectiveness is validated through both theoretical analysis and empirical observations. Based on TFI, we propose a simple yet effective Graph Feature Selection (GFS) method, which processes GNN-favored and GNN-disfavored features with GNNs and non-GNN models separately. Compared to original GNNs, GFS significantly improves the extraction of useful topological information from each feature with comparable computational costs. Extensive experiments show that after applying GFS to $\textbf{8}$ baseline and state-of-the-art (SOTA) GNN architectures across $\textbf{10}$ datasets, $\textbf{90\%}$ of the GFS-augmented cases show significant performance boosts. Furthermore, our proposed TFI metric outperforms other feature selection methods for GFS. These results verify the effectiveness of both GFS and TFI. Additionally, we demonstrate that GFS's improvements are robust to hyperparameter tuning, highlighting its potential as a universally valid method for enhancing various GNN architectures. To facilitate reproducibility and further research, we have made our code publicly available at https://github.com/KTTRCDL/graph-feature-selection.",2025,0.6477270371901683,0.6466582877904381,0.6666666666666666,0.625,ba896cf3-085b-4e35-9d86-96a813143b66,1,"[0.5, 0.625, 0.625, 0.625]","[0.95, 0.9, 0.95, 1.0]",Topological Feature Informativeness,0.5946444869348535
549026,Stochastic Steepest Descent with Acceleration for $\ell_p$-Smooth Non-Convex Optimization,"In this work, we analyze stochastic $\ell_p$ steepest descent for non-convex problems. Specifically, for $p > 2$, we establish $\epsilon$-approximate stationarity (in expectation) with respect to the dual norm $\Vert\cdot\Vert_{p^*}^{p^*}$ at a rate of $O(\epsilon^{-4})$, thereby generalizing the previous guarantees for signSGD ($p=\infty$). In addition, inspired by techniques for the convex setting, we present a new accelerated $\ell_p$ descent method, called Stacey, based on interpolated primal-dual iterate sequences that are designed for non-Euclidean smooth optimization settings. We compare our algorithm against popular methods such as SGD, Adam, AdamW, and Lion on image classification and pretraining language modeling tasks, and our results demonstrate the potential for both faster convergence and achieving higher accuracy. We further evaluate our algorithm for different values of $p$ across various models and datasets, highlighting the importance and efficiency of non-Euclidean methods as compared to standard Euclidean-based approaches.",2025,0.3409089669421938,0.3394141902150118,0.2666666666666666,0.25,72746b36-44e1-47aa-abfe-d7d4fd50cf9a,0,"[0.25, 0.25, 0.25, 0.5]","[0.9, 0.9, 0.95, 0.9]",Stacey,0.3082087486157254
549038,EFFECTIVE REGULARIZATION WITH RELATIVE-DISTANCE VARIANCES IN DEEP METRIC LEARNING,"This paper develops, for the first time, a novel method using relative-distance variance to regularize deep metric learning (DML), overcoming the drawbacks of existing pair-distance-based metrics, notably loss functions. Being a fundamental field in machine learning research, DML has been widely studied with the goal of learning a feature space where dissimilar data samples are further apart than similar ones. A typical approach of DML is to optimize the feature space by maximizing the relative distances between negative and positive pairs. Despite the rapid advancement, the pair-distance-based approach suffers from a few drawbacks that it heavily relies on the appropriate selection of margin to determine decision boundaries, and it depends on the effective selection of informative pairs, and resulting in low generalization across tasks. To address these issues, this paper explores the use of relative-distance variance and investigates its impact on DML through both empirical and theoretical studies. Based upon such investigation, we propose a novel Relative Distance Variance Constraint (RDVC) loss by regularizing the representation or embedding function learning. The proposed RDVC loss can seamlessly integrate with various pair-distance-based loss functions to ensure a robust and effective performance. Substantial experimental results have demonstrated the effectiveness of our proposed RDVC loss on both within-domain and cross-domain retrieval tasks. In particular, the RDVC loss is also shown useful in fine-grained zero-shot sketch-based image retrieval, a challenging task, revealing its general applicability to cross-domain and zero-shot learning.",2025,0.3409089669421938,0.338600626019678,0.2666666666666666,0.25,c8d8fa86-6622-4ef6-b082-a390ced6bbb8,0,"[0.25, 0.25, 0.25, 0.5]","[1.0, 0.95, 1.0, 0.95]",Relative Distance Variance Constraint,0.3032912687585266
549044,Distilling Non-Autoregressive Model Knowledge for Autoregressive De Novo Peptide Sequencing,"Autoregressive (next-token-prediction) models excel in various language generation tasks compared to non-autoregressive (parallel prediction) models. However, their advantage diminishes in certain biology-related tasks like protein modeling and de novo peptide sequencing. Notably, previous studies show that Non-Autoregressive Transformers (NAT) can largely outperform Autoregressive Transformers (AT) in amino acid sequence prediction due to their bidirectional information flow. Despite their advantages, NATs struggle with generalizing to longer sequences, scaling to larger models, and facing extreme optimization difficulties compared to AT models. Motivated by this, we propose a novel framework for directly distilling knowledge from NATs, known for encoding superior protein representations, to enhance autoregressive generation. Our approach employs joint training with a shared encoder and a specially designed cross-decoder attention module. Additionally, we introduce a new training pipeline that uses importance annealing and cross-decoder gradient blocking to facilitate effective knowledge transfer. Evaluations on a widely used 9-species benchmark show that our proposed design achieves state-of-the-art performance. Specifically, AT and NAT baseline models each excel in different types of data prediction due to their unique inductive biases. Our model combines these advantages, achieving strong performance across all data types and outperforming baselines across all evaluation metrics. This work not only advances de novo peptide sequencing but also provides valuable insights into how autoregressive generation can benefit from non-autoregressive knowledge and how next-token prediction (GPT-style) can be enhanced through bidirectional learning (BERT-style).
We release our code for reproduction in the anonymous repository here: https://anonymous.4open.science/r/CrossNovo-E263.",2025,0.443181657024852,0.4433707042830288,0.4,0.25,c60315ae-a6f1-4499-a308-86cb27eef139,0,"[0.25, 0.25, 0.5, 0.625]","[0.9, 0.95, 0.9, 0.95]",Knowledge Distillation,0.4102655440414509
549050,SGHormerVQ: Bridging Graph Transformers and Spiking Neural Networks via Spiking Vector Quantization,"Graph Transformers (GTs), which simultaneously integrate message passing and self-attention mechanisms, have achieved promising empirical results in some graph prediction tasks. Although these approaches show the potential of Transformers in capturing long-range graph topology information, issues concerning the quadratic complexity and high computing energy consumption severely impair the scalability of GTs on large-scale graphs. Recently, as brain-inspired neural networks, Spiking Neural Networks (SNNs) provide an energy-saving deep learning option with lower computational and storage overhead via their unique spike-based event-driven biological neurons. Inspired by these characteristics, we propose SGHormerVQ, which bridges efficient Graph Transformers and spiking neural networks via spiking vector quantization. Spiking vector quantization generates implied codebooks with smaller sizes and higher codebook usage to assist self-attention blocks in performing efficient global information aggregation. SGHormerVQ effectively alleviates the reliance on complex machinery (distance measure, auxiliary loss, etc.) and the \textit{codebook collapse} present in previous vector quantization-based GNNs. In experiments, we compare SGHormerVQ with other state-of-the-art baselines on node classification datasets ranging from small to large. Experimental results show that SGHormerVQ has achieved competitive performances on most datasets while maintaining up to 518× faster inference speed compared to other GTs.",2025,0.5113634504132908,0.5105197008871879,0.5333333333333333,0.5,a0df1535-9d58-4ea0-9c40-4f0be8d4b24c,0,"[0.25, 0.5, 0.5, 0.625]","[0.95, 0.95, 0.95, 0.95]",Spiking Vector Quantization,0.4687499999999999
549064,Super(ficial)-alignment: Strong Models May Deceive Weak Models in Weak-to-Strong Generalization,"Superalignment, where humans act as weak supervisors for superhuman models, has become a crucial problem with the rapid development of Large Language Models (LLMs). Recent work has preliminarily studied this problem by using weak models to supervise strong models, and discovered that weakly supervised strong students can consistently outperform weak teachers towards the alignment target, leading to a weak-to-strong generalization phenomenon. However, we are concerned that behind such a promising phenomenon, whether there exists an issue of weak-to-strong deception, where strong models deceive weak models by exhibiting well-aligned in areas known to weak models but producing misaligned behaviors in cases weak models do not know. We take an initial step towards exploring this security issue in a specific but realistic multi-objective alignment case, where there may be some alignment targets conflicting with each other (e.g., helpfulness v.s. harmlessness). We aim to explore whether, in such cases, strong models might deliberately make mistakes in areas known to them but unknown to weak models within one alignment dimension, in exchange for a higher reward in another dimension. Through extensive experiments in both the reward modeling and preference optimization scenarios, we find: (1) The weak-to-strong deception phenomenon exists across all settings. (2) The deception intensifies as the capability gap between weak and strong models increases. (3) Bootstrapping with an intermediate model can mitigate the deception to some extent, though its effectiveness remains limited. Our work highlights the urgent need to pay more attention to the true reliability of superalignment.",2025,0.7499997272728265,0.7478299509247622,0.6666666666666666,0.625,438c674c-484b-47a2-a86d-0781dca83b13,1,"[0.625, 0.625, 0.625, 0.875]","[0.9, 0.95, 0.9, 0.9]",Weak-to-strong deception,0.6832087486157254
549087,Union-over-Intersections: Object Detection beyond Winner-Takes-All,"This paper revisits the problem of predicting box locations in object detection architectures. Typically, each box proposal or box query aims to directly maximize the intersection-over-union score with the ground truth, followed by a winner-takes-all non-maximum suppression where only the highest scoring box in each region is retained. We observe that both steps are sub-optimal: the first involves regressing proposals to the entire ground truth, which is a difficult task even with large receptive fields, and the second neglects valuable information from boxes other than the top candidate. Instead of regressing proposals to the whole ground truth, we propose a simpler approach—regress only to the area of intersection between the proposal and the ground truth. This avoids the need for proposals to extrapolate beyond their visual scope, improving localization accuracy. Rather than adopting a winner-takes-all strategy, we take the union over the regressed intersections of all boxes in a region to generate the final box outputs. Our plug-and-play method integrates seamlessly into proposal-based, grid-based, and query-based detection architectures with minimal modifications, consistently improving object localization and instance segmentation. We demonstrate its broad applicability and versatility across various detection and segmentation tasks.",2025,0.8522724173554846,0.8534843854486365,0.9333333333333332,0.875,f0a957c3-469f-4d91-a1dd-7b520eeafb16,1,"[0.5, 0.875, 0.875, 0.875]","[0.95, 1.0, 0.95, 1.0]",Union-over-Intersections,0.7950630968622081
549088,Dreamguider: Improved Training free Diffusion-based Conditional Generation,"Diffusion models have emerged as a formidable tool for training-free conditional generation. However, a key hurdle in inference-time guidance techniques is the need for compute-heavy backpropagation through the diffusion network for estimating the guidance direction. Moreover, these techniques often require handcrafted parameter tuning on a case-by-case basis. Although some recent works have introduced minimal compute methods for linear inverse problems, a generic lightweight guidance solution to both linear and non-linear guidance problems is still missing. To this end, we propose Dreamguider, a method that enables inference-time guidance without compute-heavy backpropagation through the diffusion network. The key idea is to regulate the gradient flow through a time-varying factor. Moreover, we propose an empirical guidance scale that works for a wide variety of tasks, hence removing the need for handcrafted parameter tuning. We further introduce an effective lightweight augmentation strategy that significantly boosts the performance during inference-time guidance. We present experiments using Dreamguider on multiple  tasks across multiple datasets and models to show the effectiveness of the proposed modules. To facilitate further research, we will make the code public after the review process.",2025,0.4090907603306326,0.4010566532561082,0.4,0.25,00057918-e81c-42f4-9e66-bdda2400b203,0,"[0.25, 0.25, 0.5, 0.5]","[1.0, 0.95, 0.95, 0.8]",Dreamguider,0.347525835297565
549090,InfiniteAudio: Infinite-Length Audio Generation with Consistent Acoustic Attributes,"This work aims to generate long-duration audio while preserving acoustic coherence, utilizing existing text-conditional audio generation models through diffusion-based approaches. Current diffusion models, however, encounter significant challenges in generating long audio sequences due to memory constraints, as output size scales with input length. While one possible solution is to concatenate short clips, this often leads to inconsistencies due to a lack of shared temporal information across segments.
To address these challenges, we propose InfiniteAudio, a novel inference technique designed to generate long audio with consistent acoustic attributes. Our method is based on three key components. First, we implement a curved denoising approach with a fixed-size input, enabling theoretically infinite audio generation while maintaining a constant memory footprint. Second, we introduce conditional guidance alternation, a mechanism that enhances intelligibility in long speech generation. Finally, initial self-attention features are shared across future frames to maintain temporal coherence.
The effectiveness of InfiniteAudio is demonstrated through comprehensive comparisons with existing text-to-audio generation baselines. Generated audio samples are available on our anonymous project page\footnote{https://anonymousforcf.github.io/InfiniteAudio/}.",2025,0.3749998636364132,0.3743811139839378,0.2666666666666666,0.25,dce80af3-eaa4-4e8c-8359-1470c609ba8d,0,"[0.25, 0.25, 0.25, 0.625]","[0.8, 0.95, 0.95, 0.9]",InfiniteAudio,0.3384934294119572
549121,Speaking Guided by Listening: Unsupervised Text-to-Speech Generative Model Guided by End-to-End Speech Recognition,"We propose to utilize end-to-end automatic speech recognition (E2EASR) as a guidance model to realize unsupervised text-to-speech (TTS). An unconditional score-based generative model (SGM) is trained with untranscribed speech data. In the sampling stage, the unconditional score estimated by the SGM is combined with the gradients from ASR models by the Bayes rule to get the conditional score. We use a set of small ASR models trained only on $80$-hour labeled ASR data to guide the unconditional SGM and generate speech with high-quality scores in both objective and subjective evaluation. Similarly, we can also use additional speaker verification models to control speaker identity for the synthesized speech. That allows us to do the zero-shot TTS for the target speaker with a few seconds of enrollment speech. Our best unsupervised synthesized speech gets $\sim8\%$ word error rate in testing, and the best speaker-controlled TTS gets $3.3$ mean opinion score (MOS) in the speaker similarly testing.",2025,0.3409089669421938,0.3394621109976018,0.2666666666666666,0.25,fa002240-6caa-45bf-a020-7500e9eaf950,0,"[0.25, 0.25, 0.25, 0.5]","[0.95, 1.0, 0.95, 0.95]",Unsupervised Text-to-Speech,0.3075294550810014
549125,Modeling All-Atom Glycan Structures via Hierarchical Message Passing and Multi-Scale Pre-training,"Understanding the various properties of glycans with machine learning has shown some preliminary promise. However, previous methods mainly focused on modeling the backbone structure of glycans as graphs of monosaccharides (i.e., sugar units), while they neglected the atomic structures underlying each monosaccharide, which are actually important indicators of glycan properties. In this work, we fill this blank by introducing the GlycanAA model for All-Atom-wise Glycan modeling. GlycanAA models a glycan as a heterogeneous graph with monosaccharide nodes representing its global backbone structure and atom nodes representing its local atomic-level structures. Based on such a graph, GlycanAA performs hierarchical message passing to capture from local atomic-level interactions to global monosaccharide-level interactions hierarchically. To further enhance the model capability, we pre-train GlycanAA on a high-quality unlabeled glycan dataset in a self-supervised way, deriving the PreGlycanAA model. Specifically, we design a multi-scale mask prediction algorithm to endow the model with knowledge about different levels of dependencies in a glycan. Extensive benchmark results show the superiority of GlycanAA over existing glycan encoders and verify the further improvements achieved by PreGlycanAA.",2025,0.5999997818182612,0.5946282980934063,0.5333333333333333,0.5,4df51be5-fad8-4c35-b16d-55c678dd7f8a,0,"[0.5, 0.5, 0.5, 0.625, 0.625]","[0.9, 0.9, 0.95, 0.8, 0.8]",GlycanAA,0.5352806017613317
549129,Geometry Informed Tokenization of Molecules for Language Model Generation,"We consider molecule generation in 3D space using language models (LMs), which requires discrete tokenization of 3D molecular geometries. Although tokenization of molecular graphs exists, that for 3D geometries is largely unexplored. Here, we attempt to bridge this gap by proposing the Geo2Seq, which converts molecular geometries into SE(3)-invariant 1D discrete sequences. Geo2Seq consists of canonical labeling and invariant spherical representation steps, which together maintain geometric and atomic fidelity in a format conducive to LMs. Our experiments show that, when coupled with Geo2Seq, various LMs excel in molecular geometry generation, especially in controlled generation tasks.",2025,0.5454543471075102,0.5464069215035798,0.6,0.625,8d09a41c-c6b2-415d-8e4b-60ac81ed643d,0,"[0.0, 0.25, 0.5, 0.5, 0.625, 0.625, 0.625, 0.875]","[0.9, 0.9, 0.95, 1.0, 0.8, 0.9, 0.95, 0.95]",Geo2Seq,0.5111507636343605
549135,Towards Ultra-High-Definition Image Deraining: A Benchmark and An Efficient Method,"Despite significant progress has been made in image deraining, existing approaches are mostly carried out on low-resolution images. The effectiveness of these methods on high-resolution images is still unknown, especially for ultra-high-definition (UHD) images, given the continuous advancement of imaging devices. In this paper, we focus on the task of UHD image deraining, and contribute the first large-scale UHD image deraining dataset, 4K-Rain13k, that contains 13,000 image pairs at 4K resolution. Based on this dataset, we conduct a benchmark study on existing methods for processing UHD images. Furthermore, we develop an effective and efficient architecture (called UDR-Mixer) to better solve this task. Specifically, our method contains two building components: a spatial feature rearrangement layer that captures long-range information of UHD images, and a frequency feature modulation layer that facilitates high-quality UHD image reconstruction. Extensive experimental results demonstrate that our method performs favorably against the state-of-the-art approaches while maintaining a lower model complexity. The code and dataset will be available to the public.",2025,0.6818179338843877,0.6806929345162506,0.6,0.5,c0554711-1c67-4f08-ba5c-80fe5a428572,0,"[0.5, 0.5, 0.625, 0.875]","[1.0, 1.0, 0.9, 1.0]",UHD image deraining,0.6249999999999999
549148,The Utility and Complexity of In- and Out-of-Distribution Machine Unlearning,"Machine unlearning, the process of selectively removing data from trained models, is increasingly crucial for addressing privacy concerns and knowledge gaps post-deployment. Despite this importance, existing approaches are often heuristic and lack formal guarantees. In this paper, we analyze the fundamental utility, time, and space complexity trade-offs of approximate unlearning, providing rigorous certification analogous to differential privacy. For in-distribution forget data—data similar to the retain set—we show that a surprisingly simple and general procedure, empirical risk minimization with output perturbation, achieves tight unlearning-utility-complexity trade-offs, addressing a previous theoretical gap on the separation from unlearning ``for free"" via differential privacy, which inherently facilitates the removal of such data. However, such techniques fail with out-of-distribution forget data—data significantly different from the retain set—where unlearning time complexity can exceed that of retraining, even for a single sample. To address this, we propose a new robust and noisy gradient descent variant that provably amortizes unlearning time complexity without compromising utility.",2025,0.7636360859505142,0.7614786610933345,0.6666666666666666,0.625,cb7d5dc5-dc47-44ba-8581-bbc109316557,1,"[0.5, 0.625, 0.625, 0.875, 0.875]","[0.9, 0.95, 0.9, 0.9, 0.9]",Machine Unlearning,0.6958230406467553
549162,Delta - Contrastive Decoding Mitigates Text Hallucinations in Large Language Models,"Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language processing tasks. Still, they are prone to generating hallucinations—factually incorrect or fabricated content that can undermine their reliability, especially in high-stakes domains such as healthcare and legal advisory. In response to this challenge, we propose Delta, a novel inference-time approach that leverages contrastive decoding to mitigate hallucinations without requiring model retraining or additional training data. Delta works by randomly masking portions of the input prompt, then contrasting the original and masked output distribution generated by the model, effectively mitigating hallucinations through inference-only computations. Delta was evaluated on context-rich QA benchmarks like SQuAD v1.1 and v2, achieving around 3 and 6 percentage points of improvement, respectively. It also showed gains of 7 and 2 percentage points on TriviaQA and Natural Question under-sampling decoding. Delta improved SQuAD v2’s no-answer exact match by over ten percentage points. These findings suggest that Delta is particularly effective when hallucinations arise from contextual ambiguity. Delta presents a computationally efficient and scalable solution for reducing hallucinations in real-world LLM applications by focusing on inference-time enhancements.",2025,0.2727271735537551,0.2694705406989028,0.2666666666666666,0.25,1f56589b-01f3-49c2-b461-d08659291f2b,0,"[0.0, 0.25, 0.25, 0.25, 0.5]","[1.0, 1.0, 0.95, 0.95, 0.95]",Delta,0.2348186674163621
549163,Scale-Aware Contrastive Reverse Distillation for Unsupervised Medical Anomaly Detection,"Unsupervised anomaly detection using deep learning has garnered significant research attention due to its broad applicability, particularly in medical imaging where labeled anomalous data are scarce. While earlier approaches leverage generative models like autoencoders and generative adversarial networks (GANs), they often fall short due to overgeneralization. Recent methods explore various strategies, including memory banks, normalizing flows, self-supervised learning, and knowledge distillation, to enhance discrimination. Among these, knowledge distillation, particularly reverse distillation, has shown promise. Following this paradigm, we propose a novel scale-aware contrastive reverse distillation model that addresses two key limitations of existing reverse distillation methods: insufficient feature discriminability and inability to handle anomaly scale variations. Specifically, we introduce a contrastive student-teacher learning approach to derive more discriminative representations by generating and exploring out-of-normal distributions. Further, we design a scale adaptation mechanism to softly weight contrastive distillation losses at different scales to account for the scale variation issue. Extensive experiments on benchmark datasets demonstrate state-of-the-art performance, validating the efficacy of the proposed method. The code will be made publicly available.",2025,0.7499997272728265,0.7487622279678756,0.7333333333333333,0.5,f544ea80-2bed-4c4a-a791-3aef744cd925,1,"[0.5, 0.5, 0.875, 0.875]","[0.9, 0.9, 0.9, 0.9]",Scale-Aware Contrastive Reverse Distillation,0.6875
549165,World Model on Million-Length Video And Language With Blockwise RingAttention,"Enabling long-context understanding remains a key challenge in scaling existing sequence models -- a crucial component in developing generally intelligent models that can process and operate over long temporal horizons that potentially consist of millions of tokens. In this paper, we aim to address these challenges by providing a comprehensive exploration of the full development process for producing 1M context language models and video-language models, setting new benchmarks in language retrieval and new capabilities in long video understanding. We detail our long context data curation process, progressive context extension from 4K to 1M tokens, and present an efficient open-source implementation for scalable training on long sequences. Additionally, we open-source a family of 7B parameter models capable of processing long text documents and videos exceeding 1M tokens.",2025,0.6477270371901683,0.6427810608354335,0.6666666666666666,0.625,b12c50fb-3330-4104-a421-ec2343d783d4,1,"[0.25, 0.625, 0.625, 0.875]","[1.0, 1.0, 1.0, 0.95]",Blockwise RingAttention,0.5744520330368489
549174,Scaling Multimodal Theory-of-Mind with Weak-to-Strong Bayesian Reasoning,"Theory of Mind (ToM) enables individuals to understand and predict thoughts, emotions, and intentions of the others. To replicate this cognitive ability in machines, especially under complex multimodal environments, recent advances combine Bayesian-based state inference with deep learning models to estimate mental states, where the Bayesian model handles state transitions and a language model (LM) estimates the likelihood of intermediate states. However, while post-training an LM to specialise in ToM tasks improves performance, the computational cost increases as the LM scales, limiting the model size to 7 billion parameters. Despite this post-training process, smaller LMs still struggle with the physical and mental modelling demands of ToM due to their limited world knowledge and reasoning capacity. To address this, we propose a scalable solution that leverages the strengths of larger LMs (up to 70 and 405 billion parameters, respectively), including their vast world knowledge and atomic-level reasoning capabilities, without increasing post-training resource requirements. Our method transfers ToM-specific behaviours from a post-trained small LM to guide the latent reasoning of a larger LM during test time. This weak-to-strong control mechanism enables the larger LM to improve Bayesian likelihood estimation at each inference step, harnessing its reasoning power in ToM scenarios while reducing the need for additional training resources. Extensive experiments demonstrate the significant effectiveness of our scaled approach. It is better at inferring human mental states in complex and interactive environments, outperforming the state-of-the-art solution by $\sim4.6$% across multiple tasks on the multimodal ToM benchmark and unseen scenarios. Our code and datasets are available: https://anonymous.4open.science/r/scale-bayesian-tom-248B",2025,0.6477270371901683,0.644118486313171,0.6666666666666666,0.625,88273b62-af23-4bf9-bfda-33e85d653083,0,"[0.5, 0.625, 0.625, 0.625]","[0.9, 0.8, 0.95, 0.7]",Weak-to-Strong Control,0.5881949896966082
549175,What Makes a Good Time-series Forecasting Model? A Causal Perspective,"Generalization is a long-standing challenge in multivariate time series forecasting (MTSF) tasks. Most existing forecasting methods use all available variables in historical series to predict all future variables, assuming that there may be correlations among all variables. From a causal perspective, this reliance on correlated variables can compromise the model’s generalization. To address this, we aim to explore the role of causal relationships in enhancing the generalization of multivariate time series models. We examine how graphical causal models, through conditional independence constraints, can reduce the hypothesis space, thereby improving generalization. Building on this foundation, we introduce a novel causality-based MTSF algorithm CAusal Informed Transformer (CAIFormer). It first constructs a Directed Acyclic Graph (DAG) among variables using causal discovery techniques. Then we build the forecasting model by enforcing the causal constraints informed by the DAG. Empirical evaluations on benchmark datasets demonstrate that our method surpasses traditional approaches in predictive accuracy. Additionally, we present the structural causal models derived for these datasets, underscoring the practical applicability of our causality-driven framework in MTSF.",2025,0.3749998636364132,0.3785774497866436,0.2666666666666666,0.25,e4f8d05a-f4e8-430b-bee8-63d7addde2df,0,"[0.25, 0.25, 0.25, 0.625]","[0.9, 0.9, 0.9, 0.95]",CAusal Informed Transformer,0.3630606312292357
549187,Data Shapley in One Training Run,"Data Shapley offers a principled framework for attributing the contribution of data within machine learning contexts. However, the traditional notion of Data Shapley requires re-training models on various data subsets, which becomes computationally infeasible for large-scale models. Additionally, this retraining-based definition cannot evaluate the contribution of data for a specific model training run, which may often be of interest in practice. This paper introduces a novel concept, In-Run Data Shapley, which eliminates the need for model retraining and is specifically designed for assessing data contribution for a particular model of interest. In-Run Data Shapley calculates the Shapley value for each gradient update iteration and accumulates these values throughout the training process. We present several techniques that allow the efficient scaling of In-Run Data Shapley to the size of foundation models. In its most optimized implementation, our method adds negligible runtime overhead compared to standard model training. This dramatic efficiency improvement makes it possible to perform data attribution for the foundation model pretraining stage. We present several case studies that offer fresh insights into pretraining data's contribution and discuss their implications for copyright in generative AI and pretraining data curation.",2025,0.5795452438017296,0.5799874099034832,0.6666666666666666,0.625,03e83d1c-61ed-4371-83a9-dfc38298f28c,1,"[0.0, 0.625, 0.625, 0.875]","[0.9, 0.9, 0.95, 0.9]",In-Run Data Shapley,0.5376868770764118
549197,Energy-Weighted Flow Matching for Offline Reinforcement Learning,"This paper investigates energy guidance in generative modeling, where the target distribution is defined as $q(\mathbf x) \propto p(\mathbf x)\exp(-\beta \mathcal E(\mathbf x))$, with $p(\mathbf x)$ being the data distribution and $\mathcal E(\mathbf x)$ as the energy function. To comply with energy guidance, existing methods often require auxiliary procedures to learn intermediate guidance during the diffusion process. To overcome this limitation, we explore energy-guided flow matching, a generalized form of the diffusion process. We introduce energy-weighted flow matching (EFM), a method that directly learns the energy-guided flow without the need for auxiliary models. Theoretical analysis shows that energy-weighted flow matching accurately captures the guided flow. Additionally, we extend this methodology to energy-weighted diffusion models and apply it to offline reinforcement learning (RL) by proposing the Q-weighted Iterative Policy Optimization (QIPO). Empirically, we demonstrate that the proposed QIPO algorithm improves performance in offline RL tasks. Notably, our algorithm is the first energy-guided diffusion model that operates independently of auxiliary models and the first exact energy-guided flow matching model in the literature.",2025,0.7159088305786071,0.7097361960918424,0.6666666666666666,0.625,85279dfe-c7f0-428a-b085-6dcf16b00017,1,"[0.5, 0.625, 0.625, 0.875]","[1.0, 0.9, 0.95, 0.9]",Energy-Weighted Flow Matching,0.6313923297896608
549198,Transforming Ocean Analysis: Learning 4D ocean field from in-situ observations via uncertainty-aware implicit representations,"A complete and accurate representation of Earth's time-evolving ocean field is crucial for understanding global warming as well as climate dynamics. However, the sparsity of current in-situ ocean measurements presents a significant challenge in estimating values in largely unobserved regions. Traditional methods, such as objective interpolation (OI), struggle with accuracy due to their reliance on discrete grids and fixed spatial correlation structures. In this paper, we propose a novel approach to reconstruct 4D ocean fields only from raw observations using implicit neural representations (INRs). Our method improves field representations by leveraging neural networks to capture continuous, complex, and nonlinear patterns inherent in ocean data. To address uncertainties in ocean measurements and the limited availability of daily observations, we incorporate uncertainty estimates and a meta-learning strategy into existing INRs. These innovations enable our approach to provide daily, resolution-free ocean temperature reconstructions, a significant improvement over  monthly averaged discrete fields. Experiments demonstrate the accuracy and adaptability of our method compared with approaches, establishing our method as a transformative solution for future ocean analysis and climate monitoring.",2025,0.4090907603306326,0.4069519986233666,0.2666666666666666,0.25,cf3e8cf1-2123-4538-9845-6c211cf213b0,0,"[0.25, 0.25, 0.25, 0.5, 0.625]","[0.95, 0.9, 0.95, 0.95, 0.9]",Implicit Neural Representations,0.3687361083047079
549210,Step-DPO: Step-wise Preference Optimization for Long-chain Reasoning of LLMs,"Mathematical reasoning presents a significant challenge for Large Language Models (LLMs) due to the extensive and precise chain of reasoning required for accuracy. Ensuring the correctness of each reasoning step is critical. To address this, we aim to enhance the robustness and factuality of LLMs by learning from human feedback. However, Direct Preference Optimization (DPO) has shown limited benefits for long-chain mathematical reasoning, as models employing DPO struggle to identify detailed errors in incorrect answers. This limitation stems from a lack of fine-grained process supervision. We propose a simple, effective, and data-efficient method called Step-DPO, which treats individual reasoning steps as units for preference optimization rather than evaluating answers holistically. Additionally, we have developed a data construction pipeline for Step-DPO, enabling the creation of a high-quality dataset containing 10K step-wise preference pairs. We also observe that in DPO, the data generated by the policy model is more effective than that produced by humans or GPT-4, due to the former's in-distribution nature. Our findings demonstrate that as few as 10K preference data pairs and fewer than 500 Step-DPO training steps can yield a nearly 3% gain in accuracy on MATH for models with over 70B parameters. Notably, Step-DPO, when applied to Qwen2-72B-Instruct, achieves scores of 70.8% and 94.0% on the test sets of MATH and GSM8K, respectively, surpassing a series of closed-source models, including GPT-4-1106, Claude-3-Opus, and Gemini-1.5-Pro.",2025,0.3409089669421938,0.3348268643907199,0.4,0.5,f40ae524-2644-4d2b-a661-4bdc107ab71c,0,"[0.0, 0.25, 0.5, 0.5]","[0.95, 0.95, 0.9, 0.9]",Step-DPO,0.2884067357512954
549216,RED – ROBUST ENVIRONMENTAL DESIGN,"The classification of road signs by autonomous systems, especially those reliant on
visual inputs, is highly susceptible to adversarial attacks. Traditional approaches to
mitigating such vulnerabilities have focused on enhancing the robustness of classi-
fication models. In contrast, this paper adopts a fundamentally different strategy
aimed at increasing robustness through the redesign of road signs themselves. We
propose an attacker-agnostic learning scheme to automatically design road signs
that are robust to a wide array of patch-based attacks. Empirical tests conducted in
both digital and physical environments demonstrate that our approach significantly
reduces vulnerability to patch attacks, outperforming existing techniques.",2025,0.2727271735537551,0.2722771738065002,0.2666666666666666,0.25,9b6207fc-e194-42b4-b246-d1d40e064ca0,0,"[0.25, 0.25, 0.25, 0.25]","[0.95, 0.95, 1.0, 0.95]",Attacker-agnostic road sign design,0.2499999999999999
549229,MagicTailor: Component-Controllable Personalization in Text-to-Image Diffusion Models,"Recent advancements in text-to-image (T2I) diffusion models have enabled the creation of high-quality images from text prompts, but they still struggle to generate images with precise control over specific visual concepts. Existing approaches can replicate a given concept by learning from reference images, yet they lack the flexibility for fine-grained customization of the individual component within the concept. In this paper, we introduce component-controllable personalization, a novel task that pushes the boundaries of T2I models by allowing users to reconfigure and personalize specific components of concepts. This task is particularly challenging due to two primary obstacles: semantic pollution, where unwanted visual elements corrupt the personalized concept, and semantic imbalance, which causes disproportionate learning of visual semantics. To overcome these challenges, we design MagicTailor, an innovative framework that leverages Dynamic Masked Degradation (DM-Deg) to dynamically perturb undesired visual semantics and Dual-Stream Balancing (DS-Bal) to establish a balanced learning paradigm for visual semantics. Extensive comparisons, ablations, and analyses demonstrate that MagicTailor not only excels in this challenging task but also holds significant promise for practical applications, paving the way for more nuanced and creative image generation. Our code will be released.",2025,0.4545456528924899,0.4537956527137322,0.5333333333333333,0.5,8dc97c49-3ddb-47f5-8699-7a7058e6ce83,0,"[0.25, 0.5, 0.5]","[0.95, 0.95, 0.95]",Component-Controllable Personalization,0.4166666666666667
549234,Improved Off-policy Reinforcement Learning in Biological Sequence Design,"Designing biological sequences with desired properties is a significant challenge due to the combinatorially vast search space and the high cost of evaluating each candidate sequence. To address these challenges, reinforcement learning (RL) methods, such as GFlowNets, utilize proxy models for rapid reward evaluation and annotated data for policy training. Although these approaches have shown promise in generating diverse and novel sequences, the limited training data relative to the vast search space often leads to the misspecification of proxy for out-of-distribution inputs. We introduce $\delta$-Conservative Search, a novel off-policy search method for training GFlowNets designed to improve robustness against proxy misspecification. The key idea is to incorporate conservativeness, controlled by parameter $\delta$, to constrain the search to reliable regions. Specifically, we inject noise into high-score offline sequences by randomly masking tokens with a Bernoulli distribution of parameter $\delta$ and then denoise masked tokens using the GFlowNet policy. Additionally, $\delta$ is adaptively adjusted based on the uncertainty of the proxy model for each data point. This enables the reflection of proxy uncertainty to determine the level of conservativeness. Experimental results demonstrate that our method consistently outperforms existing machine learning methods in discovering high-score sequences across diverse tasks—including DNA, RNA, protein, and peptide design—especially in large-scale scenarios.",2025,0.613636140495949,0.607961166740363,0.6,0.25,d9d0920a-f673-4076-8a29-036025ba944f,0,"[0.25, 0.5, 0.625, 0.875]","[0.95, 0.9, 0.9, 0.9]",$\delta$-Conservative Search,0.541043743078627
549242,FeDeRA: Efficient Fine-tuning of Language Models in Federated Learning Leveraging Weight Decomposition,"Federated learning (FL) is a widely used privacy-preserving approach for distributed training that avoids the need to collect data from individual users. In this paper, we investigate fine-tuning pre-trained language models (PLMs) in an FL setting and leverage parameter-efficient fine-tuning (PEFT) methods to reduce computational and communication costs. However, non-IID data in federated learning significantly degrades the performance of PEFT, with the degradation worsening as data heterogeneity increases. To address this, we propose FeDeRA, an FL approach for fine-tuning PLMs that incorporates an effective extension of the low-rank adaptation (LoRA) method. Specifically, FeDeRA initializes the low-rank matrices using Singular Value Decomposition (SVD) on the pre-trained weight matrices, rather than the zero or random initialization used in the original LoRA method. Analyzing weight updates during training reveals that FeDeRA reduces weight oscillations, enabling faster and more efficient fine-tuning of PLMs in FL with non-IID data. Experimental results across multiple NLP tasks and models show that FeDeRA outperforms all PEFT-based baselines in task performance and, in some cases, even matches or exceeds the performance of full-parameter fine-tuning. FeDeRA also greatly enhances training efficiency, reducing training time by up to 97.3\% compared to full-parameter fine-tuning and up to 74.6\% compared to the fastest PEFT baseline in practical FL settings. Furthermore, FeDeRA demonstrates greater robustness to data heterogeneity than all other PEFT methods, highlighting the effectiveness of its proposed initialization in FL systems.",2025,0.2727271735537551,0.2722771738065002,0.2666666666666666,0.25,e491457a-2824-40ce-afeb-8b961580bcb4,0,"[0.25, 0.25, 0.25, 0.25]","[0.95, 0.95, 1.0, 0.95]",FeDeRA,0.2499999999999999
549274,QPM: Discrete Optimization for Globally Interpretable Image Classification,"Understanding the classifications of deep neural networks, e.g. used in safety-critical situations, is becoming increasingly important. While recent models can locally explain a single decision, to provide a faithful global explanation about an accurate model’s general behavior is a more challenging open task. Towards that goal, we introduce the Quadratic Programming Enhanced Model (QPM), which learns globally interpretable class representations. QPM represents every class with a binary assignment of very few, typically 5, features, that are also assigned to other classes, ensuring easily comparable contrastive class representations. This compact binary assignment is found using discrete optimization based on predefined similarity measures and interpretability constraints. The resulting optimal assignment is used to fine-tune the diverse features, so that each of them becomes the shared general concept between the assigned classes. Extensive evaluations show that QPM delivers unprecedented global interpretability across small and large-scale datasets while setting the state of the art for the accuracy of interpretable models.",2025,0.772726628099408,0.7730722231540152,0.6666666666666666,0.625,174ee501-f3a9-46ab-a217-2aedb3d4350b,1,"[0.625, 0.625, 0.875]","[0.9, 0.95, 0.95]",Quadratic Programming Enhanced Model,0.7151788671745611
549283,Presto! Distilling Steps and Layers for Accelerating Music Generation,"Despite advances in diffusion-based text-to-music (TTM) methods, efficient, high-quality generation remains a challenge. We introduce Presto!, an approach to inference acceleration for score-based diffusion transformers via reducing both sampling steps and cost per step. To reduce steps, we develop a new score-based distribution matching distillation (DMD) method for the EDM-family of diffusion models, the first GAN-based distillation method for TTM. To reduce the cost per step, we develop a simple, but powerful improvement to a recent layer distillation method that improves learning via better preserving hidden state variance. Finally, we combine our step and layer distillation methods together for a dual-faceted approach. We evaluate our step and layer distillation methods independently and show each yield best-in-class performance. Our combined distillation method can generate high-quality outputs with improved diversity, accelerating our base model by 10-18x (230/435ms latency for 32 second mono/stereo 44.1kHz, 15x faster than the comparable SOTA model) — the fastest TTM to our knowledge.",2025,0.8522724173554846,0.8521927025360986,0.9333333333333332,0.875,9559cf7b-2292-4bee-8dcf-f8b6be14f499,1,"[0.5, 0.875, 0.875, 0.875]","[0.95, 0.95, 0.95, 1.0]",Distillation,0.7887058173784947
549328,When SNN meets ANN: Error-Free ANN-to-SNN Conversion for Extreme Edge Efficiency,"Spiking Neural Networks (SNN) are now demonstrating comparable accuracy to convolutional neural networks (CNN), thanks to advanced ANN-to-SNN conversion techniques, all while delivering remarkable energy and latency efficiency when deployed on neuromorphic hardware. However, these conversion techniques incur a large number of time steps, and consequently, high spiking activity. In this paper, we propose a novel ANN-to-SNN conversion framework, that incurs an exponentially lower number of time steps compared to that required in the existing conversion approaches. Our framework modifies the standard integrate-and-fire (IF) neuron model used in SNNs with no change in computational complexity and shifts the bias term of each batch normalization (BN) layer in the trained ANN. To reduce spiking activity, we propose training the source ANN with a fine-grained $\ell_1$ regularizer with surrogate gradients that encourages high spike sparsity in the converted SNN. Our proposed framework thus yields lossless SNNs with low latency, low compute energy, thanks to the low time steps and high spike sparsity, and high test accuracy, for example, 75.12% with only 4 time steps on the ImageNet dataset. Codes will be made available.",2025,0.6477270371901683,0.6457379909429721,0.6666666666666666,0.625,acf07e19-70e9-4877-b583-b55c4826eafb,0,"[0.5, 0.625, 0.625, 0.625]","[0.95, 0.9, 0.9, 0.95]",ANN-to-SNN conversion,0.5897344559585493
549335,Bridging Compressed Image Latents and Multimodal Large Language Models,"This paper presents the first-ever study of adapting compressed image latents to suit the needs of downstream vision tasks that adopt Multimodal Large Language Models (MLLMs). MLLMs have extended the success of large language models to modalities (e.g. images) beyond text, but their billion scale hinders deployment on resource-constrained end devices. While cloud-hosted MLLMs could be available, transmitting raw, uncompressed images captured by end devices to the cloud requires an efficient image compression system. To address this, we focus on emerging neural image compression and propose a novel framework with a lightweight transform-neck and a surrogate loss to adapt compressed image latents for MLLM-based vision tasks. 
Given the huge scale of MLLMs, our framework excludes the entire downstream MLLM except part of its visual encoder from training our system. This stands out from most existing coding for machine approaches that involve downstream networks in training and thus could be impractical when the networks are MLLMs. The proposed framework is general in that it is applicable to various MLLMs, neural image codecs, and multiple application scenarios, where the neural image codec can be (1) pre-trained for human perception without updating, (2) fully updated for joint human and machine perception, or (3) fully updated for only machine perception. 
Extensive experiments on different neural image codecs and various MLLMs show that our method achieves great rate-accuracy performance with much less complexity.",2025,0.6818179338843877,0.6806929345162506,0.6666666666666666,0.625,923e8e83-1012-490c-9001-ceb889b1cdb8,1,"[0.625, 0.625, 0.625, 0.625]","[0.95, 0.9, 0.9, 0.8]",Compressed Image Latents,0.625
549353,Restructuring Vector Quantization with the Rotation Trick,"Vector Quantized Variational AutoEncoders (VQ-VAEs) are designed to compress a continuous input to a discrete latent space and reconstruct it with minimal distortion. 
They operate by maintaining a set of vectors---often referred to as the codebook---and quantizing each encoder output to the nearest vector in the codebook. 
However, as vector quantization is non-differentiable, the gradient to the encoder flows _around_ the vector quantization layer rather than _through_ it in a straight-through approximation.
This approximation may be undesirable as all information from the vector quantization operation is lost. 
In this work, we propose a way to propagate gradients through the vector quantization layer of VQ-VAEs. 
We smoothly transform each encoder output into its corresponding codebook vector via a rotation and rescaling linear transformation that is treated as a constant during backpropagation. 
As a result, the relative magnitude and angle between encoder output and codebook vector becomes encoded into the gradient as it propagates through the vector quantization layer and back to the encoder.
Across 11 different VQ-VAE training paradigms, we find this restructuring improves reconstruction metrics, codebook utilization, and quantization error.",2025,0.9545451074381428,0.9529701083227508,0.9333333333333332,0.875,4108257d-952a-49e7-9d18-286979839fb3,1,"[0.875, 0.875, 0.875, 0.875]","[0.95, 0.95, 0.9, 0.95]",Rotation Trick,0.874998196898315
549357,Rethinking Table Instruction Tuning,"Recent advances in table understanding have focused on instruction-tuning large language models (LLMs) for table-related tasks. 
However, existing research has overlooked the impact of hyperparameter choices and lacks a comprehensive evaluation of the out-of-domain table understanding ability and the general capabilities of these table LLMs. 
In this paper, we evaluate these abilities in existing table LLMs, and reveal significant declines in both out-of-domain table understanding and general capabilities compared to their base models. 
Through systematic analysis, we show that hyperparameters, such as learning rate, can significantly influence both table-specific and general capabilities.
Contrary to the existing table instruction-tuning works, we demonstrate that smaller learning rates and fewer training instances can enhance table understanding while preserving general capabilities.
Based on our findings, we introduce **TAMA**, a **TA**ble LLM instruction-tuned from LLa**MA** 3.1 8B Instruct, which achieves performance on par with, or surpassing GPT-3.5 and GPT-4 on table tasks, while maintaining strong out-of-domain generalization and general capabilities. 
Our findings highlight the potential for reduced data annotation costs and more efficient model development through careful hyperparameter selection.",2025,0.727272826446245,0.7260728265202324,0.6666666666666666,0.5,3dfb3276-4ba5-4edd-b9ae-15c4e5340788,0,"[0.5, 0.625, 0.875]","[0.95, 0.95, 0.95]",TAMA,0.6666666666666667
549366,PROGRESSIVE KNOWLEDGE DISTILLATION (PKD): A MODULAR APPROACH FOR ARCHITECTURE-AGNOSTIC KNOWLEDGE DISTILLATION,"\textbf{Knowledge distillation (KD)} is a key technique for training \textbf{lightweight deep neural networks}, particularly in \textbf{resource-constrained environments}. While existing KD methods utilize intermediate features to improve student models, they often overlook the proper \textbf{alignment between teacher-student layers} and fail to select the most \textbf{informative data} for training each student layer. These limitations are especially pronounced in \textbf{architecture-agnostic scenarios}, where different network architectures complicate knowledge transfer.

We propose \textbf{PKD}, a \textbf{Progressive Knowledge Distillation} framework that progressively aligns teacher and student layers through \textbf{feature-based modularization}. Each student module is trained using the most \textbf{representative features} from its corresponding teacher module, starting with the shallowest layers and progressively moving to deeper ones. This training method enables efficient, architecture-agnostic knowledge transfer across a variety of model architectures. \textbf{Experiments on CIFAR-100 and ImageNet-1K} demonstrate that PKD outperforms baseline models, achieving performance improvements of up to \textbf{4.54\%} and \textbf{6.46\%}, respectively, thereby validating its effectiveness in diverse neural network settings.",2025,0.3818180429752571,0.3806021028510687,0.2666666666666666,0.25,91d2742d-f3be-419f-8320-6b058713e683,0,"[0.25, 0.25, 0.25, 0.5, 0.5]","[0.9, 0.95, 0.95, 0.95, 0.9]",Progressive Knowledge Distillation,0.3474944433218833
549367,Chain of Ideas: Revolutionizing Research in Idea Development with LLM Agents,"Effective research ideation is a critical step for scientific research. However, the exponential increase in scientific literature makes it challenging for researchers to stay current with recent advances and identify meaningful research directions. Recent developments in large language models~(LLMs) suggest a promising avenue for automating the generation of novel research ideas. However, existing methods for idea generation either trivially prompt LLMs or directly expose LLMs to extensive literature without indicating useful information.  Inspired by the research process of human researchers, we propose a Chain-of-Ideas (CoI) agent, an LLM-based agent that organizes relevant literature in a chain structure to effectively mirror the progressive development in a research domain. This organization facilitates LLMs to capture the current advancements in research, thereby enhancing their ideation capabilities. Furthermore, we propose Idea Arena, an evaluation protocol that can comprehensively evaluate idea generation methods from different perspectives, aligning closely with the preferences of human researchers. Experimental results indicate that the CoI agent consistently outperforms other methods and shows comparable quality as humans in research idea generation. Moreover, our CoI agent is budget-friendly, with a minimum cost of \$0.50 to generate a candidate idea and its corresponding experimental design.",2025,0.6477270371901683,0.6494180892241408,0.5333333333333333,0.5,ceca425f-7e1b-4e75-8c67-8a032ecc8dd2,0,"[0.5, 0.5, 0.5, 0.875]","[0.9, 0.95, 0.9, 0.95]",Chain-of-Ideas,0.6057966321243523
549369,Natural Policy Gradient for Average Reward Non-Stationary RL,"We consider the problem of non-stationary reinforcement learning (RL) in the infinite-horizon average-reward setting. We model it by a Markov Decision Process with time-varying rewards and transition probabilities, with a variation budget of $\Delta_T$. Existing non-stationary RL algorithms focus on model-based and model-free value-based methods. Policy-based methods, however, despite their flexibility in practice, are not theoretically well understood in non-stationary RL. We propose and analyze the first model-free policy-based algorithm, Non-Stationary Natural Actor-Critic (NS-NAC), a policy gradient method with efficient exploration for change and a novel interpretation of learning rates as adapting factors. We present a dynamic regret of $\mathcal{\tilde{O}} (|\mathcal{S}|^{\frac{1}{2}}|\mathcal{A}|^{\frac{1}{2}}\Delta_T^{\frac{1}{9}}T^{\frac{8}{9}} )$, where $T$ is the time horizon, and $|\mathcal{S}|$, $|\mathcal{A}|$ are, respectively, the size of the state and action space. The regret analysis relies on adapting the Lyapunov function based analysis to dynamic environments and characterizing the effects of simultaneous updates in policy, value function estimate and environment.",2025,0.6233768642265948,0.6217351244088862,0.6666666666666666,0.625,6b9467b0-43d5-4749-9a60-c33010366f1c,0,"[0.25, 0.5, 0.5, 0.625, 0.625, 0.625, 0.875]","[0.95, 0.9, 0.9, 0.95, 0.8, 0.9, 0.95]",Non-Stationary Natural Actor-Critic,0.5703220792426292
549394,Ferret-UI 2: Mastering Universal User Interface Understanding Across Platforms,"Building a generalist model for user interface (UI) understanding is challenging due to various foundational issues, such as platform diversity, resolution variation, and data limitation. In this paper, we introduce Ferret-UI 2, a multimodal large language model (MLLM) designed for universal UI understanding across a wide range of platforms, including iPhone, Android, iPad, Webpage, and AppleTV. Building on the foundation of Ferret-UI, Ferret-UI 2 introduces three key innovations: support for multiple platform types, high-resolution perception through adaptive scaling, and advanced task training data generation powered by GPT-4o with set-of-mark visual prompting. These advancements enable Ferret-UI 2 to perform complex, user-centered interactions, making it highly versatile and adaptable for the expanding diversity of platform ecosystems. Extensive empirical experiments on referring, grounding, user-centric advanced tasks (comprising 9 subtasks $\times$ 5 platforms), GUIDE next-action prediction dataset, and GUI-World multi-platform benchmark demonstrate that Ferret-UI 2 significantly outperforms Ferret-UI, and also shows strong cross-platform transfer capabilities.",2025,0.727272826446245,0.728461241888863,0.6666666666666666,0.5,020ffaca-4486-4234-aaec-8d5fb40f5088,1,"[0.5, 0.625, 0.875]","[0.9, 1.0, 0.95]",Ferret-UI 2,0.6742216079457177
549433,Graph Distributional Analytics: Enhancing GNN Explainability through Scalable Embedding and Distribution Analysis,"Graph Neural Networks (GNNs) have achieved significant success in processing graph-structured data but often lack interpretability, limiting their practical applicability. We introduce the Graph Distributional Analytics (GDA) framework, leveraging novel combinations of scalable techniques to enhance GNN explainability. The integration of Weisfeiler-Leman (WL) graph kernels with distributional distance analysis enables GDA to efficiently quantify graph data distributions, while capturing global structural complexities without significant computational costs. GDA creates high-dimensional embeddings employing WL kernels, measures the distribution of distances from measures of categorical central tendency, and assigns distribution scores to quantify each graph's deviation from this vector We evaluate GDA on the ENZYMES, ogbg-ppa, and MalNet-Tiny datasets. Our experiments demonstrate GDA not only accurately characterizes graph distributions but also outperforms baseline methods in identifying specific structural features responsible for misclassifications. This comprehensive analysis provides deeper insights into how training data distributions affect model performance, particularly with out-of-distribution (OOD) data. By revealing the underlying structural causes of GNN predictions through a novel synergy of established techniques, GDA enhances transparency and offers a practical tool for practitioners to build more interpretable and robust graph-based models. Our framework's scalability, efficiency, and ability to integrate with various embedding methods make it a valuable addition to the suite of tools available for GNN analysis.",2025,0.3272726082645061,0.3273120143936605,0.2666666666666666,0.25,df0eeceb-144e-4ecd-8d0f-c57b7cf2e4a5,0,"[0.25, 0.25, 0.25, 0.25, 0.5]","[0.95, 0.9, 0.9, 1.0, 0.95]",Graph Distributional Analytics,0.3011233196999472
549449,fine-tuning with very large dropout,"It is impossible today to pretend that the practice of machine learning is compatible with the idea that training and testing data follow the same distribution. Several authors have recently used ensemble techniques to show how scenarios involving multiple data distributions are best served by representations that are both richer than those obtained by regularizing for the best in-distribution performance, and richer than those obtained under the influence of the implicit sparsity bias of common stochastic gradient procedures.

This contribution investigates the use of very high dropout rates instead of ensembles to obtain such rich representations. Although training a deep network from scratch using such dropout rates is virtually impossible, fine-tuning a large pre-trained model under such conditions is not only possible but also achieves out-of-distribution performances that exceed those of both ensembles and weight averaging methods such as model soups. 

This result has practical significance because the importance of the fine-tuning scenario has considerably grown in recent years. This result also provides interesting insights on the nature of rich representations and on the intrinsically linear nature of fine-tuning a large network using a comparatively small dataset.",2025,0.5454543471075102,0.5416878135271657,0.5333333333333333,0.5,66488df3-bcc5-4ae5-88ce-6db3dfc3a6a5,0,"[0.25, 0.5, 0.5, 0.625, 0.625]","[1.0, 0.9, 0.95, 0.95, 0.95]",very large dropout,0.4831179706655864
549454,A diffusion model on toric varieties with application to protein loop modeling,"The conformation spaces of loop regions in proteins as well as closed kinematic linkages in robotics can be described by systems of polynomial equations, forming Toric varieties. These are real algebraic varieties, formulated as the zero sets of polynomial equations constraining the rotor angles in a linkage or macromolecular chain. These spaces are essentially stitched manifolds and contain singularities. Diffusion models have achieved spectacular success in applications in Cartesian space and smooth manifolds but have not been extended to varieties. Here we develop a diffusion model on the underlying variety by utilizing an appropriate Jacobian, whose loss of rank indicates singularities. This allows our method to explore the variety, without encountering singular or infeasible states. We demonstrated the approach on two important protein structure prediction problems: one is prediction of Major Histocompatibility Complex (MHC) peptide interactions, a critical part in the design of  neoantigen vaccines, and the other is loop prediction for nanobodies, an important class of drugs. In both, we improve upon the state of the art open source AlphaFold.",2025,0.4772725537190714,0.4814176274420539,0.5333333333333333,0.5,b508ddae-5058-4b4d-b9a7-8ee1379cd2d9,0,"[0.25, 0.5, 0.5, 0.5]","[0.8, 0.8, 0.9, 0.95]",Toric varieties,0.4548934225737131
549456,Advancing Text-to-3D Generation with Linearized Lookahead Variational Score Distillation,"Text-to-3D generation based on score distillation of pre-trained 2D diffusion models has gained increasing interest, with variational score distillation (VSD) as a remarkable example. 
VSD proves that vanilla score distillation can be improved by introducing an extra score-based model, which characterizes the distribution of images rendered from 3D models, to correct the distillation gradient. 
Despite the theoretical foundations, VSD, in practice, is likely to suffer from slow and sometimes ill-posed convergence.
In this paper, we perform an in-depth investigation of the interplay between the introduced score model and the 3D model, and find that we can simply adjust their optimization order to improve the generation quality. 
By doing so, the score model looks ahead to the current 3D state and hence yields more reasonable corrections. 
Nevertheless, naive lookahead VSD may suffer from unstable training in practice due to the potential over-fitting. 
To address this, we propose to use a linearized variant of the model for score distillation, giving rise to the Linearized Lookahead Variational Score Distillation ($L^2$-VSD). 
$L^2$-VSD can be realized efficiently with forward-mode autodiff functionalities of existing deep learning libraries. 
Extensive experiments validate the efficacy of $L^2$-VSD, revealing its clear superiority over prior score distillation-based methods. 
We also show that our method can be seamlessly incorporated into any other VSD-based text-to-3D framework.",2025,0.4636361950413836,0.4628711954710504,0.5333333333333333,0.25,4e70f926-0d81-4cf7-88f7-ddfa93161f52,0,"[0.25, 0.25, 0.5, 0.5, 0.625]","[0.95, 0.95, 0.95, 0.95, 0.95]",$L^2$-VSD,0.425
549460,Unsupervised Learning of Facial Attribute Representations Using StyleGAN,"Facial attributes (e.g., gender, age) encompass important social cues and play a pivotal role in computer vision. While supervised methods have dominated facial attribute analysis, they often require large annotated datasets, which are costly and time-consuming to create.
In this work, we circumvent this limitation by proposing a novel unsupervised learning framework that leverages StyleGAN to learn rich and disentangled facial attribute representations. Specifically, unlike prior methods that rely on labeled datasets or supervised techniques, our approach exploits the unique inductive bias of StyleGAN, namely Hierarchical Feature Modulation, to automatically discover semantically meaningful representations of facial attributes. This inductive bias enables StyleGAN to generate disentangled and interpretable facial attribute features at different layers, benefiting a variety of downstream tasks. To leverage StyleGAN representations, we employ GAN inversion methods to represent input images as StyleGAN features and propose a simple yet effective feature reduction method based on mutual information to improve the effectiveness and efficiency of the learned representations. Extensive experiments in few-shot facial attribute analysis tasks, including clustering, classification, and facial attribute annotation demonstrate the effectiveness of our approach.",2025,0.2727271735537551,0.2722771738065002,0.2666666666666666,0.25,1d968458-a364-4afd-8372-27008ad228a9,0,"[0.25, 0.25, 0.25]","[0.9, 1.0, 0.95]",StyleGAN,0.2500000000000033
549472,V2M: Visual 2-Dimensional Mamba for Image Representation Learning,"Mamba has garnered widespread attention due to its flexible design and efficient hardware performance to process 1D sequences based on the state space model (SSM). Recent studies have attempted to apply Mamba to the visual domain by flattening 2D images into patches and then regarding them as a 1D sequence. To compensate for the 2D structure information loss (e.g., local similarity) of the original image, most existing methods focus on designing different orders to sequentially process the tokens, which could only alleviate this issue to some extent. In this paper, we propose a Visual 2-Dimensional Mamba (V2M) model as a complete solution, which directly processes image tokens in the 2D space. We first generalize SSM to the 2-dimensional space which generates the next state considering two adjacent states on both dimensions (e.g., columns and rows). We then construct our V2M based on the 2-dimensional SSM formulation and incorporate Mamba to achieve hardware-efficient parallel processing. The proposed V2M effectively incorporates the 2D locality prior yet inherits the efficiency and input-dependent scalability of Mamba. Extensive experimental results on ImageNet classification and downstream visual tasks including object detection and instance segmentation on COCO and semantic segmentation on ADE20K demonstrate the effectiveness of our V2M compared with other visual backbones.",2025,0.613636140495949,0.6205795800832515,0.6,0.25,1a96a307-9c85-403a-be55-078bdc8942bb,0,"[0.25, 0.5, 0.625, 0.875]","[0.9, 1.0, 0.95, 1.0]",Visual 2-Dimensional Mamba,0.5978012954797118
549479,KV-Dict: Sparse KV Cache Compression with Universal Dictionaries,"Transformer has become the de facto architecture for Large Language Models (LLMs), yet its substantial memory required for long contexts make it costly to deploy. Managing the memory usage of the key-value (KV) cache during inference has become a pressing challenge, as the cache grows with both model size and input length, consuming significant GPU memory.

We introduce a novel post-training KV cache compression method using KV-Dict, a universal dictionary that can accurately decompose and reconstruct key-value states. Unlike traditional quantization methods, KV-dict leverages sparse dictionary learning, allowing for flexible memory usage with minimal performance loss through fine-grained controls of sparsity levels. Moreoever, we retain competitive performance in the low memory regimes that 2-bit compression struggles to offer.

KV-Dict is remarkably universal, as it uses a small, input-agnostic dictionary that is shared across tasks and batches without scaling memory. This universality, combined with the ability to control sparsity for different memory requirements, offers a flexible and efficient solution to the KV cache bottleneck, maintaining strong performance on complex reasoning tasks, such as LongBench and GSM8k.",2025,0.5795452438017296,0.5781228558172563,0.5333333333333333,0.5,251ec9f9-d540-472c-82de-24823fc271cf,0,"[0.5, 0.5, 0.5, 0.625]","[0.95, 0.9, 0.9, 0.9]",KV-Dict,0.5291043743078626
549512,MOMENTUM MEETS VIRALITY: A NOVEL METRIC FOR UNMASKING SOCIAL BIAS IN VIRAL TWEETS,"Predicting which social media posts will go viral is a critical but complex task in the field of computational social science. Previous studies have utilized various measures to forecast the virality of tweets or Facebook posts, but these approaches exhibit limitations, particularly in the absence of a virality metric that specifically considers social biases. In this paper, we test existing metrics and introduce a new metric, $\textbf{ViralTweet Score (VTS)}$, inspired by principles of momentum from physics to better predict a tweet's virality given that it consists of social biases. We compare this new metric with others, highlighting the advantages and disadvantages of each of them as a virality measurement metric. We release the $\textbf{ViralTweets Dataset}$ with $\mathbf{88.8k}$ Hindi tweets and corresponding virality labels based on our VTS metric. We also show how social biases in posts can influence their potential to go viral. We test our hypothesis that VTS is a better metric using two methodologies and we show how VTS achieves an F1 score of 0.87 based on pairwise evaluation methodology and an overall F1 score of 0.58 based on our clustering-based verification methodology. Our work offers a novel metric for understanding tweet virality for biased tweets and opens the door for more equitable and effective social media analytics by considering the role of social biases in virality.",2025,0.2727271735537551,0.2686471745253119,0.2666666666666666,0.25,742bb5cf-3bec-43eb-b21c-dacd5ef1a1fe,0,"[0.0, 0.25, 0.25, 0.5]","[0.95, 0.95, 0.95, 0.9]",ViralTweet Score (VTS),0.2349074975657258
549513,Discrete Copula Diffusion,"Discrete diffusion models have recently shown significant progress in modeling complex data, such as natural languages and DNA sequences. However, unlike diffusion models for continuous data, which can generate high-quality samples in just a few denoising steps, modern discrete diffusion models still require hundreds or even thousands of denoising steps to perform well. In this paper, we identify a fundamental limitation that prevents discrete diffusion models from achieving strong performance with fewer steps -- they fail to capture dependencies between output variables at each denoising step. To address this issue, we provide a formal explanation and introduce a general approach to supplement the missing dependency information by incorporating another deep generative model, termed the copula model. Our method does not require fine-tuning either the diffusion model or the copula model, yet it enables high-quality sample generation with significantly fewer denoising steps. When we apply this approach to autoregressive copula models, the combined model outperforms both models individually in unconditional and conditional text generation. Specifically, the hybrid model achieves better (un)conditional text generation using 8 to 32 times fewer denoising steps than the diffusion model alone. In addition to presenting an effective discrete diffusion generation algorithm, this paper emphasizes the importance of modeling inter-variable dependencies in discrete diffusion.",2025,0.6477270371901683,0.6448667039867912,0.6666666666666666,0.625,7bdf79fe-228a-4bda-adc4-486a17e03f84,1,"[0.5, 0.625, 0.625, 0.625]","[1.0, 0.95, 0.9, 0.95]",Copula Model,0.5841536216195387
549514,Reinforcement Learning with Action Sequence for Data-Efficient Robot Learning,"Training reinforcement learning (RL) agents on robotic tasks typically requires a large number of training samples. This is because training data often consists of noisy trajectories, whether from exploration or human-collected demonstrations, making it difficult to learn value functions that understand the effect of taking each action. On the other hand, recent behavior-cloning (BC) approaches have shown that predicting a sequence of actions enables policies to effectively approximate noisy, multi-modal distributions of expert demonstrations. Can we use a similar idea for improving RL on robotic tasks? In this paper, we introduce a novel RL algorithm that learns a critic network that outputs Q-values over a sequence of actions. By explicitly training the value functions to learn the consequence of executing a series of current and future actions, our algorithm allows for learning useful value functions from noisy trajectories. We study our algorithm across various setups with sparse and dense rewards, and with or without demonstrations, spanning mobile bi-manual manipulation, whole-body control, and tabletop manipulation tasks from BiGym, HumanoidBench, and RLBench. We find that, by learning the critic network with action sequences, our algorithm outperforms various RL and BC baselines, in particular on challenging humanoid control tasks.",2025,0.5909092396693675,0.5899342396169823,0.5333333333333333,0.5,b7313bfc-429e-48db-af49-6fc04ad06434,0,"[0.5, 0.5, 0.625]","[0.95, 0.95, 0.95]",Action Sequence Critic,0.5416666666666666
549518,DA-Bench: Benchmarking Unsupervised Domain Adaptation Methods with Realistic Validation On Diverse Modalities,"Unsupervised Domain Adaptation (DA) consists of adapting a model trained on a labeled source domain to perform well on an unlabeled target domain with some data distribution shift.
While many methods have been proposed in the literature, fair and realistic evaluation remains an open question, particularly due to methodological difficulties in selecting hyperparameters in the unsupervised setting.
With DA-Bench, we propose a framework to evaluate DA methods on diverse modalities, beyond computer vision task that have been largely explored in the literature. We present a complete and fair evaluation of existing shallow algorithms, including reweighting, mapping, and subspace alignment.
Realistic hyperparameter selection is performed with nested cross-validation and various unsupervised model selection scores, on both simulated datasets with controlled shifts and real-world datasets across diverse modalities, such as images, text, biomedical, and tabular data.
Our benchmark highlights the importance of realistic validation and provides practical guidance for real-life applications, with key insights into the choice and impact of model selection approaches.
DA-Bench is open-source, reproducible, and can be easily extended with novel DA methods, datasets, and model selection criteria without requiring re-evaluating competitors.",2025,0.6477270371901683,0.6448667039867912,0.6666666666666666,0.625,dd39825f-cd7b-48cd-b874-62f88bfe6bdf,0,"[0.5, 0.625, 0.625, 0.625]","[1.0, 0.95, 0.95, 0.9]",DA-Bench,0.5841536216195388
549525,Identifiability for Gaussian Processes with Holomorphic Kernels,"Gaussian processes (GPs) are widely recognized for their robustness and flexibility across various domains, including machine learning, time series, spatial statistics, and biomedicine. In addition to their common usage in regression tasks, GP kernel parameters are frequently interpreted in various applications. For example, in spatial transcriptomics, estimated kernel parameters are used to identify spatial variable genes, which exhibit significant expression patterns across different tissue locations. However, before these parameters can be meaningfully interpreted, it is essential to establish their identifiability. Existing studies of GP parameter identifiability have focused primarily on Mat\'ern-type kernels, as their spectral densities allow for more established mathematical tools. In many real-world applications, particuarly in time series analysis, other kernels such as the squared exponential, periodic, and rational quadratic kernels, as well as their combinations, are also widely used. These kernels share the property of being holomorphic around zero, and their parameter identifiability remains underexplored.
In this paper, we bridge this gap by developing a novel theoretical framework for determining kernel parameter identifiability for kernels holomorphic near zero. Our findings enable practitioners to determine which parameters are identifiable in both existing and newly constructed kernels, supporting application-specific interpretation of the identifiable parameters, and highlighting non-identifiable parameters that require careful interpretation.",2025,0.8181815206612653,0.8243942921991501,0.8,0.625,e49a8fec-1505-45c8-b8a9-8e5b9db14160,1,"[0.625, 0.625, 0.875, 0.875]","[0.8, 0.9, 0.95, 0.95]",Holomorphic Kernels,0.777785326404024
549531,A Contrastive Teacher-Student Framework for Novelty Detection under Style Shifts,"There have been several efforts to improve Novelty Detection (ND) performance. However, ND methods often suffer significant performance drops under minor distribution shifts caused by changes in the environment, known as style shifts. This challenge arises from the ND setup, where the absence of out-of-distribution (OOD) samples during training causes the detector to be biased toward the dominant style features in the in-distribution (ID) data. As a result, the model mistakenly learns to correlate style with core features, using this shortcut for detection. Robust ND is crucial for real-world applications like autonomous driving and medical imaging, where test samples may have different styles than the training data. Motivated by this, we propose a robust ND method that crafts an auxiliary OOD set with style features similar to the ID set but with different core features. Then, a task-based knowledge distillation strategy is utilized to distinguish core features from style features and help our model rely on core features for discriminating crafted OOD and ID sets. We verified the effectiveness of our method through extensive experimental evaluations on several datasets, including synthetic and real-world benchmarks, against nine different ND methods.",2025,0.6272724991736367,0.6182957191493625,0.6666666666666666,0.625,6065a637-8a65-43c3-9a9f-efc52f6a64a2,0,"[0.25, 0.5, 0.625, 0.625, 0.875]","[1.0, 1.0, 0.95, 0.95, 0.9]",Contrastive Teacher-Student Framework,0.5371925937718582
549533,On the Convergence of FedProx with Extrapolation and Inexact Prox,"Enhancing the FedProx federated learning algorithm (Li et al., 2020) with server-side extrapolation, Li et al. (2024a) recently introduced the FedExProx method. Their theoretical analysis, however, relies on the assumption that each client computes a certain proximal operator exactly, which is impractical since this is virtually never possible to do in real settings. In this paper, we investigate the behavior of FedExProx without this exactness assumption in the smooth and globally strongly convex setting. We establish a general convergence result, showing that inexactness leads to convergence to a neighborhood of the solution. Additionally, we demonstrate that, with careful control, the adverse effects of this inexactness can be mitigated. By linking inexactness to biased compression (Beznosikov et al., 2023), we refine our analysis, highlighting robustness of extrapolation to inexact proximal updates. We also examine the local iteration complexity required by each client to achieved the required level of inexactness using various local optimizers. Our theoretical insights are validated through comprehensive numerical experiments.",2025,0.6477270371901683,0.6447675950955257,0.6666666666666666,0.625,f6869acd-45c3-4260-8bc7-37e248d1a7b3,0,"[0.5, 0.625, 0.625, 0.625]","[0.95, 0.8, 0.95, 0.9]",FedExProx,0.5868036683988817
549535,Multi-task Learning for Heterogeneous Multi-source Block-Wise Missing Data,"Multi-task learning (MTL) has emerged as an imperative machine learning tool to solve multiple learning tasks simultaneously and has been successfully applied to  healthcare, marketing, and biomedical fields. However, in order to borrow information across different tasks effectively, it is essential to utilize both homogeneous and heterogeneous information. Among the extensive literature on MTL, various forms of heterogeneity are presented in MTL problems, such as block-wise, distribution, and posterior heterogeneity. Existing methods, however, struggle to tackle these forms of heterogeneity simultaneously in a unified framework. In this paper, we propose a two-step learning strategy for MTL which addresses the aforementioned heterogeneity. First, we impute the missing blocks using shared representations extracted from homogeneous source across different tasks. Next, we disentangle the mappings between input features and responses into a shared component and a task-specific component, respectively, thereby enabling information borrowing through the shared component. Our numerical experiments and real-data analysis from the ADNI database demonstrate the superior MTL performance of the proposed method compared to a single task learning and other competing methods.",2025,0.5795452438017296,0.5772276084697805,0.5333333333333333,0.5,86129fda-54ed-4704-972e-60341477b847,0,"[0.5, 0.5, 0.5, 0.625]","[0.95, 0.95, 0.95, 0.9]",Block-Wise Missing Data,0.5255903115871472
549545,"Equally Critical: Samples, Targets, and Their Mappings in Datasets","Neural scaling laws highlight the trade-off between test error reduction and increased resources in machine learning, revealing diminishing returns as data volume, model size, and computational power increase.
This inefficiency poses sustainability challenges, as marginal performance gains necessitate exponential resource consumption.
Recent works have investigated these laws from a data-efficient standpoint, primarily concentrating on sample optimization, while largely neglecting the influence of target.
In this study, we first demonstrate that, given an equivalent training budget, employing soft targets on a 10% subset can outperform the use of one-hot targets on the full dataset. Building on this observation, we review existing paradigms in the sample-target relationship, categorizing them into distinct sample-to-target mapping strategies.
Subsequently, we propose a unified loss framework to assess their impact on training efficiency. Finally, we conduct a comprehensive analysis of how variations in target and sample types, quantities, and qualities influence training efficiency across three training strategies, providing six key insights to enhance training efficacy.",2025,0.5454543471075102,0.5365461313770037,0.6,0.625,13510e79-0109-4d41-8db2-13f8df076505,0,"[0.25, 0.5, 0.625, 0.625]","[0.9, 0.9, 0.9, 0.7]",Sample-Target Mapping,0.4802219006271105
549555,Proxy Denoising for Source-Free Domain Adaptation,"Source-Free Domain Adaptation (SFDA) aims to adapt a pre-trained source model to an unlabeled target domain with no access to the source data. Inspired by the success of large Vision-Language (ViL) models in many applications, the latest research has validated ViL's benefit for SFDA by using their predictions as pseudo supervision. However, we observe that ViL's supervision could be noisy and inaccurate at an unknown rate, potentially introducing additional negative effects during adaption. To address this thus-far ignored challenge, we introduce a novel Proxy Denoising (__ProDe__) approach. The key idea is to leverage the ViL model as a proxy to facilitate the adaptation process towards the latent domain-invariant space. Concretely, we design a proxy denoising mechanism to correct ViL's predictions. This is grounded on a proxy confidence theory that models the dynamic effect of proxy's divergence against the domain-invariant space during adaptation. To capitalize the corrected proxy, we further derive a mutual knowledge distilling regularization. Extensive experiments show that ProDe significantly outperforms the current state-of-the-art alternatives under both conventional closed-set setting and the more challenging open-set, partial-set, generalized SFDA, multi-target, multi-source, and test-time settings. Our code and data are available at https://github.com/tntek/source-free-domain-adaptation.",2025,0.9545451074381428,0.9529701083227508,0.9333333333333332,0.875,5c7e17e6-74fd-4f66-86d8-1b7507ce316f,1,"[0.875, 0.875, 0.875, 0.875]","[0.95, 0.95, 0.95, 0.9]",Proxy Denoising,0.8749981968983148
549564,SPARTUN3D: Situated Spatial Understanding of 3D World in Large Language Model,"Integrating the 3D world into large language models (3D-based LLMs) has been a promising research direction for 3D scene understanding. However, current 3D-based LLMs fall short in situated understanding due to two key limitations: 1) existing 3D datasets are constructed from a global perspective of the 3D scenes and lack situated context.
2) the architectures of the current 3D-based LLMs lack an explicit mechanism for aligning situated spatial information between 3D representations and natural language, limiting their performance in tasks requiring precise spatial reasoning. 
In this work, we address these issues by introducing a scalable situated 3D dataset, named Spartun3D, that incorporates various situated spatial information.
In addition, we propose a situated spatial alignment module to enhance the learning between 3D visual representations and their corresponding textual descriptions. Our experimental results demonstrate that both our dataset and alignment module enhance situated spatial understanding ability.",2025,0.6477270371901683,0.6453317533996528,0.6666666666666666,0.625,06f4aec6-9273-4c22-957b-31e8612a8235,1,"[0.5, 0.625, 0.625, 0.625]","[1.0, 0.95, 0.95, 0.95]",Spartun3D,0.5862941826215023
549577,SV-RAG: LoRA-Contextualizing Adaptation of  MLLMs for Long Document Understanding,"Multimodal large language models (MLLMs) have recently shown great progress in text-rich image understanding, yet they still struggle with complex, multi-page visually-rich documents. Traditional methods using document parsers for retrieval-augmented generation suffer from performance and efficiency limitations, while directly presenting all pages to MLLMs leads to inefficiencies, especially with lengthy ones.  In this work, we present a novel framework named **S**elf-**V**isual **R**etrieval-**A**ugmented **G**eneration (SV-RAG), which can broaden horizons of *any* MLLM to support long-document understanding. We demonstrate that **MLLMs themselves can be an effective multimodal retriever** to fetch relevant pages and then answer user questions based on these pages. SV-RAG is implemented with two specific MLLM adapters, one for evidence page retrieval and the other for question answering. Empirical results show state-of-the-art performance on public benchmarks, demonstrating the effectiveness of SV-RAG.",2025,0.6818179338843877,0.6806929345162506,0.6666666666666666,0.625,c51e3b70-c8cc-4df3-8e55-100e0ab5ed33,1,"[0.625, 0.625, 0.625, 0.625]","[0.9, 0.95, 0.95, 0.95]",SV-RAG,0.6250000000000001
549590,Boosting Deductive Reasoning with Step Signals In RLHF,"Logical reasoning is a crucial task for Large Language Models (LLMs), enabling them to tackle complex problems. Among reasoning tasks, multi-step reasoning poses a particular challenge. Grounded in the theory of formal logic, we have developed an automated method, Multi-step Deduction (MuseD), for deductive reasoning data. MuseD has allowed us to create training and testing datasets for multi-step reasoning. Our generation method enables control over the complexity of the generated instructions, facilitating training and evaluation of models across different difficulty levels. Through RLHF training, our training data has demonstrated significant improvements in logical capabilities for both in-domain of out-of-domain reasoning tasks. Additionally, we have conducted tests to assess the multi-step reasoning abilities of various models.",2025,0.3409089669421938,0.3376236955200603,0.2666666666666666,0.25,e72ff702-c320-4afb-95d7-a5e862ecfa31,0,"[0.25, 0.25, 0.25, 0.5]","[0.9, 1.0, 0.95, 0.9]",Multi-step Deduction,0.2996298896712799
549595,Pixel-Space Post-Training of Latent-Diffusion Models,"Latent diffusion models (LDMs) have made significant advancements in the field of image generation in recent years. One major advantage of LDMs is their ability to operate in a compressed latent space, allowing for more efficient training and deployment. However, despite these advantages, challenges with LDMs still remain. For example, it has been observed that LDMs often generate high-frequency details and complex compositions imperfectly. We hypothesize that one reason for these flaws is due to the fact that all pre- and post-training of LDMs are done in latent space, which is typically $8 \times 8$ lower spatial-resolution than the output images. To address this issue, we propose adding pixel-space supervision in the post-training process to better preserve high-frequency details. Experimentally, we show that adding a pixel-space objective significantly improves both supervised quality fine-tuning and preference-based post-training by a large margin on a state-of-the-art DiT transformer and U-Net diffusion models in both visual quality and visual flaw metrics, while maintaining the same text alignment quality.",2025,0.443181657024852,0.4363895175466302,0.4,0.25,e550031c-577c-4643-8a9b-76bc2e610972,0,"[0.25, 0.25, 0.5, 0.625]","[0.9, 0.95, 1.0, 0.8]",Pixel-Space Supervision,0.3948279603204929
549598,Cost-Sensitive Multi-Fidelity Bayesian Optimization,"In this paper, we address the problem of cost-sensitive multi-fidelity Bayesian Optimization (BO) for efficient hyperparameter optimization (HPO). Specifically, we assume a scenario where users want to early-stop the BO when performance increase is not satisfactory with respect to the required computational cost. Motivated by this scenario, we introduce \emph{utility function}, which is predefined by each user and describes the trade-off between the required BO steps and the cumulative best performance during the BO. This utility function, combined with our novel acquisition function and the stopping criteria, allows us to dynamically choose for each BO step the best configuration that we expect to achieve the maximum utility in future, and also automatically stop the BO around the maximum utility. Further, we improve the sample efficiency of existing learning curve (LC) extrapolation methods (e.g., Prior Fitted Networks) with transfer learning, while successfully capturing the correlations between different configurations to develop a sensible surrogate function for multi-fidelity BO. We validate our algorithm on various LC datasets and found it outperform all the previous multi-fidelity BO baselines, achieving significantly better trade-off between cost and performance of multi-fidelity BO.",2025,0.5181816297521347,0.515893363189433,0.5333333333333333,0.5,0d519986-1a23-44d4-8c2d-0775d1353a22,0,"[0.25, 0.5, 0.5, 0.5, 0.625]","[1.0, 0.9, 0.9, 0.95, 1.0]",Utility Function,0.4678746233064399
549620,Takin-VC: Zero-shot Voice Conversion via Jointly Hybrid Content and Memory-Augmented Context-Aware Timbre Modeling,"Zero-shot voice conversion (VC) aims to transform the source speaker timbre into an arbitrary unseen one without altering the original speech content. While recent advancements in zero-shot VC methods have shown remarkable progress, there still remains considerable potential for improvement in terms of improving speaker similarity and speech naturalness. In this paper, we propose Takin-VC, a novel zero-shot VC framework based on jointly hybrid content and memory-augmented context-aware timbre modeling to tackle this challenge. Specifically, an effective hybrid content encoder, guided by neural codec training, that leverages quantized features from pre-trained WavLM and HybridFormer is first presented to extract the linguistic content of the source speech. Subsequently, we introduce an advanced cross-attention-based context-aware timbre modeling approach that learns the fine-grained, semantically associated target timbre features. To further enhance both speaker similarity and real-time performance, we utilize a conditional flow matching model to reconstruct the Mel-spectrogram of the source speech. Additionally, we advocate an efficient memory-augmented module designed to generate high-quality conditional target inputs for the flow matching process, thereby improving the overall performance of the proposed system. Experimental results demonstrate that the proposed Takin-VC method surpasses state-of-the-art zero-shot VC systems, delivering superior performance in terms of both speech naturalness and speaker similarity.",2025,0.3409089669421938,0.3439285457567236,0.2666666666666666,0.25,4ba483fd-ddbe-4ad2-bf91-2e091b97de21,0,"[0.25, 0.25, 0.25, 0.5]","[0.95, 0.95, 0.9, 1.0]",Timbre Modeling,0.3316927567609226
549628,Retrieval Head Mechanistically Explains Long-Context Factuality,"Despite the recent progress in long-context language models, it remains elusive how transformer-based models exhibit the capability to retrieve relevant information from arbitrary locations within the long context. This paper aims to address this question. Our systematic investigation across a wide spectrum of models reveals that a special type of attention heads are largely responsible for retrieving information, which we dub retrieval heads. We identify intriguing properties of retrieval heads:(1) universal: all the explored models with long-context capability have a set of retrieval heads; (2) sparse: only a small portion (less than 5\%) of the attention heads are retrieval. (3) intrinsic: retrieval heads already exist in models pretrained with short context. When extending the context length by continual pretraining, it is still the same set of heads that perform information retrieval. (4) dynamically activated: take Llama-2 7B for example, 12 retrieval heads always attend to the required information no matter how the context is changed. The rest of the retrieval heads are activated in different contexts. (5) causal: completely pruning retrieval heads leads to failure in retrieving relevant information and results in hallucination, while pruning random non-retrieval heads does not affect the model's retrieval ability. We further show that retrieval heads strongly influence chain-of-thought (CoT) reasoning, where the model needs to frequently refer back the question and previously-generated context. Conversely, tasks where the model directly generates the answer using its intrinsic knowledge are less impacted by masking out retrieval heads. These observations collectively explain which internal part of the model seeks information from the input tokens. We believe our insights will foster future research on reducing hallucination, improving reasoning, and compressing the KV cache.",2025,0.9545451074381428,0.9529701083227508,0.9333333333333332,0.875,5fa3ffa8-0105-48c7-8e4b-17635a89eea4,1,"[0.875, 0.875, 0.875, 0.875]","[0.95, 0.9, 0.8, 0.95]",retrieval heads,0.8749937923044308
549632,EMOS: Embodiment-aware Heterogeneous Multi-robot Operating System with LLM Agents,"Heterogeneous multi-robot systems (HMRS) have emerged as a powerful ap-
proach for tackling complex tasks that single robots cannot manage alone. Current
large-language-model-based multi-agent systems (LLM-based MAS) have shown
success in areas like software development and operating systems, but applying
these systems to robot control presents unique challenges. In particular, the ca-
pabilities of each agent in a multi-robot system are inherently tied to the physical
composition of the robots, rather than predefined roles. To address this issue,
we introduce a novel multi-agent framework designed to enable effective collab-
oration among heterogeneous robots with varying embodiments and capabilities,
along with a new benchmark named Habitat-MAS. One of our key designs is
Robot Resume: Instead of adopting human-designed role play, we propose a self-
prompted approach, where agents comprehend robot URDF files and call robot
kinematics tools to generate descriptions of their physics capabilities to guide
their behavior in task planning and action execution. The Habitat-MAS bench-
mark is designed to assess how a multi-agent framework handles tasks that require
embodiment-aware reasoning, which includes 1) manipulation, 2) perception, 3)
navigation, and 4) comprehensive multi-floor object rearrangement. The experi-
mental results indicate that the robot’s resume and the hierarchical design of our
multi-agent system are essential for the effective operation of the heterogeneous
multi-robot system within this intricate problem context.",2025,0.7840906239670459,0.7827968746936882,0.8,0.875,2c41b7e8-43c9-4b2b-bbb0-273af3c5a4eb,1,"[0.5, 0.625, 0.875, 0.875]","[0.95, 0.95, 0.9, 1.0]",Embodiment-aware,0.7232224346742671
549638,INFER: A Neural-symbolic Model For Extrapolation Reasoning on Temporal Knowledge Graph,"Temporal Knowledge Graph(TKG) serves as an efficacious way to store dynamic facts in real-world. Extrapolation reasoning on TKGs, which aims at predicting possible future events, has attracted consistent research interest. Recently, some rule-based methods have been proposed, which are considered more interpretable compared with embedding-based methods. Existing rule-based methods apply rules through path matching or subgraph extraction, which falls short in inference ability and suffers from missing facts in TKGs. Besides, during rule application period, these methods consider the standing of facts as a binary 0 or 1 problem and ignores the validity as well as frequency of historical facts under temporal settings.
In this paper, by designing a novel paradigm for rule application, we propose INFER, a neural-symbolic model for TKG extrapolation. With the introduction of Temporal Validity Function, INFER firstly considers the frequency and validity of historical facts and extends the truth value of facts into continuous real number to better adapt for temporal settings. INFER builds Temporal Weight Matrices with a pre-trained static KG embedding model to enhance its inference ability. Moreover, to facilitates potential integration with existing embedding-based methods, INFER adopts a rule projection module which enables it apply rules through conducting matrices operation on GPU. This feature also improves the efficiency of rule application. 
Experimental results show that INFER achieves state-of-the-art performance on various TKG datasets and significantly outperforms existing rule-based models on our modified, more sparse TKG datasets, which demonstrates the superiority of our model in inference ability.",2025,0.613636140495949,0.6126236410646255,0.6,0.5,37b53289-805f-46c4-9f63-2c5a66b45e0f,1,"[0.5, 0.5, 0.625, 0.625]","[0.95, 0.95, 0.95, 0.95]",INFER,0.5624999999999999
549664,Compositional Generative Multiphysics and Multi-component Simulation,"Multiphysics simulation, which models the interactions between multiple physical processes, and multi-component simulation of complex structures are critical in fields like nuclear and aerospace engineering. Previous studies often rely on numerical solvers or machine learning-based surrogate models to solve or accelerate these simulations. However, multiphysics simulations typically require integrating multiple specialized solvers—each responsible for evolving a specific physical process—into a coupled program, which introduces significant development challenges.  Furthermore, no universal algorithm exists for multi-component simulations, which adds to the complexity.
Here we propose compositional  Multiphysics and Multi-component Simulation with Diffusion models (MultiSimDiff) to overcome these challenges. During diffusion-based training, MultiSimDiff learns energy functions modeling the conditional probability of one physical process/component conditioned on other processes/components. In inference, MultiSimDiff generates coupled multiphysics solutions and multi-component structures by sampling from the joint probability distribution, achieved by composing the learned energy functions in a structured way.
We test our method in three tasks. In the reaction-diffusion and nuclear thermal coupling problems, MultiSimDiff successfully predicts the coupling solution using decoupled data, while the surrogate model fails in the more complex second problem. For the thermal and mechanical analysis of the prismatic fuel element, MultiSimDiff trained for single component prediction accurately predicts a larger structure with 64 components, reducing the relative error by 40.3% compared to the surrogate model.",2025,0.6363630413225304,0.637766804402613,0.5333333333333333,0.5,f92152aa-da37-48e7-93d6-150975002e11,0,"[0.5, 0.5, 0.5, 0.5, 0.625, 0.875]","[0.9, 0.9, 0.9, 0.95, 0.95, 0.95]",MultiSimDiff,0.5940414507772019
549670,Reassessing Layer Pruning in LLMs: New Insights and Methods,"Although large language models (LLMs) have achieved remarkable success across various domains, their considerable scale necessitates substantial computational resources, posing significant challenges for deployment in resource-constrained environments. Layer pruning, as a simple yet effective compression method, removes layers of a model directly, reducing computational overhead. However, what are the best practices for layer pruning in LLMs? Are sophisticated layer selection metrics truly effective? Does the LoRA (Low-Rank Approximation) family, widely regarded as a leading method for pruned model fine-tuning, truly meet expectations when applied to post-pruning fine-tuning? To answer these questions, we dedicate thousands of GPU hours to benchmarking layer pruning in LLMs and gaining insights across multiple dimensions. Our results demonstrate that a simple approach, i.e., pruning the final 25\% of layers followed by fine-tuning the \texttt{lm\_head} and the remaining last three layer, yields remarkably strong performance. Following this guide, we prune Llama-3.1-8B-It and obtain a model that outperforms many popular LLMs of similar size, such as ChatGLM2-6B, Vicuna-7B-v1.5, Qwen1.5-7B and Baichuan2-7B.
We release the optimal model weights on Huggingface, and the code is available on GitHub.",2025,0.4772725537190714,0.4712486195547288,0.4666666666666667,0.25,39cee6d7-9b9a-4747-82f7-9fdc70bf9eb6,0,"[0.25, 0.25, 0.625, 0.625]","[1.0, 1.0, 0.9, 1.0]",Layer Pruning,0.4151464542651594
549671,Investigating the Effectiveness of HyperTuning via Gisting,"Gisting (Mu et al., 2023) is a simple method for training models to compress information into fewer token representations using a modified attention mask, and can serve as an economical approach to training Transformer-based hypernetworks. We introduce HyperLlama, a set of Gisting-based hypernetworks built on Llama-2 models that generates task-specific soft prefixes based on few-shot inputs. In experiments across P3, Super-NaturalInstructions and Symbol Tuning datasets, we show that HyperLlama models can effectively compress information from few-shot examples into soft prefixes. However, they still underperform multi-task fine-tuned language models with full attention over few-shot in-context examples. We also show that HyperLlama-generated soft prefixes can serve as better initializations for further prefix tuning. Overall, Gisting-based hypernetworks are economical and easy to implement, but have mixed empirical performance.",2025,0.4636361950413836,0.4652650563831572,0.5333333333333333,0.25,16a773eb-4d9f-4cca-a56a-6fbbe5a3ae54,0,"[0.25, 0.25, 0.5, 0.5, 0.625]","[0.9, 0.9, 0.9, 0.9, 0.95]",HyperLlama,0.43613855827532
549685,Selective Task Group Updates for Multi-Task Optimization,"Multi-task learning enables the acquisition of task-generic knowledge by training multiple tasks within a unified architecture. However, training all tasks together in a single architecture can lead to performance degradation, known as negative transfer, which is a main concern in multi-task learning. Previous works have addressed this issue by optimizing the multi-task network through gradient manipulation or weighted loss adjustments. However, their optimization strategy focuses on addressing task imbalance in shared parameters, neglecting the learning of task-specific parameters. As a result, they show limitations in mitigating negative transfer, since the learning of shared space and task-specific information influences each other during optimization. To address this, we propose a different approach to enhance multi-task performance by selectively grouping tasks and updating them for each batch during optimization. We introduce an algorithm that adaptively determines how to effectively group tasks and update them during the learning process. To track inter-task relations and optimize multi-task networks simultaneously, we propose proximal inter-task affinity, which can be measured during the optimization process. We provide a theoretical analysis on how dividing tasks into multiple groups and updating them sequentially significantly affects multi-task performance by enhancing the learning of task-specific parameters. Our methods substantially outperform previous multi-task optimization approaches and are scalable to different architectures and various numbers of tasks.",2025,0.6818179338843877,0.6806929345162506,0.6666666666666666,0.625,eadde94a-b594-4bac-b4de-aa9d23c5775e,1,"[0.625, 0.625, 0.625, 0.625]","[0.9, 0.9, 0.95, 0.9]",Proximal inter-task affinity,0.6250000000000001
549690,Deciphering Cell Lineage Gene Regulatory Network via MTGRN,"Gene regulatory network (GRN) inference is crucial for cell fate decision, as it outlines the regulations between genes, which direct cell differentiation. Although there have been some work to infer cell lineage GRN, they fail to capture the continuous nature of the differentiation process as they group cells by cell type or cluster and infer GRN in a discrete manner. In this paper, we hypothesize GRN can forecast future gene expression based on history information and transform the inference process into a multivariate time series forecasting problem, linking cells at different time to learn temporal dynamics and inferring GRN in a continuous process. We introduce MTGRN, a transformer-based model that only takes single cell data as input to infer the cell lineage GRN by forecasting gene expression. MTGRN consists of temporal blocks and spatial blocks, effectively captures the connections between cells along their developmental trajectories and leverages prior knowledge to elucidate regulatory interactions among genes. It significantly outperforms six other methods across five datasets, demonstrating superior performance even compared to multimodal approaches. Based on the inferred GRN, MTGRN pinpoints three crucial genes associated with the development of mouse embryonic stem cells and depicts the activity changes of these genes during cellular differentiation. Beyond this, MTGRN is capable of conducting perturbation experiments on key genes and accurately modeling the change of cell identity following the knockout of the Gata1 in mouse hematopoietic stem cells.",2025,0.6477270371901683,0.645785911725562,0.6666666666666666,0.625,ff7d28cf-26b9-42f1-a09b-1983741f25b2,0,"[0.5, 0.625, 0.625, 0.625]","[1.0, 1.0, 0.95, 0.95]",MTGRN,0.5891456343792634
549708,Effective and Efficient Adversarial Detection for Vision-Language Models via A Single Vector,"Visual Language Models (VLMs) are vulnerable to adversarial attacks, especially those from adversarial images, which is however under-explored in literature.
To facilitate research on this critical safety problem, we first construct a new la**R**ge-scale **A**dervsarial images dataset with **D**iverse h**A**rmful**R**esponses (RADAR), given that existing datasets are either small-scale or only contain limited types of harmful responses.
With the new RADAR dataset, we further develop a novel and effective  i**N**-time**E**mbedding-based**A**dve**RS**arial **I**mage **DE**tection (NEARSIDE) method, which exploits a single vector that distilled from the hidden states of VLMs, which we call *the attacking direction*, to achieve the detection of adversarial images against benign ones in the input. 
Extensive experiments with two victim VLMs, LLaVA and MiniGPT-4, well demonstrate the effectiveness, efficiency,
and cross-model transferrability of our proposed method. Our code is included in the supplementary file and will be made publicly available.",2025,0.4090907603306326,0.410231304904692,0.4666666666666667,0.625,383cb6db-e75b-4293-8d20-e30b1c2ff2cc,0,"[0.0, 0.25, 0.625, 0.625]","[0.95, 0.9, 0.95, 0.95]",RADAR,0.3825462512171372
549715,"Expanding the Web, Smaller Is Better: A Comprehensive Study in Post-training","General-purpose large language models (GLLMs) like GPT-4 and LLaMA have demonstrated exceptional performance across a wide range of tasks. However, their performance often falls short in domain- or task-specific applications, where deeper, specialized knowledge is essential, while maintaining general knowledge remains crucial for handling broader, unseen tasks. Post-training has been widely applied to
make LLMs specialized, typically consisting of multiple stages, including DomainAdaptive Pre-Training (DAPT) and Supervised Fine-Tuning (SFT). In this work, we conduct a comprehensive study on three key aspects of post-training taking Finance as a target domain: (1) the distinct roles of DAPT and SFT in post-training, (2) strategies to mitigate knowledge forgetting across stages, and (3) evaluation methods that capture both general and domain-specific capabilities. 
Our results show that DAPT and SFT require distinct training objectives, joint training of DAPT and SFT is essential for maintaining stage knowledge and encouraging knowledge transfer across stages, and replay mechanisms are critical for preventing forgetting. Evaluation should encompass general, seen, and unseen tasks for a complete assessment. Based on these insights, we developed a Joint-and-Replay post-training recipe and built LLaMA3-8B-Fin, a smaller yet more powerful stateof-the-art financial LLM trained through post-training. Despite its smaller size, LLaMA3-8B-Fin surpasses larger models like GPT-4o and LLaMA3.1-70b on both seen and unseen financial tasks while retaining general knowledge, demonstrating that a well-structured post-training can “expand the web” of capabilities in smaller LLMs, enabling them to outperform much larger models.",2025,0.3636358677687754,0.359794681228776,0.2666666666666666,0.25,fa45df19-ee89-4bc6-bddb-71eca431fd9c,0,"[0.25, 0.25, 0.5]","[0.95, 0.95, 0.9]",Joint-and-Replay,0.3196422656508778
549717,How and how well do diffusion models improve adversarial robustness?,"Recent findings suggest that diffusion models significantly enhance empirical adversarial robustness. While some intuitive explanations have been proposed, the precise mechanisms underlying these improvements remain unclear. In this work, we systematically investigate how and how well do diffusion models improve adversarial robustness. First, we observe that diffusion models intriguingly increase—rather than decrease—the $\ell_p$ distances to clean samples. This is the opposite of what was believed previously. Second, we find that the purified images are heavily influenced by the internal randomness of diffusion models. To properly evaluate the robustness of systems with inherent randomness, we introduce the concept of fuzzy adversarial robustness, and find that empirically a substantial fraction of adversarial examples are fuzzy in nature. Finally, by leveraging a hyperspherical cap model of adversarial regions, we show that diffusion models increase robustness by dramatically compressing the image space. Our findings provide novel insights into the mechanisms behind the robustness improvements of diffusion-model-based purification and offer guidance for the development of more efficient adversarial purification systems.",2025,0.6272724991736367,0.6155827493895546,0.5333333333333333,0.5,df5ea128-9453-49ca-a0a5-83594880221f,0,"[0.5, 0.5, 0.5, 0.5, 0.875]","[0.95, 0.95, 0.95, 0.95, 0.8]",Fuzzy adversarial robustness,0.5416886979510905
549720,On Exact Bit-level Reversible Transformers Without Changing Architectures,"Various reversible deep neural networks (DNN) models have been proposed to reduce memory consumption in the training process. However, almost all existing reversible DNNs either require special non-standard architectures or are constructed by modifying existing DNN architectures considerably to enable reversibility. In this work we present the BDIA-transformer, which is an exact bit-level reversible transformer that uses an unchanged standard architecture for inference. The basic idea is to first treat each transformer block as the Euler integration approximation for solving an ordinary differential equation (ODE) and then incorporate the technique of bidirectional integration approximation (BDIA) (originally designed for diffusion inversion) into the neural architecture, together with activation quantization to make it exactly bit-level reversible. In the training process, we let a hyper-parameter $\gamma$ in BDIA-transformer randomly take one of the two values $\{0.5, -0.5\}$ per training sample per transformer block for averaging every two consecutive integration approximations. As a result, BDIA-transformer can be viewed as training an ensemble of ODE solvers parameterized by a set of binary random variables,  which regularizes the model and results in improved validation accuracy. Lightweight side information per transformer block is required to be stored in the forward process to account for binary quantization loss to enable exact bit-level reversibility.  In the inference procedure, the expectation $\mathbb{E}(\gamma)=0$ is taken to make the resulting architectures of BDIA-transformer identical to transformers up to activation quantization. Our experiments in both image classification and language translation show that BDIA-transformers outperform their conventional counterparts significantly in terms of validation performance due to the regularization effect of the set of $\gamma$ random variables while also requiring considerably less training memory.",2025,0.4999994545456529,0.5018438610210176,0.5333333333333333,0.25,726d3961-aba5-406c-996b-4a220cac42e6,0,"[0.25, 0.25, 0.5, 0.5, 0.625, 0.625]","[0.8, 0.9, 0.8, 0.8, 0.9, 0.9]",BDIA-transformer,0.4672382671480143
549727,SAT-LDM: Provably Generalizable Image Watermarking for Latent Diffusion Models with Self-Augmented Training,"The proliferation of AI-generated images necessitates effective watermarking to protect intellectual property and identify fake content. While existing training-based watermarking methods show promise, they often struggle with generalization across diverse image styles and tend to produce noticeable artifacts. To this end, we introduce a provably generalizable image watermarking method for Latent Diffusion Models with Self-Augmented Training (SAT-LDM), which aligns the training and testing phases by a free generation distribution to bolster the watermarking module’s generalization capabilities. We theoretically consolidate our method by proving that the free generation distribution contributes to its tight generalization bound without the need to collect new data. Extensive experimental results demonstrate that SAT-LDM achieves robust watermarking while significantly improving the quality of watermarked images across diverse styles. Furthermore, we conduct experimental analyses to demonstrate the strong generalization abilities of SAT-LDM. We hope our method offers a practical and convenient solution for securing high-fidelity AI-generated content.",2025,0.613636140495949,0.6116652254128266,0.6,0.5,05a55458-aabc-499d-9303-544a76530a19,0,"[0.5, 0.5, 0.625, 0.625]","[1.0, 0.8, 0.95, 0.8]",SAT-LDM,0.5560449067526531
549760,Scalable Approximate Message Passing for Bayesian Neural Networks,"Bayesian neural networks (BNNs) offer the potential for reliable uncertainty quantification and interpretability, which are critical for trustworthy AI in high-stakes domains. However, existing methods often struggle with issues such as overconfidence, hyperparameter sensitivity, and posterior collapse, leaving room for alternative approaches. In this work, we advance message passing (MP) for BNNs and present a novel framework that models the predictive posterior as a factor graph. To the best of our knowledge, our framework is the first MP method that handles convolutional neural networks and avoids double-counting training data, a limitation of previous MP methods that causes overconfidence. We evaluate our approach on CIFAR-10 with a convolutional neural network of roughly 890k parameters and find that it can compete with the SOTA baselines AdamW and IVON, even having an edge in terms of calibration. On synthetic data, we validate the uncertainty estimates and observe a strong correlation (0.9) between posterior credible intervals and its probability of covering the true data-generating function outside the training range. While our method scales to an MLP with 5.6 million parameters, further improvements are necessary to match the scale and performance of state-of-the-art variational inference methods.",2025,0.5454543471075102,0.5342796961822384,0.6666666666666666,0.625,b34c7120-a971-4c36-95da-82b3e491b3e5,0,"[0.25, 0.625, 0.625]","[0.95, 0.8, 0.9]",Message Passing,0.4599882601205381
549761,FMS PINN: Flow-matching sampling for efficient solution of partial differential equations with source singularities,"Singularities in the source functions of partial differential equations (PDEs) can pose significant challenges for physics-informed neural networks (PINNs), often leading to numerical instability and necessitating a large number of sampling points thereby increasing the computational time. In this paper, we introduce a novel sampling point selection method to address these challenges. Our approach is based on diffusion models capable of generative sampling from the distribution of PDE residuals. Specifically, we apply the optimal transport coupling flow-matching technique to generate more sampling points in regions where the PDE residuals are higher, enhancing the accuracy and efficiency of the solution. In contrast to existing approaches in the literature, our method avoids explicit modeling of the probability density proportional to residuals, instead using the benefits of flow matching to generate novel and probable samples from more complex distributions, thereby enhancing PINN solutions for problems with singularities.
We demonstrate that this method, in certain scenarios, outperforms existing techniques such as normalizing flow-based sampling PINN. Especially, our approach demonstrates effectiveness in improving the solution quality for the linear elasticity equation in the case of material with complex geometry of inclusion. A detailed comparison of the flow matching sampling method with other approaches is also provided.",2025,0.2727271735537551,0.2722771738065002,0.2666666666666666,0.25,a68d1900-0862-44cf-a903-0b7f25fcbfb2,0,"[0.25, 0.25, 0.25, 0.25]","[0.95, 0.8, 0.8, 0.9]",Flow-matching sampling,0.2500000000000857
549766,Neural Probabilistic Logic Learning for Knowledge Graph Reasoning,"Knowledge graph (KG) reasoning is a task that aims to predict unknown facts based on known factual samples. Reasoning methods can be divided into two categories: rule-based methods and KG-embedding based methods. The former possesses precise reasoning capabilities but finds it challenging to reason efficiently over large-scale knowledge graphs. While gaining the ability to reason over large-scale knowledge graphs, the latter sacrifices reasoning accuracy. This paper aims to design a reasoning framework called Neural Probabilistic Logic Learning(NPLL) that achieves accurate reasoning on knowledge graphs. Our approach introduces a scoring module that effectively enhances the expressive power of embedding networks. We strike a balance between model simplicity and reasoning capabilities by incorporating a Markov Logic Network based on variational inference. We empirically evaluate our approach on several benchmark datasets, and the experimental results validate that our method substantially enhances the accuracy and quality of the reasoning results.",2025,0.2727271735537551,0.2759561829789737,0.2666666666666666,0.25,7f99e566-0e26-4d8d-ad99-336f70a14025,0,"[0.0, 0.25, 0.25, 0.5]","[0.9, 0.9, 0.95, 0.95]",Neural Probabilistic Logic Learning,0.266062176165803
549778,Forgetting Order of Continual Learning: What is Learned First is Forgotten Last,"Catastrophic forgetting poses a significant challenge in continual learning, where models often forget previous tasks when trained on new data. Our empirical analysis reveals a strong correlation between catastrophic forgetting and the learning speed of examples: examples learned early are rarely forgotten, while those learned later are more susceptible to forgetting. We demonstrate that replay-based continual learning methods can leverage this phenomenon by focusing on mid-learned examples for rehearsal. We introduce Goldilocks, a novel replay buffer sampling method that filters out examples learned too quickly or too slowly, keeping those learned at an intermediate speed. Goldilocks improves existing continual learning algorithms, leading to state-of-the-art performance across several image classification tasks.",2025,0.4909089123967591,0.488415150808881,0.5333333333333333,0.0,bf74e818-d1b1-4cf5-85bf-acaa2f496313,0,"[0.0, 0.25, 0.5, 0.625, 0.875]","[0.95, 1.0, 1.0, 0.95, 0.95]",Goldilocks,0.4408912004498171
549780,PLAY2PROMPT: Zero-shot Tool Instruction Optimization for LLM Agents via Tool Play,"Large language models (LLMs) are increasingly integrated with external tools to complete user requests. Many real-world applications require LLMs to use specialized tools in a zero-shot setting. To achieve this, current methods primarily rely on prompting LLMs with tool-specific information, yet tool documentation is often underspecified or noisy, limiting effectiveness. Manual improvements are inefficient and impractical, as they require domain expertise to rewrite documentation and test on carefully curated held-out datasets to evaluate performance gains. Automatic prompt engineering techniques are not applicable either, because they require labeled examples, which is unavailable in the zero-shot setting. In this work, we introduce PLAY2PROMPT, an automated framework that iteratively refines tool documentation and generates usage examples. PLAY2PROMPT enables LLMs to explore tool input-output behaviors, allowing us to effectively search the space of possible tool descriptions and examples. The generated examples not only guide LLM inference but also serve as validation data to ensure more effective tool use. Extensive experiments on real-world tasks demonstrate significant improvements in zero-shot tool performance across both open- and closed-source models.",2025,0.4363634776860081,0.4380112003938217,0.5333333333333333,0.5,a326eb30-19ac-40d4-a2be-ab39409c4895,0,"[0.25, 0.25, 0.5, 0.5, 0.5]","[0.9, 0.9, 0.95, 0.95, 0.9]",PLAY2PROMPT,0.4105509466071049
549791,Selecting Influential Samples for Long Context Alignment via Homologous Models’ Guidance and Contextual Awareness Measurement,"The expansion of large language models to effectively handle instructions with extremely long contexts has yet to be fully investigated. The primary obstacle lies in constructing a high-quality long instruction-following dataset devised for long context alignment. Existing studies have attempted to scale up the available data volume by synthesizing long instruction-following samples. However, indiscriminately increasing the quantity of data without a well-defined strategy for ensuring data quality may introduce low-quality samples and restrict the final performance. To bridge this gap, we aim to address the unique challenge of long-context alignment, i.e., modeling the long-range dependencies for handling instructions and lengthy input contexts. We propose GATEAU, a novel framework designed to identify the influential and high-quality samples enriched with long-range dependency relations by utilizing crafted Homologous Models' Guidance (HMG) and Contextual Awareness Measurement (CAM). Specifically, HMG attempts to measure the difficulty of generating corresponding responses due to the long-range dependencies, using the perplexity scores of the response from two homologous models with different context windows. Also, the role of CAM is to measure the difficulty of understanding the long input contexts due to long-range dependencies by evaluating whether the model’s attention is focused on important segments. Built upon both proposed methods, we select the most challenging samples as the influential data to effectively frame the long-range dependencies, thereby achieving better performance of LLMs. Comprehensive experiments indicate that GATEAU effectively identifies samples enriched with long-range dependency relations and the model trained on these selected samples exhibits better instruction-following and long-context understanding capabilities.",2025,0.5454543471075102,0.5445543476130005,0.5333333333333333,0.5,b12c50fb-3330-4104-a421-ec2343d783d4,0,"[0.5, 0.5, 0.5, 0.5]","[0.9, 0.95, 0.9, 0.95]",GATEAU,0.5
549798,Large Convolutional Model Tuning via Filter Subspace,"Efficient fine-tuning methods are critical to address the high computational and parameter complexity while adapting large pre-trained models to downstream tasks.
Our study is inspired by prior research that represents each convolution filter as a linear combination of a small set of filter subspace elements, referred to as filter atoms. In this paper, we propose to fine-tune pre-trained models by adjusting only filter atoms, which are responsible for spatial-only convolution, while preserving spatially-invariant channel combination knowledge in atom coefficients.
In this way, we bring a new filter subspace view for model tuning. 
Furthermore, each filter atom can be recursively decomposed as a combination of another set of atoms, which naturally expands the number of tunable parameters in the filter subspace.
By only adapting filter atoms constructed by a small number of parameters, while maintaining the rest of model parameters constant, the proposed approach is highly parameter-efficient. It effectively preserves the capabilities of pre-trained models and prevents overfitting to downstream tasks. 
Extensive experiments show that such a simple scheme surpasses previous tuning baselines for both discriminate and generative tasks.",2025,0.6818179338843877,0.6806929345162506,0.6666666666666666,0.625,66488df3-bcc5-4ae5-88ce-6db3dfc3a6a5,1,"[0.625, 0.625, 0.625]","[0.95, 0.9, 1.0]",Filter Atoms,0.6249999999999999
549809,CSGO: Content-Style Composition in Text-to-Image Generation,"The diffusion model has shown exceptional capabilities in controlled image generation, which has further fueled interest in image style transfer. Existing works mainly focus on training free-based methods (e.g., image inversion) due to the scarcity of specific data. In this study, we present a data construction pipeline for content-style-stylized image triplets that generates and automatically cleanses stylized triplets. Based on this pipeline, we construct a dataset IMAGStyle, the first large-scale style transfer dataset containing 210k image triplets, available for the community to explore and research.Equipped with IMAGStyle, we propose a simple yet effective framework CSGO, a style transfer model based on end-to-end training, which explicitly decouples content and style features employing independent feature injection.  Our CSGO implements image-driven style transfer, text-driven stylized synthesis, and text editing-driven stylized synthesis in the same model.
We conduct extensive experiments on CSGO to validate the effectiveness of synthetic stylized data for style control. Meanwhile, ablation experiments show the effectiveness of CSGO.",2025,0.5999997818182612,0.5973423569619095,0.5333333333333333,0.5,a8c205ca-ddda-4c39-91ce-296186c8d182,0,"[0.5, 0.5, 0.5, 0.625, 0.625]","[1.0, 1.0, 1.0, 0.95, 0.95]",CSGO,0.5414126689636893
549826,Semi-Supervised Underwater Object Detection with Image Enhancement Guided by Attribute-based Data Distribution,"Semi-supervised underwater object detection aims to improve the performance of detectors on unlabeled underwater images by leveraging knowledge from labeled ones. However, existing methods often overlook the distribution differences between labeled and unlabeled underwater images. In this paper, we propose a novel underwater image enhancement method guided by attribute-based data distribution (UIEG+), which focuses on reducing the discrepancies between enhanced and original unlabeled images across different attributes, thereby effectively addressing the challenges in semi-supervised underwater object detection. Specifically, we explore an underwater image enhancement strategy based on two attributes: color and scale distributions. For the color attribute, we construct a 3-dimensional grid memory, where each grid cell represents a color subspace and records the number of samples in that subspace. Similarly, for the scale attribute, we design a 1-dimensional vector memory that dynamically stores the number of samples in each scale subspace. Subsequently, we propose an effective sampling method to derive parameters for color and scale transformations based on the aforementioned distribution analysis, increasing the likelihood of transformations in low-distribution regions. To evaluate its effetiveness and superiority, massive semi-superivised underwater object deteciton experiments in multiple datasets have been conduted by integrating UIEG+ into existing semi-supervised object detection frameworks. The code will be released.",2025,0.2727271735537551,0.2722771738065002,0.2666666666666666,0.25,923ac5d4-ac9c-4bd7-9712-e2d8c4e75dd2,0,"[0.25, 0.25, 0.25, 0.25]","[0.9, 1.0, 0.95, 1.0]",UIEG+,0.2499999999999999
549838,Two Sparse Matrices are Better than One: Sparsifying Neural Networks with Double Sparse Factorization,"Neural networks are often challenging to work with due to their large size and complexity. To address this, various methods aim to reduce model size by sparsifying or decomposing weight matrices, such as magnitude pruning and low-rank or block-diagonal factorization. In this work, we present Double Sparse Factorization (DSF), where we factorize each weight matrix into two sparse matrices. Although solving this problem exactly is computationally infeasible, we propose an efficient heuristic based on alternating minimization via ADMM that achieves state-of-the-art results, enabling unprecedented sparsification of neural networks. For instance, in a one-shot pruning setting, our method can reduce the size of the LLaMA2-13B model by 50% while maintaining better performance than the dense LLaMA2-7B model. We also compare favorably with Optimal Brain Compression, the state-of-the-art layer-wise pruning approach for convolutional neural networks. Furthermore, accuracy improvements of our method persist even after further model fine-tuning.
Code available at: https://github.com/usamec/double_sparse",2025,0.727272826446245,0.7260728265202324,0.6666666666666666,0.5,88250833-6328-45d7-b85c-f0cdec6eaa1c,1,"[0.5, 0.625, 0.875]","[0.95, 0.95, 0.95]",Double Sparse Factorization,0.6666666666666667
549853,LAM Simulator: Advancing Large Action Model Training for Agent via Online Exploration and Feedback Simulation,"Large Action Models (LAMs) for AI agents have significant potential, but their development is often constrained by the reliance on supervised learning and manual data curation, which are both time-consuming and costly. To address these limitations, we present the LAM Simulator, a comprehensive framework designed for online exploration of agentic tasks with high-quality feedback. This framework includes a curated set of high-quality agentic tasks, a diverse collection of tools, and an interactive environment where agent models can call tools, receive execution responses, and obtain action feedback. Our findings indicate that the LAM Simulator significantly enhances model performance and effectively identifies and addresses potential issues. Specifically, our model, LAM-Sim-8x7B, demonstrates an 18.54\% improvement over its base LAM and significantly outperforms other state-of-the-art alternatives on ToolEval benchmark. Furthermore, we have demonstrated that LLMs lacking in agentic capability can greatly benefit from the implementation of LAM Simulator. Our experiments with a model trained on Mixtral-8x7B-Instruct-v0.1 have yielded a doubling to tripling of performance. Remarkably, the data construction process for training these models requires minimal human intervention, making the LAM Simulator a robust framework for accelerating the development of AI agents.",2025,0.6818179338843877,0.6806929345162506,0.6666666666666666,0.625,895a47b4-c3c1-4153-99c1-f8fdbfee5a63,0,"[0.625, 0.625, 0.625]","[0.8, 0.95, 0.9]",LAM Simulator,0.625
549861,Can Language Models Reason about Individualistic Human Values and Preferences?,"Recent calls for pluralistic alignment emphasize that AI systems should address the diverse needs of all people. Yet, efforts in this space often require sorting people into fixed buckets of pre-specified diversity-defining dimensions (e.g., demographics, personalities, communication styles), risking smoothing out or even stereotyping the rich spectrum of individualistic variations. To achieve an authentic representation of diversity that respects individuality, we propose individualistic alignment. While individualistic alignment can take various forms, in this paper, we introduce IndieValueCatalog, a dataset transformed from the influential World Values Survey (WVS), to study language models (LMs) on the specific challenge of individualistic value reasoning. Specifically, given a sample of an individual’s value-expressing statements, models are tasked with predicting their value judgments in novel cases. With IndieValueCatalog, we reveal critical limitations in frontier LMs’ abilities to reason about individualistic human values with accuracies, only ranging between 55% to 65%. Moreover, our results highlight that a precise description of individualistic values cannot be approximated only via demographic information. We also identify a partiality of LMs in reasoning about global individualistic values, as measured by our proposed Value Inequity Index (σINEQUITY). Finally, we train a series of Individualistic Value Reasoners (IndieValueReasoner) using IndieValueCatalog to enhance models’ individualistic value reasoning capability, revealing new patterns and dynamics into global human values. We outline future research challenges and opportunities for advancing individualistic alignment.",2025,0.4772725537190714,0.4821571322461124,0.4666666666666667,0.25,16a49715-2bb8-4856-9c26-37cf45dc8854,0,"[0.25, 0.25, 0.625, 0.625]","[0.95, 0.8, 0.9, 0.95]",Individualistic Alignment,0.453082424215312
549871,OmniSep: Unified Omni-Modality Sound Separation with Query-Mixup,"Query-based sound separation (QSS) effectively isolate sound signals that match the content of a given query, enhancing the understanding of audio data. However, most existing QSS methods rely on a single modality for separation, lacking the ability to fully leverage homologous but heterogeneous information across multiple modalities for the same sound signal. To address this limitation, we introduce Omni-modal Sound Separation (**OmniSep**), a novel framework capable of isolating clean soundtracks based on omni-modal queries, encompassing both single-modal and multi-modal composed queries. Specifically, we introduce the **Query-Mixup** strategy, which blends query features from different modalities during training. This enables OmniSep to optimize multiple modalities concurrently, effectively bringing all modalities under a unified framework for sound separation. We further enhance this flexibility by allowing queries to influence sound separation positively or negatively, facilitating the retention or removal of specific sounds as desired. Finally, OmniSep employs a retrieval-augmented approach known as **Query-Aug**, which enables open-vocabulary sound separation. Experimental evaluations on MUSIC, VGGSOUND-CLEAN+, and MUSIC-CLEAN+ datasets demonstrate effectiveness of OmniSep, achieving state-of-the-art performance in text-, image-, and audio-queried sound separation tasks. For samples and further information, please visit the demo page at \url{https://omnisep.github.io/}.",2025,0.6818179338843877,0.6806929345162506,0.6666666666666666,0.625,56de4e57-ac80-4685-b103-3d54caf20f6f,1,"[0.625, 0.625, 0.625, 0.625]","[0.9, 0.9, 0.9, 0.95]",OmniSep,0.6250000000000001
549878,Enhancing Document Understanding with Group Position Embedding: A Novel Approach to Incorporate Layout Information,"Recent advancements in document understanding have been dominated by leveraging large language models (LLMs) and multimodal large models. However, enabling LLMs to comprehend complex document layouts and structural information often necessitates intricate network modifications or costly pre-training, limiting their practical applicability. In this paper, we introduce Group Position Embedding (GPE), a novel and efficient technique to enhance the layout understanding capabilities of LLMs without architectural changes or additional pre-training. GPE achieves this by strategically grouping the attention heads and feeding each group with distinct positional embeddings, effectively encoding layout information relevant to document comprehension. This simple yet powerful method allows for effective integration of layout information within the existing LLM framework.  We evaluate GPE against several competitive baselines across five mainstream document tasks. We also introduce a challenging benchmark called BLADE, specifically designed to assess layout comprehension. 
 Extensive experiments on both established and BLADE benchmarks confirm the efficacy of GPE in significantly advancing the state-of-the-art in document understanding. Our code is available at https://github.com/antgroup/GroupPositionEmbedding.git",2025,0.7159088305786071,0.7193268872620026,0.6666666666666666,0.625,ff572331-ffa6-4111-baa5-97f422bdecb8,1,"[0.5, 0.625, 0.625, 0.875]","[0.95, 0.8, 0.95, 1.0]",Group Position Embedding,0.6799676644237634
549888,MLLMs Know Where to Look: Training-free Perception of Small Visual Details with Multimodal LLMs,"Multimodal Large Language Models (MLLMs) have experienced rapid progress in visual recognition tasks in recent years. Given their potential integration into many critical applications, it is important to understand the limitations of their visual perception. In this work, we study whether MLLMs can perceive small visual details as effectively as large ones when answering questions about images. We observe that their performance is very sensitive to the size of the visual subject of the question, and further show that this effect is in fact causal by conducting an intervention study. Next, we study the attention patterns of MLLMs when answering visual questions, and intriguingly find that they consistently know where to look, even when they provide the wrong answer. Based on these findings, we then propose training-free visual intervention methods that leverage the internal knowledge of any MLLM itself, in the form of attention and gradient maps, to enhance its perception of small visual details. We evaluate our proposed methods on two widely-used MLLMs and seven visual question answering benchmarks and show that they can significantly improve MLLMs' accuracy without requiring any training. Our results elucidate the risk of applying MLLMs to visual recognition tasks concerning small details and indicate that visual intervention using the model's internal state is a promising direction to mitigate this risk. Our code is available at: https://github.com/saccharomycetes/mllms_know.",2025,0.8181815206612653,0.8144006308117563,0.9333333333333332,0.875,95c2b42f-8c5b-4702-ad00-fd27b733a6d6,1,"[0.5, 0.875, 0.875]","[0.95, 0.9, 0.95]",Visual Intervention,0.7397316992381485
549900,INDIRECT ATTENTION: IA-DETR FOR ONE SHOT OBJECT DETECTION,"One-shot object detection presents a significant challenge, requiring the identification of objects within a target image using only a single sample image of the object class as query image. Attention-based methodologies have garnered considerable attention in the field of object detection. Specifically, the cross-attention module, as seen in DETR, plays a pivotal role in exploiting the relationships be-
tween object queries and image features. However, in the context of DETR networks for one-shot object detection, the intricate interplay among target image features, query image features, and object queries must be carefully considered.
In this study, we propose a novel module termed ”indirect attention.” We illustrate that relationships among target image features, query image features, and object queries can be effectively captured in a more concise manner compared to
cross-attention. Furthermore, we introduce a pre-training pipeline tailored specifically for one-shot object detection, addressing three primary objectives: identifying objects of interest, class differentiation, and object detection based on a given
query image. Our experimental findings demonstrate that the proposed IA-DETR (Indirect-Attention DETR) significantly outperforms state-of-the-art one-shot object detection methods on both the Pascal VOC and COCO benchmarks.",2025,0.6477270371901683,0.6466582877904381,0.5333333333333333,0.5,7edb5a27-fad3-48df-9933-ccbf299b14af,0,"[0.5, 0.5, 0.5, 0.875]","[0.95, 0.95, 0.95, 0.95]",Indirect Attention,0.59375
549934,Increment Vector Transformation for Class Incremental Learning,"Class Incremental Learning (CIL) presents a major challenge due to the phenomenon of catastrophic forgetting.
Recent studies on Linear Mode Connectivity (LMC) reveal that Naive-SGD oracle, trained with all historical data, connects to previous task minima through low-loss linear paths---a property generally absent in current CIL methods.
In this paper, we explore whether LMC holds for the CIL oracle. Our empirical results confirm the presence of LMC in the CIL oracle, showing that models can retain performance on earlier tasks by following the discovered low-loss linear paths. Motivated by this finding, we propose Increment Vector Transformation (IVT), which leverages the diagonal of the Fisher Information Matrix to approximate Hessian-based transformation, uncovering low-loss linear paths for incremental updates. 
Our method is orthogonal to existing CIL approaches, serving as a plug-in with minor extra computational costs.
Extensive experiments on CIFAR-100, ImageNet-Subset, and ImageNet-Full demonstrate significant performance improvements when integrating IVT with representative CIL methods.",2025,0.4772725537190714,0.4764850541613754,0.5333333333333333,0.5,ff984f15-2f17-426b-9a87-be0d5626e854,0,"[0.25, 0.5, 0.5, 0.5]","[0.95, 0.9, 1.0, 0.95]",Increment Vector Transformation,0.4392889738697067
549940,Learning Extrapolative Sequence Transformations from Markov Chains,"Most successful applications of deep learning involve similar training and test conditions. However, for some generative tasks, samples should improve desirable properties beyond previously known values, which requires the ability to generate novel hypotheses that extrapolate beyond training data. While large language models have been successfully extended to a variety of sequence modeling problems, greedy autoregressive sampling can struggle to explore the solution space sufficiently to extrapolate, especially when the properties of interest are global to the sequence. On the other hand, sequence-level sampling methods such as Markov chain Monte Carlo (MCMC) offer theoretical guarantees about capturing the distribution of interest, but suffer from the curse of dimensionality in discrete structured spaces. We propose a new approach that bridges the gap between MCMC and autoregressive sampling, which may be viewed as off-policy reinforcement learning. Our approach uses selected states from Markov chains as a source of training data for an autoregressive inference network, which is then able to generate novel sequences at test time that extrapolate along the sequence-level properties of interest. The proposed approach is validated on three problems: protein sequence design, text sentiment control, and text anonymization. We find that the learned inference network confers many of the same (and sometimes better) generalization benefits compared to the slow sampling process, but with the additional benefit of high sample efficiency.",2025,0.5113634504132908,0.5082510874750322,0.5333333333333333,0.5,1598800c-6dc7-4672-9476-d809fc649193,0,"[0.25, 0.5, 0.5, 0.625]","[0.95, 0.95, 0.95, 0.9]",Extrapolative Sequence Transformations,0.4593171859785785
549944,Operator Deep Smoothing for Implied Volatility,"We devise a novel method for nowcasting implied volatility based on neural operators.
Better known as implied volatility smoothing in the financial industry, nowcasting of implied volatility means constructing a smooth surface that is consistent with the prices presently observed on a given option market.
Option price data arises highly dynamically in ever-changing spatial configurations, which poses a major limitation to foundational machine learning approaches using classical neural networks.
While large models in language and image processing deliver breakthrough results on vast corpora of raw data, in financial engineering the generalization from big historical datasets has been hindered by the need for considerable data pre-processing.
In particular, implied volatility smoothing has remained an instance-by-instance, hands-on process both for neural network-based and traditional parametric strategies.
Our general *operator deep smoothing* approach, instead, directly maps observed data to smoothed surfaces.
We adapt the graph neural operator architecture to do so with high accuracy on ten years of raw intraday S&P 500 options data, using a single model instance.
The trained operator adheres to critical no-arbitrage constraints and is robust with respect to subsampling of inputs (occurring in practice in the context of outlier removal).
We provide extensive historical benchmarks and showcase the generalization capability of our approach in a comparison with classical neural networks and SVI, an industry standard parametrization for implied volatility. 
The operator deep smoothing approach thus opens up the use of neural networks on large historical datasets in financial engineering.",2025,0.6818179338843877,0.6806929345162506,0.6666666666666666,0.625,35b82ff3-656c-4bc3-bd94-0a3aee8c50e8,1,"[0.625, 0.625, 0.625]","[0.95, 0.8, 0.8]",Operator Deep Smoothing,0.6249999999999999
549977,STL-Drive: Formal Verification Guided End-to-end Automated Driving,"End-to-end automated driving behavior models require extensive training data from machine or human driver experts or interacting with the environment to learn a driving policy. Not all human driver expert data represent safe driving that the end-to-end model is learning to imitate, and similarly, neither are some of the behaviors learned during exploration while learning by trial and error. However, the models should learn from such data without being negatively affected during the learning process. We aim to provide a learning framework to incorporate formal verification methods to improve the robustness and safety of the learned models in the presence of training data that contain unsafe behaviors, dubbed as STL-Drive. We are particularly interested in utilizing this framework to enhance the safety of end-to-end automated driving models. In this work, we incorporate Signal Temporal Logic (STL) as the formal method to impose safety constraints. In addition, we utilize the Responsibility-Sensitive Safety (RSS) framework to define the safety constraints. We designed a loss function that combines the task objectives and the STL robustness score to balance the learned policy's performance and safety. We demonstrate that encoding safety constraints using STL and utilizing the robustness score during training improves the performance and safety of the driving policy. We validate our framework using open-loop predictive simulator NAVSIM and real-world data from OpenScene. The results of this study suggest a promising research direction where formal methods can enhance the safety and resilience of deep learning models. Formal verification of safety constraints for automated driving will further increase the public's trust in automated vehicles.",2025,0.2045453801653163,0.2050922366153987,0.2666666666666666,0.25,370d94f2-c698-4ac3-8b0f-6d04dfdf4338,0,"[0.0, 0.25, 0.25, 0.25]","[0.95, 1.0, 0.95, 0.95]",STL-Drive,0.1924705449194217
549995,Wavelet Latent Diffusion (WaLa): Billion-Parameter 3D Generative Model with Compact Wavelet Encodings,"Large-scale 3D generative models require substantial computational resources yet often fall short in capturing fine details and complex geometries at high resolutions. We attribute this limitation to the inefficiency of current representations, which lack the compactness required for generative networks to model effectively. To address this, we introduce Wavelet Latent Diffusion (WaLa), a novel approach that encodes 3D shapes into a wavelet-based, compact latent encodings. Specifically, we compress a $256^3$ signed distance field into a $12^3 \times 4$ latent grid, achieving an impressive 2,427× compression ratio with minimal loss of detail. This high level of compression allows our method to efficiently train large-scale generative networks without increasing inference time. Our models, both conditional and unconditional, contain approximately one billion parameters and successfully generate high-quality 3D shapes at $256^3$ resolution. Moreover, WaLa offers rapid inference, producing shapes within 2–4 seconds depending on the condition, despite the model’s scale. We demonstrate state-of-the-art performance across multiple datasets, with significant improvements in generation quality, diversity, and computational efficiency. Upon acceptance, we will open-source the code and model weights for public use and reproducibility.",2025,0.3749998636364132,0.3705038870289333,0.2666666666666666,0.25,dce1cfd2-d6ca-41b3-9343-40609d3da9fc,0,"[0.25, 0.25, 0.25, 0.625]","[1.0, 1.0, 1.0, 0.95]",Wavelet Latent Diffusion,0.3244520330368487
550010,Physics-aligned field reconstruction with diffusion bridge,"The reconstruction of physical fields from sparse measurements is pivotal in both scientific research and engineering applications. Traditional methods are increasingly supplemented by deep learning models due to their efficacy in extracting features from data. However, except for the low accuracy on complex physical systems, these models often fail to comply with essential physical constraints, such as governing equations and boundary conditions. To overcome this limitation, we introduce a novel data-driven field reconstruction framework, termed the Physics-aligned Schr\""{o}dinger Bridge (PalSB). This framework leverages a diffusion bridge mechanism that is specifically tailored to align with physical constraints. The PalSB approach incorporates a dual-stage training process designed to address both local reconstruction mapping and global physical principles. Additionally, a boundary-aware sampling technique is implemented to ensure adherence to physical boundary conditions. We demonstrate the effectiveness of PalSB through its application to three complex nonlinear systems: cylinder flow from Particle Image Velocimetry experiments, two-dimensional turbulence, and a reaction-diffusion system. The results reveal that PalSB not only achieves higher accuracy but also exhibits enhanced compliance with physical constraints compared to existing methods. This highlights PalSB's capability to generate high-quality representations of intricate physical interactions, showcasing its potential for advancing field reconstruction techniques. The source code can be found at https://github.com/lzy12301/PalSB.",2025,0.8636364132231226,0.865452600900475,0.9333333333333332,0.875,3095eacd-e515-46b4-8aa8-996c7f28c611,1,"[0.625, 0.875, 0.875]","[0.9, 0.95, 0.95]",Physics-aligned Schrödinger Bridge,0.8053577321323483
550013,Dynamic Weighting: Exploiting the Potential of a Single Weight Across Different Modes,"Weights play an essential role in determining the performance of deep networks. This paper introduces a new concept termed ``Weight Augmentation Strateg'' (WAS), which emphasizes the exploration of weight spaces rather than traditional network structure design. The core of WAS is the utilization of randomly transformed weight coefficients, referred to as Shadow Weights (SW), for deep networks to calculate the loss function and update the parameters. Differently, stochastic gradient descent is applied to Plain Weights (PW), which is referred to as the original weight of the network before the random transformation. During training, numerous SW collectively form a high-dimensional space, while PW is directly learned from the distribution of SW. To maximize the benefits of WAS, we introduce two operational modes, \textit{i.e.},  the Accuracy-Priented Mode (AOM) and the Desire-Oriented Mode (DOM). To be concrete, AOM relies on PW, which ensures that the network remains highly robust and accurate. Meanwhile, DOM utilizes SW, which is determined by the specific objective of our proposed WAS, such as reduced computational complexity or lower sensitivity to particular data. These dual modes can be switched at any time as needed, thereby providing flexibility and adaptability to different tasks. By extending the concept of augmentation from data to weights, our WAS offers an easy-to-understand and implement technique that can significantly enhance almost all networks. Our experimental results demonstrate that convolutional neural networks, including VGG-16, ResNet-18, ResNet-34, GoogleNet, MobileNetV2, and EfficientNet-Lite, benefit substantially with little to no additional costs. On the CIFAR-100 and CIFAR-10 datasets, model accuracy increases by an average of 7.32\% and 9.28\%, respectively, with the highest improvements reaching 13.42\% and 18.93\%. In addition, DOM can reduce floating point operations (FLOPs) by up to 36.33\%.",2025,0.3409089669421938,0.3376236955200603,0.2666666666666666,0.25,726d3961-aba5-406c-996b-4a220cac42e6,0,"[0.25, 0.25, 0.25, 0.5]","[1.0, 0.9, 0.95, 0.9]",Weight Augmentation Strategy,0.2996298896712799
550028,Semantic Object Navigation with Segmenting Decision Transformer,"Understanding scene semantics plays an important role in solving the object navigation task, where an embodied intelligent agent has to find an object in the scene given its semantic category. This task can be divided into two stages: exploring the scene and reaching the found target. In this work, we consider the latter stage of reaching a given semantic goal. This stage is particularly sensitive to errors in the semantic understanding of the scene. To address this challenge, we propose a multimodal and multitasking method called SegDT, which is based on the joint training of a segmentation model and a decision transformer model. Our method aggregates information from multiple multimodal frames to predict the next action and the current segmentation mask of the target object. To optimize our model, we first performed a pre-training phase using a set of collected trajectories. In the second phase, online policy fine-tuning, we addressed the problems of long-term credit assignment and poor sampling efficiency of transformer models. Using the PPO algorithm, we simultaneously trained an RNN-based policy using ground-truth segmentation and transferred its knowledge to the proposed transformer-based model, which trains the segmentation in itself through an additional segmentation loss. We conducted extensive experiments in the Habitat Sim environment and demonstrated the advantage of the proposed method over the basic navigation approach as well as current state-of-the-art methods that do not consider the auxiliary task of improving the quality of the segmentation of the current frame during training.",2025,0.3545453256198816,0.3483971587332359,0.2666666666666666,0.25,d5ca3ec1-20a6-4a61-be74-e09c4629c275,0,"[0.0, 0.25, 0.25, 0.5, 0.625]","[0.95, 0.9, 1.0, 0.9, 0.9]",SegDT,0.2999029180213082
550029,Accelerating neural network training: An analysis of the AlgoPerf competition,"The goal of the AlgoPerf: Training Algorithms competition is to evaluate practical speed-ups in neural network training achieved solely by improving the underlying training algorithms. In the external tuning ruleset, submissions must provide workload-agnostic hyperparameter search spaces, while in the self-tuning ruleset they must be completely hyperparameter-free. In both rulesets, submissions are compared on time-to-result across multiple deep learning workloads, training on fixed hardware. This paper presents the inaugural AlgoPerf competition's results, which drew 18 diverse submissions from 10 teams. Our investigation reveals several key findings: (1) The winning submission in the external tuning ruleset, using Distributed Shampoo, demonstrates the effectiveness of non-diagonal preconditioning over popular methods like Adam, even when compared on wall-clock runtime. (2) The winning submission in the self-tuning ruleset, based on the Schedule Free AdamW algorithm, demonstrates a new level of effectiveness for completely hyperparameter-free training algorithms. (3) The top-scoring submissions were surprisingly robust to workload changes. We also discuss the engineering challenges encountered in ensuring a fair comparison between different training algorithms. These results highlight both the significant progress so far, and the considerable room for further improvements.",2025,0.5727270644628857,0.5572185035210884,0.6666666666666666,0.625,b0f7f622-ef48-4f99-aa8c-a796feecc092,1,"[0.0, 0.5, 0.625, 0.625, 0.875]","[0.95, 0.95, 0.8, 0.7, 0.9]",AlgoPerf,0.4712715617969706
550034,Pseudo Physics-Informed Neural Operators,"Recent advancements in operator learning are transforming the landscape of computational physics and engineering, especially alongside the rapidly evolving field of physics-informed machine learning. The convergence of these areas offers
exciting opportunities for innovative research and applications. However, merging
these two realms often demands deep expertise and explicit knowledge of physical systems, which may be challenging or even impractical in relatively complex applications. To address this limitation, we propose a novel framework: Pseudo
Physics-Informed Neural Operator (PPI-NO). In this framework, we construct a
surrogate physics system for the target system using partial differential equations
(PDEs) derived from simple, rudimentary physics knowledge, such as basic differential operators. We then couple the surrogate system with the neural operator model, utilizing an alternating update and learning process to iteratively enhance
the model’s predictive power. While the physics derived via PPI-NO may not mirror the ground-truth underlying physical laws — hence the term “pseudo physics” — this approach significantly enhances the accuracy of current operator learning
models, particularly in data scarce scenarios. Through extensive evaluations across
five benchmark operator learning tasks and an application in fatigue modeling,
PPI-NO consistently outperforms competing methods by a significant margin. The
success of PPI-NO may introduce a new paradigm in physics-informed machine
learning, one that requires minimal physics knowledge and opens the door to
broader applications in data-driven physics learning and simulations.",2025,0.4545456528924899,0.4537956527137322,0.4,0.25,8c8c0397-7bd9-4f17-a225-9ed5a37e1306,0,"[0.25, 0.25, 0.25, 0.5, 0.625, 0.625]","[1.0, 0.95, 0.9, 0.95, 0.95, 0.95]",Pseudo Physics-Informed Neural Operator,0.413455631567344
550049,Flow Matching for Accelerated Simulation of Atomic Transport in Materials,"We introduce LiFlow, a generative framework to accelerate molecular dynamics (MD) simulations for crystalline materials that formulates the task as conditional generation of atomic displacements. The model uses flow matching, with a Propagator submodel to generate atomic displacements and a Corrector to locally correct unphysical geometries, and incorporates an adaptive prior based on the Maxwell–Boltzmann distribution to account for chemical and thermal conditions. We benchmark LiFlow on a dataset comprising 25-ps trajectories of lithium diffusion across 4,186 solid-state electrolyte (SSE) candidates at four temperatures. The model obtains a consistent Spearman rank correlation of 0.7–0.8 for lithium mean squared displacement (MSD) predictions on unseen compositions. Furthermore, LiFlow generalizes from short training trajectories to larger supercells and longer simulations while maintaining high accuracy. With speed-ups of up to 600,000× compared to first-principles methods, LiFlow enables scalable simulations at significantly larger length and time scales.",2025,0.727272826446245,0.7137735220250452,0.8,0.875,9645ef01-5c98-493f-a2cc-66129f38ccfb,0,"[0.25, 0.5, 0.625, 0.875, 0.875, 0.875]","[1.0, 1.0, 0.7, 0.8, 0.9, 0.95]",Flow Matching,0.6053052520444832
550054,Locret: Enhancing Eviction in Long-Context LLM Inference with Trained Retaining Heads,"Large language models (LLMs) have shown remarkable advances in supporting long-context comprehension and processing tasks. However, scaling the generation inference of LLMs to such long contexts incurs significant additional computation load, and demands a substantial GPU memory footprint to maintain the key-value (KV) cache of transformer-based LLMs. Existing KV cache compression methods, such as quantization, face memory bottlenecks as context length increases, while static-sized caches, such as selective eviction, suffer from inefficient policies. These limitations restrict deployment on consumer-grade devices like a single Nvidia 4090 GPU. To overcome this, we propose Locret, an efficient framework for long-context LLM inference that introduces retaining heads to evaluate the causal importance of KV cache units, allowing for more accurate eviction within a fixed cache size. Locret is fine-tuned on top of the frozen backbone LLM using a minimal amount of data from standard long-context SFT datasets. During inference, we evict low-importance cache units along with a chunked prefill pattern, significantly reducing peak GPU memory usage. We conduct an extensive empirical study to evaluate Locret, where the experimental results show that Locret outperforms the recent popular and competitive approaches, including InfLLM, Quantization, SirLLM, and MInference, in terms of memory efficiency and the quality of generated contents --- Locret achieves over a $20\times$ and $8\times$ KV cache compression ratio compared to the full KV cache for Phi-3-mini-128K and Llama-3.1-8B-instruct. Additionally, Locret can be combined with other efficient inference methods, such as quantization and token merging. To the best of our knowledge, Locret is the first framework capable of deploying Llama-3.1-8B or similar models on a single Nvidia 4090 GPU, enabling 128K long-context inference without compromising generation quality, and requiring little additional system optimizations.",2025,0.6545452165290122,0.6484128418984472,0.5333333333333333,0.5,fe467dd8-b1ec-408f-ae05-924a8e253b8d,0,"[0.25, 0.5, 0.5, 0.875, 0.875]","[1.0, 0.95, 1.0, 0.95, 0.95]",Locret,0.5726736013494517
550096,Distilling Cross-Domain Knowledge for Person Re-ID by Aligning Any Pretrained Encoder with CLIP Textual Features,"Based on the alignment of image-text pairs, CLIP has demonstrated superior performance across various tasks, even in a zero-shot setting. In person ReID, CLIP-based models achieve state-of-the-art results without explicit text descriptions for further fine-tuning. However, previous models are primarily initialized with weights from ImageNet or self-supervised methods, lacking cross-domain knowledge in both image and text areas. This paper introduces a novel approach that aligns a pure image-domain pretrained student model with CLIP textual features, distilling cross-domain knowledge from existing CLIP-ReID into the online student model. To leverage CLIP’s textual features for each ID, we address the challenge of mismatched feature dimensions between the teacher and student. A trainable adapter is inserted on the student side to match dimensions and preserve the prior knowledge within the pretrained student. For the student encoder yielding lower or equal-dimensional features compared to the teacher, the adapter is initialized as an identity matrix, while offline PCA is employed on the teacher side for dimensionality reduction. PCA eigenvectors are computed from all training images and applied to existing text features for matching with the student. In cases where the student outputs exceed the teacher's dimensions, the adapter is initialized using eigenvectors computed from the student side to retain knowledge in the pretrained student model. After dimension alignment, text features for each ID are compared with online image features, specifying cross-domain similarities, which are further constrained to mimic the teacher through a KL-divergence loss. Experiments with different pretraining encoder structures demonstrate the effectiveness of this approach, which is also compatible with relation knowledge distillation to enhance performance.",2025,0.4090907603306326,0.4119063540779497,0.4,0.25,65df8dcc-56ef-4852-8582-81f810e0b331,0,"[0.25, 0.25, 0.5, 0.5]","[0.9, 1.0, 1.0, 1.0]",Cross-Domain Knowledge Distillation,0.3899023638232271
550128,Exponential Topology-enabled Scalable Communication in Multi-agent Reinforcement Learning,"In cooperative multi-agent reinforcement learning (MARL), well-designed communication protocols can effectively facilitate consensus among agents, thereby enhancing task performance. Moreover, in large-scale multi-agent systems commonly found in real-world applications, effective communication plays an even more critical role due to the escalated challenge of partial observability compared to smaller-scale setups. In this work, we endeavor to develop a scalable communication protocol for MARL. Unlike previous methods that focus on selecting optimal pairwise communication links—a task that becomes increasingly complex as the number of agents grows—we adopt a global perspective on communication topology design. Specifically, we propose utilizing the exponential topology to enable rapid information dissemination among agents by leveraging its small-diameter and small-size properties. This approach leads to a scalable communication protocol, named ExpoComm. To fully unlock the potential of exponential graphs as communication topologies, we employ memory-based message processors and auxiliary tasks to ground messages, ensuring that they reflect global information and benefit decision-making. Extensive experiments on large-scale cooperative benchmarks, including MAgent and Infrastructure Management Planning, demonstrate the superior performance and robust zero-shot transferability of ExpoComm compared to existing communication strategies. The
code is publicly available at [https://github.com/LXXXXR/ExpoComm](https://github.com/LXXXXR/ExpoComm).",2025,0.7159088305786071,0.7185089666318878,0.6666666666666666,0.625,408fd680-697d-4f31-938d-2327ef64f6db,1,"[0.5, 0.625, 0.625, 0.875]","[0.9, 0.8, 0.95, 0.95]",ExpoComm,0.6718948533982507
550139,Evaluating Deep Unlearning in Large Language Models,"Machine unlearning has emerged as an important component in developing safe and trustworthy models. Prior work on unlearning in LLMs has mostly considered unlearning tasks where a large corpus of copyrighted material or some specific training data are required to be removed. In this work, we consider the task of unlearning a fact from LLMs, which can be challenging as related facts can be deduced from each other, and investigate how well current unlearning methods for LLMs succeed at this task. Specifically, we formally propose a framework and a definition for deep unlearning facts that are interrelated. We design the metric, recall, to quantify the extent of deep unlearning. To enable us to systematically evaluate the extent of deep unlearning undistracted by other factors, we construct a synthetic dataset EDU-RELAT, which consists of a synthetic knowledge base of family relationships and biographies, together with a realistic logical rule set that connects them. We use this dataset to test four unlearning methods in four LLMs at different sizes. Our findings reveal that in the task of deep unlearning only a single fact, they either fail to properly unlearn with high recall, or end up unlearning many other irrelevant facts. Our dataset and code are publicly available at: https://anonymous.4open.science/r/deep_unlearning_anonymous-2C73.",2025,0.5909092396693675,0.5899342396169823,0.5333333333333333,0.5,942dfdf7-b0f2-41b0-9910-4e8c9ce1d71d,0,"[0.5, 0.5, 0.625]","[1.0, 0.9, 0.95]",Deep Unlearning,0.5400914966738121
550140,DocGenome: A Large Benchmark for Multi-Modal Language Models in Real-World Academic Document Understanding,"Scientific documents record research findings and valuable human knowledge, comprising a vast corpus of high-quality data. Thus, leveraging multi-modality data extracted from these documents and assessing large models' abilities to handle scientific document-oriented tasks is meaningful. Despite promising advancements, large models still perform poorly on multi-page scientific document extraction and understanding tasks, and their capacity to process within-document data formats such as charts and equations remains under-explored. To address these issues, we present DocGenome, a structured document dataset constructed by annotating 500K scientific documents from 153 disciplines in the arXiv open-access community, using our custom auto-labeling pipeline. DocGenome features four characteristics: 1) Completeness: It is the first dataset to structure data from all modalities including 13 layout attributes along with their LaTeX source codes. 2) Logicality: It provides 6 logical relationships between different entities within each scientific document. 3) Diversity: It covers various document-oriented tasks, including document classification, visual grounding, document layout detection, document transformation, open-ended single-page QA and multi-page QA. 4) Correctness: It undergoes rigorous quality control checks conducted by a specialized team. We conduct extensive experiments to demonstrate the advantages of DocGenome and objectively evaluate the performance of current large models on our benchmark.",2025,0.7159088305786071,0.7125177796994496,0.6666666666666666,0.625,9eaf0f17-7d24-4c4e-aaf1-51b1935ed69e,0,"[0.5, 0.625, 0.625, 0.875]","[1.0, 0.95, 0.95, 0.95]",DocGenome,0.6438236377025037
550159,CausalESC: Breaking Causal Cycles for Emotional Support Conversations with Temporal Causal HMM,"Emotional Support Conversation (ESC) is a rapidly advancing task focused on alleviating a seeker's emotional distress. The intricate interplay between cognition, emotion, and behavior presents substantial challenges for existing approaches, which often struggle to capture the dynamic evolution of the seeker's internal state during conversations. To address this, we propose \textbf{CausalESC}, a model designed to dynamically represent the seeker's internal states, by assuming that the generative process governing the mutual influence among these factors follows a first-order Markov property, with \iid random variables. The model comprises a prior network, that disentangles the seeker's emotions, cognition, and behavior, and a posterior network, which decouples the support strategy factors. The prior network also models the psychological causality of the seeker within each conversation round. To account for the varying effects of support strategies on the seeker's intrinsic states, we incorporate a support intervention module to capture these impacts. Additionally, a holistic damping transfer  mechanism is designed to regulate the complex interactions among cognition, emotion, behavior, and strategy, ensuring that changes remain within a reasonable range. Our model effectively breaks causal cycles and achieves causal representation learning. Both automatic and human evaluations demonstrate the effectiveness of our model, emphasizing the advantages of modeling the evolution of the seeker's internal state under support strategies.",2025,0.443181657024852,0.4311051621573936,0.4,0.25,774c5721-0e58-491e-a426-4a645bb3e708,0,"[0.25, 0.25, 0.5, 0.625]","[0.95, 0.95, 0.9, 0.8]",CausalESC,0.3663242005893049
550161,Learning with Preserving for Continual Multitask Learning,"Artificial Intelligence (AI) drives advancements across fields, enabling capabilities previously unattainable. Modern intelligent systems integrate increasingly specialized tasks, such as improving tumor classification with tissue recognition or advancing driving assistance with lane detection. Typically, new tasks are addressed by training single-task models or re-training multitask models, which becomes impractical when prior data is unavailable or new data is limited. This paper introduces Continual Multitask Learning (CMTL), a novel problem category critical for future intelligent systems yet overlooked in current research. CMTL presents unique challenges beyond the scope of traditional Continual Learning (CL) and Multitask Learning (MTL). To address these challenges, we propose Learning with Preserving (LwP), a novel approach for CMTL that retains previously learned knowledge while supporting diverse tasks. LwP employs a Dynamically Weighted Distance Preservation loss function to maintain representation integrity, enabling learning across tasks without a replay buffer. We extensively evaluate LwP on three benchmark datasets across two modalities—inertial measurement units of multivariate time series data for quality of exercises assessment and image datasets. Results demonstrate that LwP outperforms existing continual learning baselines, effectively mitigates catastrophic forgetting, and highlights its robustness and generalizability in CMTL scenarios.",2025,0.5454543471075102,0.5427867242006487,0.6,0.625,86129fda-54ed-4704-972e-60341477b847,0,"[0.25, 0.5, 0.625, 0.625]","[1.0, 0.9, 0.95, 1.0]",Learning with Preserving,0.4903214274898714
550165,Optimal Algorithm for Max-Min Fair Bandit,"We consider a multi-player multi-armed bandit problem (MP-MAB) where $N$ players compete for $K$ arms in $T$ rounds. The reward distribution is heterogeneous where each player has a different expected reward for the same arm. When multiple players select the same arm, they collide and obtain zero reward. In this paper, we aim to find the max-min fairness matching that maximizes the reward of the player who receives the lowest reward. This paper improves the existing regret upper bound result of $O(\log T\log \log T)$ to achieve max-min fairness. More specifically, our decentralized fair elimination algorithm (DFE) deals with heterogeneity and collision carefully and attains a regret upper bounded of $O((N^2+K)\log T / \Delta)$, where $\Delta$ is the minimum reward gap between max-min value and sub-optimal arms. We assume $N\leq K$ to guarantee all players can select their arms without collisions. In addition, we also provide an $\Omega(\max\{N^2, K\} \log T / \Delta)$ regret lower bound for this problem. This lower bound indicates that our algorithm is optimal with respect to key parameters, which significantly improves the performance of algorithms in previous work. Numerical experiments again verify the efficiency and improvement of our algorithms.",2025,0.6477270371901683,0.6466582877904381,0.6666666666666666,0.625,ccc72b11-2214-4942-9824-6a1c8590a274,0,"[0.5, 0.625, 0.625, 0.625]","[0.9, 0.9, 0.9, 0.9]",Max-Min Fairness,0.5937500000000001
550176,One-step Image-function Generation via Consistency Training,"Consistency models aim to deliver a U-Net generator to map noise to images directly and enable swift inference with minimal steps, even trained in isolation with consistency training mode. However, the U-Net generator requires heavy feature extraction layers for multi-level resolutions and learning convolution kernels with specific receptive fields, resulting in the challenge that consistency models suffer from heavy training resources and fail to generate images with any user-specific resolutions. In this paper, we first validate that training the original consistency model with a small batch size via consistency training mode is pretty unstable, which motivates us to investigate efficient and flexible consistency models. To this end, we propose to use a novel Transformer-based generator to generate continuous image functions, which can then be differentially rendered as images with arbitrary resolutions. We adopt implicit neural representations (INRs) to form such continuous functions, which help to decouple the resolution of generated images and the total amount of the parameters generated from the neural network. Extensive experiments on one-step image generation demonstrate that our method greatly improves the performance of consistency models with low training resources and also provides an efficient any-resolution image sampling process.",2025,0.5795452438017296,0.5776686974913471,0.5333333333333333,0.5,e550031c-577c-4643-8a9b-76bc2e610972,0,"[0.5, 0.5, 0.5, 0.625]","[0.95, 0.9, 0.95, 0.9]",Implicit Neural Representations,0.5272344559585492
550179,Drawing the Line: Enhancing  Trustworthiness of MLLMs Through the Power of Refusal,"Multimodal large language models (MLLMs) excel at multimodal perception and understanding, yet their tendency to generate hallucinated or inaccurate responses undermines their trustworthiness. Existing methods have largely overlooked the importance of refusal responses as a means of enhancing MLLMs reliability. To bridge this gap, we present the Information Boundary-aware Learning Framework (InBoL), a novel approach that empowers MLLMs to refuse to answer user queries when encountering insufficient information. To the best of our knowledge, InBoL is the first framework that systematically defines the conditions under which refusal is appropriate for MLLMs using the concept of information boundaries proposed in our paper. This framework introduces a comprehensive data generation pipeline and tailored training strategies to improve the model’s ability to deliver appropriate refusal responses. To evaluate the trustworthiness of MLLMs, we further propose a user-centric alignment goal along with corresponding metrics. Experimental results demonstrate a significant improvement in refusal accuracy without noticeably compromising the model’s helpfulness, establishing InBoL as a pivotal advancement in building more trustworthy MLLMs.",2025,0.5909092396693675,0.589108695226001,0.5333333333333333,0.5,cbc020c4-1e4a-4f0a-891f-e1e95a40df63,0,"[0.5, 0.5, 0.625]","[0.95, 0.9, 0.9]",InBoL,0.5379375676651029
550193,Students Rather Than Experts: A New AI for Education Pipeline to Model More Human-like and Personalised Early Adolescences,"The capabilities of large language models (LLMs) have been applied in expert systems across various domains, providing new opportunities for AI in Education (AI4Education). Educational interactions involve a cyclical exchange between teachers and students. Current research predominantly focuses on using LLMs to simulate teachers, leveraging their expertise to enhance student learning outcomes. However, the simulation of students, which could improve teachers' instructional skills, has received insufficient attention due to the challenges of modeling and evaluating virtual students. This research poses the question: “Can LLMs be utilized to develop virtual student agents that mimic human-like behavior and individual variability?” Unlike expert systems focusing on knowledge delivery, virtual students must replicate learning difficulties, emotional responses, and linguistic uncertainties. These traits present significant challenges in both modeling and evaluation. To address these issues, this study focuses on language learning as a context for modeling virtual student agents. We propose a novel AI4Education framework, termed SOE (Scene - Object - Evaluation), to systematically construct LVSA (LLM-based Virtual Student Agents).  By curating a dataset of personalized teacher-student interactions with various personality traits, question types, and learning stages, and fine-tuning LLMs using LoRA, we conduct multi-dimensional evaluation experiments that integrate both subjective human evaluations and objective metrics. Specifically, we: (1) develop a theoretical framework for generating LVSA; (2) integrate human subjective evaluation metrics into GPT-4 assessments, demonstrating a strong correlation between human evaluators and GPT-4 in judging LVSA authenticity; and (3) validate that LLMs can generate human-like, personalized virtual student agents in educational contexts, laying a foundation for future applications in pre-service teacher training and multi-agent simulation environments.",2025,0.5454543471075102,0.5463219710253523,0.6,0.625,4fb4884b-fce3-4dd2-8cda-3ea0da3f9489,0,"[0.25, 0.5, 0.625, 0.625]","[0.95, 0.95, 0.95, 1.0]",LVSA,0.509941089837997
550197,Prompt-Guided Distillation from Multimodal Large Language Models to Task-specific Models for Multimodal Sentiment Analysis,"Multimodal Sentiment Analysis (MSA) has made some progress with the advent of Multimodal Large Language Models (MLLMs). However, the scalability and the closed-source nature of some MLLMs imposes challenges for efficient application in the real-word. In this study, we explore an innovative pathway to infuse the capabilities of general MLLMs into task-specific small models for MSA. We introduce the Prompt-Guided Multimodal Framework (PGMF), a refined teacher-student framework designed to transfer knowledge from powerful, general MLLMs to smaller, efficient models. The PGMF-Teacher utilizes MLLM-generated prompts and a tailored conditional alignment module to achieve better MSA, while the PGMF-Student distills this expertise to predict independently of MLLMs' guidance. Extensive evaluations on two popular MSA datasets including SIMS and MOSI demonstrate that compared to previous task-specific small models, PGMF-Teacher achieves state-of-the-art performance with the help of MLLMs' prompts, while PGMF-Student achieve competitive results with fewer parameters and without relying on MLLMs' prompts. The proposed framework offers a novel way to equip task-specific small models with the capability of MLLMs.",2025,0.6363630413225304,0.6360961116641364,0.6666666666666666,0.625,60192548-f48a-4e27-b496-381790781fc4,0,"[0.5, 0.625, 0.625]","[0.95, 1.0, 0.95]",Prompt-Guided Multimodal Framework,0.587637494021999
550208,LNUCB-TA: Linear-nonlinear Hybrid Bandit Learning with Temporal Attention,"Existing contextual multi-armed bandit (MAB) algorithms struggle to simultaneously capture long-term trends as well as local patterns across all arms, leading to suboptimal performance in complex environments with rapidly changing reward structures. Additionally, they typically employ static exploration rates, which do not adapt to dynamic conditions. To address these issues, we present LNUCB-TA, a hybrid bandit model that introduces a novel nonlinear component (adaptive $k$-Nearest Neighbors ($k$-NN)) designed to reduce time complexity, and an innovative global-and-local attention-based exploration mechanism. Our method incorporates a unique synthesis of linear and nonlinear estimation techniques, where the nonlinear component dynamically adjusts $k$ based on reward variance, thereby effectively capturing spatiotemporal patterns in the data. This is critical for reducing the likelihood of selecting suboptimal arms and accurately estimating rewards while reducing computational time. Also, our proposed attention-based mechanism prioritizes arms based on their historical performance and frequency of selection, thereby balancing exploration and exploitation in real-time without the need for fine-tuning exploration parameters. Incorporating both global attention (based on overall performance across all arms) and local attention (focusing on individual arm performance), the algorithm efficiently adapts to temporal and spatial complexities in the available context. Empirical evaluation demonstrates that LNUCB-TA significantly outperforms state-of-the-art contextual MAB algorithms, including purely linear, nonlinear, and vanilla combination of linear and nonlinear bandits based on cumulative and mean rewards, convergence performance, and demonstrates consistency of results across different exploration rates. Theoretical analysis further proves the robustness of LNUCB-TA with a sub-linear regret bound.",2025,0.443181657024852,0.4470497134555022,0.4,0.25,42dee5ad-6392-40a8-9b56-3978f034c46a,0,"[0.25, 0.25, 0.5, 0.625]","[0.9, 0.9, 0.95, 0.95]",LNUCB-TA,0.4263277202072538
550217,Leave-One-Out Stable Conformal Prediction,"Conformal prediction (CP) is an important tool for distribution-free predictive uncertainty quantification.
Yet, a major challenge is to balance computational efficiency and prediction accuracy, particularly for multiple predictions.
We propose **L**eave-**O**ne-**O**ut **Stab**le **C**onformal **P**rediction (LOO-StabCP), a novel method to speed up full conformal using algorithmic stability without sample splitting.
By leveraging *leave-one-out* stability, our method is much faster in handling a large number of prediction requests compared to existing method RO-StabCP based on *replace-one* stability.
We derived stability bounds for several popular machine learning tools: regularized loss minimization (RLM) and stochastic gradient descent (SGD), as well as kernel method, neural networks and bagging.
Our method is theoretically justified and demonstrates superior numerical performance on synthetic and real-world data.
We applied our method to a screening problem, where its effective exploitation of training data led to improved test power compared to state-of-the-art method based on split conformal.",2025,0.7159088305786071,0.7128368885471509,0.6666666666666666,0.625,5ac9e47a-31b3-49ea-aaff-50aeeca1b4bb,1,"[0.5, 0.625, 0.625, 0.875]","[0.95, 0.8, 0.95, 0.9]",LOO-StabCP,0.6457992880068532
550220,Continual Learning After Model Deployment,"This paper studies continual learning after model deployment. A real-world application environment is often an open world filled with novel or out-of-distribution (OOD) objects that have not been seen before. We can call continual learning in such an environment *open-world continual learning* (OWCL). OWCL incrementally performs two main tasks: (1) detecting OOD objects, and (2) continually learning the OOD or new objects on the fly. Although OOD detection and continual learning have been extensively studied separately, their combination for OWCL has barely been attempted. This is perhaps because in addition to the existing challenges of OOD detection and continual learning such as *catastrophic forgetting* (CF), OWCL also faces the challenge of data scarcity. As novel objects appear sporadically, when an object from a new/novel class is detected, it is difficult to learn it from one or a few samples to give good accuracy. This paper proposes a novel method called OpenLD to deal with these problems based on *linear discriminant analysis* (LDA) and a pre-trained model. This method enables OOD detection and incremental learning of the detected samples on the fly with no CF. Experimental evaluation demonstrates the effectiveness of OpenLD.",2025,0.3749998636364132,0.3743811139839378,0.2666666666666666,0.25,58377d15-e085-4dde-9398-60a5af1ed364,0,"[0.25, 0.25, 0.25, 0.625]","[1.0, 0.95, 0.9, 0.95]",Open-world continual learning,0.3410665391954398
550227,Animate Your Thoughts: Reconstruction of Dynamic Natural Vision from Human Brain Activity,"Reconstructing human dynamic vision from brain activity is a challenging task with great scientific significance.  Although prior video reconstruction methods have made substantial progress, they still suffer from several limitations, including: (1) difficulty in simultaneously reconciling semantic (e.g. categorical descriptions), structure (e.g. size and color), and consistent motion information (e.g. order of frames); (2) low temporal resolution of fMRI, which poses a challenge in decoding multiple frames of video dynamics from a single fMRI frame; (3) reliance on video generation models, which introduces ambiguity regarding whether the dynamics observed in the reconstructed videos are genuinely derived from fMRI data or are hallucinations from generative model. To overcome these limitations,  we propose a two-stage model named Mind-Animator. During the fMRI-to-feature stage, we decouple semantic, structure, and motion features from fMRI. Specifically, we employ fMRI-vision-language tri-modal contrastive learning to decode semantic feature from fMRI and design a sparse causal attention mechanism for decoding multi-frame video motion features through a next-frame-prediction task. In the feature-to-video stage, these features are integrated into videos using an inflated Stable Diffusion, effectively eliminating external video data interference.  Extensive experiments on multiple video-fMRI datasets demonstrate that our model achieves state-of-the-art performance. Comprehensive visualization analyses further elucidate the interpretability of our model from a neurobiological perspective.  Project page: https://mind-animator-design.github.io/.",2025,0.8181815206612653,0.8186470656144424,0.8,0.625,612780b2-60ca-4de7-bb3c-b1ed2b946828,1,"[0.625, 0.625, 0.875, 0.875]","[0.95, 0.9, 0.95, 0.95]",Mind-Animator,0.7575462512171373
550237,Backtracking Improves Generation Safety,"Text generation has a fundamental limitation almost by definition: there is no taking back tokens that have been generated, even when they are clearly problematic.
In the context of language model safety, when a partial unsafe generation is produced, language models by their nature tend to happily keep on generating similarly unsafe additional text.
This is in fact how safety alignment of frontier models gets circumvented in the wild, despite great efforts in improving their safety.
Deviating from the paradigm of approaching safety alignment as prevention (decreasing the probability of harmful responses), we propose backtracking, a technique that allows language models to ""undo"" and recover from their own unsafe generation through the introduction of a special [RESET] token.
Our method can be incorporated into either SFT or DPO training to optimize helpfulness and harmlessness.
We show that models trained to backtrack are consistently safer than baseline models: backtracking Llama-3-8B is four times more safe than the baseline model (6.1\% $\to$ 1.5\%) in our evaluations without regression in helpfulness.
Our method additionally provides protection against four adversarial attacks including an adaptive attack, despite not being trained to do so.",2025,0.9545451074381428,0.9529701083227508,0.9333333333333332,0.875,d785de1e-f23a-4850-8f5f-27abcfd86b1e,1,"[0.875, 0.875, 0.875, 0.875]","[0.8, 0.95, 0.95, 0.95]",Backtracking,0.8749964706534724
550243,Cross-modal Mitigation of Spurious Correlation for Prompt-tuning in VLMs with Causally Motivated Logic Alignment,"Recent studies have shown that pre-trained vision-language models can effectively adapt to diverse downstream tasks through parameter-efficient prompt tuning. Unfortunately, the tuned models can exploit spurious correlations during prediction, resulting in a failure to generalize to out-of-distribution test data, especially when the tuning dataset exhibits bias. How to achieve cross-modal mitigation of spurious correlations during prompt tuning of vision-language models remains an open question. In this paper, the challenging problem is tackled by leveraging the stable relationship between necessary and sufficient causal features and the corresponding label. On the one hand, we constrain the learning process of prompt by reinforcing the necessary and sufficient connection between the textual labels and textual features. On the other hand, the probability of necessity and sufficiency between the textual features and the filtered visual features is measured and maximized to enhance cross-modal feature alignment. By iteratively optimizing these two objectives, we can achieve cross-modal mitigation of spurious correlations because the logic equivalence between textual labels and visual features is bolstered. The theoretical analysis on generalization error indicates that our method can achieve a tighter generalization error bound than existing approaches. We evaluate the proposed method on several commonly adopted out-of-distribution datasets, and the empirical results demonstrate the superiority of our method over the state-of-the-art competitors.",2025,0.5454543471075102,0.5388822695282635,0.6,0.625,f9f48ccf-6d16-4a3d-a1c5-10f8af7373f3,0,"[0.25, 0.5, 0.625, 0.625]","[0.95, 0.9, 0.8, 0.95]",Causally Motivated Logic Alignment,0.4809131953926595
550248,Scaling Instruction-tuned LLMs to Million-token Contexts via Hierarchical Synthetic Data Generation,"Large Language Models (LLMs) struggle with long-context reasoning, not only due to the quadratic scaling of computational complexity with sequence length but also because of the scarcity and expense of annotating long-context data. There has been barely any open-source work that systematically ablates long-context data, nor is there any openly available instruction tuning dataset with contexts surpassing 100K tokens. To bridge this gap, we introduce a novel post-training synthetic data generation strategy designed to efficiently extend the context window of LLMs while preserving their general task performance. Our approach scalably extends to arbitrarily long context lengths, unconstrained by the length of available real-world data, which effectively addresses the scarcity of raw long-context data. 
Through a step-by-step rotary position embedding (RoPE) scaling training strategy, we demonstrate that our model, with a context length of up to 1M tokens, performs well on the RULER benchmark and InfiniteBench and maintains robust performance on general language tasks.",2025,0.6818179338843877,0.6806929345162506,0.6666666666666666,0.625,b12c50fb-3330-4104-a421-ec2343d783d4,1,"[0.625, 0.625, 0.625, 0.625]","[0.9, 0.9, 0.9, 0.95]",Hierarchical Synthetic Data Generation,0.6250000000000001
550262,Controlling Information Leakage in Concept Bottleneck Models with Trees,"As AI models grow larger, the demand for accountability and interpretability has become increasingly critical for understanding their decision-making processes. Concept Bottleneck Models (CBMs) have gained attention for enhancing interpretability by mapping inputs to intermediate concepts before making final predictions. However, CBMs often suffer from information leakage, where additional input data, not captured by the concepts, is used to improve task performance, complicating the interpretation of downstream predictions. In this paper, we introduce a novel approach for training both joint and sequential CBMs that allows us to identify and control leakage using decision trees. Our method quantifies leakage by comparing the decision paths of hard CBMs with their soft, leaky counterparts. Specifically, we show that soft leaky CBMs extend the decision paths of hard CBMs, particularly in cases where concept information is incomplete. Using this insight, we develop a technique to better inspect and manage leakage, isolating the subsets of data  most affected by this. Through synthetic and real-world experiments, we demonstrate that controlling leakage in this way not only improves task accuracy but also yields more informative and transparent explanations.",2025,0.4772725537190714,0.4755778266182522,0.5333333333333333,0.5,d9681b36-91d6-4d84-b90e-110395767c77,0,"[0.25, 0.5, 0.5, 0.5]","[0.95, 0.95, 0.95, 0.9]",Information Leakage,0.4337268743914313
550268,Self-Supervised Feature Re-Representation via Lennard-Jones Potential Loss,"The Lennard-Jones potential, initially developed to model molecular interactions, is characterized by a repulsive force at short distances to prevent over-clustering and an attractive force at longer distances to maintain balanced proximity, resembling the equilibrium-seeking behavior of particles in natural systems.  This offers a potential pathway for more orderly entropy reduction in higher-order features.
This paper introduces a self-supervised approach for feature re-representation, utilizing a Lennard-Jones potential loss to constrain the gradient directions between positive and negative features in computer vision tasks.  Unlike supervised learning directly driven by downstream tasks or contrastive learning with multi-label data pairs and multi-feature extractors, the proposed loss term integrates with existing task-specific losses by directly constraining gradient directions, thereby enhancing the feature learning process.
Extensive theoretical analysis and experimental results demonstrate that, across various domains, datasets, network architectures, and tasks, models incorporating the Lennard-Jones potential loss significantly outperform baseline models without this auxiliary loss in both accuracy and robustness.  This approach highlights the potential of physics-inspired loss functions to improve deep learning optimization.",2025,0.3409089669421938,0.336764388759527,0.2666666666666666,0.25,4b25321e-cf08-4fcb-9bf9-caf1eba75bbf,0,"[0.25, 0.25, 0.25, 0.5]","[0.95, 1.0, 0.95, 0.9]",Lennard-Jones potential loss,0.2968851909784909
550269,Hierarchical Object-Oriented POMDP Planning for Object Rearrangement,"We present an online planning framework for solving multi-object rearrangement problems in partially observable, multi-room environments. Current object rearrangement solutions, primarily based on Reinforcement Learning or hand-coded planning methods, often lack adaptability to diverse challenges. To address this limitation, we introduce a novel Hierarchical Object-Oriented Partially Observed Markov Decision Process (HOO-POMDP) planning approach. This approach comprises of (a) an object-oriented POMDP planner generating sub-goals, (b) a set of low-level policies for sub-goal achievement, and (c) an abstraction system converting the continuous low-level world into a representation suitable for abstract planning. We evaluate our system on varying numbers of objects, rooms, and problem types in AI2-THOR simulated environments with promising results.",2025,0.5454543471075102,0.5445543476130005,0.6,0.625,576107a1-4299-4e10-ad5c-566d0ec1d6e1,0,"[0.25, 0.5, 0.625, 0.625]","[0.9, 1.0, 0.9, 0.9]",HOO-POMDP,0.4999999999999999
550270,What Matters When Repurposing Diffusion Models for General Dense Perception Tasks?,"Extensive pre-training with large data is indispensable for downstream geometry and semantic visual perception tasks. Thanks to large-scale text-to-image (T2I) pretraining, recent works show promising results by simply fine-tuning T2I diffusion models for a few dense perception tasks. However, several crucial design decisions in this process still lack comprehensive justification, encompassing the necessity of the multi-step diffusion mechanism, training strategy, inference ensemble strategy, and fine-tuning data quality. In this work, we conduct a thorough investigation into critical factors that affect transfer efficiency and performance when using diffusion priors. Our key findings are: 1) High-quality fine-tuning data is paramount for both semantic and geometry perception tasks. 2) As a special case of the diffusion scheduler by setting its hyper-parameters, the multi-step generation can be simplified to a one-step fine-tuning paradigm without any loss of performance, while significantly speeding up inference. 3) Apart from fine-tuning the diffusion model with only latent space supervision, task-specific supervision can be beneficial to enhance fine-grained details. These observations culminate in the development of GenPercept, an effective deterministic one-step fine-tuning paradigm tailored for dense visual perception tasks exploiting diffusion priors. Different from the previous multi-step methods, our paradigm offers a much faster inference speed, and can be seamlessly integrated with customized perception decoders and loss functions for task-specific supervision, which can be critical for improving the fine-grained details of predictions. Comprehensive experiments on a diverse set of dense visual perceptual tasks, including monocular depth estimation, surface normal estimation, image segmentation, and matting, are performed to demonstrate the remarkable adaptability and effectiveness of our proposed method. Code: https://github.com/aim-uofa/GenPercept",2025,0.6818179338843877,0.6806929345162506,0.6666666666666666,0.625,c579838a-35f6-451c-801f-ff8b17700d3a,1,"[0.625, 0.625, 0.625, 0.625]","[0.95, 0.9, 0.9, 0.95]",GenPercept,0.6250000000000002
550300,SPARK: Physics-Guided Quantitative Augmentation for Dynamical System Modeling,"In dynamical system modeling, traditional numerical methods have a solid theoretical foundation but are limited by high computational costs and sensitivity to initial conditions. Current data-driven approaches use deep learning models to capture complex spatiotemporal features, but they rely heavily on large amounts of data and assume a stable data distribution, making them ineffective against data scarcity and distribution shifts. To address these challenges, we propose SPARK, a physics-guided quantized augmentation plugin. SPARK integrates boundary information and physical parameters, using a reconstruction autoencoder to build a physics-rich discrete memory bank for data compression. It then enhances selected samples for downstream tasks with this pre-trained memory bank. SPARK then utilizes an attention mechanism to model historical observations and combines fourier-enhanced graph ODE to efficiently predict long-term dynamical systems, enhancing robustness and adaptability to complex physical environments. Extensive experiments on benchmark datasets show that our approach significantly outperforms various baseline methods in handling distribution shifts and data scarcity.",2025,0.6477270371901683,0.6425741301833405,0.6666666666666666,0.625,debb69af-f3c1-43ed-b9f7-1070d93e434b,0,"[0.25, 0.625, 0.625, 0.875]","[0.95, 0.95, 0.95, 0.9]",SPARK,0.576770934761441
550303,Unpaired Single-Cell Dataset Alignment with Wavelet Optimal Transport,"Aligning single-cell samples across different datasets and modalities is an important task with the rise of high-throughput single-cell technologies. Currently, collecting multi-modality datasets with paired samples is difficult, expensive, and impossible in some cases, motivating methods to align unpaired samples from distinct uni-modality datasets. While dataset alignment problems have been addressed in various domains, single-cell data introduce additional complexity including high levels of noise, dropout, and non-isometry between data spaces. In response to these unique challenges, we propose Wavelet Optimal Transport (WOT), a multi-resolution optimal transport method that aligns samples by minimizing the spectral graph wavelet discrepancies across datasets. Filters are incorporated into the optimization process to eliminate non-essential scales and wavelets, enhancing the quality of correspondences. We demonstrate the capacity of WOT in highly noisy and non-isometric conditions, outperforming previous state-of-the-art methods by significant margins, especially on real single-cell datasets.",2025,0.5454543471075102,0.5445543476130005,0.5333333333333333,0.5,4d2fb43e-4408-4f56-a22b-ef587a3fe892,0,"[0.5, 0.5, 0.5, 0.5]","[0.95, 0.8, 1.0, 0.9]",Wavelet Optimal Transport,0.5000000000000001
550315,Different Rates for Different Weights: Decoupled Relative Learning Rate Schedules,"In this work, we introduce a novel approach for optimizing neural network training by adjusting learning rates across weights of different components in Transformer models. Traditional methods often apply a uniform learning rate across all network layers, potentially overlooking the unique dynamics of each part. Remarkably, our introduced Relative Learning Rate Schedules (RLRS) method accelerates the training process by 13.6%, particularly in complex models such as the Mixture of Experts (MoE). Hyperparameters of RLRS can be efficiently tuned on smaller models and then extrapolated to 27x larger ones. This simple and effective method results in a substantial reduction in training time and computational resources, offering a practical and scalable solution for optimizing large-scale neural networks.",2025,0.2045453801653163,0.1966451095752258,0.2666666666666666,0.25,b0f7f622-ef48-4f99-aa8c-a796feecc092,0,"[0.0, 0.25, 0.25, 0.25]","[1.0, 0.9, 0.8, 0.9]",Relative Learning Rate Schedules,0.150779783779953
550316,Efficient Residual Learning with Mixture-of-Experts for Universal Dexterous Grasping,"Universal dexterous grasping across diverse objects presents a fundamental yet formidable challenge in robot learning. Existing approaches using reinforcement learning (RL) to develop policies on extensive object datasets face critical limitations, including complex curriculum design for multi-task learning and limited generalization to unseen objects. 
To overcome these challenges, we introduce ResDex, a novel approach that integrates residual policy learning with a mixture-of-experts (MoE) framework. ResDex is distinguished by its use of geometry-agnostic base policies that are efficiently acquired on individual objects and capable of generalizing across a wide range of unseen objects. Our MoE framework incorporates several base policies to facilitate diverse grasping styles suitable for various objects. By learning residual actions alongside weights that combine these base policies, ResDex enables efficient multi-task RL for universal dexterous grasping.
ResDex achieves state-of-the-art performance on the DexGraspNet dataset comprising 3,200 objects with an 88.8% success rate. It exhibits no generalization gap with unseen objects and demonstrates superior training efficiency, mastering all tasks within only 12 hours on a single GPU. For further details and videos, visit our project page.",2025,0.8181815206612653,0.8185991448318526,0.8,0.625,d7012029-4973-48c5-ac70-650b290341c2,1,"[0.625, 0.625, 0.875, 0.875]","[0.95, 0.95, 1.0, 0.95]",ResDex,0.7599410898379971
550321,How Good is my Video LMM? Complex Video Reasoning and Robustness Evaluation Suite for Video-LMMs,"Recent advancements in Large Language Models (LLMs) have led to the development of Video Large Multi-modal Models (Video-LMMs) that can handle a wide range of video understanding tasks. These models have the potential to be deployed in real-world applications such as robotics, AI assistants, medical surgery, and autonomous vehicles. The widespread adoption of Video-LMMs in our daily lives underscores the importance of ensuring and evaluating their robust performance in mirroring human-like reasoning and interaction capabilities in complex, real-world contexts. However, existing benchmarks for Video-LMMs primarily focus on general video comprehension abilities and neglect assessing their reasoning capabilities over complex videos in the real-world context, and the robustness of these models through the lens of user prompts as text queries. In this paper, we present the Complex Video Reasoning and Robustness Evaluation Suite (CVRR-ES), a novel benchmark that comprehensively assesses the performance of Video-LMMs across 11 diverse real-world video dimensions. We evaluate 11 recent models, including both open-source and closed-source variants, and find that most of the Video-LMMs, especially open-source ones, struggle with robustness and reasoning when dealing with complex videos. Based on our analysis, we develop a training-free Dual-Step Contextual Prompting (DSCP) technique to effectively enhance the performance of existing Video-LMMs on CVRR-ES benchmark. Our findings provide valuable insights for building the next generation of human-centric AI systems with advanced robustness and reasoning capabilities. Our dataset and code will be made publicly available.",2025,0.3749998636364132,0.3730545795931526,0.2666666666666666,0.25,dcd1b540-fc25-4131-82c9-4cfb8066b54e,0,"[0.25, 0.25, 0.25, 0.625]","[0.95, 1.0, 0.95, 0.95]",CVRR-ES,0.3362941826215022
550328,Model Developmental Safety: A Safety-Centric Method and Applications in Vision-Language Models,"In the real world, a learning-enabled system usually undergoes multiple cycles of model development to enhance the system's ability to handle difficult or emerging tasks, which involve collecting new data, training a new model and validating the model.  This continual model development process raises a significant issue that the model development for acquiring new or improving existing capabilities may inadvertently lose capabilities of the old model, also known as catastrophic forgetting. Existing continual learning studies focus on mitigating catastrophic forgetting by trading off performance on previous tasks and new tasks to ensure good average performance.  However, they are inadequate for many applications especially in safety-critical domains, as failure to preserve the performance of the old model not only introduces safety risks and uncertainties but also imposes substantial expenses in the re-improving and re-validation of existing properties. To address this issue, we introduce  **model developmental safety as a guarantee** of a learning system such that in the model development process the new model should strictly preserve the existing protected capabilities of the old model while improving its performance on target tasks. 
To ensure the model developmental safety, we present a retention-centric framework by formulating the model developmental safety as data-dependent constraints. Under this framework, we study how to develop a pretrained vision-language model,specifically
the CLIP model, for acquiring new capabilities or improving existing capabilities of image classification. We propose an efficient constrained optimization algorithm with theoretical guarantee and use its insights to finetune a CLIP model with task-dependent heads for promoting the model developmental safety. Our experiments on improving vision perception capabilities in autonomous driving dataset and scene recognition dataset demonstrate the efficacy of the proposed approach.",2025,0.613636140495949,0.6135308686077487,0.6,0.5,ef48d936-4a36-4ff0-9fb1-0eab8737f6ec,0,"[0.5, 0.5, 0.625, 0.625]","[0.95, 0.9, 1.0, 0.9]",Model Developmental Safety,0.568052504717259
550332,DiRaGNN: Attention-Enhanced Entity Ranking for Sparse Graph Networks,"Sparsity in both the structural and engagement information presents a core challenge in entity ranking problems for graph networks. The interaction dynamics of entities are often characterized by limited structural and engagement information which results in inferior performance of the state-of-the-art approaches. In this work, we present DiRaGNN, an attention-enhanced entity ranking model designed
to address the problem of dimension recommendation and ranking for automated watchdogs in the cloud setting. DiRaGNN is inspired by transformer architectures and utilizes a multi-head attention mechanism to focus on heterogeneous neighbors and their attributes. Additionally, our model employs multi-faceted loss functions to optimize for relevant recommendations and reduce popularity bias. To manage computational complexity, we sample a local subgraph that includes multiple hops of neighbors. Empirical evaluations demonstrate significant improvements over existing methods, with our model achieving a 39.7% increase in MRR.",2025,0.3409089669421938,0.3376236955200603,0.4,0.5,5000ea42-d39f-4c5a-a563-0377bb6f81ce,0,"[0.0, 0.25, 0.5, 0.5]","[0.95, 0.95, 0.9, 0.95]",DiRaGNN,0.3011806231742941
550337,Enhancing Logits Distillation with Plug&Play Kendall's $\tau$ Ranking Loss,"Knowledge distillation typically employs the Kullback-Leibler (KL) divergence to constrain the output of the student model to precisely match the soft labels provided by the teacher model. However, the optimization process of KL divergence is challenging for the student and prone to suboptimal points. Also, we demonstrate that the gradients provided by KL divergence depend on channel scale and thus tend to overlook low-probability channels. The mismatch in low-probability channels also results in the neglect of inter-class relationship information, making it difficult for the student to further enhance performance. To address this issue, we propose an auxiliary ranking loss based on Kendall’s $\tau$ Coefficient, which can be plug-and-play in any logit-based distillation method, providing inter-class relationship information and balancing the attention to low-probability channels. We show that the proposed ranking loss is less affected by channel scale, and its optimization objective is consistent with that of KL divergence. Extensive experiments on CIFAR-100, ImageNet, and COCO datasets, as well as various CNN and ViT teacher-student architecture combinations, demonstrate that the proposed ranking loss can be plug-and-play on various baselines and enhance their performance.",2025,0.6818179338843877,0.6882557052958999,0.6,0.5,91d2742d-f3be-419f-8320-6b058713e683,0,"[0.5, 0.5, 0.625, 0.875]","[0.9, 0.8, 0.95, 0.95]",Kendall's τ Ranking Loss,0.652785326404473
550344,Lightweight Neural App Control,"This paper introduces a novel mobile phone control architecture, Lightweight Multi-modal App Control (LiMAC), for efficient interactions and control across various Android apps. LiMAC  takes as input a textual goal and a sequence of past mobile observations, such as screenshots and corresponding UI trees, to generate precise actions. To address the computational constraints inherent to smartphones, we introduce a small Action Transformer (AcT) integrated with a fine-tuned vision-language model (VLM) for real-time decision-making and task execution.  We evaluate LiMAC on two open-source mobile control datasets, demonstrating the superior performance of our small-form-factor approach against fine-tuned versions of open-source VLMs, such as Florence2 and Qwen2-VL. It also significantly outperforms prompt engineering baselines utilising closed-source foundation models like GPT-4o. More specifically, LiMAC increases the overall action accuracy by up to 19% compared to fine-tuned VLMs, and up to 42% compared to prompt-engineering baselines.",2025,0.886363314049704,0.883061310284889,0.9333333333333332,0.875,020ffaca-4486-4234-aaec-8d5fb40f5088,1,"[0.625, 0.875, 0.875, 0.875]","[0.95, 0.9, 0.9, 0.95]",LiMAC,0.8044689118962354
550359,Beyond the Alphabet: Deep Signal Embedding for Enhanced DNA Clustering,"The emerging field of DNA storage employs strands of DNA bases (A/T/C/G) as a storage medium for digital information to enable massive density and durability. The DNA storage pipeline includes: (1) encoding the raw data into sequences of DNA bases; (2) synthesizing the sequences as DNA strands that are stored over time as an unordered set; (3) sequencing the DNA strands to generate DNA reads; and (4) deducing the original data. The DNA synthesis and sequencing stages each generate several independent error-prone duplicates of each strand which are then utilized in the final stage to reconstruct the best estimate for the original strand. Specifically, the reads are first clustered into groups likely originating from the same strand (based on their similarity to each other), and then each group approximates the strand that led to the reads of that group. This work improves the DNA clustering stage by embedding it as part of the DNA sequencing. Traditional DNA storage solutions begin after the DNA sequencing process generates discrete DNA reads (A/T/C/G), yet we identify that there is untapped potential in using the raw signals generated by the Nanopore DNA sequencing machine before they are discretized into bases, a process known as basecalling, which is done using a deep neural network. We propose a deep neural network that clusters these signals directly, demonstrating superior accuracy, and reduced computation times compared to current approaches that cluster after basecalling.",2025,0.5113634504132908,0.5100655425612787,0.4,0.25,bb76fcc8-67a4-4dfe-8d92-3320166715ba,0,"[0.25, 0.25, 0.5, 0.875]","[0.95, 0.95, 0.9, 0.95]",Signal Embedding,0.4668634371957157
550379,Can LLMs Generate Diverse Molecules? Towards Alignment with Structural Diversity,"Recent advancements in large language models (LLMs) have demonstrated impressive performance in generating molecular structures as drug candidates, which offers significant potential to accelerate drug discovery. However, the current LLMs overlook a critical requirement for drug discovery: proposing a diverse set of molecules. This diversity is essential for improving the chances of finding a viable drug, as it provides alternative molecules that may succeed where others fail in wet-lab or clinical validations. Despite such a need for diversity, the LLMs often output structurally similar molecules from a given prompt. While decoding schemes like beam search may enhance textual diversity, this often does not align with molecular structural diversity. In response, we propose a new method for fine-tuning molecular generative LLMs to autoregressively generate a set of structurally diverse molecules, where each molecule is generated by conditioning on the previously generated molecules. Our approach consists of two stages: (1) supervised fine-tuning to adapt LLMs to autoregressively generate molecules in a sequence and (2) reinforcement learning to maximize structural diversity within the generated molecules. Our experiments show that (1) our fine-tuning approach enables the LLMs to better discover diverse molecules compared to existing decoding schemes and (2) our fine-tuned model outperforms other representative LLMs in generating diverse molecules, including the ones fine-tuned on chemical domains.",2025,0.443181657024852,0.4470497134555022,0.2666666666666666,0.25,00118d9f-6b35-41d4-b3a5-3826847a44ef,0,"[0.25, 0.25, 0.25, 0.875]","[0.9, 0.9, 0.95, 0.95]",Structural Diversity,0.4263277202072537
550396,A biologically-plausible alternative to backpropagation using pseudoinverse feedback,"Despite its successes in both practical machine learning and neural modeling, the backpropagation algorithm has long been considered biologically implausible (Crick, 1989). Previous solutions to this biological implausibility have proposed the existence of a separate, error feedback network, in which error at the final layer may be propagated backwards to earlier layers in a manner similar to backpropagation. However, biological evidence suggests that feedback connections in the cortex may function more similarly to an autoencoder, rather than being exclusively used as error feedback (Marino, 2020; Chen et al., 2024). Here, we attempt to unify these two paradigms by showing how autoencoder-like, inverse feedback connections may be used to minimize error throughout a feedforward neural network. Our proposed mechanism, Reciprocal Feedback, consists of two contributions: first we show how a modification of the Recirculation algorithm (Hinton & McClelland, 1988) is capable of learning the Moore-Penrose pseudoinverse of a pair of network weights. Then, we will show how, using a Newton-like method (Hildebrandt & Graves, 1927), locally-learned pseudoinverse feedback connections may be used to facilitate an alternative optimization method to traditional gradient descent - while alleviating the need to compute the weight transpose, or use direct feedback connections from the final layer. In the MNIST and CIFAR-10 classification tasks, our method obtains an asymptotic error similar to backpropagation, in fewer iterations than comparable biologically-plausible algorithms, such as Feedback Alignment (Lillicrap et al., 2014).",2025,0.4545456528924899,0.4490188219764709,0.5333333333333333,0.5,65f6bc7f-a4f9-4319-bc83-9f6b9d3852d5,0,"[0.25, 0.5, 0.5]","[1.0, 0.9, 0.95]",Reciprocal Feedback,0.3921057641514369
550441,L-WISE: Boosting Human Visual Category Learning Through Model-Based Image Selection and Enhancement,"The currently leading artificial neural network models of the visual ventral stream - which are derived from a combination of performance optimization and robustification methods - have demonstrated a remarkable degree of behavioral alignment with humans on visual categorization tasks. We show that image perturbations generated by these models can enhance the ability of humans to accurately report the ground truth class. Furthermore, we find that the same models can also be used out-of-the-box to predict the proportion of correct human responses to individual images, providing a simple, human-aligned estimator of the relative difficulty of each image. Motivated by these observations, we propose to augment visual learning in humans in a way that improves human categorization accuracy at test time. Our learning augmentation approach consists of (i) selecting images based on their model-estimated recognition difficulty, and (ii) applying image perturbations that aid recognition for novice learners. We find that combining these model-based strategies leads to categorization accuracy gains of 33-72% relative to control subjects without these interventions, on unmodified, randomly selected held-out test images. Beyond the accuracy gain, the training time for the augmented learning group was also shortened by 20-23%, despite both groups completing the same number of training trials. We demonstrate the efficacy of our approach in a fine-grained categorization task with natural images, as well as two tasks in clinically relevant image domains - histology and dermoscopy - where visual learning is notoriously challenging. To the best of our knowledge, our work is the first application of artificial neural networks to increase visual learning performance in humans by enhancing category-specific image features.",2025,0.6818179338843877,0.6806929345162506,0.6666666666666666,0.625,3ffdb87e-4a0f-4166-83ee-fcf2df90fed3,1,"[0.625, 0.625, 0.625]","[0.95, 0.8, 0.9]",Model-Based Image Selection,0.625
550465,Unleashing the Potential of Temperature Scaling for Multi-Label Logit Distillation,"This paper undertakes meticulous scrutiny of the pure logit-based distillation under multi-label learning through the lens of activation function. We begin with empirically clarifying a recently discovered perspective that vanilla sigmoid per se is more suitable than tempered softmax in multi-label distillation, is not entirely correct. After that, we reveal that both the sigmoid and tempered softmax have an intrinsic limitation. In particular, we conclude that ignoring the decisive factor temperature $\tau$ in the sigmoid is the essential reason for its unsatisfactory results. With this regard, we propose unleashing the potential of temperature scaling in the multi-label distillation and present Tempered Logit Distillation (TLD), an embarrassingly simple yet astonishingly performant approach. Specifically, we modify the sigmoid with the temperature scaling mechanism, deriving a new activation function, dubbed as tempered sigmoid. With theoretical and visual analysis, intriguingly, we identify that tempered sigmoid with $\tau$ smaller than 1 provides an effect of hard mining by governing the magnitude of penalties according to the sample difficulty, which is shown as the key property to its success. Our work is accompanied by comprehensive experiments on COCO, PASCAL-VOC, and NUS-WIDE over several architectures across three multi-label learning scenarios: image classification, object detection, and instance segmentation. Distillation results evidence that TLD consistently harvests remarkable performance and surpasses the prior counterparts, demonstrating its superiority and versatility.",2025,0.6818179338843877,0.6806929345162506,0.6666666666666666,0.625,45b97928-b5e1-46c5-a0be-f6fbe6ee1576,0,"[0.625, 0.625, 0.625]","[0.8, 0.9, 0.95]",Tempered Logit Distillation,0.625
550479,Uni$^2$Det: Unified and Universal Framework for Prompt-Guided Multi-dataset 3D Detection,"We present Uni$^2$Det, a brand new framework for unified and universal multi-dataset training on 3D detection, enabling robust performance across diverse domains and generalization to unseen domains. Due to substantial disparities in data distribution and variations in taxonomy across diverse domains, training such a detector by simply merging datasets poses a significant challenge. Motivated by this observation, we introduce multi-stage prompting modules for multi-dataset 3D detection, which leverages prompts based on the characteristics of corresponding datasets to mitigate existing differences. This elegant design facilitates seamless plug-and-play integration within various advanced 3D detection frameworks in a unified manner, while also allowing straightforward adaptation for universal applicability across datasets. Experiments are conducted across multiple dataset consolidation scenarios involving KITTI, Waymo, and nuScenes, demonstrating that our Uni$^2$Det outperforms existing methods by a large margin in multi-dataset training. Notably, results on zero-shot cross-dataset transfer validate the generalization capability of our proposed method. Our code is available at https://github.com/ThomasWangY/Uni2Det.",2025,0.8181815206612653,0.8112367700521248,0.8,0.625,c3e4e655-58bb-4c49-b34c-01b3595fbfe1,1,"[0.625, 0.625, 0.875, 0.875]","[0.95, 0.95, 0.95, 0.8]",Uni$^2$Det,0.7321549966009517
550480,Knowledge-localized Unlearning for Faithful Forgetting in Language Models,"Large language models are exposed to privacy risks since they are trained on large text corpus, which may include sensitive or private information. Therefore, existing studies have attempted to unlearn undesirable knowledge exposed without permission from a language model. However, they are limited in that they have overlooked the complex and interconnected nature of knowledge, where related knowledge must be carefully examined. Specifically, they have failed to evaluate whether an unlearning method faithfully erases interconnected knowledge that should be removed, retaining knowledge that appears relevant but exists in a completely different context. To resolve this problem, we first define a new concept called superficial unlearning, which refers to the phenomenon where an unlearning method either fails to erase the interconnected knowledge it should remove or unintentionally erases irrelevant knowledge. Based on the definition, we introduce a new benchmark, FaithUnBench, to analyze and evaluate the faithfulness of unlearning in real-world knowledge QA settings. Furthermore, we propose a novel unlearning method, KLUE, which identifies and updates only knowledge-related neurons to achieve faithful unlearning. KLUE categorizes knowledge neurons using an explainability method and updates only those neurons using selected unforgotten samples.  Experimental results demonstrate that widely-used unlearning methods fail to ensure faithful unlearning, while our method shows significant effectiveness in real-world QA settings.",2025,0.5454543471075102,0.5427388034180588,0.6,0.625,942dfdf7-b0f2-41b0-9910-4e8c9ce1d71d,0,"[0.25, 0.5, 0.625, 0.625]","[0.95, 0.95, 0.9, 0.95]",KLUE,0.4924537487828628
550486,Cometh: A continuous-time discrete-state graph diffusion model,"Discrete-state denoising diffusion models led to state-of-the-art performance in graph generation, especially in the molecular domain. Recently, they have been transposed to continuous time, allowing more flexibility in the reverse process and a better trade-off between sampling efficiency and quality. Here, to leverage the benefits of both approaches, we propose Cometh, a continuous-time discrete-state graph diffusion model, tailored to the specificities of graph data. In addition, we also successfully replaced the set of structural encodings previously used in the discrete graph diffusion model with a single random-walk-based encoding, providing a simple and principled way to boost the model's expressive power. Empirically, we show that integrating continuous time leads to significant improvements across various metrics over state-of-the-art discrete-state diffusion models on a large set of molecular and non-molecular benchmark datasets. In terms of VUN samples, Cometh obtains a near-perfect performance of $99.5$% on the planar graph dataset and outperforms DiGress by $12.6$% on the large GuacaMol dataset.",2025,0.4090907603306326,0.4066481372973985,0.4,0.25,a5d259e3-38b9-42da-98f4-8827901f332f,0,"[0.25, 0.25, 0.5, 0.5]","[1.0, 0.95, 0.95, 0.95]",Cometh,0.3650589101620029
550494,MatExpert: Decomposing Materials Discovery By Mimicking Human Experts,"Material discovery is a critical research area with profound implications for various industries. In this work, we introduce MatExpert, a novel framework that leverages Large Language Models (LLMs) and contrastive learning to accelerate the discovery and design of new solid-state materials. Inspired by the workflow of human materials design experts, our approach integrates three key stages: retrieval, transition, and generation. First, in the retrieval stage, MatExpert identifies an existing material that closely matches the desired criteria. Second, in the transition stage, MatExpert outlines the necessary modifications to transform this material formulation to meet specific requirements outlined by the initial user query. Third, in the generation state, MatExpert performs detailed computations and structural generation to create a new material based on the provided information. Our experimental results demonstrate that MatExpert outperforms state-of-the-art methods in material generation tasks, achieving superior performance across various metrics including validity, distribution, and stability. As such, MatExpert represents a meaningful advancement in computational material discovery using langauge-based generative models.",2025,0.6818179338843877,0.6806929345162506,0.6666666666666666,0.625,3540b999-138b-465b-a3df-d8f1f07766eb,1,"[0.625, 0.625, 0.625, 0.625]","[0.9, 0.9, 0.95, 0.9]",MatExpert,0.6250000000000001
550495,FusionSAM: Visual Multimodal Learning with Segment Anything Model,"Multimodal image fusion and semantic segmentation are critical for autonomous driving.   Despite advancements, current models often struggle with segmenting densely packed elements due to a lack of comprehensive fusion features for guidance during training.   While the Segment Anything Model (SAM) allows precise control during fine-tuning through its flexible prompting encoder, its potential remains largely unexplored in the context of multimodal segmentation for natural images. In this paper, we introduce SAM into multimodal image segmentation for the first time, proposing a novel framework that combines Latent Space Token Generation (LSTG) and Fusion Mask Prompting (FMP) modules. This approach transforms the training methodology for multimodal segmentation from a traditional black-box approach to a controllable, prompt-based mechanism. Specifically, we obtain latent space features for both modalities through vector quantization and embed them into a cross-attention-based inter-domain fusion module to establish long-range dependencies between modalities. We then use these comprehensive fusion features as prompts to guide precise pixel-level segmentation. Extensive experiments on multiple public datasets demonstrate that our method significantly outperforms SAM and SAM2 in multimodal autonomous driving scenarios, achieving at least a 3.9$\%$ improvement in segmentation mIoU over state-of-the-art methods.",2025,0.4090907603306326,0.4066927907539028,0.4,0.25,c9035fa4-af38-45c1-ad45-d7d03b4554c6,0,"[0.25, 0.25, 0.5, 0.5]","[1.0, 1.0, 1.0, 0.95]",Fusion Mask Prompting,0.366423125794155
550497,Initializing the Layer-wise Learning Rate,"Weight initialization schemes have been devised with heavy emphasis in the initial training dynamics, assuming the optimizer automatically handles appropriate step sizes in prolonged training. The optimizer typically calculates the step sizes using a single, global learning rate across all parameters, focusing exclusively on the (exponentially averaged) in-training time gradient. Motivated from hierarchical structure inherent in deep networks, this work explores assigning non-adaptive layer-wise learning rates based on the differences in gradient magnitude at initialization as a practical and effective optimization strategy. The gradient magnitude used to preset the layer-wise learning rates is measured at fan-in initialization, as stable activation variance is considered a desirable property during training, and so is assumed to largely hold true in prolonged training. Experiments on convolutional and transformer architectures show the proposed layer-wise learning rate can improve training stability and convergence in image classification and autoregressive language modeling",2025,0.3636358677687754,0.3678126994430298,0.2666666666666666,0.25,b0f7f622-ef48-4f99-aa8c-a796feecc092,0,"[0.25, 0.25, 0.5]","[0.9, 0.95, 1.0]",Layer-wise Learning Rate,0.357894235848563
550524,Conformal Prediction for Dose-Response Models with Continuous Treatments,"Understanding the dose-response relation between a continuous treatment and the outcome for an individual can greatly drive decision-making, particularly in areas like personalized drug dosing and personalized healthcare interventions. Point estimates are often insufficient in these high-risk environments, highlighting the need for uncertainty quantification to support informed decisions. Conformal prediction, a distribution-free and model-agnostic method for uncertainty quantification, has seen limited application in continuous treatments or dose-response models. To address this gap, we propose a novel methodology that frames the causal dose-response problem as a covariate shift, leveraging weighted conformal prediction. By incorporating propensity estimation, conformal predictive systems, and likelihood ratios, we present a practical solution for generating prediction intervals for dose-response models. Additionally, our method approximates local coverage for every treatment value by applying kernel functions as weights in weighted conformal prediction. Finally, we use a new synthetic benchmark dataset to demonstrate the significance of covariate shift assumptions in achieving robust prediction intervals for dose-response models.",2025,0.6545452165290122,0.6582529389598141,0.5333333333333333,0.5,bdc38627-0810-496c-a518-30fbf53a5e12,0,"[0.25, 0.5, 0.5, 0.875, 0.875]","[0.95, 0.8, 0.9, 0.95, 0.95]",Weighted Conformal Prediction,0.6170257052544448
550529,HandsOnVLM: Vision-Language Models for Hand-Object Interaction Prediction,"How can we predict future interaction trajectories of human hands in a scene given high-level colloquial task specifications in the form of natural language? In this paper, we extend the classic hand trajectory prediction task to two tasks involving explicit or implicit language queries. Our proposed tasks require extensive understanding of human daily activities and reasoning abilities about what is happening next given cues from the current scene. We also develop new benchmarks to evaluate the proposed two tasks, Vanilla Hand Prediction (VHP) and Reasoning-Based Hand Prediction (RBHP). We enable solving these tasks by integrating high-level world knowledge and reasoning capabilities of Vision-Language Models (VLMs) with the auto-regressive nature of low-level ego-centric hand trajectories. Our
model, HandsOnVLM is a novel VLM that can generate textual responses and produce future hand trajectories through natural-language conversations. Our experiments show that HandsOnVLM outperforms existing task-specific methods and other VLM baselines on proposed tasks, and demonstrates its ability to effectively utilize world knowledge for reasoning about low-level human hand trajectories based on the provided context.",2025,0.727272826446245,0.7260728265202324,0.6666666666666666,0.5,6c1d19e0-8a33-4f2d-8980-0fd1fc02c7e2,0,"[0.5, 0.625, 0.875]","[0.95, 0.95, 0.95]",HandsOnVLM,0.6666666666666667
550531,DiverseAgentEntropy: Quantifying Black-Box LLM Uncertainty through Diverse Perspectives and Multi-Agent Interaction,"Quantifying the uncertainty in the factual parametric knowledge of Large Language Models (LLMs), especially in a black-box setting, poses a significant challenge. Existing methods, which gauge a model’s uncertainty through evaluating self-consistency in responses to the original query, do not always capture true uncertainty. Models might respond consistently to the origin query with a wrong answer, yet respond correctly to varied questions from different perspectives about the same query, and vice versa. In this paper, we propose a novel method, DiverseAgentEntropy, for evaluating a model's uncertainty using multi-agent interaction under the assumption that if a model is certain, it should consistently recall the answer to the original query across a diverse collection of questions about the same original query. We further implement an abstention policy to withhold responses when uncertainty is high. Our method offers a more accurate prediction of the model's reliability and further detects hallucinations, outperforming other self-consistency-based methods. Additionally, it demonstrates that existing models often fail to consistently retrieve the correct answer to the same query under diverse varied questions even when knowing the correct answer.",2025,0.4772725537190714,0.4764850541613754,0.4666666666666667,0.25,0a44d0c9-5058-418e-a6d5-d48f828313e3,0,"[0.25, 0.25, 0.625, 0.625]","[0.95, 0.95, 0.95, 0.95]",DiverseAgentEntropy,0.4374999999999999
550538,VN-EGNN: E(3)- and SE(3)-Equivariant Graph Neural Networks with Virtual Nodes Enhance Protein Binding Site Identification,"Being able to identify regions within or around proteins, to which ligands can potentially bind, is an essential step in developing new drugs. Binding site identification methods can now profit from the availability of large amounts of 3D structures in protein structure databases or from AlphaFold predictions. Current binding site identification methods heavily rely on graph neural networks (GNNs), usually designed to output E($3$)-equivariant predictions. Such methods turned out to be very beneficial for physics-related tasks like binding energy or motion trajectory prediction. However, the performance of GNNs at binding site identification is still limited potentially due to a lack of expressiveness capable of modeling higher-order geometric entities, such as binding pockets. In this work, we extend E($n$)-equivariant graph neural networks (EGNNs) by adding virtual nodes and applying an extended message passing scheme. The virtual nodes in these graphs are dedicated entities to learn representations of binding sites, which leads to improved predictive performance. In our experiments, we show that our proposed method, VN-EGNN, sets a new state-of-the-art at locating binding site centers on COACH420, HOLO4K and PDBbind2020.",2025,0.5727270644628857,0.564830284192023,0.5333333333333333,0.5,124956f1-0259-4fb3-992e-a54c8c9ca94f,0,"[0.25, 0.5, 0.5, 0.5, 0.875]","[1.0, 0.9, 0.95, 0.95, 0.9]",VN-EGNN,0.490429827220643
550558,Approximating Multiple Robust Optimization Solutions in One Pass via Proximal Point Methods,"Robust optimization provides a principled and unified framework to model many problems in modern operations research and computer science applications, such as risk measures minimization and adversarially robust machine learning. To use a robust solution (e.g., to implement an investment portfolio or perform robust machine learning inference), the user has to a priori decide the trade-off between efficiency (nominal performance) and robustness (worst-case performance) of the solution by choosing the uncertainty level hyperparameters.  In many applications, this amounts to solving the problem many times and comparing them, each from a different hyperparameter setting.  This makes robust optimization practically cumbersome or even intractable. We present a novel procedure based on the proximal point method (PPM) to approximate many Pareto-efficient robust solutions using the PPM trajectory.  Compared with the existing method with computation cost $N\times T_{\mathrm{RC}}$, the cost of our method is $T_{\mathrm{RC}} + (N-1)\times T_{\mathrm{\widetilde{PPM}}}$, where $N$ is the number of robust solutions to be generated,  $T_{\mathrm{RC}}$ is the cost of solving a single robust optimization problem, and $T_{\mathrm{\widetilde{PPM}}}$ is cost of a single step of an approximate PPM. We prove exact PPM can produce exact Pareto efficient robust solutions for a class of robust linear optimization problems. For robust optimization problems with nonlinear and differentiable objective functions, compared with the existing method, our method equipped with first-order approximate PPMs is computationally cheaper and generates robust solutions with comparable performance.",2025,0.5454543471075102,0.5445543476130005,0.5333333333333333,0.5,db8570c6-34c4-47a7-9cc0-10ecc333774b,0,"[0.5, 0.5, 0.5, 0.5]","[0.8, 0.8, 0.8, 0.95]",Proximal Point Methods,0.5
550563,Stateful Dynamics for Training of Binary Activation Recurrent Networks,"The excessive energy and memory consumption of neural networks has inspired a recent interest in quantized neural networks. 
Due to the discontinuity, training binary neural networks (BNNs) requires modifications or alternatives to standard backpropagation, typically in the form of surrogate gradient descent. Multiple surrogate methods exist for feedforward BNNs; however, their success has been limited when applied to recurrent BNNs, but successful when used in binary-like spiking neural networks (SNNs), which contain intrinsic temporal dynamics. We show that standard binary activation approaches fail to train when applied to layer with explicit recurrent weights, and present a theoretical argument for the necessity of temporal continuity in network behavior. By systematically incorporating mechanisms from SNN models, we find that integrative state enables recurrent binary activation networks to reach similar performance as floating-point approaches, while explicit reset and leakage terms do not affect performance. These results show how spiking units enable the training of binary recurrent neural networks and identify the minimally complex units required to make recurrent binary activations trainable with current surrogate methods.",2025,0.443181657024852,0.4424504074355629,0.2666666666666666,0.25,8ab4a591-68d1-47f6-abc8-9cd232b0e8db,0,"[0.25, 0.25, 0.25, 0.875]","[0.95, 0.8, 0.95, 0.9]",Stateful Dynamics,0.3974890490199287
550575,Efficient Machine Unlearning for Deep Generative Models by Mitigating Optimization Conflicts,"Machine unlearning of deep generative model refers to the process of modifying
or updating a pre-trained generative model to forget or remove certain patterns
or information it has learned. Existing research on Bayesian-based unlearning
from various deep generative models has highlighted low efficiency as a significant
drawback due to two primary causes. Firstly, Bayesian methods often overlook
correlations between data to forget and data to remember, leading to conflicts during
gradient descent and much slower convergence. Additionally, they require aligning
updated model parameters with the original ones to maintain the generation ability
of the updated model, further reducing efficiency. To address these limitations,
we propose an Efficient Bayesian-based Unlearning method for various deep
generative models called EBU. By identifying the relevant weights pertaining to
the data to forget and the data to remember, EBU only preserves the parameters
related to data to remember, improving the efficiency. Additionally, EBU balances
the gradient descent directions of shared parameters to adeptly manage the conflicts
caused by the correlations between data to forget and data to remember, leading to
a more efficient unlearning process. Extensive experiments on multiple generative
models demonstrate the superiority of our proposed EBU.",2025,0.3409089669421938,0.3319548847614089,0.2666666666666666,0.25,2b847230-ff65-49b8-86b6-8014e2b3a6ba,0,"[0.25, 0.25, 0.25, 0.5]","[0.95, 0.95, 0.95, 0.8]",Efficient Bayesian-based Unlearning,0.2857324949014276
550604,Scale-Invariant Continuous Implicit Neural Representations For Object Counting,"Many object counting methods rely on density map estimation (DME) using convolutional neural networks (CNNs) on discrete grid image representations. However, these methods struggle with large variations in object size or input image resolution, typically due to different imaging conditions and perspective effects. Worse yet, discrete grid representations of density maps result in information loss with blurred or vanished details for low-resolution inputs.
To overcome these limitations, we design Scale-Invariant Implicit neural representations for counting (SI-INR) to map arbitrary-scale input signals into a continuous function space, where each function produces density values over continuous spatial coordinates. SI-INR achieves robust counting performances with respect to changing object sizes, extensive experiments on commonly used diverse datasets have validated the proposed method.",2025,0.5454543471075102,0.5428085063745531,0.6,0.625,6e77c126-91c8-4dcb-a59c-6aa22ac4326a,0,"[0.25, 0.5, 0.625, 0.625]","[1.0, 0.95, 0.95, 1.0]",Scale-Invariant Implicit Neural Representations,0.4907912687585266
550613,DeepLTL: Learning to Efficiently Satisfy Complex LTL Specifications for Multi-Task RL,"Linear temporal logic (LTL) has recently been adopted as a powerful formalism for specifying complex, temporally extended tasks in multi-task reinforcement learning (RL). However, learning policies that efficiently satisfy arbitrary specifications not observed during training remains a challenging problem. Existing approaches suffer from several shortcomings: they are often only applicable to finite-horizon fragments of LTL, are restricted to suboptimal solutions, and do not adequately handle safety constraints. In this work, we propose a novel learning approach to address these concerns. Our method leverages the structure of Büchi automata, which explicitly represent the semantics of LTL specifications, to learn policies conditioned on sequences of truth assignments that lead to satisfying the desired formulae. Experiments in a variety of discrete and continuous domains demonstrate that our approach is able to zero-shot satisfy a wide range of finite- and infinite-horizon specifications, and outperforms existing methods in terms of both satisfaction probability and efficiency. Code available at: https://deep-ltl.github.io/",2025,0.9545451074381428,0.9529701083227508,0.9333333333333332,0.875,fb583b3e-4b78-4cea-a18f-34afc65f5296,1,"[0.875, 0.875, 0.875, 0.875]","[1.0, 1.0, 0.95, 0.95]",DeepLTL,0.8749998102934814
550614,WILTing Trees: Interpreting the Distance Between MPNN Embeddings,"We investigate the distance function implicitly learned by message passing neural networks (MPNNs) on specific tasks. 
Our goal is to capture the functional distance that is implicitly learned by an MPNN for a given task. 
This contrasts previous work which relates MPNN distances on arbitrary tasks to structural distances that ignore the task at hand.
To this end, we distill the distance between MPNN embeddings into an interpretable graph distance.
Our distance is an optimal transport on the Weisfeiler Leman Labeling Tree (WILT), whose edge weights reveal subgraphs that strongly influence the distance between MPNN embeddings.
Moreover, it generalizes the metrics of two well-known graph kernels and is computable in linear time.
Through extensive experiments, we show that MPNNs define the relative position of embeddings by focusing on a small number of subgraphs known by domain experts to be functionally important.",2025,0.4909089123967591,0.4832168350065673,0.5333333333333333,0.0,2b696481-a0d1-4c71-9151-448ae48a6101,0,"[0.0, 0.25, 0.5, 0.625, 0.875]","[1.0, 0.9, 0.8, 0.9, 0.95]",WILT,0.4101834709978896
550627,AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out Context Attribution,"The influence of contextual input on the behavior of large language models (LLMs) has prompted the development of context attribution methods that aim to quantify each context span's effect on an LLM's generations. The leave-one-out (LOO) error, which measures the change in the likelihood of the LLM's response when a given span of the context is removed, provides a principled way to perform context attribution, but can be prohibitively expensive to compute for large models. In this work, we introduce AttriBoT, a series of novel techniques for efficiently computing an approximation of the LOO error for context attribution. Specifically, AttriBoT uses cached activations to avoid redundant operations, performs hierarchical attribution to reduce computation, and emulates the behavior of large target models with smaller proxy models. Taken together, AttriBoT can provide a 300x speedup while remaining more faithful to a target model's LOO error than prior context attribution methods. This stark increase in performance makes computing context attributions for a given response $30\times$ faster than generating the response itself, empowering real-world applications that require computing attributions at scale. We release a user-friendly and efficient implementation of AttriBoT to enable efficient LLM interpretability as well as encourage future development of efficient context attribution methods.",2025,0.7207786469895829,0.7211086690873922,0.6666666666666666,0.625,7594ef13-8b44-42db-a548-9516216ca77a,1,"[0.625, 0.625, 0.625, 0.625, 0.625, 0.625, 0.875]","[0.95, 0.95, 0.9, 0.8, 0.95, 0.9, 0.95]",AttriBoT,0.6663576306373723
550628,SIMPL: Scalable and hassle-free optimisation of neural representations from behaviour,"Neural activity in the brain is known to encode low-dimensional, time-evolving, behaviour-related variables. A long-standing goal of neural data analysis has been to identify these variables and their mapping to neural activity. A productive and canonical approach has been to simply visualise neural ""tuning curves"" as a function of behaviour. However, significant discrepancies between behaviour and the true latent variables -- such as an agent thinking of position Y whilst located at position X -- distort and blur the tuning curves, decreasing their interpretability. To address this, latent variable models propose to learn the latent variable from data; these are typically expensive, hard to tune, or scale poorly, complicating their adoption. Here we propose SIMPL (Scalable Iterative Maximization of Population-coded Latents), an EM-style algorithm which iteratively optimises latent variables and tuning curves. SIMPL is fast, scalable and exploits behaviour as an initial condition to further improve convergence and identifiability. It can accurately recover latent variables in spatial and non-spatial tasks. When applied to a large hippocampal dataset SIMPL converges on smaller, more numerous, and more uniformly sized place fields than those based on behaviour, suggesting the brain may encode space with greater resolution than previously thought.",2025,0.7159088305786071,0.7119677798083603,0.6666666666666666,0.625,ea2954c8-e961-475d-b377-e6a27ad31f10,1,"[0.5, 0.625, 0.625, 0.875]","[0.95, 0.9, 0.95, 0.9]",SIMPL,0.6442033678756475
550640,ChinaTravel: A Real-World Benchmark for Language Agents in Chinese Travel Planning,"Recent advances in Large Language Models (LLMs), particularly in language reasoning and tool-use capabilities have sparked the rapid development of \emph{Language Agents} to assist humans across various real-world applications. Among these, travel planning stands out as a significant domain, presenting both academic challenges and practical value due to its inherent complexity and real-world relevance. However, existing travel plan benchmarks do not test language agents with human users or their ability to follow customized requirements, both of which are vital for deploying them in real-world applications. In this paper, we propose ChinaTravel, a new benchmark tailored to authentic Chinese travel requirements, aiming to provide a more realistic evaluation framework for future language agents. We collect the travel requirements through questionnaires and employ an efficient and faithful evaluation process with 46 metrics covering feasibility, constraint satisfaction, and preference comparison. Moreover, we identify three challenges in the real-world deployments of travel planning, including \emph{constraint recognition}, \emph{concept openness}, and \emph{customized preference}. The empirical studies show that even state-of-the-art neural-symbolic agents succeed in 51.3\% constraint validation of human queries. Our findings point to the need for methods that can improve the ability of agents to understand diverse intentions or keep track of constraints with emerging concepts from human requirements.",2025,0.5454543471075102,0.5445543476130005,0.5333333333333333,0.5,be45365b-98c7-4dfe-89a1-f840babc446c,0,"[0.5, 0.5, 0.5, 0.5]","[0.9, 0.95, 0.95, 0.95]",ChinaTravel,0.5
550645,Distilling the Knowledge in Data Pruning,"With the increasing size of datasets used for training neural networks, data pruning has gained traction in recent years.
However, most current data pruning algorithms are limited in their ability to preserve accuracy compared to models trained on the full data, especially in high pruning regimes. 
In this paper we explore the application of data pruning while incorporating knowledge distillation (KD) when training on a pruned subset. 
That is, rather than relying solely on ground-truth labels, we also use the soft predictions from a teacher network pre-trained on the complete data.
By integrating KD into training, we demonstrate significant improvement across datasets, pruning methods, and on all pruning fractions. 
We first establish a theoretical motivation for employing self-distillation to improve training on pruned data.
Then, we empirically make a compelling and highly practical observation: using KD, simple random pruning is comparable or superior to sophisticated pruning methods across all pruning regimes.
On ImageNet for example, we achieve superior accuracy despite training on a random subset of only 50\% of the data. 
Additionally, we demonstrate a crucial connection between the pruning factor and the optimal knowledge distillation weight. This helps mitigate the impact of samples with noisy labels and low-quality images retained by typical pruning algorithms.
Finally, we make an intriguing observation: when using lower pruning fractions, larger teachers lead to accuracy degradation, while surprisingly, employing teachers with a smaller capacity than the student's may improve results.
Our code will be made available.",2025,0.5909092396693675,0.5883136458784859,0.5333333333333333,0.5,45b97928-b5e1-46c5-a0be-f6fbe6ee1576,0,"[0.5, 0.5, 0.625]","[0.95, 0.95, 0.9]",Knowledge Distillation,0.5348211328254389
550667,Multi-Atlas Brain Network Classification through Consistency Distillation and Complementary Information Fusion,"In the realm of neuroscience, identifying distinctive patterns associated with neurological disorders via brain networks is crucial. Resting-state functional magnetic resonance imaging (fMRI) serves as a primary tool for mapping these networks by correlating blood-oxygen-level-dependent (BOLD) signals across different brain regions, defined as regions of interest (ROIs). Constructing these brain networks involves using atlases to parcellate the brain into ROIs based on various hypotheses of brain division. However, there is no standard atlas for brain network classification, leading to limitations in detecting abnormalities in disorders. Some recent methods have proposed utilizing multiple atlases, but they neglect consistency across atlases and lack ROI-level information exchange. To tackle these limitations, we propose an Atlas-Integrated Distillation and Fusion network (AIDFusion) to improve brain network classification using fMRI data. AIDFusion addresses the challenge of utilizing multiple atlases by employing a disentangle Transformer to filter out inconsistent atlas-specific information and distill distinguishable connections across atlases. It also incorporates subject- and population-level consistency constraints to enhance cross-atlas consistency. Additionally, AIDFusion employs an inter-atlas message-passing mechanism to fuse complementary information across brain regions. Experimental results on four datasets of different diseases demonstrate the effectiveness and efficiency of AIDFusion compared to state-of-the-art methods. A case study illustrates AIDFusion extract patterns that are both interpretable and consistent with established neuroscience findings.",2025,0.6818179338843877,0.6788773903213089,0.6,0.5,7d573cbb-555c-4244-9166-e53aa8db689a,0,"[0.5, 0.5, 0.625, 0.875]","[0.9, 0.95, 1.0, 0.9]",AIDFusion,0.6176823943885389
550681,Align Your Intents: Offline Imitation Learning via Optimal Transport,"Offline reinforcement learning (RL) addresses the problem of sequential decision-making by learning optimal policy through pre-collected data, without interacting with the environment. As yet, it has remained somewhat impractical, because one rarely knows the reward explicitly and it is hard to distill it retrospectively. Here, we show that an imitating agent can still learn the desired behavior merely from observing the expert, despite the absence of explicit rewards or action labels. In our method, AILOT (Aligned Imitation Learning via Optimal Transport), we involve special representation of states in a form of intents that incorporate pairwise spatial distances within the data. Given such representations, we define intrinsic reward function via optimal transport distance between the expert's and the agent's trajectories. We report that AILOT outperforms state-of-the art offline imitation learning algorithms on D4RL benchmarks and improves the performance of other offline RL algorithms by dense reward relabelling in the sparse-reward tasks.",2025,0.5454543471075102,0.540875338440527,0.6,0.625,c31aa0fd-f171-4009-8cdd-237c718de957,0,"[0.25, 0.5, 0.625, 0.625]","[0.95, 0.95, 0.9, 0.9]",AILOT,0.4839378238341968
550685,Meta Flow Matching: Integrating Vector Fields on the Wasserstein Manifold,"Numerous biological and physical processes can be modeled as systems of interacting entities evolving continuously over time, e.g. the dynamics of communicating cells or physical particles. Learning the dynamics of such systems is essential for predicting the temporal evolution of populations across novel samples and unseen environments. Flow-based models allow for learning these dynamics at the population level - they model the evolution of the entire distribution of samples. However, current flow-based models are limited to a single initial population and a set of predefined conditions which describe different dynamics. We argue that multiple processes in natural sciences have to be represented as vector fields on the Wasserstein manifold of probability densities. That is, the change of the population at any moment in time depends on the population itself due to the interactions between samples. In particular, this is crucial for personalized medicine where the development of diseases and their respective treatment response depend on the microenvironment of cells specific to each patient. We propose *Meta Flow Matching* (MFM), a practical approach to integrate along these vector fields on the Wasserstein manifold by amortizing the flow model over the initial populations. Namely, we embed the population of samples using a Graph Neural Network (GNN) and use these embeddings to train a Flow Matching model. This gives MFM the ability to generalize over the initial distributions, unlike previously proposed methods. We demonstrate the ability of MFM to improve the prediction of individual treatment responses on a large-scale multi-patient single-cell drug screen dataset.",2025,0.7159088305786071,0.7222903520217124,0.6666666666666666,0.625,63a1d6cc-f30b-41e1-88a3-dd5468c18916,1,"[0.5, 0.625, 0.625, 0.875]","[0.8, 0.95, 0.9, 0.95]",Meta Flow Matching,0.6822831362084589
550705,IRIS: LLM-Assisted Static Analysis for Detecting Security Vulnerabilities,"Software is prone to security vulnerabilities. Program analysis tools to detect them have limited effectiveness in practice due to their reliance on human labeled specifications. Large language models (or LLMs) have shown impressive code generation capabilities but they cannot do complex reasoning over code to detect such vulnerabilities especially since this task requires whole-repository analysis. We propose IRIS, a neuro-symbolic approach that systematically combines LLMs with static analysis to perform whole-repository reasoning for security vulnerability detection. Specifically, IRIS leverages LLMs to infer taint specifications and perform contextual analysis, alleviating needs for human specifications and inspection. For evaluation, we curate a new dataset, CWE-Bench-Java, comprising 120 manually validated security vulnerabilities in real-world Java projects. A state-of-the-art static analysis tool CodeQL detects only 27 of these vulnerabilities whereas IRIS with GPT-4 detects 55 (+28) and improves upon CodeQL's average false discovery rate by 5% points.
Furthermore, IRIS identifies 4 previously unknown vulnerabilities which cannot be found by existing tools. IRIS is available publicly at https://github.com/iris-sast/iris.",2025,0.6477270371901683,0.6466582877904381,0.6666666666666666,0.625,da6874cc-d1ae-4706-937d-e17127b042c3,1,"[0.25, 0.625, 0.625, 0.875]","[0.95, 0.95, 0.95, 0.95]",IRIS,0.5937499999999999
550711,Certified PEFTSmoothing: Parameter-Efficient Fine-Tuning with Randomized Smoothing,"Randomized smoothing is the primary certified robustness method for accessing the robustness of deep learning models to adversarial perturbations in the $l_2$-norm, by taking a majority vote over the multiple predictions of a random Gaussian perturbed input of the base classifier. To fulfill the certified bound and empirical accuracy of randomized smoothing, the base model either needs to be retrained from scratch to learn Gaussian noise or adds an auxiliary denoiser to eliminate it. In this work, we propose \textit{PEFTSmoothing}, which teach the base model to learn the Gaussian noise-augmented data with Parameter-Efficient Fine-Tuning (PEFT) methods in both white-box and black-box settings. This design is based on the intuition that large-scale models have the potential to learn diverse data patterns, including the noise data distributions. In addition, we explore the possibility of combining \textit{PEFTSmoothing} with the fine-tuning for downstream task adaptation, which allows us to simultaneously obtain a robust version of the large vision model and its adaptation tailored to downstream datasets. Extensive results demonstrate the effectiveness and efficiency of \textit{PEFTSmoothing}, which allow us to certify over 98\% accuracy for ViT on CIFAR-10, 20\% higher than SoTA denoised smoothing, and over 61\% accuracy on ImageNet which is 30\% higher than CNN-based denoiser and comparable to the Diffusion-based denoiser.",2025,0.3749998636364132,0.3729826984192676,0.2666666666666666,0.25,2803dc59-18c4-423f-ae86-23a4cd4fa2fa,0,"[0.25, 0.25, 0.25, 0.625]","[0.9, 0.9, 0.95, 0.9]",PEFTSmoothing,0.337313122923588
550713,An Optimal Discriminator Weighted Imitation Perspective for Reinforcement Learning,"We introduce Iterative Dual Reinforcement Learning (IDRL), a new method that takes an optimal discriminator-weighted imitation view of solving RL. Our method is motivated by a simple experiment in which we find training a discriminator using the offline dataset plus an additional expert dataset and then performing discriminator-weighted behavior cloning gives strong results on various types of datasets. That optimal discriminator weight is quite similar to the learned visitation distribution ratio in Dual-RL, however, we find that current Dual-RL methods do not correctly estimate that ratio. In IDRL, we propose a correction method to iteratively approach the optimal visitation distribution ratio in the offline dataset given no addtional expert dataset. During each iteration, IDRL removes zero-weight suboptimal transitions using the learned ratio from the previous iteration and runs Dual-RL on the remaining subdataset. This can be seen as replacing the behavior visitation distribution with the optimized visitation distribution from the previous iteration, which theoretically gives a curriculum of improved visitation distribution ratios that are closer to the optimal discriminator weight. We verify the effectiveness of IDRL on various kinds of offline datasets, including D4RL datasets and more realistic corrupted demonstrations. IDRL beats strong Primal-RL and Dual-RL baselines in terms of both performance and stability, on all datasets.",2025,0.6477270371901683,0.6500138316804294,0.6666666666666666,0.625,c31aa0fd-f171-4009-8cdd-237c718de957,1,"[0.5, 0.625, 0.625, 0.625]","[0.8, 0.9, 0.9, 0.95]",Iterative Dual Reinforcement Learning,0.6044612165629093
550720,3D-Properties: Identifying Challenges in DPO and Charting a Path Forward,"Aligning large language models (LLMs) with human preferences has gained significant attention, with Proximal Policy Optimization (PPO) as a standard yet computationally expensive method and Direct Preference Optimization (DPO) as a more efficient alternative. While DPO offers simplicity, it remains underutilized in state-of-the-art LLMs, suggesting potential limitations. In this work, we revisit DPO, analyzing its theoretical foundations and empirical performance to bridge this gap. We identify three key properties—termed \textbf{3D}-properties—that emerge from DPO’s learning process: \textbf{D}rastic drop in rejected response likelihood, \textbf{D}egradation into response suppression, and \textbf{D}ispersion effect on unseen responses. We show that these issues arise from DPO’s optimization dynamics, where the interaction between chosen and rejected response gradients leads to instability. Our findings are supported by experiments on both a controlled toy model and real-world LLM tasks, including mathematical problem-solving and instruction following. To address these challenges, we propose simple regularization techniques that improve training stability and performance. Additionally, we examine how preference data distribution impacts DPO’s effectiveness, offering insights into how alignment models handle out-of-domain (OOD) data. Our work connects these observations to broader research and provides a theoretical explanation for DPO’s limitations. We hope these insights will guide future advancements in reward-model-free preference learning, bringing it closer to reward-model-based approaches.",2025,0.7159088305786071,0.7240993615644828,0.6666666666666666,0.625,19c7e4bf-1e28-4594-a9ce-77b73cc38f6e,1,"[0.5, 0.625, 0.625, 0.875]","[0.7, 0.9, 0.95, 0.9]",3D-properties,0.676569308179923
550727,Mask-Guided Video Generation: Enhancing Motion Control and Quality with Limited Data,"Recent advancements in diffusion models have brought new vitality into visual content creation. However, current text-to-video generation models still face challenges such as high training costs, substantial data requirements, and difficulties in maintaining consistency between given text and motion of the foreground object. To address these challenges, we propose mask-guided video generation, which requires only a small amount of data and is trained on a single GPU. Furthermore, to mitigate the impact of background interference on controllable text-to-video generation, we utilize mask  sequences obtained through drawing or extraction, along with the first-frame content, to guide video generation. Specifically, our model introduces foreground masks into existing architectures to learn region-specific attention, precisely matching text features and the motion of the foreground object.  Subsequently, video generation is guided by the mask sequences to prevent the sudden disappearance of foreground objects. Our model also incorporates a first-frame sharing strategy during inference, leading to better stability in the video generation. Additionally, our approach allows for incrementally  generation of longer video sequences. By employing this method, our model achieves efficient resource utilization and ensures controllability and consistency in video generation using mask sequences. Extensive qualitative and quantitative experiments demonstrate that this approach excels in various video generation tasks, such as video editing and generating artistic videos, outperforming previous methods in terms of consistency and quality.",2025,0.2727271735537551,0.2722771738065002,0.2666666666666666,0.25,a00ceaa8-acea-47b7-9868-83bac9fc8014,0,"[0.25, 0.25, 0.25, 0.25]","[1.0, 0.95, 1.0, 1.0]",Mask-Guided Video Generation,0.25
550736,GIFT-Eval: A Benchmark for General Time Series Forecasting Model Evaluation,"Time series foundation models excel in zero-shot forecasting, handling diverse tasks without explicit training. However, the advancement of these models has been hindered by the lack of comprehensive benchmarks. To address this gap, we introduce the **G**eneral T**I**me Series **F**orecas**T**ing Model **Eval**uation, **GIFT-EVAL**, a pioneering benchmark aimed at promoting evaluation across diverse datasets. GIFT-EVAL encompasses 28 datasets over 144,000 time series and 177 million data points, spanning seven domains, 10 frequencies, multivariate inputs, and prediction lengths ranging from short to long-term forecasts. To facilitate the effective pretraining and evaluation of foundation models, we also provide a non-leaking pretraining dataset containing approximately 230 billion data points. Additionally, we provide a comprehensive analysis of 20 baselines, which includes statistical models, deep learning models, and foundation models. We discuss each model in the context of various benchmark characteristics and offer a qualitative analysis that spans both deep learning and foundation models. We believe the insights from this analysis, along with access to this new standard zero-shot time series forecasting benchmark, will guide future developments in time series foundation models.",2025,0.5795452438017296,0.5794613704036891,0.5333333333333333,0.5,61be19a7-6abf-4a3a-83d1-88aeddc6e161,0,"[0.5, 0.5, 0.5, 0.625]","[0.95, 1.0, 0.95, 1.0]",GIFT-EVAL,0.5358543656207366
550767,Edge Prompt Tuning for Graph Neural Networks,"Pre-training powerful Graph Neural Networks (GNNs) with unlabeled graph data in a self-supervised manner has emerged as a prominent technique in recent years. However, inevitable objective gaps often exist between pre-training and downstream tasks. To bridge this gap, graph prompt tuning techniques design and learn graph prompts by manipulating input graphs or reframing downstream tasks as pre-training tasks without fine-tuning the pre-trained GNN models. While recent graph prompt tuning methods have proven effective in adapting pre-trained GNN models for downstream tasks, they overlook the crucial role of edges in graph prompt design, which can significantly affect the quality of graph representations for downstream tasks.
In this study, we propose EdgePrompt, a simple yet effective graph prompt tuning method from the perspective of edges. Unlike previous studies that design prompt vectors on node features, EdgePrompt manipulates input graphs by learning additional prompt vectors for edges and incorporates the edge prompts through message passing in the pre-trained GNN models to better embed graph structural information for downstream tasks. 
Our method is compatible with prevalent GNN architectures pre-trained under various pre-training strategies and is universal for different downstream tasks.
We provide comprehensive theoretical analyses of our method regarding its capability of handling node classification and graph classification as downstream tasks.
Extensive experiments on ten graph datasets under four pre-training strategies demonstrate the superiority of our proposed method against six baselines. Our code is available at https://github.com/xbfu/EdgePrompt.",2025,0.613636140495949,0.6135308686077487,0.6,0.5,d8678682-fe59-4dc6-b571-ee33d1d7e98d,1,"[0.5, 0.5, 0.625, 0.625]","[0.9, 0.95, 0.95, 0.95]",EdgePrompt,0.5662731256085687
550789,Multi-Granularity Semantic Revision for Large Language Model Distillation,"Knowledge distillation plays a key role in compressing the Large Language Models (LLMs), which boosts a small-size student model under large teacher models' guidance. However, existing LLM distillation methods overly rely on student-generated outputs, which may introduce generation errors and misguide the distillation process.  Moreover, the distillation loss functions introduced in previous works struggle to align the most informative part due to the complex distribution of LLMs' outputs. To address these problems, we propose a multi-granularity semantic revision method for LLM distillation.  At the sequence level, we propose a sequence correction and re-generation (SCRG) strategy. SCRG first calculates the semantic cognitive difference between the teacher and student to detect the error token, then corrects it with the teacher-generated one, and re-generates the sequence to reduce generation errors and enhance generation diversity. At the token level, we design a distribution adaptive clipping Kullback-Leibler (DAC-KL) loss as the distillation objective function. DAC-KL loss exploits a learnable sub-network to adaptively extract semantically dense areas from the teacher's output, avoiding the interference of redundant information in the distillation process. Finally, at the span level, we leverage the span priors of a sequence to compute the probability correlations within spans, and constrain the teacher and student's probability correlations to be consistent, further enhancing the transfer of semantic information. Extensive experiments across different model families with parameters ranging from 0.1B to 13B demonstrate the superiority of our method compared to existing methods.",2025,0.6818179338843877,0.6844231317973997,0.6,0.5,90d1f82f-cfdb-4a18-9895-ea6f5f26aef1,0,"[0.5, 0.5, 0.625, 0.875]","[0.9, 0.9, 0.9, 0.95]",Multi-Granularity Semantic Revision,0.6421650055370983
550817,In-batch Ensemble Drafting: Toward Fast and Robust Speculative Decoding for Multimodal Language Models,"Multimodal Large Language Models (MLLMs) have emerged as powerful tools for processing modalities beyond text by combining a visual encoder with Large Language Models (LLMs) to incorporate visual context. This integration, however, leads to higher computational costs during LLM inference, specifically in the Prefill and Decoding stages. Existing MLLM acceleration methods primarily focus on reducing the cost of long prefills caused by visual context, but this approach has limitations: (1) From a latency perspective, it mainly benefits the prefill stage, offering minimal improvements for decoding. (2) It does not guarantee output distributions that are identical to those of the original MLLM. To ensure identical output distribution while mitigating decoding latency, we focus on speculative decoding (SD)—an acceleration technique that uses a smaller draft model verified by a larger model. Despite its importance for LLM acceleration, SD's application to MLLMs remains largely unexplored, even though decoding constitutes a significant portion of MLLM inference latency. We investigate various drafting techniques—multimodal, text-only, image-pooling, and caption-based—for multimodal scenarios and analyze their integration with MLLMs. Building on these insights, we propose In-batch Ensemble Drafting, which combines probability distributions from multiple drafting methods via batch inference during the SD draft phase. This approach requires no additional model parameters, incurs minimal overhead, and significantly increases the likelihood of draft tokens passing verification, thereby enhancing performance and robustness across diverse input scenarios.",2025,0.3409089669421938,0.3376236955200603,0.2666666666666666,0.25,887f4b94-f5dd-4a9d-9534-adf47b80bf87,0,"[0.25, 0.25, 0.25, 0.5]","[0.95, 0.95, 0.95, 0.9]",In-batch Ensemble Drafting,0.3011806231742941
550838,IndianRoad: A Video Dataset of Diverse Atomic Visual Elements in Dense and Unpredictable Environments,"Most existing traffic video datasets including Waymo are structured, focusing predominantly on Western traffic, which hinders global applicability. Specifically, most Asian scenarios are far more complex, involving numerous objects with distinct motions and behaviors. Addressing this gap, we present a new dataset, IndianRoad, designed for evaluating perception methods with high representation of Vulnerable Road Users (VRUs: e.g. pedestrians, animals, motorbikes, and bicycles) in complex and unpredictable environments. IndianRoad is a manually annotated dataset encompassing 16 diverse actor categories (spanning animals, humans, vehicles, etc.) and 16 action types (complex and rare cases like cut-ins, zigzag movement, U-turn, etc.), which require high reasoning ability. IndianRoad densely annotates over 13 million bounding boxes (bboxes) actors with identification, and more than 1.6 million boxes are annotated with both actor identification and action/behavior details. The videos within IndianRoad are collected based on a broad spectrum of factors, such as weather conditions, the time of day, road scenarios, and traffic density. IndianRoad can benchmark video tasks like Tracking, Detection, Spatiotemporal Action Localization, Language-Visual Moment retrieval, and Multi-label Video Action Recognition. Given the critical importance of accurately identifying VRUs to prevent accidents and ensure road safety, in IndianRoad, vulnerable road users constitute 41.13% of instances, compared to 23.71% in Waymo. IndianRoad provides an invaluable resource for the development of more sensitive and accurate visual perception algorithms in the complex real world.
  Our experiments show that existing methods suffer degradation in performance when evaluated on IndianRoad, highlighting its benefit for future video recognition research.",2025,0.4090907603306326,0.4066481372973985,0.4,0.25,7e741fdd-8e3d-40d8-9f31-c7ac52c555ee,0,"[0.25, 0.25, 0.5, 0.5]","[0.95, 1.0, 0.95, 0.95]",IndianRoad,0.3650589101620029
550853,SciLitLLM: How to Adapt LLMs for Scientific Literature Understanding,"Scientific literature understanding is crucial for extracting targeted information and garnering insights, thereby significantly advancing scientific discovery.
Despite the remarkable success of Large Language Models (LLMs), they face challenges in scientific literature understanding, primarily due to (1) a lack of scientific knowledge and (2) unfamiliarity with specialized scientific tasks.
To develop an LLM specialized in scientific literature understanding, we propose a hybrid strategy that integrates continual pre-training (CPT) and supervised fine-tuning (SFT), to simultaneously infuse scientific domain knowledge and enhance instruction-following capabilities for domain-specific tasks.
In this process, we identify two key challenges: (1) constructing high-quality CPT corpora, and (2) generating diverse SFT instructions. 
We address these challenges through a meticulous pipeline, including PDF text extraction, parsing content error correction, quality filtering, and synthetic instruction creation.
Applying this strategy, we present a suite of LLMs: SciLitLLM, specialized in scientific literature understanding.
These models demonstrate promising performance on scientific literature understanding benchmarks.
(1) We present an effective framework that integrates CPT and SFT to adapt LLMs to scientific literature understanding, which can also be easily adapted to other domains.
(2) We propose an LLM-based synthesis method to generate diverse and high-quality scientific instructions, resulting in a new instruction set -- SciLitIns -- for less-represented scientific domains. 
(3) SciLitLLM achieves promising performance in scientific literature understanding benchmarks.",2025,0.7159088305786071,0.7115506511780889,0.6666666666666666,0.625,ceca425f-7e1b-4e75-8c67-8a032ecc8dd2,1,"[0.5, 0.625, 0.625, 0.875]","[0.95, 0.95, 0.95, 0.9]",SciLitLLM,0.6430440603700096
550854,Certified Training with Branch-and-Bound: A Case Study on Lyapunov-stable Neural Control,"We study the problem of learning Lyapunov-stable neural controllers which provably satisfy the Lyapunov asymptotic stability condition within a region-of-attraction.  Compared to previous works which commonly used counterexample guided training on this task, we develop a new and generally formulated certified training framework named CT-BaB, and we optimize for differentiable verified bounds, to produce verification-friendly models. In order to handle the relatively large region-of-interest, we propose a novel framework of training-time branch-and-bound to dynamically maintain a training dataset of subregions throughout training, such that the hardest subregions are iteratively split into smaller ones whose verified bounds can be computed more tightly to ease the training. We demonstrate that our new training framework can produce models which can be more efficiently verified at test time. On the largest 2D quadrotor dynamical system, verification for our model is more than 5X faster compared to the baseline, while our size of region-of-attraction is 16X larger than the baseline.",2025,0.5795452438017296,0.5799155287295983,0.5333333333333333,0.5,023021c8-b4e4-4783-8fda-0077cc275dda,0,"[0.5, 0.5, 0.5, 0.625]","[0.95, 0.95, 0.95, 1.0]",CT-BaB,0.5387058173784979
550857,Hybrid Fine-Tuning of LLMs: Theoretical Insights on Generalized Smoothness and Convergence,"Applying either Parameter-Efficient Fine-Tuning (PEFT) or full fine-tuning to Large Language Models (LLMs) often results in its inherent limitations.  To overcome this issue, we propose a novel ""hybrid fine-tuning"" approach that jointly updates both  LLMs and PEFT modules  using a combination of zeroth-order and first-order optimization methods. To analyze this approach, we develop a theoretical framework centered on the concept of ""hybrid generalized smoothness"", which accounts for the heterogeneous nature of the optimization landscape in joint LLM and PEFT training. We provide a rigorous convergence analysis for the convergence of SGD algorithm under multiple learning rates and demonstrate its effectiveness through extensive empirical studies across various downstream tasks and model architectures. Our work not only offers a solution to the practical challenge of LLM fine-tuning but also contributes a broader theoretical foundation for analyzing hybrid optimization problems in machine learning.",2025,0.4090907603306326,0.404833682211152,0.4,0.25,ab14602b-4c36-41a8-a90c-04bd1522be2d,0,"[0.25, 0.25, 0.5, 0.5]","[1.0, 0.95, 0.9, 0.95]",Hybrid Generalized Smoothness,0.3575962171087842
550859,Neutral residues: revisiting adapters for model extension,"We address the problem of extending a pretrained large language model to a new domain that was not seen at training time, like adding a language for which the original model has seen no or little training data.  Popular solutions like fine-tuning or low-rank adaptation are successful at domain adaptation, but formally they do not add any extra capacity and degrade the performance in the original domain. 

Our paper analyzes this extension problem under three angles: data, architecture and training procedure, which are advantageously considered jointly. In particular, we improve adapters and make it possible to learn an entire new language while ensuring that the output of the neural network is  almost unchanged in the original domain. For this purpose, we modify the new residual blocks in a way that leads each new residual block to output near-zeros in the original domain. 

This solution of neutral residues, which borrows architectural components from mixture of experts, is effective: with only 20% extra learnable weights compared to an original model trained on English, we get results that are significantly better than concurrent approaches (fine-tuning, low-rank or vanilla adapters) in terms of the trade-off between learning a new language and not forgetting English.",2025,0.4545456528924899,0.4521750589752358,0.5333333333333333,0.5,fa45df19-ee89-4bc6-bddb-71eca431fd9c,0,"[0.25, 0.5, 0.5]","[0.95, 0.95, 0.9]",Neutral residues,0.4098211328254389
550860,Order-aware Interactive Segmentation,"Interactive segmentation aims to accurately segment target objects with minimal user interactions. However, current methods often fail to accurately separate target objects from the background, due to a limited understanding of order, the relative depth between objects in a scene. To address this issue, we propose OIS: order-aware interactive segmentation, where we explicitly encode the relative depth between objects into order maps. We introduce a novel order-aware attention, where the order maps seamlessly guide the user interactions (in the form of clicks) to attend to the image features. We further present an object-aware attention module to incorporate a strong object-level understanding to better differentiate objects with similar order.  Our approach allows both dense and sparse integration of user clicks, enhancing both accuracy and efficiency as compared to prior works.  Experimental results demonstrate that OIS achieves state-of-the-art performance, improving mIoU after one click by 7.61 on the HQSeg44K dataset and 1.32 on the DAVIS dataset as compared to the previous state-of-the-art SegNext, while also doubling inference speed compared to current leading methods.",2025,0.6818179338843877,0.6806929345162506,0.6666666666666666,0.625,c9eb6a18-70ef-4e35-9425-cce99b226988,1,"[0.5, 0.5, 0.625, 0.625, 0.625, 0.875]","[0.9, 1.0, 0.95, 1.0, 1.0, 0.95]",Order-aware attention,0.622836191870313
550863,Counterfactual Learning under Rank Preservation,"Counterfactual inference aims to estimate the counterfactual outcome given knowledge of an observed treatment and the factual outcome, with broad applications in fields such as epidemiology, econometrics, and management science. In this paper, we propose a principled approach for identifying and estimating the counterfactual outcome.  Specifically, we introduce a simple and intuitive rank preservation assumption to identify the counterfactual outcome without relying on a known structural causal model. Building on this, we propose a novel ideal loss for theoretically unbiased learning of the counterfactual outcome and further develop a kernel-based estimator for its empirical estimation. Our theoretical analysis shows that the proposed ideal loss is convex, and the proposed estimator is unbiased. Extensive semi-synthetic and real-world experiments are conducted to demonstrate the effectiveness of the proposed method.",2025,0.443181657024852,0.4457144661951552,0.4,0.25,4921c3c2-9506-41ba-8ca0-3efb4f052e62,0,"[0.25, 0.25, 0.5, 0.625]","[0.9, 0.9, 0.9, 0.95]",Rank Preservation,0.4212693798449611
550871,LoLCATs: On Low-Rank Linearizing of Large Language Models,"Recent works show we can linearize large language models (LLMs)—swapping the quadratic attentions of popular Transformer-based LLMs with subquadratic analogs, such as linear attention—avoiding the expensive pretraining costs. However, linearizing LLMs often significantly degrades model quality, still requires training over billions of tokens, and remains limited to smaller 1.3B to 7B LLMs. We thus propose Low-rank Linear Conversion via Attention Transfer (LoLCATs), a simple two-step method that improves LLM linearizing quality with orders of magnitudes less memory and compute. We base these steps on two findings. First, we can replace an LLM's softmax attentions with closely-approximating linear attentions, simply by *training* the linear attentions to match their softmax counterparts with an output MSE loss (“attention transfer”). Then, this enables adjusting for approximation errors and recovering LLM quality simply with *low-rank* adaptation (LoRA). LoLCATs significantly improves linearizing quality, training efficiency, and scalability. We significantly reduce the linearizing quality gap and produce state-of-the-art subquadratic LLMs from Llama 3 8B and Mistral 7B v0.1, leading to 20+ points of improvement on 5-shot MMLU. Furthermore, LoLCATs does so with only 0.2% of past methods' model parameters and 0.04-0.2% of their training tokens. Finally, we apply LoLCATs to create the first linearized 70B and 405B LLMs (50$\times$ that of prior work). When compared with prior approaches under the same compute budgets, LoLCATs significantly improves linearizing quality, closing the gap between linearized and original Llama 3.1 70B and 405B LLMs by 77.8\% and 78.1\% on 5-shot MMLU.",2025,0.7840906239670459,0.7781975686737488,0.8,0.875,67adc0aa-9410-40f2-9f74-5e1b7f0c9ff8,1,"[0.5, 0.625, 0.875, 0.875]","[0.95, 0.95, 0.9, 0.9]",LoLCATs,0.6986722797927462
550874,GameGen-X: Interactive Open-world Game Video Generation,"We introduce GameGen-$\mathbb{X}$, the first diffusion transformer model specifically designed for both generating and interactively controlling open-world game videos. 
    This model facilitates high-quality, open-domain generation by approximating various game elements, such as innovative characters, dynamic environments, complex actions, and diverse events. 
    Additionally, it provides interactive controllability, predicting and altering future content based on the current clip, thus allowing for gameplay simulation.
    To realize this vision, we first collected and built an Open-World Video Game Dataset (OGameData) from scratch. 
    It is the first and largest dataset for open-world game video generation and control, which comprises over one million diverse gameplay video clips with informative captions.
    GameGen-$\mathbb{X}$ undergoes a two-stage training process, consisting of pre-training and instruction tuning. 
    Firstly, the model was pre-trained via text-to-video generation and video continuation, enabling long-sequence open-domain game video generation with improved fidelity and coherence.
    Further, to achieve interactive controllability, we designed InstructNet to incorporate game-related multi-modal control signal experts.
    This allows the model to adjust latent representations based on user inputs, advancing the integration of character interaction and scene content control in video generation.
    During instruction tuning, only the InstructNet is updated while the pre-trained foundation model is frozen, enabling the integration of interactive controllability without loss of diversity and quality of generated content. 
    GameGen-$\mathbb{X}$ contributes to advancements in open-world game design using generative models. 
    It demonstrates the potential of generative models to serve as auxiliary tools to traditional rendering techniques, demonstrating the potential for merging creative generation with interactive capabilities.
    The project will be available at https://github.com/GameGen-X/GameGen-X.",2025,0.8181815206612653,0.8185991448318526,0.8,0.625,5c8064a7-47be-4f9d-96b8-a58417016751,1,"[0.625, 0.625, 0.875, 0.875]","[0.95, 0.95, 0.95, 1.0]",GameGen-$\mathbb{X}$,0.7599410898379971
550906,SEPARATE: A Simple Low-rank Projection for Gradient Compression in Modern Large-scale Model Training Process,"Training Large Language Models (LLMs) presents a significant communication bottleneck, predominantly due to the growing scale of the gradient to communicate across multi-device clusters. However, how to mitigate communication overhead in practice remains a formidable challenge due to the weakness of the methodology of the existing compression methods, especially the neglect of the characteristics of the gradient. In this paper, we consider and demonstrate the low-rank properties of gradient and Hessian observed in LLMs training dynamic, and take advantage of such natural properties to design SEPARATE, a simple low-rank projection for gradient compression in modern large-scale model training processes. SEPARATE realizes dimensional reduction by common random Gaussian variables and an improved moving average error-feedback technique. We theoretically demonstrate that SEPARATE-based optimizers maintain the original convergence rate for SGD and Adam-Type optimizers for general non-convex objectives. Experimental results show that SEPARATE accelerates training speed by up to 2× for GPT-2-Medium pre-training, and improves performance on various benchmarks for LLAMA2-7B fine-tuning.",2025,0.6818179338843877,0.6806929345162506,0.6666666666666666,0.625,0c8e13d2-04b4-4e46-b519-cd0ea6f19bef,1,"[0.625, 0.625, 0.625, 0.625]","[0.9, 0.8, 0.9, 0.9]",Low-rank Projection,0.6250000000000001
550912,GDrag:Towards General-Purpose Interactive Editing with Anti-ambiguity Point Diffusion,"Recent interactive point-based image manipulation methods have gained considerable attention for being user-friendly. However, these methods still face two types of ambiguity issues that can lead to unsatisfactory outcomes, namely, intention ambiguity which misinterprets the purposes of users, and content ambiguity where target image areas are distorted by distracting elements. To address these issues and achieve general-purpose manipulations, we propose a novel task-aware, training-free framework called GDrag. Specifically, GDrag defines a taxonomy of atomic manipulations, which can be parameterized and combined unitedly to represent complex manipulations, thereby reducing intention ambiguity. Furthermore, GDrag introduces two strategies to mitigate content ambiguity, including an anti-ambiguity dense trajectory calculation method (ADT) and a self-adaptive motion supervision method (SMS). Given an atomic manipulation, ADT converts the sparse user-defined handle points into a dense point set by selecting their semantic and geometric neighbors, and calculates the trajectory of the point set. Unlike previous motion supervision methods relying on a single global scale for low-rank adaption, SMS jointly optimizes point-wise adaption scales and latent feature biases. These two methods allow us to model fine-grained target contexts and generate precise trajectories. As a result, GDrag consistently produces precise and appealing results in different editing tasks. Extensive experiments on the challenging DragBench dataset demonstrate that GDrag outperforms state-of-the-art methods significantly. The code of GDrag will be released upon acceptance.",2025,0.7363633685951387,0.7328307459741098,0.6666666666666666,0.625,a57b335a-77d2-4e93-8d8d-9dbafa96c6a6,1,"[0.625, 0.625, 0.625, 0.625, 0.875]","[1.0, 0.95, 0.95, 0.9, 0.9]",GDrag,0.6644809108059282
550925,Synthesizing Programmatic Reinforcement Learning Policies with Large Language Model Guided Search,"Programmatic reinforcement learning (PRL) has been explored for representing policies through programs as a means to achieve interpretability and generalization. Despite promising outcomes, current state-of-the-art PRL methods are hindered by sample inefficiency, necessitating tens of millions of program-environment interactions. To tackle this challenge, we introduce a novel LLM-guided search framework (LLM-GS). Our key insight is to leverage the programming expertise and common sense reasoning of LLMs to enhance the efficiency of assumption-free, random-guessing search methods. We address the challenge of LLMs' inability to generate precise and grammatically correct programs in domain-specific languages (DSLs) by proposing a Pythonic-DSL strategy — an LLM is instructed to initially generate Python codes and then convert them into DSL programs. To further optimize the LLM-generated programs, we develop a search algorithm named Scheduled Hill Climbing, designed to efficiently explore the programmatic search space to improve the programs consistently. Experimental results in the Karel domain demonstrate our LLM-GS framework's superior effectiveness and efficiency. Extensive ablation studies further verify the critical role of our Pythonic-DSL strategy and Scheduled Hill Climbing algorithm. Moreover, we conduct experiments with two novel tasks, showing that LLM-GS enables users without programming skills and knowledge of the domain or DSL to describe the tasks in natural language to obtain performant programs.",2025,0.772726628099408,0.7745817278055985,0.6666666666666666,0.625,fb583b3e-4b78-4cea-a18f-34afc65f5296,1,"[0.625, 0.625, 0.875]","[0.95, 0.95, 1.0]",LLM-Guided Search,0.7255499760879962
551001,HYBRID MODEL COLLABORATION FOR SIGN LANGUAGE TRANSLATION WITH VQ-VAE AND RAG ENHANCED LLMS,"Data shortages and the phonetic disparity between sign and spoken languages have historically limited the quality of sign language translation.  On another front, endowed with substantial prior knowledge, large language models perform exceptionally well across diverse tasks, significantly diminishing the demand for domain-specific training data. Building on these foundation, this paper presents VRG-SLT, an innovative framework that translates sign language into spoken language, facilitating communication between signing and non-signing communities. In practice, VRG-SLT utilizes a hierarchical VQ-VAE to convert continuous sign sequences into discrete representations, referred as sign codes, which are subsequently aligned with text by a fine-tuned pre-trained language model. Additionally, retrieval-augmented generation (RAG) is employed to extend and enhance the language model, producing more semantically coherent and precise spoken text. Featuring a hierarchical VQ-VAE and pre-trained large language models, VRG-SLT demonstrates state-of-the-art performance. It excels on modish benchmarks like How2Sign and PHOENIX-2014T. Moreover, the incorporation of additional factual knowledge through RAG further improves the accuracy of the generated text.",2025,0.5454543471075102,0.5427388034180588,0.4666666666666667,0.25,020d0859-8766-4dbe-9d22-7935b93b36cf,0,"[0.25, 0.25, 0.625, 0.875]","[0.95, 0.95, 0.9, 0.95]",VRG-SLT,0.4924537487828628
551003,Path Selection Makes BERT-family Good Generators,"The Mask-Predict decoding algorithm has been widely used to enhance the generation capacity of traditional non-autoregressive (NAR) models and provide a good recipe for adapting the pre-trained BERT-like masked language models (MLMs) to NAR generation scenarios.
However, these models, which we denote as NAR-MLMs, are still regarded as inferior to competitive autoregressive (AR) models in terms of performance.
In this paper, we further explore the core problems leading to the performance gap of NAR-MLMs and delve into effective solutions for technological innovation.
Specifically, most related works neglect the impact of the training sequence decomposition format, i.e., 
Unlike the AR models which can naturally decompose the text sequence in a left-to-right manner for training and inference, NAR-MLMs are trained with a random decomposition but aim to find a determined optimal composition (denoted as decoding paths) during inference.
To alleviate this mismatching, we propose decoding path selection to increase the search space for finding a better 
composition, and path optimization methods to enable the model decoding path preference during the training process. 
Results on various zero-shot common sense reasoning and reading comprehension tasks and several task-specific generation tasks demonstrate that our NAR-MLM achieves significant performance improvements on common benchmarks with the methods mentioned above, reaching performance levels comparable to even outperforming AR pre-trained models. Our model and code will be available at Github.",2025,0.3749998636364132,0.3716213125502351,0.2666666666666666,0.25,7a13bdf9-57c4-400b-bb99-2f45e1b0a24c,0,"[0.25, 0.25, 0.25, 0.625]","[0.95, 0.9, 0.95, 0.9]",Decoding Path Selection,0.3317033678756477
551006,SC-OmniGS: Self-Calibrating Omnidirectional Gaussian Splatting,"360-degree cameras streamline data collection for radiance field 3D reconstruction by capturing comprehensive scene data. However, traditional radiance field methods do not address the specific challenges inherent to 360-degree images. We present SC-OmniGS, a novel self-calibrating omnidirectional Gaussian splatting system for fast and accurate omnidirectional radiance field reconstruction using 360-degree images. Rather than converting 360-degree images to cube maps and performing perspective image calibration, we treat 360-degree images as a whole sphere and derive a mathematical framework that enables direct omnidirectional camera pose calibration accompanied by 3D Gaussians optimization. Furthermore, we introduce a differentiable omnidirectional camera model in order to rectify the distortion of real-world data for performance enhancement. Overall, the omnidirectional camera intrinsic model, extrinsic poses, and 3D Gaussians are jointly optimized by minimizing weighted spherical photometric loss. Extensive experiments have demonstrated that our proposed SC-OmniGS is able to recover a high-quality radiance field from noisy camera poses or even no pose prior in challenging scenarios characterized by wide baselines and non-object-centric configurations. The noticeable performance gain in the real-world dataset captured by consumer-grade omnidirectional cameras verifies the effectiveness of our general omnidirectional camera model in reducing the distortion of 360-degree images.",2025,0.8181815206612653,0.8168315214195008,0.8,0.625,a5511f57-93a8-44c3-b20d-9052d69e81cc,1,"[0.625, 0.625, 0.875, 0.875]","[0.9, 0.95, 0.9, 0.95]",Omnidirectional Gaussian Splatting,0.75
551013,3D-Prover: Diversity Driven Theorem Proving With Determinantal Point Processes,"A key challenge in automated formal reasoning is the intractable search space, which grows exponentially with the depth of the proof. This branching is caused by the large number of candidate proof tactics which can be applied to a given goal. Nonetheless, many of these tactics are semantically similar or lead to an execution error, wasting valuable resources in both cases. We address the problem of effectively pruning this search, using only synthetic data generated from previous proof attempts. We first demonstrate that it is possible to generate semantically aware tactic representations which capture the effect on the proving environment, likelihood of success and execution time. We then propose a novel filtering mechanism which leverages these representations to select semantically diverse and high quality tactics, using Determinantal Point Processes. Our approach, 3D-Prover, is designed to be general, and to augment any underlying tactic generator. We demonstrate the effectiveness of 3D-Prover on the miniF2F-valid and miniF2F-test benchmarks by augmenting the ReProver LLM. We show that our approach leads to an increase in the overall proof rate, as well as a significant improvement in the tactic success rate, execution time and diversity.",2025,0.6818179338843877,0.6806929345162506,0.6,0.5,a7c90a80-10ff-488e-845d-dae88b18cb48,0,"[0.5, 0.5, 0.625, 0.875]","[0.95, 0.95, 0.9, 0.95]",Determinantal Point Processes,0.625
551014,PACE: Physics Informed Uncertainty Aware Climate Emulator,"Climate models serve as critical tools for evaluating the effects of climate change and projecting future climate scenarios. However, the reliance on numerical simulations of physical equations renders them computationally intensive and inefficient. While deep learning methodologies have made significant progress in weather forecasting, they are still unstable for climate emulation tasks. Here, we propose PACE, a lightweight 684K parameter Physics Informed Uncertainty Aware Climate Emulator. PACE emulates temperature and precipitation stably for 86 years while only being trained on emissions data. We incorporate a fundamental physical law of advection-diffusion in PACE accounting for boundary conditions and empirically estimating the diffusion co-efficient and flow velocities from concentrations data. PACE has been trained on 15 climate models provided by ClimateSet outperforming baselines across most of the climate models and advancing a new state of the art in a climate diagnostic task.",2025,0.2727271735537551,0.2743246981535251,0.2666666666666666,0.25,cf3e8cf1-2123-4538-9845-6c211cf213b0,0,"[0.0, 0.25, 0.25, 0.25, 0.25, 0.25, 0.5]","[0.9, 0.95, 0.95, 0.95, 1.0, 0.95, 0.95]",PACE,0.2579985980263794
551029,ActiveAD: Planning-Oriented Active Learning for End-to-End Autonomous Driving,"End-to-end differentiable learning has emerged as a prominent paradigm in autonomous driving (AD). A significant bottleneck in this approach is its substantial demand for high-quality labeled data, such as 3D bounding boxes and semantic segmentation, which are especially expensive to annotate manually. This challenge is exacerbated by the long tailed distribution in AD datasets, where a substantial portion of the collected data might be trivial (e.g. simply driving straight on a straight road) and only a minority of instances are critical to safety.  In this paper, we propose ActiveAD, a planning-oriented active learning strategy designed to enhance sampling and labeling efficiency in end-to-end autonomous driving. ActiveAD progressively annotates parts of collected raw data based on our newly developed metrics. We design innovative diversity metrics to enhance initial sample selection, addressing the cold-start problem. Furthermore, we develop uncertainty metrics to select valuable samples for the ultimate purpose of route planning during subsequent batch selection. Empirical results demonstrate that our approach significantly surpasses traditional active learning methods. Remarkably, our method achieves comparable results to state-of-the-art end-to-end AD methods - by using only 30% data in both open-loop nuScenes and closed-loop CARLA evaluation.",2025,0.3749998636364132,0.3717628966806145,0.2666666666666666,0.25,924dddaf-0734-4140-9a66-9bdeb5a708d7,0,"[0.25, 0.25, 0.25, 0.625]","[0.95, 1.0, 1.0, 0.95]",ActiveAD,0.3299369031377899
551047,NextBestPath: Efficient 3D Mapping of Unseen Environments,"This work addresses the problem of active 3D mapping, where an agent must find an efficient trajectory to exhaustively reconstruct a new scene.
Previous approaches mainly predict the next best view near the agent's location, which is prone to getting stuck in local areas. Additionally, existing indoor datasets are insufficient due to limited geometric complexity and inaccurate ground truth meshes.
To overcome these limitations, we introduce a novel dataset AiMDoom with a map generator for the Doom video game, enabling to better benchmark active 3D mapping in diverse indoor environments.
Moreover, we propose a new method we call next-best-path (NBP), which predicts long-term goals rather than focusing solely on short-sighted views.
The model jointly predicts accumulated surface coverage gains for long-term goals and obstacle maps, allowing it to efficiently plan optimal paths with a unified model.
By leveraging online data collection, data augmentation and curriculum learning, NBP significantly outperforms state-of-the-art methods on both the existing MP3D dataset and our AiMDoom dataset, achieving more efficient mapping in indoor environments of varying complexity.",2025,0.5454543471075102,0.5498583069587512,0.6,0.625,ee285534-3096-4528-a724-d0f6869eb22e,1,"[0.25, 0.5, 0.625, 0.625]","[0.9, 1.0, 0.95, 1.0]",Next-Best-Path,0.5224338928776924
551061,PointSeg: A Training-Free Paradigm for 3D Scene Segmentation via Foundation Models,"Recent success of vision foundation models have shown promising performance for the 2D perception tasks. However, it is difficult to train a 3D foundation network directly due to the limited dataset and it remains under explored whether existing foundation models can be lifted to 3D space seamlessly. In this paper, we present PointSeg, a novel training-free paradigm that leverages off-the-shelf vision foundation models to address 3D scene perception tasks. PointSeg can segment anything in 3D scene by acquiring accurate 3D prompts to align their corresponding pixels across frames. Concretely, we design a two-branch prompts learning structure to construct the 3D point-box prompts pairs, combining with the bidirectional matching strategy for accurate point and proposal prompts generation. Then, we perform the iterative post-refinement adaptively when cooperated with different vision foundation models. Moreover, we design a affinity-aware merging algorithm to improve the final ensemble masks. PointSeg demonstrates impressive segmentation performance across various datasets, all without training. Specifically, our approach significantly surpasses the state-of-the-art specialist training-free model by 16.3$\%$, 14.9$\%$, and 15$\%$ mAP on ScanNet, ScanNet++, and KITTI-360 datasets, respectively. On top of that, PointSeg can incorporate with various foundation models and even surpasses the specialist training-based methods by 5.6$\%$-8$\%$ mAP across various datasets, serving as an effective generalist model.",2025,0.5795452438017296,0.5785889943388131,0.5333333333333333,0.5,8191a8a6-7dda-4179-9ede-32b62afb1de2,0,"[0.5, 0.5, 0.5, 0.625]","[0.9, 0.9, 0.9, 0.9]",PointSeg,0.53125
551098,SynthCLIP: Are We Ready for a Fully Synthetic CLIP Training?,"We present SynthCLIP, a CLIP model trained on entirely synthetic text-image pairs. Leveraging recent text-to-image (TTI) networks and large language models (LLM), we generate synthetic datasets of images and corresponding captions at scale, with no human intervention. In this work, we provide an analysis on CLIP models trained on synthetic data. We provide insights on the data generation strategy, number of samples required, scaling trends, and resulting properties. We also introduce SynthCI-30M, a purely synthetic dataset comprising 30 million captioned images. Our work focuses on showing the advantages and disadvantages of synthetic data for training CLIP models. Our code, trained models, and data, will be released as open source.",2025,0.5113634504132908,0.5105197008871879,0.5333333333333333,0.5,5317e759-aaab-41c5-9961-6ee469cdb65b,0,"[0.25, 0.5, 0.5, 0.625]","[0.95, 0.95, 0.95, 0.95]",SynthCLIP,0.4687499999999999
551101,Federated Learning in Streaming Subspace,"Federated learning (FL) has received widespread attention due to its distributed training and privacy protection. However, existing federated learning methods encounter significant challenges, such as increased communication costs and degraded model performance, when processing non-independently and identically distributed (non-IID) data. This paper jointly alleviates these problems by analyzing and exploiting the low-rank properties of global model trajectories.

Primarily, we introduce a streaming subspace update strategy and then propose a general federated learning framework, $\\textbf{F}$erated $\\textbf{L}$earning in $\\textbf{S}$treaming $\\textbf{S}$ubspace ($\\texttt{FLSS}$). In $\\texttt{FLSS}$, local model updates are restricted to the global streaming subspace, resulting in low-dimensional trajectories. The server then aggregates these trajectories to update the global model. Comprehensive experiments verify the effectiveness of our framework. In Cifar100, the $\\texttt{FLSS}$-equipped FL method outperforms the baseline by 2.14$\\%$ and reduces the communication cost by 80$\\%$. $\\texttt{FLSS}$ utilizes the early training information of the global model to simultaneously improve the performance and communication efficiency of federated learning.",2025,0.5113634504132908,0.5127295024298015,0.5333333333333333,0.5,a8c02da4-1d17-4f42-8dc0-8d4c5a30db1b,0,"[0.25, 0.5, 0.5, 0.625]","[0.95, 0.95, 0.95, 1.0]",FLSS,0.4811763622974963
551112,M3GIA: A Cognition Inspired Multilingual and Multimodal General Intelligence Ability Benchmark,"As recent multi-modal large language models (MLLMs) have shown formidable proficiency on various complex tasks, there has been increasing attention on debating whether these models could eventually mirror human intelligence. However, existing benchmarks mainly focus on evaluating solely on task performance, such as the accuracy of identifying the attribute of an object. Combining well-developed cognitive science to understand the intelligence of MLLMs beyond superficial achievements remains largely unexplored. To this end, we introduce the first cognitive-driven multi-lingual and multi-modal benchmark to evaluate the general intelligence ability of MLLMs, dubbed M3GIA. Specifically, we identify five key cognitive factors based on the well-recognized Cattell-Horn-Carroll (CHC) model of intelligence and propose a novel evaluation metric. In addition, since most MLLMs are trained to perform in different languages, we go beyond English to encompass other languages, including Chinese, French, Spanish, Portuguese and Korean, to construct our M3GIA. We make sure all the data relevant to the cultural backgrounds are collected from their native context to avoid English-centric bias. We collected a significant corpus of data from human participants, revealing that the most advanced MLLM barely reaches the lower boundary of human performance in English, and there remains a pronounced disparity in the other five languages. Importantly, we found that designing IQ tests for MLLMs is crucial, as the evaluation of M3GIA achieves a significantly stronger alignment with human preferences compared to traditional task-oriented benchmarks. Moreover, grounded in CHC theory, we discovered that the number of samples seen by the vision encoder has a greater influence on the model's visual capabilities than its parameter size.",2025,0.4545456528924899,0.4585724834509934,0.5333333333333333,0.5,732aa2ca-2716-4a0f-82af-57b8e9eeac45,0,"[0.25, 0.5, 0.5]","[0.9, 1.0, 0.95]",M3GIA,0.4380772291961873
551125,Multi-Label Test-Time Adaptation with Bound Entropy Minimization,"Mainstream test-time adaptation (TTA) techniques endeavor to mitigate distribution shifts via entropy minimization for multi-class classification, inherently increasing the probability of the most confident class. However, when encountering multi-label instances, the primary challenge stems from the varying number of labels per image, and prioritizing only the highest probability class inevitably undermines the adaptation of other positive labels. To address this issue, we investigate TTA within multi-label scenario (ML--TTA), developing Bound Entropy Minimization (BEM) objective to simultaneously increase the confidence of multiple top predicted labels. Specifically, to determine the number of labels for each augmented view, we retrieve a paired caption with yielded textual labels for that view. These labels are allocated to both the view and caption, called weak label set and strong label set with the same size k. Following this, the proposed BEM considers the highest top-k predicted labels from view and caption as a single entity, respectively, learning both view and caption prompts concurrently. By binding top-k predicted labels, BEM overcomes the limitation of vanilla entropy minimization, which exclusively optimizes the most confident class. Across the MSCOCO, VOC, and NUSWIDE multi-label datasets, our ML--TTA framework equipped with BEM exhibits superior performance compared to the latest SOTA methods, across various model architectures, prompt initialization, and varying label scenarios. The code is available at https://github.com/Jinx630/ML-TTA.",2025,0.7159088305786071,0.7133661953730307,0.6666666666666666,0.625,7e59b5bd-da5e-4a8d-a95f-68e2b685d299,1,"[0.5, 0.625, 0.625, 0.875]","[0.9, 1.0, 0.95, 0.9]",Bound Entropy Minimization,0.6498149448356398
551144,On the (un) interpretability of Ensembles: A Computational Analysis,"Despite the widespread adoption of ensemble models, it is widely acknowledged within the ML community that they offer limited interpretability. For instance, while a single decision tree is considered interpretable, ensembles of decision trees (e.g., boosted-trees) are usually regarded as black-boxes. Although this reduced interpretability is widely acknowledged, the topic has received only limited attention from a theoretical and mathematical viewpoint.  In this work, we provide an elaborate analysis of the interpretability of ensemble models through the lens of *computational complexity* theory. In a nutshell, we explore different forms of explanations, and analyze whether obtaining explanations for ensembles is strictly computationally less tractable than for their constituent base models. We show that this is indeed the case for ensembles that consist of interpretable models, such as decision trees or linear models; but this is not the case for ensembles consisting of more complex models, such as neural networks. Next, we perform a fine-grained analysis using parameterized complexity to measure the impact of different problem parameters on an ensemble's interpretability. Our findings reveal that even if we shrink the *size* of all base models in an ensemble substantially, the ensemble as a whole remains intractable to interpret. However, an analysis of the *number* of base models yields a surprising dynamic --- while ensembles consisting of a limited number of decision trees can be interpreted efficiently, ensembles that consist of a small (even *constant*) number of linear models are computationally intractable to interpret.",2025,0.6272724991736367,0.6165357194978773,0.6666666666666666,0.625,2d19038e-ff1f-4f9b-bdc3-7f20f8a3548f,0,"[0.25, 0.5, 0.625, 0.625, 0.875]","[0.9, 0.95, 0.8, 0.9, 0.8]",Computational Complexity,0.5433313175902488
551145,"Open Eyes, Then Reason: Fine-grained Visual Mathematical Understanding in MLLMs","Current multimodal large language models (MLLMs) often underperform on mathematical problem-solving tasks that require fine-grained visual understanding. The limitation primarily arises from inadequate perception of geometric primitives during image-level contrastive pre-training (e.g., CLIP). Current efforts to enhance MLLM performance have focused on scaling up mathematical visual instruction datasets and employing stronger LLM backbones, yet these approaches often neglect persistent visual recognition errors in MLLMs. In this paper, we systematically evaluate the visual grounding capabilities of state-of-the-art MLLMs and uncover a negative correlation between their visual grounding accuracy and problem-solving performance. Notably, even advanced models like GPT-4o demonstrate a significant error rate (70\%) when identifying geometric entities, highlighting that fine-grained visual understanding remains a crucial bottleneck in visual mathematical reasoning. To address this, we propose a novel approach, SVE-Math (Selective Vision-Enhanced Mathematical MLLM), featuring a geometric-grounded vision encoder and a feature router that dynamically adjusts the contribution of hierarchical visual feature maps. Our model recognizes accurate visual primitives and generates precise visual prompts tailored to the language model's reasoning needs. In experiments, SVE-Math-Deepseek-7B outperforms other 7B models by 7.7\% on MathVerse and is compatible with GPT-4V on MathVista. Despite being trained on smaller datasets, SVE-Math-7B matches the performance of models trained on significantly larger datasets, evaluated on GeoQA. Our findings provide critical insights for future research, highlighting the need for more effective integration of fine-grained visual understanding in MLLMs.  We will release model weights, code, and instructions upon acceptance.",2025,0.5454543471075102,0.5445543476130005,0.5333333333333333,0.5,a78c7b29-4d32-40c3-9263-47e14b78694d,0,"[0.5, 0.5, 0.5, 0.5]","[1.0, 0.95, 0.95, 1.0]",SVE-Math,0.4999999999999999
551147,Core Context Aware Attention for Long Context Language Modeling,"Transformer-based Large Language Models (LLMs) have exhibited remarkable success in various natural language processing tasks primarily attributed to self-attention mechanism, which requires a token to consider all preceding tokens as its context to compute the attention score. However, when the context length L becomes very large (e.g., 32K), more redundant context information will be included w.r.t. any tokens, making the self-attention suffer from two main limitations: 1) The computational and memory complexity scales quadratically w.r.t. L; 2) The presence of redundant context information may hamper the model to capture dependencies among crucial tokens, which may degrade the representation performance. In this paper, we propose a plug-and-play Core Context Aware (CCA) Attention for efficient long-range context modeling, which consists of two components: 1) Globality-pooling attention that divides input tokens into groups and then dynamically merges tokens within each group into one core token based on their significance; 2) Locality-preserved attention that incorporates neighboring tokens into the attention calculation. The two complementary attentions will then be fused to the final attention, maintaining comprehensive modeling ability as the full self-attention. In this way, the core context information w.r.t. a given token will be automatically focused and strengthened, while  the context information in redundant groups will be diminished during the learning process. As a result, the computational and memory complexity will be significantly reduced. More importantly, the CCA-Attention can improve the long-context modeling ability by diminishing the  redundant context information. Extensive experimental results demonstrate that our CCA-Attention significantly outperforms state-of-the-art models in terms of computational efficiency and long-context modeling ability.",2025,0.4999994545456529,0.5032270290639547,0.5333333333333333,0.25,3695bf58-8528-43cd-8419-52d5b8f48e45,0,"[0.25, 0.5, 0.625]","[0.9, 0.95, 0.95]",Core Context Aware Attention,0.4754471679364026
551159,UniEEG: Advancing Universal EEG Representation with Electrode-Wise Time-Frequency Pretraining,"Previous electroencephalogram (EEG) models typically exhibit limited performance and generalization by collecting data specifically for targeted EEG tasks. Recognizing this limitation, we propose UniEEG, the first electrode-wise time-frequency pretraining model, designed to overcome barriers across diverse tasks and data in EEG modeling. We collect data from nearly 20 publicly available EEG datasets, including 6 EEG tasks, significantly extending the data volume. The collected EEG data are standardized and split to individual electrodes as the input of UniEEG, enabling full compatibility with diverse EEG data from different acquisition devices and task paradigms. Meanwhile, leveraging a time-frequency transform method, UniEEG adeptly processes EEG signals characterized by signal noises and time delays. In the training phase, we employ an encoder-decoder architecture and a mask signal modeling strategy on time-frequency dimension, learning the electrode-wise universal EEG representation. In the fine-tuning phase, multi-electrode EEG signals from various tasks are consolidated into individual electrodes. The predictions for downstream tasks are then obtained through the pre-trained encoder and an additional prediction module. Furthermore, the proposed UniEEG achieves state-of-the-art performance across different EEG tasks, demonstrating an amazing ability to universal EEG feature representation.
Code, data and models would be available upon acceptance.",2025,0.1363635867768775,0.1361385869032501,0.1333333333333333,0.0,f89536e6-52a4-40eb-92b2-20e95fcb5626,0,"[0.0, 0.0, 0.25, 0.25]","[1.0, 0.95, 0.95, 1.0]",Electrode-wise Time-Frequency Pretraining,0.1250001897065186
551165,Towards World Simulator: Crafting Physical Commonsense-Based Benchmark for Video Generation,"Text-to-video (T2V) models like Sora have made significant strides in visualizing complex prompts, which is increasingly viewed as a promising path towards constructing the universal world simulator. Cognitive psychologists believe that the foundation for achieving this goal is the ability to understand intuitive physics. However, the capacity of these models to accurately represent intuitive physics remains largely unexplored. To bridge this gap, we introduce PhyGenBench, a comprehensive \textbf{Phy}sics \textbf{Gen}eration \textbf{Ben}chmark designed to evaluate physical commonsense correctness in T2V generation. PhyGenBench comprises 160 carefully crafted prompts across 27 distinct physical laws, spanning four fundamental domains, which could comprehensively assesses models' understanding of physical commonsense. Alongside PhyGenBench, we propose a novel evaluation framework called PhyGenEval. This framework employs a hierarchical evaluation structure utilizing appropriate advanced vision-language models and large language models to assess physical commonsense. Through PhyGenBench and PhyGenEval, we can conduct large-scale automated assessments of T2V models' understanding of physical commonsense, which align closely with human feedback. Our evaluation results and in-depth analysis demonstrate that current models struggle to generate videos that comply with physical commonsense. Moreover, simply scaling up models or employing prompt engineering techniques is insufficient to fully address the challenges presented by PhyGenBench (e.g., dynamic scenarios). We hope this study will inspire the community to prioritize the learning of physical commonsense in these models beyond entertainment applications. We will release the data and codes at https://github.com/PhyGenBench/PhyGenBench",2025,0.5727270644628857,0.5717820649936505,0.5333333333333333,0.5,add292d2-19cc-4b9d-ba4e-883f3dcb4a64,0,"[0.5, 0.5, 0.5, 0.5, 0.625]","[0.95, 0.95, 0.95, 0.95, 0.95]",PhyGenBench,0.5249999999999999
551167,Breaking Free: Hacking Diffusion Models for Generating Adversarial Examples and Bypassing Safety Guardrails,"Deep neural networks can be exploited using natural adversarial samples, which do not impact human perception. Current approaches often rely on synthetically altering the distribution of adversarial samples compared to the training distribution. In contrast, we propose EvoSeed, a novel evolutionary strategy-based algorithmic framework that uses auxiliary Conditional Diffusion and Classifier models to generate photo-realistic natural adversarial samples. We employ CMA-ES to optimize the initial seed vector search, which, when processed by the Conditional Diffusion Model, results in the natural adversarial sample misclassified by the Classifier Model. Experiments show that generated adversarial images are of high image quality, raising concerns about generating harmful content bypassing safety classifiers. We also show that beyond generating adversarial images, EvoSeed can also be used as a red-teaming tool to understand classification systems' misclassification. Our research opens new avenues for understanding the limitations of current safety mechanisms and the risk of plausible attacks against classifier systems using image generation.",2025,0.3636358677687754,0.3582590379685073,0.2666666666666666,0.25,558976b6-46d1-4ab2-9a28-4e005ca35383,0,"[0.25, 0.25, 0.5]","[0.95, 1.0, 0.9]",EvoSeed,0.3119227708038127
551185,Zodiac: A Cardiologist-Level LLM Framework for Multi-Agent Diagnostics,"Large language models (LLMs) have demonstrated remarkable progress in healthcare. However, a significant gap remains regarding LLMs' professionalism in domain-specific clinical practices, limiting their application in real-world diagnostics. In this work, we introduce ZODIAC, an LLM-powered framework with cardiologist-level professionalism designed to engage LLMs in cardiological diagnostics. ZODIAC assists cardiologists by extracting clinically relevant characteristics from patient data, detecting significant arrhythmias, and generating preliminary reports for the review and refinement by cardiologists. To achieve cardiologist-level professionalism, ZODIAC is built on a multi-agent collaboration framework, enabling the processing of patient data across multiple modalities. Each LLM agent is fine-tuned using real-world patient data adjudicated by cardiologists, reinforcing the model's professionalism. ZODIAC undergoes rigorous clinical validation with independent cardiologists, evaluated across eight metrics that measure clinical effectiveness and address security concerns. Results show that ZODIAC outperforms industry-leading models, including OpenAI's GPT-4o, Meta's Llama-3.1-405B, and Google's Gemini-pro, as well as medical-specialist LLMs like Microsoft's BioGPT. ZODIAC demonstrates the transformative potential of specialized LLMs in healthcare by delivering domain-specific solutions that meet the stringent demands of medical practice. Notably, ZODIAC has been successfully integrated into electrocardiography (ECG) devices, exemplifying the growing trend of embedding LLMs into Software-as-Medical-Device (SaMD).",2025,0.4772725537190714,0.4737622824233104,0.4666666666666667,0.25,be6414be-b373-4eb4-be23-b494abbae11b,0,"[0.25, 0.25, 0.625, 0.625]","[0.95, 0.95, 0.9, 0.95]",ZODIAC,0.4261806231742939
551189,Reflect-then-Plan: Offline Model-Based Planning through a Doubly Bayesian Lens,"Offline reinforcement learning (RL) is essential when online exploration is costly or unsafe, but it often struggles with high epistemic uncertainty due to limited data. Existing methods learn fixed conservative policies, which limit adaptivity and generalization. To tackle these challenges, we propose __Reflect-then-Plan (RefPlan)__, a novel _doubly Bayesian_ approach for offline model-based (MB) planning that enhances offline-learned policies for improved adaptivity and generalization. RefPlan integrates uncertainty modeling and MB planning in a unified probabilistic framework, recasting planning as Bayesian posterior estimation. During deployment, it updates a belief distribution over environment dynamics based on real-time observations. By incorporating this uncertainty into MB planning via marginalization, RefPlan derives plans that account for unknowns beyond the agent's limited knowledge. Empirical results on standard benchmarks show that RefPlan significantly improves the performance of conservative offline RL policies. In particular, RefPlan maintains robust performance under high epistemic uncertainty and limited data, while demonstrating resilience to changing environment dynamics, improving the flexibility, generalizability, and robustness of offline-learned policies.",2025,0.4909089123967591,0.4857675275707866,0.5333333333333333,0.5,d9b9edab-0536-49db-b71e-e0eaa6f2d3e1,0,"[0.25, 0.5, 0.5, 0.5, 0.5]","[0.95, 0.8, 0.8, 0.95, 0.9]",Reflect-then-Plan,0.4337373239682353
551199,MTU-Bench: A Multi-granularity Tool-Use Benchmark for Large Language Models,"Large Language Models (LLMs) have displayed massive improvements in reason- ing and decision-making skills and can hold natural conversations with users. Recently, many tool-use benchmark datasets have been proposed. However, existing datasets have the following limitations: (1). Insufficient evaluation scenarios (e.g., only cover limited tool-use scenes). (2). Extensive evaluation costs (e.g., GPT API costs). To address these limitations, in this work, we propose a multi-granularity tool-use benchmark for large language models called MTU-Bench. For the ""multi-granularity"" property, our MTU-Bench covers five tool usage scenes (i.e., single-turn and single-tool, single-turn and multiple-tool, multiple-turn and single-tool, multiple-turn and multiple-tool, and out-of-distribution tasks). Besides, all evaluation metrics of our MTU-Bench are based on the prediction results and the ground truth without using any GPT or human evaluation metrics. Moreover, our MTU-Bench is collected by transforming existing high-quality datasets to simulate real-world tool usage scenarios, and we also propose an instruction dataset called MTU-Instruct data to enhance the tool-use abilities of existing LLMs. Comprehensive experimental results demonstrate the effectiveness of our MTU-Bench.",2025,0.6477270371901683,0.6471004659206998,0.6666666666666666,0.625,84ab0000-c018-47d3-83cb-16fa5b99f8ea,1,"[0.5, 0.625, 0.625, 0.625]","[0.95, 0.95, 0.95, 1.0]",MTU-Bench,0.5962352724594994
551211,ManiSkill-HAB: A Benchmark for Low-Level Manipulation in Home Rearrangement Tasks,"High-quality benchmarks are the foundation for embodied AI research, enabling significant advancements in long-horizon navigation, manipulation and rearrangement tasks. However, as frontier tasks in robotics get more advanced, they require faster simulation speed, more intricate test environments, and larger demonstration datasets. To this end, we present MS-HAB, a holistic benchmark for low-level manipulation and in-home object rearrangement. First, we provide a GPU-accelerated implementation of the Home Assistant Benchmark (HAB). We support realistic low-level control and achieve over 3x the speed of prior magical grasp implementations at a fraction of the GPU memory usage. Second, we train extensive reinforcement learning (RL) and imitation learning (IL) baselines for future work to compare against. Finally, we develop a rule-based trajectory filtering system to sample specific demonstrations from our RL policies which match predefined criteria for robot behavior and safety. Combining demonstration filtering with our fast environments enables efficient, controlled data generation at scale.",2025,0.6477270371901683,0.6506368018540987,0.5333333333333333,0.5,7a037afb-1874-4177-948b-17623d5810f7,1,"[0.5, 0.5, 0.5, 0.875]","[0.95, 0.95, 0.95, 1.0]",ManiSkill-HAB,0.6161174521354934
551240,"SafetyAnalyst: Interpretable, transparent, and steerable LLM safety moderation","The ideal LLM content moderation system would be both structurally interpretable (so its decisions can be explained to users) and steerable (to reflect a community's values or align to safety standards).  However, current systems fall short on both of these dimensions. To address this gap, we present SafetyAnalyst, a novel LLM safety moderation framework. Given a prompt, SafetyAnalyst creates a structured ""harm-benefit tree,"" which identifies 1) the actions that could be taken if a compliant response were provided, 2) the harmful and beneficial effects of those actions (along with their likelihood, severity, and immediacy), and 3) the stakeholders that would be impacted by those effects.  It then aggregates this structured representation into a harmfulness score based on a parameterized set of safety preferences, which can be transparently aligned to particular values. To demonstrate the power of this framework, we develop, test, and release a prototype system, SafetyReporter, including a pair of LMs specializing in generating harm-benefit trees through symbolic knowledge distillation and an interpretable algorithm that aggregates the harm-benefit trees into safety labels.  SafetyReporter is trained on 18.5 million harm-benefit features generated by SOTA LLMs on 19k prompts. On a comprehensive set of prompt safety benchmarks, we show that our system (average F1=0.75) outperforms existing LLM safety moderation systems (average F1$<$0.72) on prompt safety classification, while offering the additional advantages of interpretability and steerability.",2025,0.3181820661156123,0.2974878618835917,0.2666666666666666,0.0,46f3538a-462c-4d60-9a3c-d4bb52c17310,0,"[0.0, 0.25, 0.625]","[0.95, 0.95, 0.8]",Harm-benefit tree,0.2250475851522577
551249,STAR: Stability-Inducing Weight Perturbation for Continual Learning,"Humans can naturally learn new and varying tasks in a sequential manner. 
  Continual learning is a class of learning algorithms that updates its learned model as it sees new data (on potentially new tasks) in a sequence.
  A key challenge in continual learning is that as the model is updated to learn new tasks, it becomes susceptible to \textit{catastrophic forgetting}, where knowledge of previously learned tasks is lost. A popular approach to mitigate forgetting during continual learning is to maintain a small buffer of previously-seen samples, and to replay them during training. However, this approach is limited by the small buffer size and, while forgetting is reduced, it is still present.  In this paper, we propose
a novel loss function STAR that exploits the worst-case parameter perturbation that reduces the KL-divergence of model predictions with that of its local parameter neighborhood to promote stability and alleviate forgetting. STAR can be combined with almost any existing rehearsal-based methods as a plug-and-play component. We empirically show that STAR consistently improves performance of existing methods by up to $\sim15\\%$ across varying baselines, and achieves superior or competitive accuracy to that of state-of-the-art methods aimed at improving rehearsal-based continual learning.  Our implementation is available at https://github.com/Gnomy17/STAR_CL.",2025,0.6818179338843877,0.6789253111038988,0.6,0.5,bf74e818-d1b1-4cf5-85bf-acaa2f496313,1,"[0.5, 0.5, 0.625, 0.875]","[1.0, 0.95, 0.95, 0.95]",STAR,0.615058910162003
551259,Hallucination Detox: Sensitive Neuron Dropout (SeND) for Large Language Model Training,"As large language models (LLMs) are increasingly deployed across various industries, concerns regarding their reliability, particularly due to hallucinations—outputs that are factually inaccurate or irrelevant to user input—have grown. Our research investigates the relationship between the training process and the emergence of hallucinations  to address a key gap in existing research that focuses primarily on post hoc detection and mitigation strategies. Using models from the Pythia suite (70M–12B parameters) and several hallucination detection metrics, we analyze hallucination trends throughout training and explore LLM internal dynamics. We introduce Sensitivity Dropout SenD, a novel training protocol designed to mitigate hallucinations by reducing variance during training. SenD achieves this by deterministically dropping embedding indices with significant variability, referred to as Sensitive Embedding Indices. In addition, we develop an unsupervised hallucination detection metric, Efficient EigenScore (EES), which approximates the traditional EigenScore in 2x speed. This efficient metric is integrated into our protocol, allowing SenD to be both computationally scalable and effective at reducing hallucinations. Our empirical evaluation demonstrates that our approach improves LLM reliability at test time by up to 40\% compared to normal training while also providing an efficient method to improve factual accuracy when adapting LLMs to Wikipedia, Medical, and LegalBench domains.",2025,0.4772725537190714,0.4800671326599737,0.5333333333333333,0.5,1f56589b-01f3-49c2-b461-d08659291f2b,0,"[0.25, 0.5, 0.5, 0.5]","[0.9, 1.0, 0.95, 0.95]",Sensitive Neuron Dropout,0.4531148090215091
551262,Identifying single molecule force spectroscopy data using deep learning with physics augmentation,"Deciphering the pathways of protein folding and unfolding under tension is essential for deepening our understanding of fundamental biological mechanisms. Such insights offer the potential to develop treatments for a range of incurable and fatal debilitating conditions, including muscular disorders like Duchenne Muscular Dystrophy and neurodegenerative diseases such as Parkinson’s disease. Single molecule force spectroscopy (SMFS) is a powerful technique for investigating forces when domains in proteins fold and unfold. Currently, manual visual inspection remains the primary method for classifying force curves resulting from single proteins; a time-consuming task demanding significant expertise. In this work, we develop a classification strategy to detect measurements arising from single molecules by augmenting deep learning models with the physics of the protein being investigated. We develop a novel physics-based Monte Carlo engine to generate simulated datasets comprising of force curves that originate from a single molecule, multiple molecules, or failed experiments. We show that pre-training deep learning models with the simulated dataset enables high throughput classification of SMFS experimental data with average accuracies of $75.3 \pm 5.3$\% and ROC-AUC of $0.87 \pm 0.05$. Our physics augmentation strategy does not need expensive expert adjudication of the experimental data where models trained using our strategy show up to 25.9\% higher ROC-AUC over the models trained solely on the limited SMFS experimental data. Furthermore, we show that incorporating a small subset of experimental data ($\sim 100$ examples) through transfer learning improves accuracy by 6.8\% and ROC-AUC by 0.06. We have validated our results on three new SMFS experimental datasets. To facilitate further research in this area, we make our datasets available and provide a Python-based toolbox (\url{https://anonymous.4open.science/r/AFM_ML-2B8C}).",2025,0.4999994545456529,0.5391088041368705,0.5333333333333333,0.0,b508ddae-5058-4b4d-b9a7-8ee1379cd2d9,0,"[0.0, 0.5, 0.875]","[0.7, 0.9, 0.9]",Physics augmentation,0.5604099473773554
551297,12-Lead ECG Generation via a PDE-Based GAN,"Synthesizing realistic 12-lead electrocardiogram (ECG) data is a complex task due to the intricate spatial and temporal dynamics of cardiac electrophysiology. Traditional generative models often struggle to capture the nuanced interdependencies among ECG leads, which are essential for accurate medical analysis. In this paper, we introduce a novel method that integrates partial differential equations (PDEs) into a generative adversarial network (GAN) framework to model the spatiotemporal behavior of the heart's electrical activity. By embedding PDE-based representations directly into the generative process, our approach effectively captures both the temporal evolution and spatial relationships between ECG leads. This results in the production of high-fidelity synthetic 12-lead ECG data that closely mirrors real physiological signals. We conduct extensive experiments to evaluate the efficacy of our PDECGAN model, demonstrating that classifiers trained on our synthetic data outperform those trained on data generated by conventional methods in detecting cardiac abnormalities, with statistically significant improvements. Our work highlights the potential of combining PDE-driven cardiac models with advanced generative techniques to enhance the quality and utility of synthetic biomedical datasets.",2025,0.3749998636364132,0.3743811139839378,0.2666666666666666,0.25,46e9191f-f41f-402d-876b-2297219a5d72,0,"[0.25, 0.25, 0.25, 0.625]","[0.95, 0.95, 0.95, 0.95]",PDECGAN,0.34375
551318,M-Longdoc: A Benchmark For Multimodal Super-Long Document Understanding And A Retrieval-Aware Tuning Framework,"The ability to understand and answer questions over documents can be useful in many business and practical applications. However, documents often contain lengthy and diverse multimodal contents such as texts, figures, and tables, which are very time-consuming for humans to read thoroughly. Hence, there is an urgent need to develop effective and automated methods to aid humans in this task. In this work, we introduce M-LongDoc, a benchmark of 851 samples, and an automated framework to evaluate the performance of large multimodal models. We further propose a retrieval-aware tuning approach for efficient and effective multimodal document reading. Compared to existing works, our benchmark consists of more recent and lengthy documents with hundreds of pages, while also requiring open-ended solutions and not just extractive answers. To our knowledge, our training framework is the first to directly address the retrieval setting for multimodal long documents. To enable tuning open-source models, we construct a training corpus in a fully automatic manner for the question-answering task over such documents. Experiments show that our tuning approach achieves a relative improvement of 4.6% for the correctness of model responses, compared to the baseline open-source models.",2025,0.6818179338843877,0.6806929345162506,0.5333333333333333,0.5,c51e3b70-c8cc-4df3-8e55-100e0ab5ed33,0,"[0.5, 0.5, 0.875]","[0.95, 0.95, 0.95]",M-Longdoc,0.6249999999999999
551328,Hydra-MDP++: Advancing End-to-End Driving via Hydra-Distillation with Expert-Guided Decision Analysis,"We introduce HydraMDP++, a novel end-to-end autonomous driving framework that integrates rule-based and neural planners by learning from human demonstrations and distilling knowledge from rule-based experts. We propose a teacher-student knowledge distillation framework with a multi-head student decoder that integrates feedback from rule-based expert teachers. The student model achieves state-of-the-art performance on the NAVSIM benchmark with a tiny image encoder. Moreover, to address limitations in existing evaluation metrics, we expand the teacher model to include traffic light compliance, lane-keeping ability, and extended comfort. This is intended to ensure a more robust decision synthesis in driving. HydraMDP++ demonstrates robust and efficient performance across diverse driving scenarios, achieving a 91.0% drive score on NAVSIM by simply scaling the image encoder. Our work contributes to developing more reliable and adaptable autonomous driving systems that combine the strengths of rule-based and neural planning approaches.",2025,0.5113634504132908,0.5051460385849429,0.5333333333333333,0.5,cd5e27a5-24da-4bbb-b9f7-b24d5e692c20,0,"[0.25, 0.5, 0.5, 0.625]","[1.0, 0.95, 0.95, 0.9]",Hydra-Distillation,0.4417498387283229
551334,NIAQUE: Neural Interpretable Any-Quantile Estimation - Towards Large Probabilistic Regression Models,"State-of-the-art computer vision and language models largely owe their success to the ability to represent massive prior knowledge contained in multiple datasets by learning over multiple tasks. However, large-scale cross-dataset studies of deep probabilistic regression models are missing, presenting a significant research gap. To bridge this gap, in this paper we propose, analyze, and evaluate a novel probabilistic regression model, capable of solving multiple regression tasks represented by different datasets. To demonstrate the feasibility of such operation and the efficacy of our model, we define a novel multi-dataset probabilistic regression benchmark LPRM-101. Our results on this benchmark imply that the proposed model is capable of solving a probabilistic regression problem jointly over multiple datasets. The model, which we call NIAQUE, learns a meaningful cross-dataset representation, scores favorably against strong tree-based baselines and Transformer and exhibits positive transfer on unseen datasets after fine-tuning.",2025,0.613636140495949,0.6107329483697133,0.6,0.5,6c2171fa-052d-4f61-9f61-bada5d0fa3f4,0,"[0.5, 0.5, 0.625, 0.625]","[0.95, 0.9, 0.8, 0.95]",NIAQUE,0.557305858594896
551337,Archon: An Architecture Search Framework for Inference-Time Techniques,"Inference-time techniques are emerging as highly effective tools to enhance large language model (LLM) capabilities. However, best practices for developing systems that combine these techniques remain underdeveloped due to our limited understanding of the utility of individual inference-time techniques and the interactions between them. Additionally, efficiently and automatically searching the space of model choices, inference-time techniques, and their compositions is challenging due to the large design space. To address these challenges, we introduce Archon, a modular framework for selecting, combining, and stacking layers of inference-time techniques to construct optimized LLM systems for target benchmarks. Rather than relying on a single LLM called once, we leverage a diverse set of LLMs and inference-time techniques, creating LLM systems greater than the sum of their parts. Archon defines an extensible design space, encompassing techniques such as generation ensembling, repeated sampling, ranking, fusion, critiquing, verification, and unit testing. It transforms the problem of building LLM systems into a hyperparameter optimization objective. Given the available LLMs, inference-time techniques, and compute budget, Archon utilizes hyperparameter search techniques to discover optimized architectures for target benchmark(s). We evaluate Archon architectures across a range of instruction-following, reasoning, and coding benchmarks, including MT-Bench, Arena-Hard-Auto, AlpacaEval 2.0, MixEval, MixEval Hard, MATH, and CodeContests. Archon architectures outperform frontier models, such as GPT-4o and Claude 3.5 Sonnet, on these benchmarks, achieving an average accuracy increase of 15.1 percentage points by using all available LLMs.",2025,0.5113634504132908,0.5100404930612885,0.5333333333333333,0.5,8fd2a597-2fa0-4c61-98a7-181d0fbe37a5,0,"[0.25, 0.5, 0.5, 0.625]","[0.9, 0.95, 0.8, 0.9]",Archon,0.4682959642261253
551345,TTVD: Towards a Geometric Framework for Test-Time Adaptation Based on Voronoi Diagram,"Deep learning models often struggle with generalization when deploying on real-world data, due to the common distributional shift to the training data. Test-time adaptation (TTA) is an emerging scheme used at inference time to address this issue. In TTA, models are adapted online at the same time when making predictions to test data. Neighbor-based approaches have gained attention recently, where prototype embeddings provide location information to alleviate the feature shift between training and testing data. However, due to their inherit limitation of simplicity, they often struggle to learn useful patterns and encounter performance degradation. To confront this challenge, we study the TTA problem from a geometric point of view. We first reveal that the underlying structure of neighbor-based methods aligns with the Voronoi Diagram, a classical computational geometry model for space partitioning. Building on this observation, we propose the Test-Time adjustment by Voronoi Diagram guidance (TTVD), a novel framework that leverages the benefits of this geometric property. Specifically, we explore two key structures: 1) Cluster-induced Voronoi Diagram (CIVD): This integrates the joint contribution of self-supervision and entropy-based methods to provide richer information. 2) Power Diagram (PD): A generalized version of the Voronoi Diagram that refines partitions by assigning weights to each Voronoi cell. Our experiments under rigid, peer-reviewed settings on CIFAR-10-C, CIFAR-100-C, ImageNet-C, and ImageNet-R shows that TTVD achieves remarkable improvements compared to state-of-the-art methods. Moreover, extensive experimental results also explore the effects of batch size and class imbalance, which are two scenarios commonly encountered in real-world applications. These analyses further validate the robustness and adaptability of our proposed framework.",2025,0.6818179338843877,0.6806929345162506,0.6666666666666666,0.625,7e59b5bd-da5e-4a8d-a95f-68e2b685d299,1,"[0.625, 0.625, 0.625, 0.625]","[0.8, 0.95, 0.95, 0.9]",Voronoi Diagram,0.625
551346,Neuroacoustic Patterns: Constant Q Cepstral Coefficients for the Classification of Neurodegenerative Disorders,"Early identification of neurodegenerative diseases is crucial for effective diagnosis in neurological disorders. However, the quasi-periodic nature of vocal tract sampling often results in inadequate spectral resolution in traditional spectral features, such as Mel Frequency Cepstral Coefficients (MFCC), thereby limiting their classification effectiveness. In this study, we propose the use of Constant Q Cepstral Coefficients (CQCC), which leverage geometrically spaced frequency bins to provide superior spectrotemporal resolution, particularly for capturing the fundamental frequency and its harmonics in speech signals associated with neurodegenerative disorders. Our results demonstrate that CQCC, when integrated with Random Forest and Support Vector Machine classifiers, significantly outperform MFCC, achieving absolute improvements of 5.6 % and 7.7 %, respectively. Furthermore, CQCC show enhanced performance over traditional acoustic measures, such as Jitter, Shimmer, and Teager Energy. The effectiveness of CQCC is underpinned by the form-invariance property of the Constant Q Transform (CQT), which ensures consistent feature representation across varying pitch and tonal conditions, thereby enhancing classification robustness. Furthermore, the robustness of CQCC features against MFCC features are validated using LDA plots. These findings are validated using the Italian Parkinson’s database and the Minsk2019 database of Amyotrophic Lateral Sclerosis, underscoring the potential of CQCC to advance the classification of neurodegenerative disorders.",2025,0.3749998636364132,0.3659829968350501,0.4,0.0,7e397825-511a-47b4-bf20-adfd7c47cfe3,0,"[0.0, 0.25, 0.5, 0.625]","[1.0, 1.0, 0.9, 0.95]",Constant Q Cepstral Coefficients,0.3019029209848227
551351,SEAT: Sparsified Enhancements for Attention Mechanisms in Time Series Transformers,"Transformer models excel in time series tasks due to their attention mechanisms. However, they often suffer from ""block-like"" attention patterns caused by high feature correlation, leading to feature confusion and reduced performance. In this study, we mathematically prove and quantify this limitation, demonstrating how it affects the sparsity of the attention matrix and hinders effective feature representation. To overcome this issue, we propose a novel, model-agnostic, and plug-and-play method called SEAT (Sparsification-Enhanced Attention Transformer) that leverages frequency domain sparsification. By transforming time series data into the frequency domain, our method induces inherent sparsity, reduces feature similarity, and mitigates block-like attention, allowing the attention mechanism to focus more precisely on relevant features. Experiments on benchmark datasets demonstrate that our approach significantly enhances the accuracy and robustness of Transformer models while maintaining computational efficiency. This provides a mathematically grounded solution to inherent flaws in attention mechanisms, offering a versatile and effective approach for advancing time series analysis.",2025,0.3863638595040511,0.3851131910666948,0.2666666666666666,0.25,289d7a79-121d-4b21-b267-52dcb2062197,0,"[0.25, 0.25, 0.25, 0.25, 0.5, 0.625]","[0.9, 0.95, 0.9, 0.95, 0.95, 0.9]",Sparsification-Enhanced Attention,0.3514896373056995
551361,ComaDICE: Offline Cooperative Multi-Agent Reinforcement Learning with Stationary Distribution Shift Regularization,"Offline reinforcement learning (RL) has garnered significant attention for its ability to learn effective policies from pre-collected datasets without the need for further environmental interactions. While promising results have been demonstrated in single-agent settings, offline multi-agent reinforcement learning (MARL) presents additional challenges due to the large joint state-action space and the complexity of multi-agent behaviors. A key issue in offline RL is the distributional shift, which arises when the target policy being optimized deviates from the behavior policy that generated the data. This problem is exacerbated in MARL due to the interdependence between agents' local policies and the expansive joint state-action space. Prior approaches have primarily addressed this challenge by incorporating regularization in the space of either Q-functions or policies. In this work, we propose a novel type of regularizer in the space of stationary distributions to address the distributional shift more effectively. Our algorithm, ComaDICE, provides a principled framework for offline cooperative MARL to correct the stationary distribution of the global policy, which is then leveraged to derive local policies for individual agents. Through extensive experiments on the offline multi-agent MuJoCo and StarCraft II benchmarks, we demonstrate that ComaDICE achieves superior performance compared to state-of-the-art offline MARL methods across nearly all tasks.",2025,0.7499997272728265,0.7487622279678756,0.6666666666666666,0.625,178c1174-117a-4505-a785-e3339e9491bd,1,"[0.625, 0.625, 0.625, 0.875]","[0.9, 1.0, 0.95, 0.95]",Stationary Distribution Shift Regularization,0.6857110261302932
551366,ETGL-DDPG: A Deep Deterministic Policy Gradient Algorithm for Sparse Reward Continuous Control,"We consider deep deterministic policy gradient (DDPG) in the context of reinforcement learning with sparse rewards. To enhance exploration, we introduce a search procedure, \emph{${\epsilon}{t}$-greedy}, which generates exploratory options for exploring less-visited states. We prove that search using $\epsilon t$-greedy has polynomial sample complexity under mild MDP assumptions. To more efficiently use the information provided by rewarded transitions, we develop a new dual experience replay buffer framework, \emph{GDRB}, and implement \emph{longest n-step returns}. The resulting algorithm, \emph{ETGL-DDPG}, integrates all three techniques: \bm{$\epsilon t$}-greedy, \textbf{G}DRB, and \textbf{L}ongest $n$-step, into DDPG. We evaluate ETGL-DDPG on standard benchmarks and demonstrate that it outperforms DDPG, as well as other state-of-the-art methods, across all tested sparse-reward continuous environments. Ablation studies further highlight how each strategy individually enhances the performance of DDPG in this setting.",2025,0.4090907603306326,0.4046343753199257,0.4,0.25,adbd4063-12c8-4158-bb5c-d6c9cdcd4e87,0,"[0.25, 0.25, 0.5, 0.5]","[0.95, 0.9, 0.8, 0.95]",ETGL-DDPG,0.3646117171897921
551395,Tool Decoding: A Plug-and-Play Approach to Enhancing Language Models for Tool Usage,"Despite the significant advancements in large language models (LLMs), their tool-use capabilities remain limited. This limitation stems from the fact that existing approaches often merely adapt strategies designed for basic natural language tasks, overlooking the specific challenges inherent in tool usage, such as precise tool selection, strict predefined formats, and accurate parameter assignment.
To bridge this gap, we conduct a fine-grained analysis of the tool usage process, breaking it down into three critical stages: tool awareness, tool selection, and tool call. Our analysis reveals that most failures stem from selection errors, format violations, and parameter mis-assignments.
Building on these insights, we propose \textbf{Tool Decoding}, a novel, training-free approach that directly incorporates tool-specific information into the decoding process. Tool Decoding employs constrained decoding to ensure format correctness and eliminate hallucinations, while leveraging order consistency to improve parameter accuracy through structured sampling and a majority-voting mechanism. This approach effectively addresses many common tool-use errors in a plug-and-play manner, allowing for seamless generalization to new tools as long as they are accompanied by well-structured documentation to guide the decoding process. 
Experimental evaluations on benchmarks like API-Bank and BFCL V2 • Live show that Tool Decoding leads to significant improvements across a diverse set of more than 10 models, including both generalist and tool-finetuned models. Almost all models demonstrate performance gains exceeding 70\% on both benchmarks. Among the 7B-level models, five outperform GPT-3.5 on key tasks, with two even surpassing GPT-4.",2025,0.6818179338843877,0.6806929345162506,0.6666666666666666,0.625,f90f91ea-fbbf-4f3b-8ccc-5a33d41e6d27,1,"[0.625, 0.625, 0.625, 0.625]","[0.8, 0.95, 0.95, 0.95]",Tool Decoding,0.625
551417,Memory retaining finetuning via distillation,"Large language models (LLMs) pretrained on large corpora of internet text possess much of the world knowledge.
Following pretraining, one often needs to conduct continued pretraining on certain capabilities such as math and coding, or ""posttraining"" (a.k.a., alignment) techniques to make the models follow users' instructions and align them with human preferences.
One challenge during these finetuning stages is that the model can lose the pretraining knowledge or forget certain capabilities (e.g., in-context learning ability).
Moreover, although there exist strong open-weight LLMs such as Llama 3, both their pretraining and posttraining data are not open to the public, making it difficult to mix the finetuning data with the models' own pretraining data as a solution for mitigating forgetting.
We propose label annealing, a method that mitigates forgetting during finetuning without requiring access to the original pretraining data.
Label annealing distills pretraining knowledge during finetuing by adding a KL divergence term in the loss function, regularizing the divergence between the finetuned model's predictions to those of the initial pretrained model.
In mathematics and code finetuning, label annealing improves the model's performance in target domains without sacrificing other capabilities of the pretrained model.
In alignment finetuning, our method introduces a smooth tradeoff between the instruction-following capability and the pretraining knowledge.
We complement our empirical investigation with a mathematical model with overparameterized linear regression that provides geometric intuition why label annealing would help.",2025,0.4999994545456529,0.4950489118715026,0.5333333333333333,0.25,ab14602b-4c36-41a8-a90c-04bd1522be2d,0,"[0.25, 0.5, 0.625]","[0.95, 0.9, 0.9]",Label Annealing,0.4396878383255143
551419,You Only Prune Once: Designing Calibration-Free Model Compression With Policy Learning,"The ever-increasing size of large language models (LLMs) presents significant challenges for deployment due to their heavy computational and memory requirements. Current model pruning techniques attempt to alleviate these issues by relying heavily on external calibration datasets to determine which parameters to prune or compress, thus limiting their flexibility and scalability across different compression ratios. Moreover, these methods often cause severe performance degradation, particularly in downstream tasks, when subjected to higher compression rates. In this paper, we propose *PruneNet*, a novel model compression method that addresses these limitations by reformulating model pruning as a policy learning process. PruneNet decouples the pruning process from the model architecture, eliminating the need for calibration datasets. It learns a stochastic pruning policy to assess parameter importance solely based on intrinsic model properties while preserving the spectral structure to minimize information loss. PruneNet can compress the LLaMA-2-7B model in just 15 minutes, achieving over 80\% retention of its zero-shot performance with a 30\% compression ratio, outperforming existing methods that retain only 75\% performance. Furthermore, on complex multitask language understanding tasks, PruneNet demonstrates its robustness by preserving up to 80\% performance of the original model, proving itself a superior alternative to conventional structured compression techniques.",2025,0.6818179338843877,0.6806929345162506,0.6666666666666666,0.625,39cee6d7-9b9a-4747-82f7-9fdc70bf9eb6,1,"[0.625, 0.625, 0.625, 0.625]","[0.9, 0.9, 0.9, 0.95]",PruneNet,0.6250000000000001
551421,MutualNeRF: Improve the Performance of NeRF under Limited Samples with Mutual Information Theory,"This paper introduces MutualNeRF, a framework enhancing Neural Radiance Field (NeRF) performance under limited samples using Mutual Information Theory. While NeRF excels in 3D scene synthesis, challenges arise with limited data and existing methods that aim to introduce prior knowledge lack theoretical support in a unified framework. We introduce a simple but theoretically robust concept, Mutual Information, as a metric to uniformly measure the correlation between images, considering both macro (semantic) and micro (pixel) levels.
  For sparse view sampling, we strategically select additional viewpoints containing more non-overlapping scene information by minimizing mutual information without knowing the ground truth images beforehand. Our framework employs a greedy algorithm, offering a near-optimal solution for this task.
  For few-shot view synthesis, we maximize the mutual information between inferred images and ground truth, expecting inferred images to gain more relevant information from known images. This is achieved by incorporating efficient, plug-and-play regularization terms.
  Experiments under limited samples show consistent improvement over state-of-the-art baselines in different settings, affirming the efficacy of our framework.",2025,0.4772725537190714,0.4764850541613754,0.5333333333333333,0.5,cebcd4be-7cca-4568-8c9e-59129798edc5,0,"[0.25, 0.5, 0.5, 0.5]","[0.95, 1.0, 0.95, 0.9]",MutualNeRF,0.4392889738697067
551466,Simultaneous Online System Identification and Control using Composite Adaptive Lyapunov-Based Deep Neural Networks,"Although deep neural network (DNN)-based controllers are popularly used to control uncertain nonlinear dynamic systems, most results use DNNs that are pretrained offline and the corresponding controller is implemented post-training. Recent advancements in adaptive control have developed controllers with Lyapunov-based update laws (i.e., control and update laws derived from a Lyapunov-based stability analysis) for updating the DNN weights online to ensure the system states track a desired trajectory. However, the update laws are based on the tracking error, and offer guarantees on only the tracking error convergence, without providing any guarantees on system identification. This paper provides the first result on simultaneous online system identification and trajectory tracking control of nonlinear systems using adaptive updates for all layers of the DNN. A combined Lyapunov-based stability analysis is provided, which guarantees that the tracking error, state-derivative estimation error, and DNN weight estimation errors are uniformly ultimately bounded. Under the persistence of excitation (PE) condition, the tracking and weight estimation errors are shown to exponentially converge to a neighborhood of the origin, where the rate of convergence and the size of this neighborhood depends on the gains and a factor quantifying PE, thus achieving system identification and enhanced trajectory tracking performance. As an outcome of the system identification, the DNN model can be propagated forward to predict and compensate for the uncertainty in dynamics under intermittent loss of state feedback. Comparative simulation results are provided on a two-link manipulator system and an unmanned underwater vehicle system with intermittent loss of state feedback, where the developed method yields significant performance improvement compared to baseline methods.",2025,0.7840906239670459,0.7846875673886005,0.8,0.875,1d2fbcca-997b-4158-9317-fba3c8ed0534,0,"[0.5, 0.625, 0.875, 0.875]","[0.95, 0.8, 0.9, 0.95]",Composite Adaptive Lyapunov-Based DNNs,0.7221919512090897
551467,Haste Makes Waste: Teaching Image Restoration to Learn Distributions from Pixels to Patterns,"In this paper, we revisit the image restoration (IR) task and propose a new training strategy that models the IR problem as a distribution mapping challenge from two perspectives, i.e., (1) the intra-pixel regression and (2) the inter-pixel interaction. At the beginning of optimization, due to the pattern distribution involving a group of pixels within a neighborhood, it is not very easy for the model to capture such multi-pixel distribution mapping. A more optimal solution would be firstly teaching the model to learn a relatively simple yet important distribution w.r.t the pixel-by-pixel mapping between the degraded/clean pixels, as warming up. By doing so, the learned distribution is served as a prior, regarded as an injection of a kind of inductive bias into the model's whole optimization procedure. Subsequently, as conventional, the model is shifted to focus on the mapping distribution of the cross-pixel patterns, which ensures the consistency and fidelity of the image patterns. The final learned mapping is a joint distribution, which transfers the knowledge from the pixel distributions to the pattern ones. Experimental results indicate that under the compact and elegant training paradigm, the newly learned joint distribution is closer to the ideal one and yields a stronger representation ability, to circumvent the dilemma of the difficulty for existing methods to learn the patterns mapping distribution between degraded/clean images right off the bat.",2025,0.4999994545456529,0.4967860402403881,0.5333333333333333,0.25,e9edf7f1-d157-4f98-b2a9-c3edc32b78e3,0,"[0.25, 0.5, 0.625]","[0.95, 1.0, 0.9]",Joint Distribution Mapping,0.4507783920542821
551473,Causal-aware Graph Neural Architecture Search under Distribution Shifts,"Graph neural architecture search (Graph NAS) has emerged as a promising approach for autonomously designing graph neural network architectures by leveraging the correlations between graphs and architectures. However, the existing methods fail to generalize under distribution shifts that are ubiquitous in real-world graph scenarios, mainly because the graph-architecture correlations they exploit might be spurious and varying across distributions. In this paper, we propose to handle the distribution shifts in the graph architecture search process by discovering and exploiting the causal relationship between graphs and architectures to search for the optimal architectures that can generalize under distribution shifts. The problem remains unexplored with the following critical challenges: 1) how to discover the causal graph-architecture relationship that has stable predictive abilities across distributions, 2) how to handle distribution shifts with the discovered causal graph-architecture relationship to search the generalized graph architectures. To address these challenges, we propose a novel approach, Causal-aware Graph Neural Architecture Search (CARNAS), which is able to capture the causal graph-architecture relationship during the architecture search process and discover the generalized graph architecture under distribution shifts. Specifically, we propose Disentangled Causal Subgraph Identification to capture the causal subgraphs that have stable prediction abilities across distributions. Then, we propose Graph Embedding Intervention to intervene on causal subgraphs within the latent space, ensuring that these subgraphs encapsulate essential features for prediction while excluding non-causal elements. Additionally, we propose Invariant Architecture Customization to reinforce the causal invariant nature of the causal subgraphs, which are utilized to tailor generalized graph architectures. Extensive experiments on synthetic and real-world datasets demonstrate that our proposed CARNAS achieves advanced out-of-distribution generalization ability by discovering the causal relationship between graphs and architectures during the search process.",2025,0.5454543471075102,0.5372932599419288,0.6,0.625,979e7b61-58f2-4637-8ac0-b318d2ab600c,0,"[0.25, 0.5, 0.625, 0.625]","[1.0, 0.95, 0.9, 0.9]",Causal-aware Graph Neural Architecture Search,0.4631547699080418
551479,Graph-Guided Scene Reconstruction from Images with 3D Gaussian Splatting,"This paper investigates an open research challenge of reconstructing high-quality, large-scale 3D open scenes from images. It is observed existing methods have various limitations, such as requiring precise camera poses for input and dense viewpoints for supervision. 
To perform effective and efficient 3D scene reconstruction, we propose a novel graph-guided 3D scene reconstruction framework, GraphGS. Specifically, given a set of images captured by RGB cameras on a scene, we first design a spatial prior-based scene structure estimation method. This is then used to create a camera graph that includes information about the camera topology. Further, we propose to apply the graph-guided multi-view consistency constraint and adaptive sampling strategy to the 3D Gaussian Splatting optimization process. This greatly alleviates the issue of Gaussian points overfitting to specific sparse viewpoints and expedites the 3D reconstruction process. We demonstrate GraphGS achieves high-fidelity 3D reconstruction from images, which presents state-of-the-art performance through quantitative and qualitative evaluation across multiple datasets.",2025,0.6818179338843877,0.6806929345162506,0.6666666666666666,0.625,a5511f57-93a8-44c3-b20d-9052d69e81cc,1,"[0.625, 0.625, 0.625]","[0.95, 0.9, 1.0]",GraphGS,0.6249999999999999
551487,Understanding Self-supervised Learning as an Approximation of Supervised Learning,"Self-supervised representation learning has mainly advanced in an empirical rather than theoretical manner. Many successful algorithms combine multiple techniques that are supported by experiments. This approach makes it difficult for the community to understand self-supervised learning fundamentally. To help settle this situation, we take a principled approach. We theoretically formulate a self-supervised learning problem as an approximation of a supervised learning problem. From the formulated problem, we derive a loss that is closely related to existing contrastive losses, thereby providing a foundation for these losses. The concepts of prototype representation bias and balanced contrastive loss are naturally introduced in the derivation, which provide insights to help understand self-supervised learning. We discuss how components of our framework align with practices of self-supervised learning algorithms, focusing on SimCLR. We also investigate the impact of balancing the attracting force between positive pairs and the repelling force between negative pairs. The proofs of our theorems are provided in the appendix, and the code to reproduce experimental results is provided in the supplementary material.",2025,0.6477270371901683,0.6425741301833405,0.5333333333333333,0.5,10da7d48-f36a-43c1-b176-1ab94ec0a369,0,"[0.5, 0.5, 0.5, 0.875]","[0.95, 0.95, 0.95, 0.9]",Balanced Contrastive Loss,0.5767709347614411
551496,Correlating and Predicting Human Evaluations of Language Models from Natural Language Processing Benchmarks,"The field of natural language processing (NLP) historically evaluated language models using benchmarks with automated metrics. However, the recent advent of highly capable chat language models (LMs) has caused a tectonic shift from NLP benchmarks to human evaluations. The relationship between these two evaluation processes is unclear and underexplored for chat LMs. Broadly, to what extent are human evaluations and NLP benchmarks correlated with one another? How well can computationally inexpensive and automated benchmarks predict expensive and time-intensive human evaluations? Which benchmarks provide predictive signals for human preference for LMs? What role, if any, should benchmarks play in the era of chat LMs? To answer these questions, we conducted a large-scale study of the relationships between human evaluations and benchmarks. We show that benchmarks are broadly highly correlated with human evaluations, and we identify which benchmarks exhibit strong correlations with human evaluations and which do not. Having established that reliable correlations exist, we fit models to predict a language model’s human evaluation scores from its academic evaluation scores and provide evidence that such predictive models can generalize across LM scales.",2025,0.5113634504132908,0.5191421744272922,0.4,0.25,54fc6269-da97-44a5-a257-4edc1b9c3093,0,"[0.25, 0.25, 0.5, 0.875]","[0.9, 0.95, 0.9, 1.0]",Human Evaluations,0.5158176890793753
551511,POMONAG: Pareto-Optimal Many-Objective Neural Architecture Generator,"Neural Architecture Search (NAS) automates the design of neural network architectures, minimising dependence on human expertise and iterative experimentation. While NAS methods are often computationally intensive and dataset-specific, employing auxiliary predictors to estimate architecture properties has proven extremely beneficial. These predictors substantially reduce the number of models requiring training, thereby decreasing overall search time. This strategy is frequently utilised to generate architectures satisfying multiple computational constraints.
Recently, Transferable Neural Architecture Search (Transferable NAS) has emerged, generalising the search process from being dataset-dependent to task-dependent. In this domain, DiffusionNAG stands as a state-of-the-art method. This diffusion-based method streamlines computation, generating architectures optimised for accuracy on unseen datasets without the need for further adaptation. However, by concentrating exclusively on accuracy, DiffusionNAG neglects other crucial objectives like model complexity, computational efficiency, and inference latency -- factors essential for deploying models in resource-constrained, real-world environments.
This paper introduces the Pareto-Optimal Many-Objective Neural Architecture Generator (POMONAG), extending DiffusionNAG through a many-objective diffusion process. POMONAG simultaneously considers accuracy, the number of parameters, multiply-accumulate operations (MACs), and inference latency. It integrates Performance Predictor models to estimate these secondary metrics and guide the diffusion gradients. POMONAG's optimisation is enhanced by expanding its training Meta-Dataset, applying Pareto Front Filtering to generated architectures, and refining embeddings for conditional generation. These enhancements enable POMONAG to generate Pareto-optimal architectures that outperform the previous state-of-the-art in both performance and efficiency.
Results were validated on two distinct search spaces -- NASBench201 and MobileNetV3 -- and evaluated across 15 image classification datasets.",2025,0.4636361950413836,0.4566316917561006,0.5333333333333333,0.25,64cc894f-dbdb-4ffd-a7d2-d62543e9fb88,0,"[0.25, 0.25, 0.5, 0.5, 0.625]","[1.0, 1.0, 0.9, 1.0, 0.9]",POMONAG,0.3967511622758469
551517,S7: Selective and Simplified State Space Layers for Sequence Modeling,"A central challenge in sequence modeling is efficiently handling tasks with extended contexts. While recent state-space models (SSMs) have made significant progress in this area, they often lack input-dependent filtering or require substantial increases in model complexity to handle input variability. We address this gap by introducing S7, a simplified yet powerful SSM that can handle input dependence while incorporating stable reparameterization and specific design choices to dynamically adjust state transitions based on input content, maintaining efficiency and performance. We prove that this reparameterization ensures stability in long-sequence modeling by keeping state transitions well-behaved over time. Additionally, it controls the gradient norm, enabling efficient training and preventing issues like exploding or vanishing gradients. S7 significantly outperforms baselines across various sequence modeling tasks, including neuromorphic event-based datasets, Long Range Arena benchmarks, and various physical and biological time series. Overall, S7 offers a more straightforward approach to sequence modeling without relying on complex, domain-specific inductive biases, achieving significant improvements across key benchmarks.",2025,0.3409089669421938,0.338600626019678,0.2666666666666666,0.25,1fcbd642-9df2-47db-a8f9-1d257a743f2a,0,"[0.25, 0.25, 0.25, 0.5]","[0.95, 1.0, 1.0, 0.95]",S7,0.3032912687585266
551536,Effective Learning with Node Perturbation in Multi-Layer Neural Networks,"Backpropagation (BP) remains the dominant and most successful method for training parameters of deep neural network models.
However, BP relies on two computationally distinct phases, does not provide a satisfactory explanation of biological learning, and can be challenging to apply for training of networks with discontinuities or noisy node dynamics.
By comparison, node perturbation (NP) proposes learning by the injection of noise into network activations, and subsequent measurement of the induced loss change. NP relies on two forward (inference) passes, does not make use of network derivatives, and has been proposed as a model for learning in biological systems.
However, standard NP is highly data inefficient and unstable due to its unguided noise-based search process.
In this work, we investigate different formulations of NP and relate it to the concept of directional derivatives as well as combining it with a decorrelating mechanism for layer-wise inputs.
We find that a closer alignment with directional derivatives together with input decorrelation at every layer strongly enhances performance of NP learning with large improvements in parameter convergence and much higher performance on the test data, approaching that of BP.
Furthermore, our novel formulation allows for application to noisy systems in which the noise process itself is inaccessible.",2025,0.5454543471075102,0.5445543476130005,0.6666666666666666,0.625,f1b4055c-ac82-4e58-b748-3bed5c23c913,0,"[0.25, 0.625, 0.625]","[0.95, 0.95, 0.95]",Node Perturbation,0.5
551545,A-Bench: Are LMMs Masters at Evaluating AI-generated Images?,"How to accurately and efficiently assess AI-generated images (AIGIs) remains a critical challenge for generative models. Given the high costs and extensive time commitments required for user studies, many researchers have turned towards employing large multi-modal models (LMMs) as AIGI evaluators, the precision and validity of which are still questionable. Furthermore, traditional benchmarks often utilize mostly natural-captured content rather than AIGIs to test the abilities of LMMs, leading to a noticeable gap for AIGIs. Therefore, we introduce **A-Bench** in this paper, a benchmark designed to diagnose *whether LMMs are masters at evaluating AIGIs*. Specifically, **A-Bench** is organized under two key principles: 1) Emphasizing both high-level semantic understanding and low-level visual quality perception to address the intricate demands of AIGIs. 2) Various generative models are utilized for AIGI creation, and various LMMs are employed for evaluation, which ensures a comprehensive validation scope. Ultimately, 2,864 AIGIs from 16 text-to-image models are sampled, each paired with question-answers annotated by human experts. We hope that **A-Bench** will significantly enhance the evaluation process and promote the generation quality for AIGIs.",2025,0.613636140495949,0.613507997325149,0.6,0.25,d8cd1264-cbfc-4a9e-ae7c-a065414c190e,1,"[0.25, 0.5, 0.625, 0.875]","[1.0, 0.9, 0.95, 1.0]",A-Bench,0.5656888300918911
551548,A case for data valuation transparency via DValCards,"Following the rise in popularity of data-centric machine learning (ML), various data valuation methods have been proposed to quantify the contribution of each datapoint to desired ML model performance metrics (e.g., accuracy). Beyond the technical applications of data valuation methods (e.g., data cleaning, data acquisition, etc.), it has been suggested that within the context of data markets, data buyers might utilize such methods to fairly compensate data owners. Here we demonstrate that data valuation metrics are inherently biased and unstable under simple algorithmic design choices, resulting in both technical and ethical implications. By analyzing 9 tabular classification datasets and 6 data valuation methods, we illustrate how (1) common and inexpensive data pre-processing techniques can drastically alter estimated data values; (2) subsampling via data valuation metrics may increase class imbalance; and (3) data valuation metrics may undervalue underrepresented group data. Consequently, we argue in favor of increased transparency associated with data valuation in-the-wild and introduce the novel Data Valuation Cards (DValCards) framework towards this aim. The proliferation of DValCards will reduce misuse of data valuation metrics, including in data pricing, and build trust in responsible ML systems.",2025,0.443181657024852,0.4410890215665304,0.4,0.25,7a05e47a-3dd4-4d27-84cc-670d41ac1265,0,"[0.25, 0.25, 0.5, 0.625]","[0.95, 0.95, 0.9, 0.95]",DValCards,0.4005903115871469
551559,Graph Fourier Neural Kernels (G-FuNK): Learning Solutions of Nonlinear Diffusive Parametric PDEs on Multiple Domains,"Understanding and predicting the time-dependent dynamics of complex systems governed by non-linear partial differential equations (PDEs), with varying parameters and domains, is a difficult problem that is motivated by applications in many fields. We introduce a novel family of neural operators based on a Graph Fourier Neural Kernel (G-FuNK), for learning solution generators of nonlinear PDEs with varying coefficients, across multiple domains, for which the highest-order term in the PDE is diffusive. G-FuNKs are constructed by combining components that are parameter- and domain-adapted, with others that are not. The latter components are learned from training data, using a variation of Fourier Neural Operators, and are transferred directly across parameters and domains. The former, parameter- and domain-adapted components are constructed as soon as a parameter and a domain on which the PDE needs to be solved are given. They are obtained by constructing a weighted graph on the (discretized) domain, with weights chosen so that the Laplacian on that weighted graph approximates the highest order, diffusive term in the generator of the PDE, which is parameter- and domain-specific, and satisfies the boundary conditions. This approach proves to be a natural way to embed geometric and directionally-dependent information about the domains, allowing for improved generalization to new test domains without need for retraining. Finally, we equip G-FuNK with an integrated ordinary differential equation (ODE) solver to enable the temporal evolution of the system's state. Our experiments demonstrate G-FuNK's ability to accurately approximate heat, reaction diffusion, and cardiac electrophysiology equations on multiple geometries and varying anisotropic diffusivity fields. We achieve low relative errors on unseen domains and fiber fields, significantly speeding up prediction capabilities compared to traditional finite-element solvers.",2025,0.5113634504132908,0.5105197008871879,0.4,0.25,9361bc48-7582-4802-90de-18a194463a4d,0,"[0.25, 0.25, 0.5, 0.875]","[0.9, 0.9, 0.9, 0.9]",G-FuNK,0.46875
551569,Toward Principled Transformers for Knowledge Tracing,"Knowledge tracing aims to reason about changes in students' knowledge and to predict students' performance in educational learning settings. We propose knowledge tracing set transformers (KTSTs), a straightforward model class for knowledge tracing prediction tasks. This model class is conceptually simpler than previous state-of-the-art approaches, which are overly complex due to domain-inspired components, and which are in part based on suboptimal design choices and flawed evaluation. In contrast, for KTSTs we propose principled set representations of student interactions and a simplified variant of learnable modification of attention matrices for positional information in a student's learning history. While being largely domain-agnostic, the proposed model class thus accounts for characteristic traits of knowledge tracing tasks. In extensive empirical experiments on standardized benchmark datasets, KTSTs establish new state-of-the-art performance.",2025,0.2727271735537551,0.2834666765412522,0.2666666666666666,0.25,8afae186-bd38-48df-af13-7e7b02f59564,0,"[0.0, 0.25, 0.25, 0.5]","[0.8, 0.95, 0.95, 0.95]",KTSTs,0.2856900067980965
551570,Low Variance: A Bottleneck in Diffusion-Based Graph Imputation,"In this paper, we tackle learning tasks on graphs with missing features, improving the applicability of graph neural networks to real-world graph-structured data. Existing imputation methods based upon graph diffusion produce channels that have nearly identical values within each channel, and these low-variance channels contribute very little to performance in graph learning tasks. To prevent diffusion-based imputation from producing low-variance channels, we introduce synthetic features that address the cause of the production, thereby increasing variance in low-variance channels. Since the synthetic features prevent diffusion-based imputation models from generating meaningless feature values shared across all nodes, our synthetic feature propagation design prevents significant performance degradation, even under extreme missing rates. Extensive experiments demonstrate the effectiveness of our scheme across various graph learning tasks with missing features, ranging from low to extremely high missing rates. Moreover, we provide empirical evidence and theoretical proof that validate the low-variance problem.",2025,0.443181657024852,0.447824069737808,0.4,0.25,eba80d7c-8b7a-4937-a199-780b85be0990,0,"[0.25, 0.25, 0.5, 0.625]","[0.9, 0.95, 0.95, 1.0]",Synthetic Feature Propagation,0.4332501612716771
551576,WenXinGPT: A Multimodal Conversational Model for Enhancing Orthopedic Expert Consultations,"Inspired by the hospital expert consultation model, this paper proposes a conversational medical visual language model for orthopedics, named WenXinGPT (Multi-disciplinary Collaboration). The core concept of this work focuses on aligning medical visual and textual representations to leverage high-quality data for generating expert consultation dialogues across hospital departments. The primary objective is to uncover orthopedic knowledge within medical intelligence models and enhance their reasoning abilities in an interpretable manner without requiring additional training. Our research particularly emphasizes zero-shot scenarios, and the results from experiments on 16 datasets provided by Peking Union Medical College Hospital demonstrate that the proposed WenXinGPT framework excels at mining and utilizing medical expertise within large language models, while also expanding their reasoning capabilities. Based on these findings, we conducted manual evaluations to identify and categorize common errors in our methods, along with ablation studies aimed at understanding the impact of various factors on overall performance.",2025,0.2727271735537551,0.2722771738065002,0.2666666666666666,0.25,c1cd8a83-dd7e-4923-802f-b624446b7eb0,0,"[0.25, 0.25, 0.25]","[0.95, 0.95, 1.0]",WenXinGPT,0.2500000000000004
551578,Unlocking Point Processes through Point Set Diffusion,"Point processes model the distribution of random point sets in mathematical spaces, such as spatial and temporal domains, with applications in fields like seismology, neuroscience, and economics.
Existing statistical and machine learning models for point processes are predominantly constrained by their reliance on the characteristic intensity function, introducing an inherent trade-off between efficiency and flexibility.
In this paper, we introduce Point Set Diffusion, a diffusion-based latent variable model that can represent arbitrary point processes on general metric spaces without relying on the intensity function.
By directly learning to stochastically interpolate between noise and data point sets, our approach effectively captures the distribution of point processes and enables efficient, parallel sampling and flexible generation for complex conditional tasks.
Experiments on synthetic and real-world datasets demonstrate that Point Set Diffusion achieves state-of-the-art performance in unconditional and conditional generation of spatial and spatiotemporal point processes while providing up to orders of magnitude faster sampling.",2025,0.8181815206612653,0.815015977224559,0.8,0.625,39b59c1f-7914-459d-ad53-d2e119e70eaf,1,"[0.625, 0.625, 0.875, 0.875]","[1.0, 0.9, 0.9, 0.95]",Point Set Diffusion,0.738894990565482
551584,HyperFace: Generating Synthetic Face Recognition Datasets by Exploring Face Embedding Hypersphere,"Face recognition datasets are often collected by crawling Internet and without individuals' consents, raising  ethical and privacy concerns. Generating synthetic datasets for training face recognition models has emerged as a promising alternative. However, the generation of synthetic datasets remains  challenging as it entails adequate inter-class and intra-class variations. While advances in generative models have made it easier to increase intra-class variations in face datasets (such as pose, illumination, etc.), generating sufficient inter-class variation is still a difficult task. In this paper, we formulate the dataset generation as a packing problem on the embedding space (represented on a hypersphere) of a face recognition model and propose a new synthetic dataset generation approach, called HyperFace. We formalize our packing problem as an optimization problem and solve it with a gradient descent-based approach. Then, we use a conditional face generator model to synthesize face images from the optimized embeddings. We use our generated datasets to train face recognition models and evaluate the trained models on several benchmarking real datasets. Our experimental results show that models trained with HyperFace achieve state-of-the-art performance in training face recognition using synthetic datasets. Project page: https://www.idiap.ch/paper/hyperface",2025,0.5795452438017296,0.5790202813821225,0.5333333333333333,0.5,1d968458-a364-4afd-8372-27008ad228a9,1,"[0.5, 0.5, 0.5, 0.625]","[0.95, 1.0, 1.0, 1.0]",HyperFace,0.5333942185514613
551592,Understanding Scale Shift in Domain Generalization for Crowd Localization,"Crowd localization plays a crucial role in visual scene understanding towards predicting each pedestrian location in a crowd, thus being applicable to various downstream tasks.
However, existing approaches suffer from significant performance degradation due to differences in head scale distributions (scale shift) between training and testing data, a challenge known as domain generalization (DG). This paper aims to comprehend the nature of scale shift within the context of domain generalization for crowd localization models.
To this end, we address three key questions: (i) how to quantify the scale shift influence on DG task, (ii) why does this influence occur, (iii) how to mitigate the influence.
Specifically, we first establish a benchmark, ScaleBench, and reproduce 20 advanced DG algorithms, to quantify the influence. 
Through extensive experiments, we demonstrate the limitations of existing algorithms and highlight the under-explored nature of this issue.
To further understand its behind reason, we provide a rigorous theoretical analysis on scale shift. 
Building on this analysis, we further propose a simple yet effective algorithm called Semantic Hook to mitigate the influence of scale shift on DG, which also serves as a case study revealing three significant insights for future research. Our results emphasize the importance of this novel and applicable research direction, which we term $\textit{Scale Shift Domain Generalization}$.",2025,0.4909089123967591,0.4878302994395447,0.5333333333333333,0.5,5addfdc4-8097-40a4-aa99-d3b8ecd6d88e,0,"[0.25, 0.5, 0.5, 0.5, 0.5]","[1.0, 0.95, 0.95, 0.95, 0.95]",Scale Shift Domain Generalization,0.4370697395989224
551615,LoCA: Location-Aware Cosine Adaptation for Parameter-Efficient Fine-Tuning,"Low-rank adaptation (LoRA) has become a prevalent method for adapting pre-trained large language models to downstream tasks. However, the simple low-rank decomposition form may constrain the optimization flexibility. To address this limitation, we introduce Location-aware Cosine Adaptation (LoCA), a novel frequency-domain parameter-efficient fine-tuning method based on inverse Discrete Cosine Transform (iDCT) with selective locations of learnable components. We begin with a comprehensive theoretical comparison between frequency-domain and low-rank decompositions for fine-tuning pre-trained large models. Our analysis reveals that frequency-domain decomposition with carefully selected frequency components can surpass the expressivity of traditional low-rank-based methods. Furthermore, we demonstrate that iDCT offers a more efficient implementation compared to inverse Discrete Fourier Transform (iDFT), allowing for better selection and tuning of frequency components while maintaining equivalent expressivity to the optimal iDFT-based adaptation. By employing finite-difference approximation to estimate gradients for discrete locations of learnable coefficients on the DCT spectrum, LoCA dynamically selects the most informative frequency components during training. Experiments on diverse language and vision fine-tuning tasks demonstrate that LoCA offers enhanced parameter efficiency while maintains computational feasibility comparable to low-rank-based methods.",2025,0.6590910330578063,0.6603353147850861,0.6666666666666666,0.625,75d20a88-0652-4373-b934-9ca1b5631802,1,"[0.5, 0.625, 0.625, 0.625, 0.625, 0.625]","[0.8, 0.9, 0.9, 0.95, 0.8, 1.0]",LoCA,0.6122131224846405
551621,Can We Further Elicit Reasoning in LLMs? Critic-Guided Planning with Retrieval-Augmentation for Solving Challenging Tasks,"State-of-the-art large language models (LLMs) exhibit impressive problem-solving capabilities but may struggle with complex reasoning and factual correctness. Existing methods harness the strengths of chain-of-thought (CoT) and retrieval-augmented generation (RAG) to decompose a complex problem into simpler steps and apply retrieval to improve factual correctness. These methods work well on straightforward reasoning tasks but often falter on challenging tasks such as competitive programming and mathematics, due to frequent reasoning errors and irrelevant knowledge retrieval. To address this, we introduce Critic-guided planning with Retrieval-augmentation, CR-Planner, a novel framework that leverages fine-tuned critic models to guide both reasoning and retrieval processes through planning.
CR-Planner solves a problem by iteratively selecting and executing sub-goals. Initially, it identifies the most promising sub-goal from reasoning, query generation, and retrieval, guided by rewards given by a critic model named sub-goal critic.  It then executes this sub-goal through sampling and selecting the optimal output based on evaluations from another critic model named execution critic.
This iterative process, informed by retrieved information and critic models, enables CR-Planner to effectively navigate the solution space towards the final answer.
We employ Monte Carlo Tree Search (MCTS) to collect the data for training the critic models, allowing for a systematic exploration of action sequences and their long-term impacts.
We validate CR-Planner on challenging domain-knowledge-intensive and reasoning-heavy tasks, including competitive programming, theorem-driven math reasoning, and complex domain retrieval problems. Our experiments demonstrate that CR-Planner significantly outperforms baselines, highlighting its effectiveness in addressing challenging problems by improving both reasoning and retrieval.",2025,0.3749998636364132,0.3716213125502351,0.2666666666666666,0.25,79816780-6252-447c-aeeb-d8a6350520ff,0,"[0.25, 0.25, 0.25, 0.625]","[0.95, 0.95, 0.9, 0.9]",CR-Planner,0.3317033678756477
551629,ND-SDF: Learning Normal Deflection Fields for High-Fidelity Indoor Reconstruction,"Neural implicit reconstruction via volume rendering has demonstrated its effectiveness in recovering dense 3D surfaces. However, it is non-trivial to simultaneously recover meticulous geometry and preserve smoothness across regions with differing characteristics. To address this issue, previous methods typically employ geometric priors, which are often constrained by the performance of the prior models. In this paper, we propose ND-SDF, which learns a Normal Deflection field to represent the angular deviation between the scene normal and the prior normal. Unlike previous methods that uniformly apply geometric priors on all samples, introducing significant bias in accuracy, our proposed normal deflection field dynamically learns and adapts the utilization of samples based on their specific characteristics, thereby improving both the accuracy and effectiveness of the model. Our method not only obtains smooth weakly textured regions such as walls and floors but also preserves the geometric details of complex structures. In addition, we introduce a novel ray sampling strategy based on the deflection angle to facilitate the unbiased rendering process, which significantly improves the quality and accuracy of intricate surfaces, especially on thin structures. Consistent improvements on various challenging datasets demonstrate the superiority of our method.",2025,0.886363314049704,0.8822488351982505,0.9333333333333332,0.875,a5511f57-93a8-44c3-b20d-9052d69e81cc,1,"[0.625, 0.875, 0.875, 0.875]","[1.0, 0.9, 0.95, 1.0]",Normal Deflection Field,0.7996325973979529
551630,Hindsight Preference Learning for Offline Preference-based Reinforcement Learning,"Offline preference-based reinforcement learning (RL), which focuses on optimizing policies using human preferences between pairs of trajectory segments selected from an offline dataset, has emerged as a practical avenue for RL applications. Existing works rely on extracting step-wise reward signals from trajectory-wise preference annotations, assuming that preferences correlate with the cumulative Markovian rewards. However, such methods fail to capture the holistic perspective of data annotation: Humans often assess the desirability of a sequence of actions by considering the overall outcome rather than the immediate rewards. To address this challenge, we propose to model human preferences using rewards conditioned on future outcomes of the trajectory segments, i.e. the hindsight information. For downstream RL optimization, the reward of each step is calculated by marginalizing over possible future outcomes, the distribution of which is approximated by a variational auto-encoder trained using the offline dataset. Our proposed method, Hindsight Preference Learning (HPL), can facilitate credit assignment by taking full advantage of vast trajectory data available in massive unlabeled datasets. Comprehensive empirical studies demonstrate the benefits of HPL in delivering robust and advantageous rewards across various domains.",2025,0.5727270644628857,0.575629886013884,0.5333333333333333,0.5,9aa28bf1-8df6-4c67-85b3-1b6c624ad215,0,"[0.25, 0.5, 0.5, 0.5, 0.875]","[0.9, 0.9, 0.9, 0.95, 0.95]",Hindsight Preference Learning,0.5421452882365453
551674,Weighted Diversified Sampling for Efficient Data-Driven Single-Cell Gene-Gene Interaction Discovery,"Gene-gene interactions play a crucial role in the manifestation of complex human diseases. Uncovering significant gene-gene interactions is a challenging task. Here, we present an innovative approach utilizing data-driven computational tools, leveraging an advanced Transformer model, to unearth noteworthy gene-gene interactions. Despite the efficacy of Transformer models, their parameter intensity presents a bottleneck in data ingestion, hindering data efficiency.  To mitigate this, we introduce a novel weighted diversified sampling algorithm. This algorithm computes the diversity score of each data sample in just two passes of the dataset, facilitating efficient subset generation for interaction discovery. Our extensive experimentation demonstrates that by sampling a mere 1% of the single-cell dataset, we achieve performance comparable to that of utilizing the entire dataset.",2025,0.2727271735537551,0.2722771738065002,0.2666666666666666,0.0,ff7d28cf-26b9-42f1-a09b-1983741f25b2,0,"[0.0, 0.25, 0.5]","[0.95, 0.8, 0.95]",Weighted Diversified Sampling,0.2500000000011425
551688,Tailoring Mixup to Data for Calibration,"Among all data augmentation techniques proposed so far, linear interpolation of training samples, also called Mixup, has found to be effective for a large panel of applications. 
  Along with improved predictive performance, Mixup is also a good technique for improving calibration.
  However, mixing data carelessly can lead to manifold mismatch, i.e., synthetic data lying outside original  class manifolds, which can deteriorate calibration.
  In this work, we show that the likelihood of assigning a wrong label with mixup increases with the distance between data to mix. 
  To this end, we propose to dynamically change the underlying distributions of interpolation coefficients 
  depending on the similarity between samples to mix, and define a flexible framework to do so without losing in diversity. We provide extensive experiments for classification and regression tasks, showing that our proposed method improves predictive performance 
  and calibration of models, while being much more efficient.",2025,0.7499997272728265,0.7487622279678756,0.6666666666666666,0.625,fb3baa7c-1255-466a-ab52-c11b4652be9d,1,"[0.625, 0.625, 0.625, 0.875]","[0.9, 0.9, 0.9, 0.9]",Manifold Mismatch,0.6875
551703,CerebroVoice: A Stereotactic EEG Dataset and Benchmark for Bilingual Brain-to-Speech Synthesis and Activity Detection,"Brain signal to speech synthesis offers a new way of speech communication, enabling innovative services and applications. With high temporal and spatial resolution, invasive brain sensing such as stereotactic electroencephalography (sEEG) becomes one of the promising solutions to decode complex brain dynamics. However, such data are hard to come by. In this paper, we introduce a bilingual brain-to-speech synthesis (CerebroVoice) dataset: the first publicly accessible sEEG recordings curated for bilingual brain-to-speech synthesis. Specifically, the CerebroVoice dataset comprises sEEG signals recorded while the speakers are reading Chinese Mandarin words, English words, and Chinese Mandarin digits. 
We establish benchmarks for two tasks on the CerebroVoice dataset: speech synthesis and voice activity detection (VAD). For the speech synthesis task, the objective is to reconstruct the speech uttered by the participants based on their sEEG recordings. We propose a novel framework, Mixture of Bilingual Synergy Experts (MoBSE), which uses a language-aware dynamic organization of low-rank expert weights to enhance the efficiency of language-specific decoding tasks. The proposed MoBSE framework achieves significant performance improvements  over current state-of-the-art methods, producing more natural and intelligible reconstructed speech. 
The VAD task aims to determine whether the speaker is actively speaking. In this benchmark, we adopt three established architectures and provide comprehensive evaluation metrics to assess their performance. Our findings indicate that low-frequency signals consistently outperform high-gamma activity across all metrics, suggesting that low-frequency filtering is more effective for VAD tasks. This finding provides valuable insights for advancing brain-computer interfaces in clinical applications. 
The CerebroVoice dataset and benchmarks are publicly available on Zenodo and GitHub for research purposes.",2025,0.5113634504132908,0.5082510874750322,0.5333333333333333,0.5,397a6037-759c-40b7-93c9-a5c9a0522e94,0,"[0.25, 0.5, 0.5, 0.625]","[0.95, 0.95, 0.95, 0.9]",CerebroVoice,0.4593171859785785
551705,DiffStroke: High-Quality Mask-free Image Manipulation with Partial Sketches,"Sketches offer a simple yet powerful way to represent object configurations, making them ideal for local image structure manipulation. Traditional methods often treat sketch-based editing as an image inpainting task, requiring both user-provided strokes and masks, which hinders the user experience. Although recent mask-free stroke-based editing methods are more convenient, they often produce significant artifacts or unintentionally modify irrelevant regions. To overcome these challenges, we propose DiffStroke, a mask-free method for high-quality image editing using only partial sketches. Trainable plug-and-play Image-Stroke Fusion (ISF) modules and an effective mask estimator are developed to address the limitations of previous conditional control diffusion models in preserving style consistency and protecting irrelevant areas. The ISF modules fuse stroke encodings with source image features as input conditions, enabling DiffStroke to control local shapes while preserving overall style consistency. The mask estimator automatically predicts masks to preserve irrelevant regions without the need for manual input. Specifically, DiffStroke blends the estimated clean latent image with the encoded source image using the predicted mask, with the mask estimator trained to minimize the error between the blended result and the latent target image. Experimental results on natural and facial images demonstrate that DiffStroke outperforms previous methods in both simple and complex stroke-based image editing tasks.",2025,0.5454543471075102,0.5445543476130005,0.5333333333333333,0.5,1545700e-0efd-4b1c-8453-84d292b035f2,0,"[0.5, 0.5, 0.5, 0.5]","[0.95, 0.95, 0.95, 0.95]",DiffStroke,0.5
551717,Simulating Human-like Daily Activities with Desire-driven Autonomy,"Desires motivate humans to interact autonomously with the complex world. In contrast, current AI agents require explicit task specifications, such as instructions or reward functions, which constrain their autonomy and behavioral diversity. In this paper, we introduce a Desire-driven Autonomous Agent (D2A) that can enable a large language model (LLM) to autonomously propose and select tasks, motivated by satisfying its multi-dimensional desires. Specifically, the motivational framework of D2A is mainly constructed by a dynamic $Value\ System$, inspired by the Theory of Needs. It incorporates an understanding of human-like desires, such as the need for social interaction, personal fulfillment, and self-care. At each step, the agent evaluates the value of its current state, proposes a set of candidate activities, and selects the one that best aligns with its intrinsic motivations. We conduct experiments on Concordia, a text-based simulator, to demonstrate that our agent generates coherent, contextually relevant daily activities while exhibiting variability and adaptability similar to human behavior. A comparative analysis with other LLM-based agents demonstrates that our approach significantly enhances the rationality of the simulated activities.",2025,0.6818179338843877,0.6806929345162506,0.6666666666666666,0.625,3a87156a-cf58-4153-88ca-5b6ac3c8e7d8,1,"[0.625, 0.625, 0.625, 0.625]","[0.8, 0.9, 0.8, 0.9]",Desire-driven Autonomous Agent,0.6249999999999999
551753,Broaden your SCOPE! Efficient Multi-turn Conversation Planning for LLMs with Semantic Space,"Large language models (LLMs) are used in chatbots or AI assistants to hold conversations with a human user. In such applications, the quality (e.g., user engagement, safety) of a conversation is important and can only be exactly known at the end of the conversation. To maximize its expected quality, conversation planning reasons about the stochastic transitions within a conversation to select the optimal LLM response at each turn. Existing simulation-based conversation planning algorithms typically select the optimal response by simulating future conversations with a large number of LLM queries at every turn. However, this process is extremely time-consuming and hence impractical for real-time conversations. This paper presents a novel approach called Semantic space COnversation Planning with improved Efficiency (SCOPE) that exploits the dense semantic representation of conversations to perform conversation planning efficiently. In particular, SCOPE models the stochastic transitions in conversation semantics and their associated rewards to plan entirely within the semantic space. This allows us to select the optimal LLM response at every conversation turn without needing additional LLM queries for simulation. As a result, SCOPE can perform conversation planning 70 times faster than conventional simulation-based planning algorithms when applied to a wide variety of conversation starters and two reward functions seen in the real world, yet achieving a higher reward within a practical planning budget. Our code can be found at: https://github.com/chenzhiliang94/convo-plan-SCOPE.",2025,0.8636364132231226,0.8622114134234825,0.9333333333333332,0.875,774c5721-0e58-491e-a426-4a645bb3e708,1,"[0.625, 0.875, 0.875]","[0.95, 0.95, 0.95]",SCOPE,0.791666666616496
551767,FDTDNet: Privacy-Preserving Lensless Object Segmentation via Feature Demultiplexing and Task Decoupling,"Camera-based vision systems pose privacy risks, whereas lensless cameras present a viable alternative by omitting visual semantics from their measurements due to the absence of lenses. However, these captured lensless measurements pose challenges for existing computer vision tasks such as object segmentation that usually require visual input. To address this problem, we propose a lensless object segmentation network via feature demultiplexing and task decoupling (FDTDNet) to perform object segmentation for lensless measurements. Specifically, we propose an optical-aware feature demultiplexing mechanism to get meaningful features from lensless measurements without visual reconstruction and design a multi-task learning framework decoupling the lensless object segmentation task into two subtasks, i.e., the reason for contour distribution maps (CDM) and body distribution maps (BDM), respectively. Extensive experiments demonstrate that our FDTDNet achieves highly accurate segmentation effect, which sheds light on privacy-preserving high-level vision with compact lensless cameras.",2025,0.7159088305786071,0.7174873826757658,0.6666666666666666,0.625,cc1d8fbc-2c23-4b76-a964-22062e603d00,0,"[0.5, 0.625, 0.625, 0.875]","[0.9, 0.95, 0.9, 0.95]",FDTDNet,0.6682966321243524
551769,KAAN: Kolmogorov-Arnold Activation Network --- a Flexible Activation Enhanced KAN,"Kolmogorov-Arnold Networks (KANs) have led to a significant breakthrough in the foundational structures of machine learning by applying the Kolmogorov-Arnold representation theorem. Through this approach, the target conditional distribution is expressed as the summation of multiple continuous univariate B-spline functions. The unique and complex computational structure of B-splines makes it hard to understand directly since the properties of each grid are not determined by its own parameters but are also influenced by the parameters of adjacent grids. Besides, it is challenging to trim and splice at components level under B-spline. To address this issue, we analyze the structural configurations of Multi-Layer Perceptrons (MLPs) and KANs, finding that MLP can be represented in a form conforming to Kolmogorov-Arnold representation Theorem (KAT). Therefore, we propose MLP style KAN framework Kolmogorov-Arnold Activation Network (KAAN), which is more straightforward, flexible and transferable. To verify the flexibility and transferability of our approach, we extend it to Convolutional Neural Network (CNN). Also, we demonstrate that parameter sharing is beneficial not only for efficiency but also for effectiveness. KAAN shows better representation capacity than MLP on several benchmarks. Furthermore, our experiment results lead us to conclude that this method is feasible for integrating modern network approaches such as CNNs.",2025,0.443181657024852,0.4438488230002331,0.4,0.25,795fbfd5-6b03-47e4-b575-dd9600cbd007,0,"[0.25, 0.25, 0.5, 0.625]","[0.9, 0.9, 0.95, 0.9]",KAAN,0.4126868770764118
551785,IPDreamer: Appearance-Controllable 3D Object Generation with Complex Image Prompts,"Recent advances in 3D generation have been remarkable, with methods such as DreamFusion leveraging large-scale text-to-image diffusion-based models to guide 3D object generation. These methods enable the synthesis of detailed and photorealistic textured objects. However, the appearance of 3D objects produced by such text-to-3D models is often unpredictable, and it is hard for single-image-to-3D methods to deal with images lacking a clear subject, complicating the generation of appearance-controllable 3D objects from complex images. To address these challenges, we present IPDreamer, a novel method that captures intricate appearance features from complex **I**mage **P**rompts and aligns the synthesized 3D object with these extracted features, enabling high-fidelity, appearance-controllable 3D object generation. Our experiments demonstrate that IPDreamer consistently generates high-quality 3D objects that align with both the textual and complex image prompts, highlighting its promising capability in appearance-controlled, complex 3D object generation.",2025,0.5454543471075102,0.5426636549180881,0.6,0.625,dce1cfd2-d6ca-41b3-9343-40609d3da9fc,1,"[0.25, 0.5, 0.625, 0.625]","[0.9, 0.95, 0.95, 0.8]",IPDreamer,0.4983102389869245
551789,RelitLRM: Generative Relightable Radiance for Large Reconstruction Models,"We propose RelitLRM, a Large Reconstruction Model (LRM) for generating high-quality Gaussian splatting representations of 3D objects under novel illuminations from sparse (4-8) posed images captured under unknown static lighting. Unlike prior inverse rendering methods requiring dense captures and slow optimization, often causing artifacts like incorrect highlights or shadow baking, RelitLRM adopts a feed-forward transformer-based model with a novel combination of a geometry reconstructor and a relightable appearance generator based on diffusion. The model is trained end-to-end on synthetic multi-view renderings of objects under varying known illuminations. This architecture design enables to effectively decompose geometry and appearance, resolve the ambiguity between material and lighting, and capture the multi-modal distribution of shadows and specularity in the relit appearance. We show our sparse-view feed-forward RelitLRM offers competitive relighting results to state-of-the-art dense-view optimization-based baselines while being significantly faster. Our project page is available at: https://relit-lrm.github.io/.",2025,0.8636364132231226,0.865452600900475,0.9333333333333332,0.875,1b08d518-e647-460c-8c90-f45da8b025d8,1,"[0.625, 0.875, 0.875]","[0.9, 0.9, 1.0]",RelitLRM,0.808197946051538
551812,Justice or Prejudice? Quantifying Biases in LLM-as-a-Judge,"LLM-as-a-Judge has been widely utilized as an evaluation method in various benchmarks and served as supervised rewards in model training. However, despite their excellence in many domains, potential issues are under-explored, undermining their reliability and the scope of their utility. 
Therefore, we identify 12 key potential biases and propose a new automated bias quantification framework—CALM—which systematically quantifies and analyzes each type of bias in LLM-as-a-Judge by using automated and principle-guided modification. Our experiments cover multiple popular language models, and the results indicate that while advanced models have achieved commendable overall performance, significant biases persist in certain specific tasks. Empirical results suggest that there remains room for improvement in the reliability of LLM-as-a-Judge. Moreover, we also discuss the explicit and implicit influence of these biases and give some suggestions for the reliable application of LLM-as-a-Judge. Our work highlights the need for stakeholders to address these issues and remind users to exercise caution in LLM-as-a-Judge applications.",2025,0.7840906239670459,0.7827968746936882,0.8,0.875,5739ee26-dd5b-449f-84a1-7c62cffc2f5d,1,"[0.5, 0.625, 0.875, 0.875]","[0.95, 0.95, 1.0, 0.9]",CALM,0.723222434674267
551820,Not All Prompts Are Made Equal: Prompt-based Pruning of Text-to-Image Diffusion Models,"Text-to-image (T2I) diffusion models have demonstrated impressive image generation capabilities. Still, their computational intensity prohibits resource-constrained organizations from deploying T2I models after fine-tuning them on their internal *target* data. While pruning techniques offer a potential solution to reduce the computational burden of T2I models, static pruning methods use the same pruned model for all input prompts, overlooking the varying capacity requirements of different prompts. Dynamic pruning addresses this issue by utilizing a separate sub-network for each prompt, but it prevents batch parallelism on GPUs. To overcome these limitations, we introduce Adaptive Prompt-Tailored Pruning (APTP), a novel prompt-based pruning method designed for T2I diffusion models. Central to our approach is a *prompt router* model, which learns to determine the required capacity for an input text prompt and routes it to an architecture code, given a total desired compute budget for prompts. Each architecture code represents a specialized model tailored to the prompts assigned to it, and the number of codes is a hyperparameter. We train the prompt router and architecture codes using contrastive learning, ensuring that similar prompts are mapped to nearby codes. Further, we employ optimal transport to prevent the codes from collapsing into a single one. We demonstrate APTP's effectiveness by pruning Stable Diffusion (SD) V2.1 using CC3M and COCO as *target* datasets. APTP outperforms the single-model pruning baselines in terms of FID, CLIP, and CMMD scores. Our analysis of the clusters learned by APTP reveals they are semantically meaningful. We also show that APTP can automatically discover previously empirically found challenging prompts for SD, *e.g.,* prompts for generating text images, assigning them to higher capacity codes.",2025,0.7159088305786071,0.7185089666318878,0.6666666666666666,0.625,13972d7f-301b-4851-b10f-1d8c7862cc23,1,"[0.5, 0.625, 0.625, 0.875]","[0.9, 0.8, 0.95, 0.95]",Adaptive Prompt-Tailored Pruning,0.6718948533982507
551829,Refining CLIP's Spatial Awareness: A Visual-Centric Perspective,"Contrastive Language-Image Pre-training (CLIP) excels in global alignment with language but exhibits limited sensitivity to spatial information, leading to strong performance in zero-shot classification tasks but underperformance in tasks requiring precise spatial understanding. Recent approaches have introduced Region-Language Alignment (RLA) to enhance CLIP's performance in dense multimodal tasks by aligning regional visual representations with corresponding text inputs. However, we find that CLIP ViTs fine-tuned with RLA suffer from notable loss in spatial awareness, which is crucial for dense prediction tasks. To address this, we propose the Spatial Correlation Distillation (SCD) framework, which preserves CLIP's inherent spatial structure and mitigates above degradation. To further enhance spatial correlations, we introduce a lightweight Refiner that extracts refined correlations directly from CLIP before feeding them into SCD, based on an intriguring finding that CLIP naturally capture high-quality dense features. Together, these components form a robust distillation framework that enables CLIP ViTs to integrate both visual-language and visual-centric improvements, achieving state-of-the-art results across various open-vocabulary dense prediction benchmarks.",2025,0.6818179338843877,0.6806929345162506,0.6666666666666666,0.625,42c1acda-3261-4a1e-9440-f62bf70eb6b0,1,"[0.625, 0.625, 0.625]","[0.95, 1.0, 0.95]",Spatial Correlation Distillation,0.6250000000000002
551832,Physics-Informed Autoencoder for Enhancing Data Quality to Improve the Forecasting Reliability of Carbon Dioxide Emissions from Agricultural Fields,"Missing values in measurements for carbon dioxide emissions on drained peatlands remains an open challenge for training forecasting techniques to achieve net zero. Existing methods struggle to model $\ce{CO_2}$ emissions to fill gaps at the field scale, especially in nighttime measurements. We propose novel Physics-Informed Autoencoders (PIAEs) for stochastic differential equations (SDEs), which combine the generative capabilities of Autoencoders with the reliability of physical models of Net Ecosystem Exchange (NEE) that quantify $\ce{CO_2}$ exchanges between the atmosphere and major carbon pools. Our method integrates an SDE describing the changes in NEE and associated uncertainties to fill gaps in the NEE measurements from eddy covariance (EC) flux towers. We define this SDE as a Wiener process with a deterministic drift term based on day and night time NEE physics models, and stochastic noise term. In the PIAE model, various sensor measurements are encoded into the latent space, and a set of deterministic decoders approximate the SDE parameters, and a probabilistic decoder predicts noise term. These are then used to predict the drift in NEE and thereby the optimal NEE forecast at the next time instance using the SDE. Finally, we use a loss function as a weighted sum of the Mean Squared Error (MSE) and Maximum Mean Discrepancy (MMD) between the measurements and the reconstructed samples and the associated noise and drift. PIAE outperforms the current state-of-the-art Random Forest Robust on predicting nighttime NEE measurements on various distribution-based and data-fitting metrics. We present a significant improvement in capturing temporal trends in the NEE at daily, weekly, monthly and quarterly scales.",2025,0.5113634504132908,0.5017676234123518,0.5333333333333333,0.5,7891c1fe-e8c6-40be-8291-a3fdba99c668,0,"[0.25, 0.5, 0.5, 0.625]","[1.0, 0.8, 0.8, 0.9]",Physics-Informed Autoencoders,0.4230818254117163
551851,NL-Eye: Abductive NLI For Images,"Will a Visual Language Model (VLM)-based bot warn us about slipping if it detects a wet floor? Recent VLMs have demonstrated impressive capabilities, yet their ability to infer outcomes and causes remains underexplored. To address this, we introduce NL-Eye, a benchmark designed to assess VLMs' visual abductive reasoning skills. NL-Eye adapts the abductive Natural Language Inference (NLI) task to the visual domain, requiring models to evaluate the plausibility of hypothesis images based on a premise image and explain their decisions. NL-Eye consists of 350 carefully curated triplet examples (1,050 images) spanning diverse reasoning categories: physical, functional, logical, emotional, cultural, and social. The data curation process involved two steps—writing textual descriptions and generating images using text-to-image models, both requiring substantial human involvement to ensure high-quality and challenging scenes. Our experiments show that VLMs struggle significantly on NL-Eye, often performing at random baseline levels, while humans excel in both plausibility prediction and explanation quality. This demonstrates a deficiency in the abductive reasoning capabilities of modern VLMs. NL-Eye represents a crucial step toward developing VLMs capable of robust multimodal reasoning for real-world applications, including accident-prevention bots and generated video verification.",2025,0.6545452165290122,0.6522682866795472,0.6666666666666666,0.625,fa4a0ae2-6368-40d8-b5c3-dcad35ede617,1,"[0.5, 0.625, 0.625, 0.625, 0.625]","[0.95, 0.8, 0.95, 0.95, 0.9]",NL-Eye,0.5957435736863887
551861,SEE: See Everything Every Time - Broader Light Range Image Enhancement via Events,"Event cameras, with a high dynamic range exceeding $120dB$, significantly outperform traditional cameras, robustly recording detailed changing information under various lighting conditions, including both low- and high-light situations.
However, recent research on utilizing event data has primarily focused on low-light image enhancement, neglecting image enhancement and brightness adjustment across a broader range of lighting conditions, such as normal or high illumination.
Based on this, we propose a novel research question: how to employ events to enhance and adjust the brightness of images captured under broader lighting conditions.
To investigate this question, we first collected a new dataset, \textbf{SEE-600K}, consisting of 610,126 images and corresponding events across 202 scenarios, each featuring an average of four lighting conditions with over a 1000-fold variation in illumination.
Subsequently, we propose a framework that effectively utilizes events to smoothly adjust image brightness through the use of prompts.
Our framework captures color through sensor patterns, uses cross-attention to model events as a brightness dictionary, and adjusts the image's dynamic range to form a broader light-range representation (BLR), which is then decoded at the pixel level based on the brightness prompt.
Experimental results demonstrate that our method not only performs well on the low-light enhancement dataset but also shows robust performance on broader light-range image enhancement using the SEE-600K dataset.
Additionally, our approach enables pixel-level brightness adjustment, providing flexibility for post-processing and inspiring more imaging applications.",2025,0.4090907603306326,0.4066481372973985,0.4,0.25,c3ed0f65-df1f-4015-9e0a-c4c1cffd66c4,0,"[0.25, 0.25, 0.5, 0.5]","[1.0, 0.95, 0.95, 0.95]",Broader Light Range,0.3650589101620029
551864,Prompt-Agnostic Erasure for Diffusion Models Using Task Vectors,"With the rapid growth of text-to-image models, a variety of techniques have been suggested to prevent undesirable image generations. Yet, these methods often only protect against specific user prompts and have been shown to allow undesirable generations with other inputs. Here we focus on \textit{unconditionally} erasing a concept from a text-to-image model rather than conditioning the erasure on the user's prompt. We first show that compared to input-dependent erasure methods, concept erasure that uses Task Vectors (TV) is more robust to unexpected user inputs, not seen during training. However, TV-based erasure can also affect the core performance of the edited model, particularly when the required edit strength is unknown. To this end, we propose a method called \textit{Diverse Inversion}, which we use to estimate the required strength of the TV edit. Diverse Inversion finds within the model input space a large set of word embeddings, each of which induces the generation of the target concept. We find that encouraging diversity in the set makes our estimation more robust to unexpected prompts. Finally, we show that Diverse Inversion enables us to apply a TV edit only to a subset of the model weights, enhancing the erasure capabilities while better maintaining model utility.",2025,0.7159088305786071,0.7151817395679724,0.6666666666666666,0.625,666b69d8-eeb3-4fee-8d54-f7cdace4c91e,0,"[0.5, 0.625, 0.625, 0.875]","[0.95, 0.9, 0.95, 0.95]",Diverse Inversion,0.6581365628042842
551874,Null Counterfactual Factor Interactions for Goal-Conditioned Reinforcement Learning,"Hindsight relabeling is a powerful tool for overcoming sparsity in goal-conditioned reinforcement learning (GCRL), especially in certain domains such as navigation and locomotion. However, hindsight relabeling can struggle in object-centric domains. For example, suppose that the goal space consists of a robotic arm pushing a particular target block to a goal location. In this case, hindsight relabeling will give high rewards to any trajectory that does not interact with the block. However, these behaviors are only useful when the object is already at the goal---an extremely rare case in practice. A dataset dominated by these kinds of trajectories can complicate learning and lead to failures. In object-centric domains, one key intuition is that meaningful trajectories are often characterized by object-object interactions such as pushing the block with the gripper. To leverage this intuition, we introduce Hindsight Relabeling using Interactions (HInt), which combines interactions with hindsight relabeling to improve the sample efficiency of downstream RL. However, interactions do not have a consensus statistical definition that is tractable for downstream GCRL. Therefore, we propose a definition of interactions based on the concept of _null counterfactual_: a cause object is interacting with a target object if, in a world where the cause object did not exist, the target object would have different transition dynamics. We leverage this definition to infer interactions in Null Counterfactual Interaction Inference (NCII), which uses a ``nulling'' operation with a learned model to simulate absences and infer interactions. We demonstrate that NCII is able to achieve significantly improved interaction inference accuracy in both simple linear dynamics domains and dynamic robotic domains in Robosuite, Robot Air Hockey, and Franka Kitchen. Furthermore, we demonstrate that HInt improves sample efficiency by up to $4\times$ in these domains as goal-conditioned tasks.",2025,0.7840906239670459,0.7837171715411542,0.8,0.875,1a8c5203-6610-409a-a4af-5c55c37abe31,1,"[0.5, 0.625, 0.875, 0.875]","[0.9, 0.95, 0.95, 0.9]",Null Counterfactual Interaction Inference,0.7227655440414508
551881,ELBOing Stein: Variational Bayes with Stein Mixture Inference,"Stein variational gradient descent (SVGD) (Liu & Wang, 2016) performs approximate Bayesian inference by representing the posterior with a set of particles.
However, SVGD suffers from variance collapse, i.e. poor predictions due to underestimating uncertainty (Ba et al., 2021), even for moderately-dimensional models
such as small Bayesian neural networks (BNNs). To address this issue, we generalize SVGD by letting each particle parameterize a component distribution in
a mixture model. Our method, Stein Mixture Inference (SMI), optimizes a lower
bound to the evidence (ELBO) and introduces user-specified guides parameterized
by particles. SMI extends the Nonlinear SVGD framework (Wang & Liu, 2019) to
the case of variational Bayes. SMI effectively avoids variance collapse, judging by
a previously described test developed for this purpose, and performs well on standard data sets. In addition, SMI requires considerably fewer particles than SVGD
to accurately estimate uncertainty for small BNNs. The synergistic combination of
NSVGD, ELBO optimization and user-specified guides establishes a promising
approach towards variational Bayesian inference in the case of tall and wide data.",2025,0.7499997272728265,0.7535553953355653,0.6666666666666666,0.625,75d03413-a6a3-4544-a465-dd467633906f,1,"[0.625, 0.625, 0.625, 0.875]","[0.8, 0.9, 0.9, 0.95]",Stein Mixture Inference,0.7071062900303199
551923,Derivative-Free Guidance in Continuous and Discrete Diffusion Models with Soft Value-Based Decoding,"Diffusion models excel at capturing the natural design spaces of images, molecules, DNA, RNA, and protein sequences. However, rather than merely generating designs that are natural, we often aim to optimize downstream reward functions while preserving the naturalness of these design spaces. Existing methods for achieving this goal often require differentiable proxy models (e.g., classifier guidance or DPS) or involve computationally expensive fine-tuning of diffusion models (e.g., classifier-free guidance, RL-based fine-tuning). In our work, we propose a new method to address these challenges. Our algorithm is an iterative sampling method that integrates soft value functions, which looks ahead to how intermediate noisy states lead to high rewards in the future, into the standard inference procedure of pre-trained diffusion models. Notably, our approach avoids fine-tuning generative models and eliminates the need to construct differentiable models. This enables us to (1) directly utilize non-differentiable features/reward feedback, commonly used in many scientific domains, and (2) apply our method to recent discrete diffusion models in a principled way. Finally, we demonstrate the effectiveness of our algorithm across several domains, including image generation, molecule generation, and DNA/RNA sequence generation.",2025,0.3818180429752571,0.3729010152671257,0.2666666666666666,0.25,0c241e69-dbec-4b0d-997e-d9ba5a56a9fe,0,"[0.25, 0.25, 0.25, 0.5, 0.5]","[0.95, 1.0, 0.95, 0.8, 0.9]",Soft Value-Based Decoding,0.3188891268858027
551940,MotifExplainer: a Motif-based Graph Neural Network Explainer,"We consider the explanation problem of Graph Neural Networks (GNNs). Most existing GNN explanation methods identify the most important edges or nodes but fail to consider substructures, which are more important for graph data. One method considering subgraphs tries to search all possible subgraphs and identifies the most significant ones. However, the subgraphs identified may not be recurrent or statistically important for interpretation. This work proposes a novel method, named MotifExplainer, to explain GNNs by identifying important motifs, which are recurrent and statistically significant patterns in graphs. Our proposed motif-based methods can provide better human-understandable explanations than methods based on nodes, edges, and regular subgraphs. Given an instance graph and a pre-trained GNN model, our method first extracts motifs in the graph using domain-specific motif extraction rules. Then, a motif embedding is encoded by feeding motifs into the pre-trained GNN. Finally, we employ an attention-based method to identify the most influential motifs as explanations for the prediction results. The empirical studies on both synthetic and real-world datasets demonstrate the effectiveness of our method.",2025,0.3409089669421938,0.3403464672581253,0.2666666666666666,0.25,df0eeceb-144e-4ecd-8d0f-c57b7cf2e4a5,0,"[0.25, 0.25, 0.25, 0.5]","[0.95, 0.95, 0.95, 0.95]",MotifExplainer,0.3125
551948,Learning a Bi-directional Driving Data Generator via Large Multi-modal Model Tuning,"Understanding human driving behaviors is crucial for developing a reliable vehicle and transportation system. Yet, data for learning these behaviors is scarce and must be carefully labeled with events, causes, and consequences. Such data may be more difficult to obtain in rare driving domains, such as in high-performance multi-car racing. While large language models (LLMs) show promise in interpreting driving behaviors, the integration of multi-modal inputs (e.g., language, trajectory, and more) and generation of multi-modal output in low-data regimes remains under-explored. In this paper, we introduce Bi-Gen: a Bi-directional Driving Data Generator, Bi-Gen is a bi-directional  multi-modal model that connects a trained encoder-decoder architecture with a pre-trained LLM, enabling both auto-annotation and generation of human driving behaviors. Our experiments show that Bi-Gen, despite its smaller size, matches the performance of much larger models like GPT-4o in annotating driving data. Additionally, Bi-Gen generates diverse, human-like driving behaviors, offering a valuable tool for synthetic data generation in resource-constrained settings. Taken together, our experiments are a significant step towards applying LLMs to complex, multi-agent driving data.",2025,0.3545453256198816,0.3548294346872406,0.2666666666666666,0.25,88a37cd6-6f3e-4584-9d1e-e0d23f9d9e9c,0,"[0.25, 0.25, 0.25, 0.25, 0.625]","[0.9, 1.0, 0.9, 0.95, 0.95]",Bi-Gen,0.326684979549921
551963,Gradient correlation is a key ingredient to accelerate SGD with momentum,"Empirically, it has been observed that adding momentum to Stochastic Gradient Descent (SGD) accelerates the convergence of the algorithm.
However, the literature has been rather pessimistic, even in the case of convex functions, about the possibility of theoretically proving this observation.
We investigate the possibility of obtaining accelerated convergence of the Stochastic Nesterov Accelerated Gradient (SNAG), a momentum-based version of SGD, when minimizing a sum of functions in a convex setting. 
We demonstrate that the average correlation between gradients allows to verify the strong growth condition, which is the key ingredient to obtain acceleration with SNAG.
Numerical experiments, both in linear regression and deep neural network optimization, confirm in practice our theoretical results.",2025,0.7499997272728265,0.7506017325541124,0.6666666666666666,0.625,c7780ae7-6ce5-4c3c-8d5b-5c2a281df2e4,1,"[0.625, 0.625, 0.625, 0.875]","[0.9, 0.9, 0.95, 0.95]",Gradient correlation,0.6955310880829015
551984,MultiMedia-Agent: A Multimodal Agent for Multimedia Content Generation,"With the advancement of AIGC (AI-generated content) technologies, an increasing number of generative models are revolutionizing fields such as video editing, music generation, and even film production. However, due to the limitations of current AIGC models, most models can only serve as individual components within specific application scenarios and are not capable of completing tasks end-to-end in real-world applications. In real-world applications, editing experts often work with a wide variety of images and video inputs, producing multimodal outputs---a video typically includes audio, text, and other elements. This level of integration across multiple modalities is something current models are unable to achieve effectively. However, the rise of agent-based systems has made it possible to use AI tools to tackle complex content generation tasks.
To deal with the complex scenarios, in this paper, we propose a multimedia content generation agent system designed to automate complex content creation. Our agent system includes a data generation pipeline, a tool library for content creation, and a set of metrics for evaluating preference alignment. Notably, we introduce the skill acquisition theory to model the training data curation and agent training. We designed a two-stage correlation strategy for plan optimization, including self-correlation and model preference correlation. 
Additionally, we utilized the generated plans to train the MultiMedia-Agent via a three stage approach including base/success plan finetune and preference optimization. The comparison results demonstrate that the our approaches are effective and the MultiMedia-Agent can generate better multimedia content compared to GPT4o.",2025,0.4090907603306326,0.4066002165148086,0.4,0.25,c5ab79ae-7b28-4177-bc0f-02a5341f4516,0,"[0.25, 0.25, 0.5, 0.5]","[0.95, 0.95, 0.95, 0.9]",MultiMedia-Agent,0.3674537487828628
552015,Projected Neural Differential Equations for Learning Constrained Dynamics,"Neural differential equations offer a powerful approach for learning dynamics from data.
  However, they do not impose known constraints that should be obeyed by the learned model.
  It is well-known that enforcing constraints in surrogate models can enhance their generalizability and numerical stability.
  In this paper, we introduce projected neural differential equations (PNDEs), a new method for constraining neural differential equations based on projection of the learned vector field to the tangent space of the constraint manifold.
  In tests on several challenging examples, including chaotic dynamical systems and state-of-the-art power grid models, PNDEs outperform existing methods while requiring fewer hyperparameters.
  The proposed approach demonstrates significant potential for enhancing the modeling of constrained dynamical systems, particularly in complex domains where accuracy and reliability are essential.",2025,0.5113634504132908,0.503354454781296,0.5333333333333333,0.5,4e384e96-117d-4ade-8fc3-f812343d6ad6,0,"[0.0, 0.5, 0.5, 0.875]","[1.0, 0.9, 0.95, 0.95]",Projected Neural Differential Equations,0.4276810256735947
552016,Conformal Structured Prediction,"Conformal prediction has recently emerged as a promising strategy for quantifying the uncertainty of a predictive model; these algorithms modify the model to output sets of labels that are guaranteed to contain the true label with high probability. However, existing conformal prediction algorithms have largely targeted classification and regression settings, where the structure of the prediction set has a simple form as a level set of the scoring function. However, for complex structured outputs such as text generation, these prediction sets might include a large number of labels and therefore be hard for users to interpret. In this paper, we propose a general framework for conformal prediction in the structured prediction setting, that modifies existing conformal prediction algorithms to output structured prediction sets that implicitly represent sets of labels. In addition, we demonstrate how our approach can be applied in domains where the prediction sets can be represented as a set of nodes in a directed acyclic graph; for instance, for hierarchical labels such as image classification, a prediction set might be a small subset of coarse labels implicitly representing the prediction set of all their more fine-descendants. We demonstrate how our algorithm can be used to construct prediction sets that satisfy a desired coverage guarantee in several domains.",2025,0.6363630413225304,0.6353130425122688,0.6666666666666666,0.625,5ac9e47a-31b3-49ea-aaff-50aeeca1b4bb,1,"[0.5, 0.625, 0.625]","[0.95, 0.9, 1.0]",Conformal Structured Prediction,0.5849085033261877
552021,HELMET: How to Evaluate Long-context Models Effectively and Thoroughly,"Many benchmarks exist for evaluating long-context language models (LCLMs), yet developers often rely on synthetic tasks such as needle-in-a-haystack (NIAH) or an arbitrary subset of tasks. However, it remains unclear whether these benchmarks reflect the diverse downstream applications of LCLMs, and such inconsistencies further complicate model comparison. We investigate the underlying reasons behind these practices and find that existing benchmarks often provide noisy signals due to limited coverage of applications, insufficient context lengths, unreliable metrics, and incompatibility with base models. In this work, we introduce HELMET (How to Evaluate Long-context Models Effectively and Thoroughly), a comprehensive benchmark encompassing seven diverse, application-centric categories. We also address several issues in previous benchmarks by adding controllable lengths up to 128K tokens, model-based evaluation for reliable metrics, and few-shot prompting for robustly evaluating base models. Consequently, we demonstrate that HELMET offers more reliable and consistent rankings of frontier LCLMs. Through a comprehensive study of 59 LCLMs, we find that (1) synthetic tasks like NIAH do not reliably predict downstream performance; (2) the diverse categories in HELMET exhibit distinct trends and low correlations with each other; and (3) while most LCLMs achieve perfect NIAH scores, open-source models significantly lag behind closed ones when tasks require full-context reasoning or following complex instructions---the gap widens as length increases. Finally, we recommend using our RAG tasks for fast model development, as they are easy to run and better predict other downstream performance; ultimately, we advocate for a holistic evaluation across diverse tasks.",2025,0.6818179338843877,0.6806929345162506,0.6666666666666666,0.625,877dc675-3680-42ac-9c97-1cef46f7471b,1,"[0.625, 0.625, 0.625, 0.625]","[1.0, 0.9, 0.95, 0.95]",HELMET,0.6250000000000001
552051,HoTPP Benchmark: Are We Good at the Long Horizon Events Forecasting?,"Accurately forecasting multiple future events within a given time horizon is crucial for applications in finance, retail, social networks, and healthcare. Event timing and labels are typically modeled using Marked Temporal Point Processes (MTPP), with evaluations often focused on next-event prediction quality. While some studies have extended evaluations to a fixed number of future events, we demonstrate that this approach leads to inaccuracies in handling false positives and false negatives. To address these issues, we propose a novel evaluation method inspired by object detection techniques from computer vision. Specifically, we introduce Temporal mean Average Precision (T-mAP), a temporal variant of mAP, which overcomes the limitations of existing long-horizon evaluation metrics. Our extensive experiments demonstrate that models with strong next-event prediction accuracy can yield poor long-horizon forecasts, and vice versa, indicating that specialized methods are needed for each task. To support further research, we release HoTPP, the first benchmark specifically designed for evaluating long-horizon MTPP predictions. HoTPP includes large-scale datasets with up to 43 million events and provides optimized procedures for both autoregressive and parallel inference, paving the way for future advancements in the field.",2025,0.4090907603306326,0.4125413044472664,0.4,0.25,beba52bd-395c-49ad-b16d-f7750a4714ce,0,"[0.25, 0.25, 0.5, 0.5]","[0.8, 0.8, 0.9, 0.8]",Temporal mean Average Precision,0.3899555375909459
552054,SurFhead: Affine Rig Blending for Geometrically Accurate 2D Gaussian Surfel Head Avatars,"Recent advancements in head avatar rendering using Gaussian primitives have achieved significantly high-fidelity results. Although precise head geometry is crucial for applications like mesh reconstruction and relighting, current methods struggle to capture intricate geometric details and render unseen poses due to their reliance on similarity transformations, which cannot handle stretch and shear transforms essential for detailed deformations of geometry. To address this, we propose SurFhead, a novel method that reconstructs riggable head geometry from RGB videos using 2D Gaussian surfels, which offer well-defined geometric properties, such as precise depth from fixed ray intersections and normals derived from their surface orientation, making them advantageous over 3D counterparts. SurFhead ensures high-fidelity rendering of both normals and images, even in extreme poses, by leveraging classical mesh-based deformation transfer and affine transformation interpolation. SurFhead introduces precise geometric deformation and blends surfels through polar decomposition of transformations, including those affecting normals. Our key contribution lies in bridging classical graphics techniques, such as mesh-based deformation, with modern Gaussian primitives, achieving state-of-the-art geometry reconstruction and rendering quality. Unlike previous avatar rendering approaches, SurFhead enables efficient reconstruction driven by Gaussian primitives while preserving high-fidelity geometry.",2025,0.7499997272728265,0.7525436133577004,0.6666666666666666,0.625,7c15245c-afaf-4af9-87c5-e42e449f0c94,1,"[0.625, 0.625, 0.625, 0.875]","[0.95, 0.8, 0.9, 0.95]",SurFhead,0.7013926632022364
552097,Long Context Compression with Activation Beacon,"Long context compression is a critical research problem due to its significance in reducing the high computational and memory costs associated with LLMs. In this paper, we propose Activation Beacon, a plug-in module for transformer-based LLMs that targets effective, efficient, and flexible compression of long contexts. To achieve this, our method introduces the following technical designs. 
1) We directly compress the activations (i.e. keys and values at every layer), rather than leveraging soft prompts to relay information (which constitute a major bottleneck to encapsulate the complex information within long contexts).
2) We tailor the compression workflow, where each fine-grained input unit is progressively compressed, enabling high-quality compression and efficient computation during both training and inference. 
3) We train the model through compression-based auto-regression, making full use of plain texts and instructional data to optimize the model's compression performance.
4) During training, we randomly sample a compression ratio at each step, teaching the model to support a wide range of compression configurations. 

Extensive evaluations are conducted on various long-context tasks whose lengths (e.g., 128K) may far exceed the maximum training length (20K), such as document understanding, few-shot learning, and Needle-in-a-Haystack. Whilst existing methods struggle to handle these challenging tasks, Activation Beacon maintains a comparable performance to the uncompressed baseline across various scenarios, 
achieving a 2x acceleration in inference time and an 8x reduction of memory costs for KV cache.",2025,0.8181815206612653,0.8113859779433708,0.8,0.625,c4327f22-4527-4c2d-816c-2c2663160678,1,"[0.625, 0.625, 0.875, 0.875]","[1.0, 0.95, 0.9, 0.9]",Activation Beacon,0.7242597793425599
552153,Open-Vocabulary Object Detection for Incomparable Spaces,"In open-vocabulary object detection (OVDet), specifying the object of interest at inference time opens up powerful possibilities, allowing users to define new categories without retraining the model. These objects can be identified through text descriptions, image examples, or a combination of both. However, visual and textual data, while complementary, encode different data types, making direct comparison or alignment challenging. Naive fusion approaches often lead to misaligned predictions, particularly when one modality is ambiguous or incomplete. In this work, we propose an approach for OVDet that aligns relational structures across these incomparable spaces, ensuring optimal correspondence between visual and textual inputs. This shift from feature fusion to relational alignment bridges the gap between these spaces, enabling robust detection even when input from one modality is weak.  Our evaluation on the challenging datasets demonstrates that our model sets a new benchmark in detecting rare objects, outperforming existing OVDet models. Additionally, we show that our multi-modal classifiers outperform single-modality models and even surpass fully-supervised detectors.",2025,0.5909092396693675,0.589108695226001,0.5333333333333333,0.5,c9035fa4-af38-45c1-ad45-d7d03b4554c6,0,"[0.5, 0.5, 0.625]","[0.9, 0.95, 0.9]",Relational Alignment,0.5379375676651029
552198,Chain-of-Action: Faithful and Multimodal Question Answering through Large Language Models,"We present a Chain-of-Action (CoA) framework for multimodal and retrieval-augmented Question-Answering (QA). Compared to the literature, CoA overcomes two major challenges of current QA applications: (i) unfaithful hallucination that is inconsistent with real-time or domain facts and (ii) weak reasoning performance over compositional information. Our key contribution is a novel reasoning-retrieval mechanism that decomposes a complex question into a reasoning chain via systematic prompting and pre-designed actions.  Methodologically, we propose three types of domain-adaptable `Plug-and-Play'  actions for retrieving real-time information from heterogeneous sources. We also propose a multi-reference faith score to verify conflicts in the answers.
In addition, our system demonstrates that detecting the knowledge boundaries of LLMs can significantly reduce both LLM interaction frequency and tokens usage in QA tasks. Empirically, we exploit both public benchmarks and a Web3 case study to demonstrate the capability of CoA over other methods.",2025,0.8181815206612653,0.8193070654837494,0.9333333333333332,0.875,1822dd00-806c-4cda-92c5-2137ff61b863,1,"[0.5, 0.875, 0.875]","[0.9, 0.95, 0.9]",Chain-of-Action,0.7611872969987442
552199,Watermark Smoothing Attacks against Language Models,"Statistical watermarking is a technique used to embed a hidden signal in the probability distribution of text generated by large language models (LLMs), enabling the attribution of the text to the originating model. We introduce the smoothing attack and show that existing statistical watermarking methods are not robust against minor modifications of text. In particular, with the help of a weaker language model, an adversary can smooth out the distribution perturbation caused by watermarks. The resulting generated text achieves comparable quality to the original (unwatermarked) model while bypassing the watermark detector. Our attack reveals a fundamental limitation of a wide range of watermarking techniques.",2025,0.443181657024852,0.4415301105880969,0.4,0.25,dc2dfa45-cde7-4eca-b482-d11a4cbb1d49,0,"[0.25, 0.25, 0.5, 0.625]","[0.9, 0.95, 0.95, 0.9]",Smoothing Attack,0.4022344559585493
552203,Chordal Graph Sampling-Based Mini-batch Training Algorithm for Large Graphs,"Graph Neural Networks (GNNs) are powerful models for learning representations of attributed graphs. To scale GNNs to large graphs, many methods use various techniques, such as sampling and decoupling, to alleviate the “neighbor explosion” problem during mini-batch training. However, these sampling-based mini-batch training methods often suffer from greater information loss than decoupling-based methods or full-batch GCNs. Besides, most original segmentation methods for large graphs usually lose a large number of edges, resulting in suboptimal performance when performing mini-batch training. Therefore, we propose a Chordal Graph Sampling-based mini-batch Training algorithm for GNNs on large-scale graph datasets, called CGST. CGST includes a balanced chordal graph partition module and a batch random aggregation module to improve performance on node classification tasks while maintaining main information of the original graph structure. Experiments on three large-scale graph datasets prove the effectiveness of CGST.",2025,0.2727271735537551,0.2722771738065002,0.2666666666666666,0.25,4ca46fed-320c-4db8-9136-d39c2de405a2,0,"[0.25, 0.25, 0.25, 0.25]","[0.95, 0.95, 0.9, 0.9]",CGST,0.25
552204,A Computation and Communication Efficient Projection-free Algorithm for Decentralized Constrained Optimization,"Decentralized constrained optimization problems arise in numerous real-world applications, where a major challenge lies in the computational complexity of projecting onto complex sets, especially in large-scale systems. 
The projection-free method, Frank-Wolfe (FW), is popular for the constrained optimization problem with complex sets due to its efficiency in tackling the projection process. 
However, when applying FW methods to decentralized constrained finite-sum optimization problems, previous studies provide suboptimal incremental first-order oracle (IFO) bounds in both convex and non-convex settings. 
In this paper, we propose a stochastic algorithm named Decentralized Variance Reduction Gradient Tracking Frank-Wolfe ($\texttt{DVRGTFW}$), which incorporates the techniques of variance reduction, gradient tracking, and multi-consensus in the FW update to obtain tight bounds. 
We present a novel convergence analysis, diverging from previous decentralized FW methods, and demonstrating $\tilde{\mathcal{O}}(n+\sqrt{\frac{n}{m}}L\varepsilon^{-1})$ and $\mathcal{O}(\sqrt{\frac{n}{m}}L^2\varepsilon^{-2})$ IFO complexity bounds in convex and non-convex settings, respectively. 
To the best of our knowledge, these bounds are the best achieved in the literature to date. Besides, in the non-convex case, $\texttt{DVRGTFW}$ achieves $\mathcal{O}(\frac{L^2\varepsilon^{-2}}{\sqrt{1-\lambda_2(W)}})$ communication complexity which is closed to the lower bound $\Omega(\frac{L\varepsilon^{-2}}{\sqrt{1-\lambda_2(W)}})$. 
Empirical results validate the convergence properties of $\texttt{DVRGTFW}$ and highlight its superior performance over other related methods.",2025,0.4999994545456529,0.4943976248717574,0.5333333333333333,0.25,c59c4aae-7055-4cff-b203-9bfdc84333af,0,"[0.25, 0.5, 0.625]","[1.0, 0.9, 0.95]",DVRGTFW,0.432197260825249
552212,AVID: Adapting Video Diffusion Models to World Models,"Large-scale generative models have achieved remarkable success in a number of domains. However, for sequential decision-making problems, such as robotics, action-labelled data is often scarce and therefore scaling-up foundation models for decision-making remains a challenge. A potential solution lies in leveraging widely-available unlabelled videos to train world models that simulate the consequences of actions. If the world model is accurate, it can be used to optimize decision-making in downstream tasks. Image-to-video diffusion models are already capable of generating highly realistic synthetic videos. However, these models are not action-conditioned, and the most powerful models are closed source which means they cannot be finetuned. In this work, we propose to adapt pretrained video diffusion models to action-conditioned world models, without access to the parameters of the pretrained model. Our approach, AVID, trains an adapter on a small domain-specific dataset of action-labelled videos. AVID uses a learnt mask to modify the intermediate outputs of the pretrained model and generate accurate action-conditioned videos. We evaluate AVID on video game and real-world robotics data, and show that it outperforms existing baselines for diffusion model adaptation. Our results demonstrate that if utilized correctly, pretrained video models have the potential to be powerful tools for embodied AI.",2025,0.6477270371901683,0.645259872225768,0.6666666666666666,0.625,7e9ede6d-fea9-433a-a60b-5f89867b4e61,0,"[0.5, 0.625, 0.625, 0.625]","[0.95, 0.9, 0.9, 0.9]",AVID,0.587313122923588
552224,Factor Graph-based Interpretable Neural Networks,"Comprehensible neural network explanations are foundations for a better understanding of decisions, especially when the input data are infused with malicious perturbations. Existing solutions generally mitigate the impact of perturbations through adversarial training, yet they fail to generate comprehensible explanations under unknown perturbations. To address this challenge, we propose AGAIN, a factor graph-based interpretable neural network, which is capable of generating comprehensible explanations under unknown perturbations. Instead of retraining like previous solutions, the proposed AGAIN directly integrates logical rules by which logical errors in explanations are identified and rectified during inference. Specifically, we construct the factor graph to express logical rules between explanations and categories. By treating logical rules as exogenous knowledge, AGAIN can identify incomprehensible explanations that violate real-world logic. Furthermore, we propose an interactive intervention switch strategy rectifying explanations based on the logical guidance from the factor graph without learning perturbations, which overcomes the inherent limitation of adversarial training-based methods in defending only against known perturbations. Additionally, we theoretically demonstrate the effectiveness of employing factor graph by proving that the comprehensibility of explanations is strongly correlated with factor graph. Extensive experiments are conducted on three datasets and experimental results illustrate the superior performance of AGAIN compared to state-of-the-art baselines.",2025,0.7090906512397632,0.7085065923749322,0.6666666666666666,0.625,89dfd646-0b1d-4da6-b4bf-8c067d61f793,1,"[0.5, 0.625, 0.625, 0.625, 0.875]","[0.95, 0.9, 0.9, 0.95, 0.95]",AGAIN,0.6525055566781167
552241,Replicate and Quantize: A Plug-and-Play Strategy for Load Balancing in Sparse Mixture-of-Experts LLMs,"While the rapid increase in the number of model parameters poses significant benefits to the development of large language models (LLMs), computational costs are also raised. In order to tackle this difficulty, the sparse mixture-of-experts(SMoE) model was introduced to tackle LLM scaling by activating a subset of experts per input. Therefore, how to leverage the knowledge of multiple experts will be an important topic. Normally, in the most extreme scenario, employing a balanced expert allocation system will result in a time-saving of $n$ times compared to utilizing only a single expert. Thus, in this paper we (1) systematically analyzed the performance and functionality of each expert. (2) Introduced a metric to fill the blank of evaluating load balance for the sparse mixture-of-experts(SMoE) model, based on the observation. (3) Proposed a dynamic plug-and-play strategy that is both trainingless and near-lossless, effectively resolving the load balancing problem, in contrast to previous works that focused on training strategies.",2025,0.5113634504132908,0.4997963366739927,0.5333333333333333,0.5,b253fdce-4476-412b-97f6-7e9f010455a3,0,"[0.25, 0.5, 0.5, 0.625]","[1.0, 0.95, 0.9, 0.8]",Load Balancing,0.4243089311825112
552269,GRIC: General Representation and Informative Content for Enhanced Out-of-Distribution Detection,"Out-of-distribution (OOD) detection is crucial for ensuring the robustness of machine learning models in open-world scenarios by identifying inputs from unknown classes. Vision-language models like CLIP have enabled zero-shot OOD detection without requiring labels or training on in-distribution (ID) data. However, current approaches are limited by their dependence on \textit{closed-set text-based labels} and \textit{full image feature representations}, constraining CLIP’s capacity to generalize across diverse labels. In this work, we propose GRIC, a novel method that improves zero-shot multi-modal OOD detection by leveraging two key insights: (1) OOD detection is driven by general ID representations rather than class-specific features, and (2) large language models (LLMs) can enrich the model’s understanding of ID data and simulate potential OOD scenarios without actual OOD samples. GRIC is simple yet highly effective, reducing the false positive rate at $95\%$ recall (FPR95) by up to $19\%$, significantly surpassing state-of-the-art methods.",2025,0.6477270371901683,0.6471004659206998,0.6666666666666666,0.625,21366401-ae03-4b0b-ba80-244b2485a54b,0,"[0.5, 0.625, 0.625, 0.625]","[0.95, 0.95, 0.95, 1.0]",GRIC,0.5962352724594994
552271,On Bits and Bandits: Quantifying the Regret-Information Trade-off,"In many sequential decision problems, an agent performs a repeated task. He then suffers regret and obtains information that he may use in the following rounds. However, sometimes the agent may also obtain information and avoid suffering regret by querying external sources. We study the trade-off between the information an agent accumulates and the regret it suffers. We invoke information-theoretic methods for obtaining regret lower bounds, that also allow us to easily re-derive several known lower bounds. We introduce the first Bayesian regret lower bounds that depend on the information an agent accumulates. We also prove regret upper bounds using the amount of information the agent accumulates. These bounds show that information measured in bits, can be traded off for regret, measured in reward. Finally, we demonstrate the utility of these bounds in improving the performance of a question-answering task with large language models, allowing us to obtain valuable insights.",2025,0.7499997272728265,0.7487622279678756,0.6666666666666666,0.625,ed3355e6-75e5-4c86-a17e-d1c93450a745,1,"[0.625, 0.625, 0.625, 0.875]","[0.9, 0.9, 0.9, 0.9]",Regret-Information Trade-off,0.6875
552276,VD3D: Taming Large Video Diffusion Transformers for 3D Camera Control,"Modern text-to-video synthesis models demonstrate coherent, photorealistic generation of complex videos from a text description. However, most existing models lack fine-grained control over camera movement, which is critical for downstream applications related to content creation, visual effects, and 3D vision. Recently, new methods demonstrate the ability to generate videos with controllable camera poses---these techniques leverage pre-trained U-Net-based diffusion models that explicitly disentangle spatial and temporal generation. Still, no existing approach enables camera control for new, transformer-based video diffusion models that process spatial and temporal information jointly. Here, we propose to tame video transformers for 3D camera control using a ControlNet-like conditioning mechanism that incorporates spatiotemporal camera embeddings based on Plucker coordinates. The approach demonstrates state-of-the-art performance for controllable video generation after fine-tuning on the RealEstate10K dataset. To the best of our knowledge, our work is the first to enable camera control for transformer-based video diffusion models.",2025,0.7090906512397632,0.7005462969215254,0.6666666666666666,0.625,a00ceaa8-acea-47b7-9868-83bac9fc8014,1,"[0.25, 0.625, 0.625, 0.875, 0.875]","[1.0, 1.0, 0.95, 0.95, 0.9]",3D Camera Control,0.6137373799035276
552283,Large Language Model Alignment via Inverse Reinforcement Learning from Demonstrations,"Aligning Large Language Models (LLMs) is crucial for enhancing their safety and utility. However, existing methods, primarily based on preference datasets, face challenges such as noisy labels, high annotation costs, and privacy concerns. 
In this work, we introduce **_Alignment from Demonstrations_** (AfD), a novel approach leveraging high-quality demonstration data to overcome these challenges. We formalize AfD within a sequential decision-making framework, highlighting its unique challenge of missing reward signals. Drawing insights from forward and inverse reinforcement learning, we introduce divergence minimization objectives for AfD.
Analytically, we elucidate the mass-covering and mode-seeking behaviors of various approaches, explaining when and why certain methods are superior.
Practically, we propose a computationally efficient algorithm that extrapolates over a tailored reward model for AfD. We validate our key insights through experiments on the Harmless and Helpful tasks, demonstrating their strong empirical performance while maintaining simplicity.",2025,0.5795452438017296,0.5781468162085512,0.5333333333333333,0.5,4c56c1c6-ca7b-4375-82b0-5a662a2fa778,0,"[0.5, 0.5, 0.5, 0.625]","[0.95, 1.0, 0.95, 0.95]",Alignment from Demonstrations,0.5287647275405007
552290,Comparing Targeting Strategies for Maximizing Social Welfare with Limited Resources,"Machine learning is increasingly used to select which individuals receive limited-resource interventions in domains such as human services, education, development, and more. However, it is often not apparent what the right quantity is for models to predict. In particular, policymakers rarely have access to data from a randomized controlled trial (RCT) that would enable accurate estimates of treatment effects -- which individuals would benefit more from the intervention. Observational data is more likely to be available, creating a substantial risk of bias in treatment effect estimates.  Practitioners instead commonly use a technique termed ""risk-based targeting"" where the model is just used to predict each individual's status quo outcome (an easier, non-causal task). Those with higher predicted risk are offered treatment. There is currently almost no empirical evidence to inform which choices lead to the most effect machine learning-informed targeting strategies in social domains. In this work, we use data from 5 real-world RCTs in a variety of domains to empirically assess such choices. We find that when treatment effects can be estimated reliably (which we simulate by using direct outcome observations), treatment effect based targeting substantially outperforms risk-based targeting, even when treatment effect estimates are biased. Moreover, these results hold even when the policymaker has strong normative preferences for assisting higher-risk individuals. However, when treatment effects must be predicted from features alone (as is always the case in practice), performance can degrade significantly due to limited data making it difficult to learn accurate mappings from features to treatment effects. Our results suggest treatment effect targeting has significant potential benefits, but realizing these benefits requires careful attention to model training and validation.",2025,0.4772725537190714,0.4737622824233104,0.6,0.625,2321aa57-fcd4-490b-b5c2-f4b07e3cdc01,1,"[0.0, 0.5, 0.625, 0.625]","[0.95, 0.95, 0.95, 0.9]",treatment effect targeting,0.426180623174294
552310,CHARGE DIRICHLET ENERGY: Geometric Perspectives on Over-smoothing in Deep Graph Neural Networks,"Over-smoothing is regarded as a key issue affecting the performance of deep Graph Neural Networks (GNNs). As the number of GNN layers increases, model performance degrades significantly, due to node embeddings converging into indistinguishable vectors. This phenomenon stems from the recursive aggregation of neighbor node representations, which impairs the distinguishability of node embeddings. From an energy perspective, this is associated with the convergence of node embeddings to a fixed point solution during the minimization of Dirichlet energy, hindering the model's ability to learn underlying geometric structures. While Graph Convolutional Networks (GCNs) have achieved success in modeling graph-structured data, there is still insufficient understanding of how the underlying geometry contributes to the trainability of deep GCNs.
In this paper, we present a novel geometric perspective to understand the poor performance of deep GCNs during training, a method called Charge Dirichlet Energy (\model). We argue that maintaining a healthy geometric structure can significantly enhance the trainability of GCNs and enable state-of-the-art performance, even in base GCN architectures. Subsequently, we analyze the importance and feasibility of learning geometric shapes, demonstrating the critical role of geometric information in training deep GNNs. Extensive empirical validation on multiple benchmark datasets shows that our method improves the geometric shape of deep base GCNs, significantly enhancing their performance and outperforming many state-of-the-art methods in competitive settings. Our contributions include not only a new approach to mitigating over-smoothing and over-compression but also comprehensive theoretical and empirical verification of the importance of geometric structures for the trainability of deep GNNs.",2025,0.3272726082645061,0.3261651829375875,0.2666666666666666,0.25,1f0727b3-3ad3-4bcd-bbeb-9515542f51a3,0,"[0.25, 0.25, 0.25, 0.25, 0.5]","[0.95, 1.0, 0.9, 1.0, 0.95]",Charge Dirichlet Energy,0.2957584804708688
552325,Self-Informed Generative Active Learning,"Active learning has been a cost-efficient approach to obtaining high-performance AI models with fewer selective annotations. In scenarios where the acquisition of original unlabeled data poses significant challenges, active learning harnessing synthesized data instances is more promising than traditional pool-based methods. In this paper, we propose the Self-Informed Generative Active Learning (SIGnAL) framework as an effective solution to actively generate and select data instances for annotation and downstream model training. In SIGnAL, we propose to guide the data generation based on a reinforcement learning policy, where the generator is self-informed by the reward to generate more informative instances. In addition, we introduce an acquisition function that measures both the informativeness and relevance of instances. Such acquisition function can be transformed to the reward seamlessly for generator optimization. Our experiments on the text classification task validate the effectiveness of our framework, especially when the original data scale is limited.",2025,0.2727271735537551,0.2722771738065002,0.2666666666666666,0.25,4ea9ef77-a39b-4e5e-bed4-047790493a1d,0,"[0.25, 0.25, 0.25, 0.25, 0.25]","[0.9, 0.95, 0.95, 0.95, 0.95]",SIGnAL,0.25
552341,LLM Compression with Convex Optimization—Part 1: Weight Quantization,"In recent years, compression of large language models (LLMs) has emerged as an important problem to enable language model deployment on resource-constrained devices, reduce computational costs, and mitigate the environmental footprint of large-scale AI infrastructure. In this paper, we lay down the foundation for LLM quantization from a convex optimization perspective and propose a quantization technique that builds on this foundation for optimum quantization outcomes. Our quantization framework, CVXQ, scales to models containing hundreds of billions of weight parameters and provides users with the flexibility to compress models to any specified model size, post-training. A reference implementation of CVXQ can be obtained from.",2025,0.2727271735537551,0.2722771738065002,0.2666666666666666,0.25,26ed76d2-f48f-4ace-84bb-39564b70294e,0,"[0.25, 0.25, 0.25, 0.25]","[0.95, 0.95, 1.0, 0.95]",CVXQ,0.2499999999999999
552345,CaLMol: Disentangled Causal Graph LLM for Molecular Relational Learning,"Molecular Relational Learning (MRL), focused on understanding interactions between molecular pairs, is essential for drug design by utilizing both structural properties and textual knowledge, such as expert documents. However, most existing MRL methods assume static molecular distributions, meaning the distributions remain consistent across training and testing stages. This assumption may lead to the exploitation of variant correlations between structures and texts regarding interactions, thereby failing in the ubiquitous scenarios involving new drug predictions. To bridge this gap, we investigate zero-shot MRL by leveraging invariant relationships between molecular texts and structures w.r.t interactions for new molecules, which is largely unexplored in the literature and is highly non-trivial with following challenges: 1) How to disentangle molecular structure components between each pair to intrinsically determine interactions and address potential structural distribution shift issues for new drugs? 2) How to align molecular structures with semantic textual information to achieve invariant molecular relation predictions for new drugs? To tackle these challenges, we propose a novel Causally Disentangled Invariant Graph Large Language Model (LLM) for Molecular Relational Learning (CaLMol), capable of exploiting invariant molecular relationships to predict interactions for new drugs. Specifically, we propose Causal Molecule Substructure Disentanglement to capture the invariant well-recognized substructure pair for a specific molecule interaction. Then, we propose Molecule Structure and Property aware LLM Alignment to use molecule (with invariant substructure)-textual property pair to align structure information to semantic information, and use them together to guide the interaction prediction. On this basis, LLM can also provide further explanations.
Extensive experiments on qualitative and quantitative tasks including 7 datasets demonstrate that our proposed CaLMol achieves advanced performance on predicting molecule interactions involving new molecules.",2025,0.3409089669421938,0.3403464672581253,0.2666666666666666,0.25,ed186ac6-b96a-4e1b-af0c-f79587e4514a,0,"[0.25, 0.25, 0.25, 0.5]","[0.9, 1.0, 0.95, 0.95]",Causal Molecule Substructure Disentanglement,0.3107110261302933
552348,LEMMA-RCA: A Large Multi-modal Multi-domain Dataset for Root Cause Analysis,"Root cause analysis (RCA) is crucial for enhancing the reliability and performance of complex systems. However, progress in this field has been hindered by the lack of large-scale, open-source datasets tailored for RCA. To bridge this gap, we introduce LEMMA-RCA, a large dataset designed for diverse RCA tasks across multiple domains and modalities. LEMMA-RCA features various real-world fault scenarios from  Information Technology (IT) and Operational Technology (OT), encompassing microservices, water distribution, and water treatment systems, with hundreds of system entities involved. We evaluate the performance of fourteen baseline methods on LEMMA-RCA across various settings, including offline and online modes, as well as single and multi-modal configurations The dataset is publicly available at https://lemma-rca.github.io/.",2025,0.5795452438017296,0.5790431526647223,0.5333333333333333,0.5,fc81bc4c-1288-402f-a1d2-e259db637907,0,"[0.5, 0.5, 0.5, 0.625]","[0.95, 0.95, 0.9, 0.95]",LEMMA-RCA,0.5331365628042843
552361,Unstable Unlearning: The Hidden Risk of Concept Resurgence in Diffusion Models,"Text-to-image diffusion models rely on massive, web-scale datasets. Training them from scratch is computationally expensive, and as a result, developers often prefer to make incremental updates to existing models. These updates often compose fine-tuning steps (to learn new concepts or improve model performance) with “unlearning” steps (to “forget” existing concepts, such as copyrighted data or the ability to generate explicit content). In this work, we demonstrate a critical and previously unknown vulnerability that arises in this paradigm: even under benign, non-adversarial conditions, fine-tuning a text-to-image diffusion model on seemingly unrelated images can cause it to “relearn” concepts that were previously “unlearned.” We comprehensively investigate the causes and scope of this phenomenon, which we term concept resurgence, by performing a series of experiments based on fine-tuning Stable Diffusion v1.4 alongside “mass concept erasure”, the current state of the art for unlearning in text-to-image diffusion models (Lu et al., 2024). Our findings underscore the fragility of composing incremental model updates, and raise new serious concerns about current approaches to ensuring the safety and alignment of text-to-image diffusion models.",2025,0.4090907603306326,0.4066002165148086,0.4,0.25,666b69d8-eeb3-4fee-8d54-f7cdace4c91e,0,"[0.25, 0.25, 0.5, 0.5]","[0.95, 0.95, 0.9, 0.95]",Concept Resurgence,0.3674537487828628
552384,Cognitive Insights and Stable Coalition Matching for Fostering Multi-Agent Cooperation,"Cognitive abilities, such as Theory of Mind (ToM), play a vital role in facilitating cooperation in human social interactions. However, Large Language Model (LLM) agents with higher ToM abilities do not necessarily exhibit better cooperative behavior compared to those with lower ToM abilities, highlighting the complexity of translating human cognitive processes to artificial intelligent agents. To address this challenge, we propose a novel matching coalition mechanism that leverages the strengths of agents with different ToM levels by explicitly considering belief alignment and specialized abilities when forming coalitions. Our proposed stable coalition formation algorithm seeks to find the team that maximizes the potential for cooperative trends and ensures long-term viability. By incorporating cognitive insights into the design of multi-agent systems, our work demonstrates the potential of leveraging ToM to create more sophisticated and human-like coordination strategies that foster cooperation and improve overall system performance.",2025,0.4772725537190714,0.4651398088832061,0.4666666666666667,0.25,35e982eb-fee3-4b2d-a08d-206c7234454b,0,"[0.25, 0.25, 0.625, 0.625]","[0.95, 0.95, 0.8, 0.9]",Coalition Formation,0.3958220103932906
552391,Faster Language Models with Better Multi-Token Prediction Using Tensor Decomposition,"We propose a new model for multi-token prediction in transformers, aiming to enhance sampling efficiency without compromising accuracy. Motivated by recent work that predicts the probabilities of subsequent tokens using multiple heads, we connect this approach to rank-1 canonical tensor decomposition. By generalizing it to a rank-r canonical probability decomposition, we develop an improved model that predicts multiple tokens simultaneously. This model can also be interpreted as a mixture of experts, allowing us to leverage successful techniques from that domain for efficient and robust training. Importantly, the overall overhead for training and sampling remains low. Our method demonstrates significant improvements in inference speed for both text and code generation tasks, proving particularly beneficial within the self-speculative decoding paradigm. It maintains its effectiveness across various model sizes and training epochs, highlighting its robustness and scalability.",2025,0.5454543471075102,0.5445543476130005,0.5333333333333333,0.5,b1820e4b-e869-4ca2-bef4-21c1d32c8e0e,0,"[0.5, 0.5, 0.5, 0.5]","[0.95, 0.9, 0.95, 0.95]",Tensor Decomposition,0.5
552416,MV-CLAM: Multi-View Molecular Interpretation with Cross-Modal Projection via Language Model,"Large language models (LLMs) have shown significant potential in the biomolecular domain, particularly by demonstrating that effective adaptation of molecular representations for LLMs can greatly improve the quality of molecular captions. Most previous works have focused on aligning unimodal molecular structures with text, overlooking the diversity of modalities. Naive approaches to aligning multi-modal molecular structures with text often lead to (1) separately aligned embeddings, (2) inconsistent textual representations, and (3) increased computational overhead. To address these challenges, we propose LLM framework MV-CLAM equipped with MQ-Former, a novel multi-querying transformer. This architecture introduces a cross-model projector facilitating the simultaneous alignment of 2D and 3D molecular representations to a unified text token. By employing a shared self-attention layer, MQ-Former preserves rich molecular embeddings across different dimensions while consolidating them into a universal molecular token. Our approach outperforms baseline models in both molecule-text retrieval and molecule captioning tasks. Additionally, our framework shows promising results for zero-shot molecule editing and molecule-related question answering. By effectively integrating multi-view molecular data into a format conducive to LLMs, our method serves as a valuable tool for enhancing the characterization and understanding of chemical structures, facilitating a more seamless transition from molecular data to textual descriptions. The source code of MV-CLAM is available in https://anonymous.4open.science/r/mv-clam-4827.",2025,0.4090907603306326,0.4066002165148086,0.4,0.25,2172e192-f7de-4644-a978-139dd3e81102,0,"[0.25, 0.25, 0.5, 0.5]","[0.95, 0.95, 0.95, 0.9]",MV-CLAM,0.3674537487828628
552428,Ensembles provably learn equivariance through data augmentation,"Recently, it was proved that group equivariance emerges in ensembles of neural networks as the result of full augmentation in the limit of infinitely wide neural networks (neural tangent kernel limit). In this paper, we extend this result significantly. We provide a proof that this emergence does not depend on the neural tangent kernel limit at all. We also consider stochastic settings, and furthermore general architectures. For the latter, we provide a simple sufficient condition on the relation between the architecture and the action of the group for our results to hold. We validate our findings through simple numeric experiments.",2025,0.5454543471075102,0.5396043485931983,0.6666666666666666,0.625,a7c4b6d0-6801-4118-a81b-f87952cba973,0,"[0.25, 0.625, 0.625]","[0.95, 0.9, 0.9]",Equivariance,0.4776254059906172
