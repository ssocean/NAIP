id,title,cites,TNCSI,TNCSI_SP,abstract,OA,is_practical,new_task,new_dataset,SOTA,is_broad,RQM,SMP,ARQ,Ref_num
81e8c72c-a2f6-4f1b-bc18-2417787fae71,Deep Surface Reconstruction from Point Clouds with Visibility Information,8,0.0358284,0.303985,"Most current neural networks for reconstructing surfaces from point clouds
ignore sensor poses and only operate on raw point locations. Sensor visibility,
however, holds meaningful information regarding space occupancy and surface
orientation. In this paper, we present two simple ways to augment raw point
clouds with visibility information, so it can directly be leveraged by surface
reconstruction networks with minimal adaptation. Our proposed modifications
consistently improve the accuracy of generated surfaces as well as the
generalization ability of the networks to unseen shape domains. Our code and
data is available at https://github.com/raphaelsulzer/dsrv-data.",1,1,0,0,0,0,0.318903,6.0,0.572264,53
0a4816bf-dba2-48e2-a644-572502ecc99b,Models and Datasets for Cross-Lingual Summarisation,39,0.379716,0.950971,"We present a cross-lingual summarisation corpus with long documents in a
source language associated with multi-sentence summaries in a target language.
The corpus covers twelve language pairs and directions for four European
languages, namely Czech, English, French and German, and the methodology for
its creation can be applied to several other languages. We derive cross-lingual
document-summary instances from Wikipedia by combining lead paragraphs and
articles' bodies from language aligned Wikipedia titles. We analyse the
proposed cross-lingual summarisation task with automatic metrics and validate
it with a human study. To illustrate the utility of our dataset we report
experiments with multi-lingual pre-trained models in supervised, zero- and
few-shot, and out-of-domain scenarios.",0,1,0,1,0,0,0.655742,7.0,0.779259,46
4dc6069d-b75f-4bd4-b14a-53cb45073003,Efficient Knowledge Distillation from Model Checkpoints,15,0.200383,0.716099,"Knowledge distillation is an effective approach to learn compact models
(students) with the supervision of large and strong models (teachers). As
empirically there exists a strong correlation between the performance of
teacher and student models, it is commonly believed that a high performing
teacher is preferred. Consequently, practitioners tend to use a well trained
network or an ensemble of them as the teacher. In this paper, we make an
intriguing observation that an intermediate model, i.e., a checkpoint in the
middle of the training procedure, often serves as a better teacher compared to
the fully converged model, although the former has much lower accuracy. More
surprisingly, a weak snapshot ensemble of several intermediate models from a
same training trajectory can outperform a strong ensemble of independently
trained and fully converged models, when they are used as teachers. We show
that this phenomenon can be partially explained by the information bottleneck
principle: the feature representations of intermediate models can have higher
mutual information regarding the input, and thus contain more ""dark knowledge""
for effective distillation. We further propose an optimal intermediate teacher
selection algorithm based on maximizing the total task-related mutual
information. Experiments verify its effectiveness and applicability.",1,0,0,0,0,0,0.62354,9.0,0.818582,60
d801e848-a2cf-4718-a58c-363b91db4ab4,Retrieval-Augmented Multilingual Keyphrase Generation with Retriever-Generator Iterative Training,11,0.3113,0.193468,"Keyphrase generation is the task of automatically predicting keyphrases given
a piece of long text. Despite its recent flourishing, keyphrase generation on
non-English languages haven't been vastly investigated. In this paper, we call
attention to a new setting named multilingual keyphrase generation and we
contribute two new datasets, EcommerceMKP and AcademicMKP, covering six
languages. Technically, we propose a retrieval-augmented method for
multilingual keyphrase generation to mitigate the data shortage problem in
non-English languages. The retrieval-augmented model leverages keyphrase
annotations in English datasets to facilitate generating keyphrases in
low-resource languages. Given a non-English passage, a cross-lingual dense
passage retrieval module finds relevant English passages. Then the associated
English keyphrases serve as external knowledge for keyphrase generation in the
current language. Moreover, we develop a retriever-generator iterative training
algorithm to mine pseudo parallel passage pairs to strengthen the cross-lingual
passage retriever. Comprehensive experiments and ablations show that the
proposed approach outperforms all baselines.",1,1,1,1,0,0,0.869172,7.0,0.8715,63
9e35788e-ff51-4ca1-8ea7-3100f479d923,Realistic Blur Synthesis for Learning Image Deblurring,26,0.228904,0.737305,"Training learning-based deblurring methods demands a tremendous amount of
blurred and sharp image pairs. Unfortunately, existing synthetic datasets are
not realistic enough, and deblurring models trained on them cannot handle real
blurred images effectively. While real datasets have recently been proposed,
they provide limited diversity of scenes and camera settings, and capturing
real datasets for diverse settings is still challenging. To resolve this, this
paper analyzes various factors that introduce differences between real and
synthetic blurred images. To this end, we present RSBlur, a novel dataset with
real blurred images and the corresponding sharp image sequences to enable a
detailed analysis of the difference between real and synthetic blur. With the
dataset, we reveal the effects of different factors in the blur generation
process. Based on the analysis, we also present a novel blur synthesis pipeline
to synthesize more realistic blur. We show that our synthesis pipeline can
improve the deblurring performance on real blurred images.",1,1,0,1,0,0,0.653579,7.0,0.778418,58
245d9f7d-c88a-45b6-9894-6c1c0bfd216b,Neural Space-filling Curves,1,0.0241371,0.159292,"We present Neural Space-filling Curves (SFCs), a data-driven approach to
infer a context-based scan order for a set of images. Linear ordering of pixels
forms the basis for many applications such as video scrambling, compression,
and auto-regressive models that are used in generative modeling for images.
Existing algorithms resort to a fixed scanning algorithm such as Raster scan or
Hilbert scan. Instead, our work learns a spatially coherent linear ordering of
pixels from the dataset of images using a graph-based neural network. The
resulting Neural SFC is optimized for an objective suitable for the downstream
task when the image is traversed along with the scan line order. We show the
advantage of using Neural SFCs in downstream applications such as image
compression. Code and additional results will be made available at
https://hywang66.github.io/publication/neuralsfc.",0,1,0,0,0,0,0.324625,22.0,0.884332,46
fb3ba4ea-94f2-4610-bde7-67f5855a62ae,Adversarial Robustness through the Lens of Convolutional Filters,13,0.0524157,0.54567,"Deep learning models are intrinsically sensitive to distribution shifts in
the input data. In particular, small, barely perceivable perturbations to the
input data can force models to make wrong predictions with high confidence. An
common defense mechanism is regularization through adversarial training which
injects worst-case perturbations back into training to strengthen the decision
boundaries, and to reduce overfitting. In this context, we perform an
investigation of 3x3 convolution filters that form in adversarially-trained
models. Filters are extracted from 71 public models of the linf-RobustBench
CIFAR-10/100 and ImageNet1k leaderboard and compared to filters extracted from
models built on the same architectures but trained without robust
regularization. We observe that adversarially-robust models appear to form more
diverse, less sparse, and more orthogonal convolution filters than their normal
counterparts. The largest differences between robust and normal models are
found in the deepest layers, and the very first convolution layer, which
consistently and predominantly forms filters that can partially eliminate
perturbations, irrespective of the architecture. Data & Project website:
https://github.com/paulgavrikov/cvpr22w_RobustnessThroughTheLens",0,0,0,0,0,0,0.504681,5.0,0.607506,70
fbbaaf8d-e935-4c93-a6e6-f7d3c7485369,Natural Logic-guided Autoregressive Multi-hop Document Retrieval for Fact Verification,7,0.0332123,0.639928,"A key component of fact verification is thevevidence retrieval, often from
multiple documents. Recent approaches use dense representations and condition
the retrieval of each document on the previously retrieved ones. The latter
step is performed over all the documents in the collection, requiring storing
their dense representations in an index, thus incurring a high memory
footprint. An alternative paradigm is retrieve-and-rerank, where documents are
retrieved using methods such as BM25, their sentences are reranked, and further
documents are retrieved conditioned on these sentences, reducing the memory
requirements. However, such approaches can be brittle as they rely on
heuristics and assume hyperlinks between documents. We propose a novel
retrieve-and-rerank method for multi-hop retrieval, that consists of a
retriever that jointly scores documents in the knowledge source and sentences
from previously retrieved documents using an autoregressive formulation and is
guided by a proof system based on natural logic that dynamically terminates the
retrieval process if the evidence is deemed sufficient. This method is
competitive with current state-of-the-art methods on FEVER, HoVer and
FEVEROUS-S, while using $5$ to $10$ times less memory than competing systems.
Evaluation on an adversarial dataset indicates improved stability of our
approach compared to commonly deployed threshold-based methods. Finally, the
proof system helps humans predict model decisions correctly more often than
using the evidence alone.",1,1,0,0,1,0,0.134727,7.0,0.493935,43
6a458e74-88a3-44d7-b48a-34e583c39b46,Trustworthy Social Bias Measurement,7,0.0781577,0.812324,"How do we design measures of social bias that we trust? While prior work has
introduced several measures, no measure has gained widespread trust: instead,
mounting evidence argues we should distrust these measures. In this work, we
design bias measures that warrant trust based on the cross-disciplinary theory
of measurement modeling. To combat the frequently fuzzy treatment of social
bias in NLP, we explicitly define social bias, grounded in principles drawn
from social science research. We operationalize our definition by proposing a
general bias measurement framework DivDist, which we use to instantiate 5
concrete bias measures. To validate our measures, we propose a rigorous testing
protocol with 8 testing criteria (e.g. predictive validity: do measures predict
biases in US employment?). Through our testing, we demonstrate considerable
evidence to trust our measures, showing they overcome conceptual, technical,
and empirical deficiencies present in prior measures.",1,0,0,0,0,1,0.67287,7.0,0.785938,134
df97a8bd-87fb-47b9-8cc9-72f694052435,Explainable Deep Belief Network based Auto encoder using novel Extended Garson Algorithm,1,0.00364907,0.0674135,"The most difficult task in machine learning is to interpret trained shallow
neural networks. Deep neural networks (DNNs) provide impressive results on a
larger number of tasks, but it is generally still unclear how decisions are
made by such a trained deep neural network. Providing feature importance is the
most important and popular interpretation technique used in shallow and deep
neural networks. In this paper, we develop an algorithm extending the idea of
Garson Algorithm to explain Deep Belief Network based Auto-encoder (DBNA). It
is used to determine the contribution of each input feature in the DBN. It can
be used for any kind of neural network with many hidden layers. The
effectiveness of this method is tested on both classification and regression
datasets taken from literature. Important features identified by this method
are compared against those obtained by Wald chi square (\c{hi}2). For 2 out of
4 classification datasets and 2 out of 5 regression datasets, our proposed
methodology resulted in the identification of better-quality features leading
to statistically more significant results vis-\`a-vis Wald \c{hi}2.",0,0,0,0,0,0,0.00511203,30.0,0.770566,48
b1ce890f-bbca-4839-9d5d-d2a662da8d75,Interpretable Multimodal Emotion Recognition using Hybrid Fusion of Speech and Image Data,9,0.214732,0.495376,"This paper proposes a multimodal emotion recognition system based on hybrid
fusion that classifies the emotions depicted by speech utterances and
corresponding images into discrete classes. A new interpretability technique
has been developed to identify the important speech & image features leading to
the prediction of particular emotion classes. The proposed system's
architecture has been determined through intensive ablation studies. It fuses
the speech & image features and then combines speech, image, and intermediate
fusion outputs. The proposed interpretability technique incorporates the divide
& conquer approach to compute shapely values denoting each speech & image
feature's importance. We have also constructed a large-scale dataset (IIT-R
SIER dataset), consisting of speech utterances, corresponding images, and class
labels, i.e., 'anger,' 'happy,' 'hate,' and 'sad.' The proposed system has
achieved 83.29% accuracy for emotion recognition. The enhanced performance of
the proposed system advocates the importance of utilizing complementary
information from multiple modalities for emotion recognition.",1,1,0,1,0,0,0.488795,10.0,0.799155,50
003e02e0-e2f3-475d-9fae-3e648f26e0ca,SAFER: Safe Collision Avoidance using Focused and Efficient Trajectory Search with Reinforcement Learning,3,0.0265224,0.446559,"Collision avoidance is key for mobile robots and agents to operate safely in
the real world. In this work we present SAFER, an efficient and effective
collision avoidance system that is able to improve safety by correcting the
control commands sent by an operator. It combines real-world reinforcement
learning (RL), search-based online trajectory planning, and automatic emergency
intervention, e.g. automatic emergency braking (AEB). The goal of the RL is to
learn an effective corrective control action that is used in a focused search
for collision-free trajectories, and to reduce the frequency of triggering
automatic emergency braking. This novel setup enables the RL policy to learn
safely and directly on mobile robots in a real-world indoor environment,
minimizing actual crashes even during training. Our real-world experiments show
that, when compared with several baselines, our approach enjoys a higher
average speed, lower crash rate, less emergency intervention, smaller
computation overhead, and smoother overall control.",0,1,0,0,0,0,0.062172,11.0,0.604058,44
2c6b167f-a2a4-4046-a8c4-b425e275d242,Iterative Scene Graph Generation,13,0.342955,0.513511,"The task of scene graph generation entails identifying object entities and
their corresponding interaction predicates in a given image (or video). Due to
the combinatorially large solution space, existing approaches to scene graph
generation assume certain factorization of the joint distribution to make the
estimation feasible (e.g., assuming that objects are conditionally independent
of predicate predictions). However, this fixed factorization is not ideal under
all scenarios (e.g., for images where an object entailed in interaction is
small and not discernible on its own). In this work, we propose a novel
framework for scene graph generation that addresses this limitation, as well as
introduces dynamic conditioning on the image, using message passing in a Markov
Random Field. This is implemented as an iterative refinement procedure wherein
each modification is conditioned on the graph generated in the previous
iteration. This conditioning across refinement steps allows joint reasoning
over entities and relations. This framework is realized via a novel and
end-to-end trainable transformer-based architecture. In addition, the proposed
framework can improve existing approach performance. Through extensive
experiments on Visual Genome and Action Genome benchmark datasets we show
improved performance on the scene graph generation.",0,0,0,0,1,0,0.95587,7.0,0.932656,72
016ae676-d168-48c1-918b-8210cbdc35af,Controllable Dynamic Multi-Task Architectures,22,0.326531,0.549198,"Multi-task learning commonly encounters competition for resources among
tasks, specifically when model capacity is limited. This challenge motivates
models which allow control over the relative importance of tasks and total
compute cost during inference time. In this work, we propose such a
controllable multi-task network that dynamically adjusts its architecture and
weights to match the desired task preference as well as the resource
constraints. In contrast to the existing dynamic multi-task approaches that
adjust only the weights within a fixed architecture, our approach affords the
flexibility to dynamically control the total computational cost and match the
user-preferred task importance better. We propose a disentangled training of
two hypernetworks, by exploiting task affinity and a novel branching
regularized loss, to take input preferences and accordingly predict
tree-structured models with adapted weights. Experiments on three multi-task
benchmarks, namely PASCAL-Context, NYU-v2, and CIFAR-100, show the efficacy of
our approach. Project page is available at https://www.nec-labs.com/~mas/DYMU.",1,0,0,0,0,0,0.921352,9.0,0.924869,60
05f108de-6f83-4f87-9e63-4607eab42a49,Improving Neural Machine Translation of Indigenous Languages with Multilingual Transfer Learning,3,0.0556965,0.271443,"Machine translation (MT) involving Indigenous languages, including those
possibly endangered, is challenging due to lack of sufficient parallel data. We
describe an approach exploiting bilingual and multilingual pretrained MT models
in a transfer learning setting to translate from Spanish to ten South American
Indigenous languages. Our models set new SOTA on five out of the ten language
pairs we consider, even doubling performance on one of these five pairs. Unlike
previous SOTA that perform data augmentation to enlarge the train sets, we
retain the low-resource setting to test the effectiveness of our models under
such a constraint. In spite of the rarity of linguistic information available
about the Indigenous languages, we offer a number of quantitative and
qualitative analyses (e.g., as to morphology, tokenization, and orthography) to
contextualize our results.",0,1,0,0,1,0,0.593179,6.0,0.714086,45
01c11285-22bc-4846-93d7-2e824e2dba8f,Latent Evolution Model for Change Point Detection in Time-varying Networks,5,0.0638488,0.337876,"Graph-based change point detection (CPD) play an irreplaceable role in
discovering anomalous graphs in the time-varying network. While several
techniques have been proposed to detect change points by identifying whether
there is a significant difference between the target network and successive
previous ones, they neglect the natural evolution of the network. In practice,
real-world graphs such as social networks, traffic networks, and rating
networks are constantly evolving over time. Considering this problem, we treat
the problem as a prediction task and propose a novel CPD method for dynamic
graphs via a latent evolution model. Our method focuses on learning the
low-dimensional representations of networks and capturing the evolving patterns
of these learned latent representations simultaneously. After having the
evolving patterns, a prediction of the target network can be achieved. Then, we
can detect the change points by comparing the prediction and the actual network
by leveraging a trade-off strategy, which balances the importance between the
prediction network and the normal graph pattern extracted from previous
networks. Intensive experiments conducted on both synthetic and real-world
datasets show the effectiveness and superiority of our model.",0,0,0,0,0,0,0.0300089,11.0,0.536318,55
29ea0493-0993-4e9e-8e0c-a1c6e6e33106,"Multi-modal Misinformation Detection: Approaches, Challenges and Opportunities",9,0.143008,0.469513,"As social media platforms are evolving from text-based forums into
multi-modal environments, the nature of misinformation in social media is also
transforming accordingly. Taking advantage of the fact that visual modalities
such as images and videos are more favorable and attractive to the users and
textual contents are sometimes skimmed carelessly, misinformation spreaders
have recently targeted contextual connections between the modalities e.g., text
and image. Hence many researchers have developed automatic techniques for
detecting possible cross-modal discordance in web-based content. We analyze,
categorize and identify existing approaches in addition to challenges and
shortcomings they face in order to unearth new research opportunities in the
field of multi-modal misinformation detection.",0,0,0,0,0,0,0.714623,4.0,0.654214,115
08831fb6-4171-40aa-b754-0130931a4cc0,WaveGAN: Frequency-aware GAN for High-Fidelity Few-shot Image Generation,26,0.194637,0.513385,"Existing few-shot image generation approaches typically employ fusion-based
strategies, either on the image or the feature level, to produce new images.
However, previous approaches struggle to synthesize high-frequency signals with
fine details, deteriorating the synthesis quality. To address this, we propose
WaveGAN, a frequency-aware model for few-shot image generation. Concretely, we
disentangle encoded features into multiple frequency components and perform
low-frequency skip connections to preserve outline and structural information.
Then we alleviate the generator's struggles of synthesizing fine details by
employing high-frequency skip connections, thus providing informative frequency
information to the generator. Moreover, we utilize a frequency L1-loss on the
generated and real images to further impede frequency information loss.
Extensive experiments demonstrate the effectiveness and advancement of our
method on three datasets. Noticeably, we achieve new state-of-the-art with FID
42.17, LPIPS 0.3868, FID 30.35, LPIPS 0.5076, and FID 4.96, LPIPS 0.3822
respectively on Flower, Animal Faces, and VGGFace. GitHub:
https://github.com/kobeshegu/ECCV2022_WaveGAN",1,0,0,0,1,0,0.682941,7.0,0.789881,46
4fc877a8-fd84-455b-be49-b09099c79b6a,Perception Prioritized Training of Diffusion Models,124,0.318531,0.903955,"Diffusion models learn to restore noisy data, which is corrupted with
different levels of noise, by optimizing the weighted sum of the corresponding
loss terms, i.e., denoising score matching loss. In this paper, we show that
restoring data corrupted with certain noise levels offers a proper pretext task
for the model to learn rich visual concepts. We propose to prioritize such
noise levels over other levels during training, by redesigning the weighting
scheme of the objective function. We show that our simple redesign of the
weighting scheme significantly improves the performance of diffusion models
regardless of the datasets, architectures, and sampling strategies.",0,0,0,0,0,0,0.870682,5.0,0.821239,55
efd69a82-4f4d-4486-8c9f-684344001294,Unsupervised Homography Estimation with Coplanarity-Aware GAN,19,0.59918,0.473362,"Estimating homography from an image pair is a fundamental problem in image
alignment. Unsupervised learning methods have received increasing attention in
this field due to their promising performance and label-free training. However,
existing methods do not explicitly consider the problem of plane-induced
parallax, which will make the predicted homography compromised on multiple
planes. In this work, we propose a novel method HomoGAN to guide unsupervised
homography estimation to focus on the dominant plane. First, a multi-scale
transformer network is designed to predict homography from the feature pyramids
of input images in a coarse-to-fine fashion. Moreover, we propose an
unsupervised GAN to impose coplanarity constraint on the predicted homography,
which is realized by using a generator to predict a mask of aligned regions,
and then a discriminator to check if two masked feature maps are induced by a
single homography. To validate the effectiveness of HomoGAN and its components,
we conduct extensive experiments on a large-scale dataset, and the results show
that our matching error is 22% lower than the previous SOTA method. Code is
available at https://github.com/megvii-research/HomoGAN.",1,1,0,0,1,0,0.948138,10.0,0.947547,43
82c5c59c-48f9-4914-aa12-514ab2137510,DisPositioNet: Disentangled Pose and Identity in Semantic Image Manipulation,2,0.0167464,0.0735788,"Graph representation of objects and their relations in a scene, known as a
scene graph, provides a precise and discernible interface to manipulate a scene
by modifying the nodes or the edges in the graph. Although existing works have
shown promising results in modifying the placement and pose of objects, scene
manipulation often leads to losing some visual characteristics like the
appearance or identity of objects. In this work, we propose DisPositioNet, a
model that learns a disentangled representation for each object for the task of
image manipulation using scene graphs in a self-supervised manner. Our
framework enables the disentanglement of the variational latent embeddings as
well as the feature representation in the graph. In addition to producing more
realistic images due to the decomposition of features like pose and identity,
our method takes advantage of the probabilistic sampling in the intermediate
features to generate more diverse images in object replacement or addition
tasks. The results of our experiments show that disentangling the feature
representations in the latent manifold of the model outperforms the previous
works qualitatively and quantitatively on two public benchmarks. Project Page:
https://scenegenie.github.io/DispositioNet/",0,1,0,0,1,0,0.372531,9.0,0.736347,65
f50d1a22-c91f-4df9-9e19-437d56cc46d7,Structural Bias for Aspect Sentiment Triplet Extraction,7,0.295195,0.8039,"Structural bias has recently been exploited for aspect sentiment triplet
extraction (ASTE) and led to improved performance. On the other hand, it is
recognized that explicitly incorporating structural bias would have a negative
impact on efficiency, whereas pretrained language models (PLMs) can already
capture implicit structures. Thus, a natural question arises: Is structural
bias still a necessity in the context of PLMs? To answer the question, we
propose to address the efficiency issues by using an adapter to integrate
structural bias in the PLM and using a cheap-to-compute relative position
structure in place of the syntactic dependency structure. Benchmarking
evaluation is conducted on the SemEval datasets. The results show that our
proposed structural adapter is beneficial to PLMs and achieves state-of-the-art
performance over a range of strong baselines, yet with a light parameter demand
and low latency. Meanwhile, we give rise to the concern that the current
evaluation default with data of small scale is under-confident. Consequently,
we release a large-scale dataset for ASTE. The results on the new dataset hint
that the structural adapter is confidently effective and efficient to a large
scale. Overall, we draw the conclusion that structural bias shall still be a
necessity even with PLMs.",1,1,0,1,1,0,0.936988,5.0,0.88148,37
a8221d15-35d9-4b59-b780-3f99341aec1c,Few-shot Metric Learning: Online Adaptation of Embedding for Retrieval,6,0.0460142,0.233086,"Metric learning aims to build a distance metric typically by learning an
effective embedding function that maps similar objects into nearby points in
its embedding space. Despite recent advances in deep metric learning, it
remains challenging for the learned metric to generalize to unseen classes with
a substantial domain gap. To tackle the issue, we explore a new problem of
few-shot metric learning that aims to adapt the embedding function to the
target domain with only a few annotated data. We introduce three few-shot
metric learning baselines and propose the Channel-Rectifier Meta-Learning
(CRML), which effectively adapts the metric space online by adjusting channels
of intermediate layers. Experimental analyses on miniImageNet, CUB-200-2011,
MPII, as well as a new dataset, miniDeepFashion, demonstrate that our method
consistently improves the learned metric by adapting it to target classes and
achieves a greater gain in image retrieval when the domain gap from the source
classes is larger.",0,0,1,1,0,0,0.497092,11.0,0.819604,46
ad85bab8-391f-4fac-a799-50853a202553,GIAOTracker: A comprehensive framework for MCMOT with global information and optimizing strategies in VisDrone 2021,60,0.336098,0.86646,"In recent years, algorithms for multiple object tracking tasks have benefited
from great progresses in deep models and video quality. However, in challenging
scenarios like drone videos, they still suffer from problems, such as small
objects, camera movements and view changes. In this paper, we propose a new
multiple object tracker, which employs Global Information And some Optimizing
strategies, named GIAOTracker. It consists of three stages, i.e., online
tracking, global link and post-processing. Given detections in every frame, the
first stage generates reliable tracklets using information of camera motion,
object motion and object appearance. Then they are associated into trajectories
by exploiting global clues and refined through four post-processing methods.
With the effectiveness of the three stages, GIAOTracker achieves
state-of-the-art performance on the VisDrone MOT dataset and wins the 3rd place
in the VisDrone2021 MOT Challenge.",0,1,0,0,1,0,0.486981,9.0,0.776251,80
022b079e-9f46-4c32-8b48-e72ea1ee06b0,Deep Vehicle Detection in Satellite Video,3,0.0345623,0.169957,"This work presents a deep learning approach for vehicle detection in
satellite video. Vehicle detection is perhaps impossible in single EO satellite
images due to the tininess of vehicles (4-10 pixel) and their similarity to the
background. Instead, we consider satellite video which overcomes the lack of
spatial information by temporal consistency of vehicle movement. A new
spatiotemporal model of a compact $3 \times 3$ convolutional, neural network is
proposed which neglects pooling layers and uses leaky ReLUs. Then we use a
reformulation of the output heatmap including Non-Maximum-Suppression (NMS) for
the final segmentation. Empirical results on two new annotated satellite videos
reconfirm the applicability of this approach for vehicle detection. They more
importantly indicate that pre-training on WAMI data and then fine-tuning on few
annotated video frames for a new video is sufficient. In our experiment only
five annotated images yield a $F_1$ score of 0.81 on a new video showing more
complex traffic patterns than the Las Vegas video. Our best result on Las Vegas
is a $F_1$ score of 0.87 which makes the proposed approach a leading method for
this benchmark.",0,1,0,0,1,0,0.136573,11.0,0.67929,69
bacc71c0-59a8-42a3-b127-c0139c41c53b,Knowledge Removal in Sampling-based Bayesian Inference,20,0.0399207,0.903963,"The right to be forgotten has been legislated in many countries, but its
enforcement in the AI industry would cause unbearable costs. When single data
deletion requests come, companies may need to delete the whole models learned
with massive resources. Existing works propose methods to remove knowledge
learned from data for explicitly parameterized models, which however are not
appliable to the sampling-based Bayesian inference, i.e., Markov chain Monte
Carlo (MCMC), as MCMC can only infer implicit distributions. In this paper, we
propose the first machine unlearning algorithm for MCMC. We first convert the
MCMC unlearning problem into an explicit optimization problem. Based on this
problem conversion, an {\it MCMC influence function} is designed to provably
characterize the learned knowledge from data, which then delivers the MCMC
unlearning algorithm. Theoretical analysis shows that MCMC unlearning would not
compromise the generalizability of the MCMC models. Experiments on Gaussian
mixture models and Bayesian neural networks confirm the effectiveness of the
proposed algorithm. The code is available at
\url{https://github.com/fshp971/mcmc-unlearning}.",1,0,1,0,0,0,0.0339591,12.0,0.585432,57
0ea03d3a-43d1-4906-8fae-b5468a31b3da,A knowledge graph representation learning approach to predict novel kinase-substrate interactions,3,0.0682009,0.479691,"The human proteome contains a vast network of interacting kinases and
substrates. Even though some kinases have proven to be immensely useful as
therapeutic targets, a majority are still understudied. In this work, we
present a novel knowledge graph representation learning approach to predict
novel interaction partners for understudied kinases. Our approach uses a
phosphoproteomic knowledge graph constructed by integrating data from iPTMnet,
Protein Ontology, Gene Ontology and BioKG. The representation of kinases and
substrates in this knowledge graph are learned by performing directed random
walks on triples coupled with a modified SkipGram or CBOW model. These
representations are then used as an input to a supervised classification model
to predict novel interactions for understudied kinases. We also present a
post-predictive analysis of the predicted interactions and an ablation study of
the phosphoproteomic knowledge graph to gain an insight into the biology of the
understudied kinases.",1,0,0,0,0,0,0.846957,12.0,0.918355,36
302f9631-01e5-4395-93dd-f55a8b23f6d8,Rethinking the Role of Scale for In-Context Learning: An Interpretability-based Case Study at 66 Billion Scale,24,0.0526477,0.763926,"Language models have been shown to perform better with an increase in scale
on a wide variety of tasks via the in-context learning paradigm. In this paper,
we investigate the hypothesis that the ability of a large language model to
in-context learn-perform a task is not uniformly spread across all of its
underlying components. Using a 66 billion parameter language model (OPT-66B)
across a diverse set of 14 downstream tasks, we find this is indeed the case:
$\sim$70% of attention heads and $\sim$20% of feed forward networks can be
removed with minimal decline in task performance. We find substantial overlap
in the set of attention heads (un)important for in-context learning across
tasks and number of in-context examples. We also address our hypothesis through
a task-agnostic lens, finding that a small set of attention heads in OPT-66B
score highly on their ability to perform primitive induction operations
associated with in-context learning, namely, prefix matching and copying. These
induction heads overlap with task-specific important heads, reinforcing
arguments by Olsson et al. (arXiv:2209.11895) regarding induction head
generality to more sophisticated behaviors associated with in-context learning.
Overall, our study provides several insights that indicate large language
models may be under-trained for in-context learning and opens up questions on
how to pre-train language models to more effectively perform in-context
learning.",1,0,0,0,0,0,0.740619,4.0,0.672566,50
80ff701b-bc14-4cbd-b835-bde928341c30,"DiRA: Discriminative, Restorative, and Adversarial Learning for Self-supervised Medical Image Analysis",45,0.69049,0.971302,"Discriminative learning, restorative learning, and adversarial learning have
proven beneficial for self-supervised learning schemes in computer vision and
medical imaging. Existing efforts, however, omit their synergistic effects on
each other in a ternary setup, which, we envision, can significantly benefit
deep semantic representation learning. To realize this vision, we have
developed DiRA, the first framework that unites discriminative, restorative,
and adversarial learning in a unified manner to collaboratively glean
complementary visual information from unlabeled medical images for fine-grained
semantic representation learning. Our extensive experiments demonstrate that
DiRA (1) encourages collaborative learning among three learning ingredients,
resulting in more generalizable representation across organs, diseases, and
modalities; (2) outperforms fully supervised ImageNet models and increases
robustness in small data regimes, reducing annotation cost across multiple
medical imaging applications; (3) learns fine-grained semantic representation,
facilitating accurate lesion localization with only image-level annotation; and
(4) enhances state-of-the-art restorative approaches, revealing that DiRA is a
general mechanism for united representation learning. All code and pre-trained
models are available at https: //github.com/JLiangLab/DiRA.",1,0,0,0,0,0,0.96952,5.0,0.928132,60
3a437b23-0b99-444f-9e5a-b72b3fc9e4fa,BD-SHS: A Benchmark Dataset for Learning to Detect Online Bangla Hate Speech in Different Social Contexts,11,0.206679,0.763829,"Social media platforms and online streaming services have spawned a new breed
of Hate Speech (HS). Due to the massive amount of user-generated content on
these sites, modern machine learning techniques are found to be feasible and
cost-effective to tackle this problem. However, linguistically diverse datasets
covering different social contexts in which offensive language is typically
used are required to train generalizable models. In this paper, we identify the
shortcomings of existing Bangla HS datasets and introduce a large manually
labeled dataset BD-SHS that includes HS in different social contexts. The
labeling criteria were prepared following a hierarchical annotation process,
which is the first of its kind in Bangla HS to the best of our knowledge. The
dataset includes more than 50,200 offensive comments crawled from online social
networking sites and is at least 60% larger than any existing Bangla HS
datasets. We present the benchmark result of our dataset by training different
NLP models resulting in the best one achieving an F1-score of 91.0%. In our
experiments, we found that a word embedding trained exclusively using 1.47
million comments from social media and streaming sites consistently resulted in
better modeling of HS detection in comparison to other pre-trained embeddings.
Our dataset and all accompanying codes is publicly available at
github.com/naurosromim/hate-speech-dataset-for-Bengali-social-media",1,1,0,1,1,0,0.744827,7.0,0.814616,28
f0da26f0-4685-4b1e-bd96-60ca2811ab6c,Causal Intervention Improves Implicit Sentiment Analysis,7,0.123941,0.234672,"Despite having achieved great success for sentiment analysis, existing neural
models struggle with implicit sentiment analysis. This may be due to the fact
that they may latch onto spurious correlations (""shortcuts"", e.g., focusing
only on explicit sentiment words), resulting in undermining the effectiveness
and robustness of the learned model. In this work, we propose a causal
intervention model for Implicit Sentiment Analysis using Instrumental Variable
(ISAIV). We first review sentiment analysis from a causal perspective and
analyze the confounders existing in this task. Then, we introduce an
instrumental variable to eliminate the confounding causal effects, thus
extracting the pure causal effect between sentence and sentiment. We compare
the proposed ISAIV model with several strong baselines on both the general
implicit sentiment analysis and aspect-based implicit sentiment analysis tasks.
The results indicate the great advantages of our model and the efficacy of
implicit sentiment reasoning.",1,0,0,0,0,0,0.372262,8.0,0.703275,63
4d5e6337-6ee1-439d-843e-015113b01080,NoisyQuant: Noisy Bias-Enhanced Post-Training Activation Quantization for Vision Transformers,17,0.188699,0.63117,"The complicated architecture and high training cost of vision transformers
urge the exploration of post-training quantization. However, the heavy-tailed
distribution of vision transformer activations hinders the effectiveness of
previous post-training quantization methods, even with advanced quantizer
designs. Instead of tuning the quantizer to better fit the complicated
activation distribution, this paper proposes NoisyQuant, a quantizer-agnostic
enhancement for the post-training activation quantization performance of vision
transformers. We make a surprising theoretical discovery that for a given
quantizer, adding a fixed Uniform noisy bias to the values being quantized can
significantly reduce the quantization error under provable conditions. Building
on the theoretical insight, NoisyQuant achieves the first success on actively
altering the heavy-tailed activation distribution with additive noisy bias to
fit a given quantizer. Extensive experiments show NoisyQuant largely improves
the post-training quantization performance of vision transformer with minimal
computation overhead. For instance, on linear uniform 6-bit activation
quantization, NoisyQuant improves SOTA top-1 accuracy on ImageNet by up to
1.7%, 1.1% and 0.5% for ViT, DeiT, and Swin Transformer respectively, achieving
on-par or even higher performance than previous nonlinear, mixed-precision
quantization.",0,1,0,0,1,0,0.778775,5.0,0.760346,48
5b23eeff-5a27-402a-87c8-2125d926a5d4,UniGeo: Unifying Geometry Logical Reasoning via Reformulating Mathematical Expression,25,0.651597,0.370562,"Geometry problem solving is a well-recognized testbed for evaluating the
high-level multi-modal reasoning capability of deep models. In most existing
works, two main geometry problems: calculation and proving, are usually treated
as two specific tasks, hindering a deep model to unify its reasoning capability
on multiple math tasks. However, in essence, these two tasks have similar
problem representations and overlapped math knowledge which can improve the
understanding and reasoning ability of a deep model on both two tasks.
Therefore, we construct a large-scale Unified Geometry problem benchmark,
UniGeo, which contains 4,998 calculation problems and 9,543 proving problems.
Each proving problem is annotated with a multi-step proof with reasons and
mathematical expressions. The proof can be easily reformulated as a proving
sequence that shares the same formats with the annotated program sequence for
calculation problems. Naturally, we also present a unified multi-task Geometric
Transformer framework, Geoformer, to tackle calculation and proving problems
simultaneously in the form of sequence generation, which finally shows the
reasoning ability can be improved on both two tasks by unifying formulation.
Furthermore, we propose a Mathematical Expression Pretraining (MEP) method that
aims to predict the mathematical expressions in the problem solution, thus
improving the Geoformer model. Experiments on the UniGeo demonstrate that our
proposed Geoformer obtains state-of-the-art performance by outperforming
task-specific model NGS with over 5.6% and 3.2% accuracies on calculation and
proving problems, respectively.",1,0,1,1,1,0,0.764155,6.0,0.793064,41
b7afc19c-9f61-46d5-b633-6aff4f0cd243,Guess What Moves: Unsupervised Video and Image Segmentation by Anticipating Motion,29,0.220482,0.610219,"Motion, measured via optical flow, provides a powerful cue to discover and
learn objects in images and videos. However, compared to using appearance, it
has some blind spots, such as the fact that objects become invisible if they do
not move. In this work, we propose an approach that combines the strengths of
motion-based and appearance-based segmentation. We propose to supervise an
image segmentation network with the pretext task of predicting regions that are
likely to contain simple motion patterns, and thus likely to correspond to
objects. As the model only uses a single image as input, we can apply it in two
settings: unsupervised video segmentation, and unsupervised image segmentation.
We achieve state-of-the-art results for videos, and demonstrate the viability
of our approach on still images containing novel objects. Additionally we
experiment with different motion models and optical flow backbones and find the
method to be robust to these change. Project page and code available at
https://www.robots.ox.ac.uk/~vgg/research/gwm.",1,1,0,0,1,0,0.439474,7.0,0.691986,108
6eb76a34-857d-4db6-91e8-ea38afff69ed,Contrastive Decoding: Open-ended Text Generation as Optimization,135,0.848238,0.980781,"Given a language model (LM), maximum probability is a poor decoding objective
for open-ended generation, because it produces short and repetitive text. On
the other hand, sampling can often produce incoherent text that drifts from the
original topics. We propose contrastive decoding (CD), a reliable decoding
approach that optimizes a contrastive objective subject to a plausibility
constraint. The contrastive objective returns the difference between the
likelihood under a large LM (called the expert, e.g. OPT-13B) and a small LM
(called the amateur, e.g. OPT-125M), and the constraint ensures that the
outputs are plausible. CD is inspired by the fact that the failures of larger
LMs (e.g., repetition, incoherence) are even more prevalent in smaller LMs, and
that this difference signals which texts should be preferred. CD requires zero
additional training, and produces higher quality text than decoding from the
larger LM alone. It also works across model scales (OPT-13B and GPT2-1.5B) and
significantly outperforms four strong decoding algorithms (e.g., nucleus,
top-k) in automatic and human evaluations across wikipedia, news and story
domains.",0,1,0,0,0,1,0.729113,8.0,0.832196,39
2754c8f2-6388-4b41-b3bc-62ec168ce5f6,GaitTAKE: Gait Recognition by Temporal Attention and Keypoint-guided Embedding,12,0.137619,0.547193,"Gait recognition, which refers to the recognition or identification of a
person based on their body shape and walking styles, derived from video data
captured from a distance, is widely used in crime prevention, forensic
identification, and social security. However, to the best of our knowledge,
most of the existing methods use appearance, posture and temporal feautures
without considering a learned temporal attention mechanism for global and local
information fusion. In this paper, we propose a novel gait recognition
framework, called Temporal Attention and Keypoint-guided Embedding (GaitTAKE),
which effectively fuses temporal-attention-based global and local appearance
feature and temporal aggregated human pose feature. Experimental results show
that our proposed method achieves a new SOTA in gait recognition with rank-1
accuracy of 98.0% (normal), 97.5% (bag) and 92.2% (coat) on the CASIA-B gait
dataset; 90.4% accuracy on the OU-MVLP gait dataset.",0,1,0,0,1,0,0.31618,15.0,0.828209,19
78d25e79-0f51-469b-996a-c7164fd64f43,Parameter-Parallel Distributed Variational Quantum Algorithm,2,0.0509571,0.149433,"Variational quantum algorithms (VQAs) have emerged as a promising near-term
technique to explore practical quantum advantage on noisy intermediate-scale
quantum (NISQ) devices. However, the inefficient parameter training process due
to the incompatibility with backpropagation and the cost of a large number of
measurements, posing a great challenge to the large-scale development of VQAs.
Here, we propose a parameter-parallel distributed variational quantum algorithm
(PPD-VQA), to accelerate the training process by parameter-parallel training
with multiple quantum processors. To maintain the high performance of PPD-VQA
in the realistic noise scenarios, a alternate training strategy is proposed to
alleviate the acceleration attenuation caused by noise differences among
multiple quantum processors, which is an unavoidable common problem of
distributed VQA. Besides, the gradient compression is also employed to overcome
the potential communication bottlenecks. The achieved results suggest that the
PPD-VQA could provide a practical solution for coordinating multiple quantum
processors to handle large-scale real-word applications.",0,1,0,0,0,0,0.933971,7.0,0.912905,60
5c5de268-071e-4c7b-9168-cd0158ffd7d4,Reducing Position Bias in Simultaneous Machine Translation with Length-Aware Framework,13,0.125172,0.532069,"Simultaneous machine translation (SiMT) starts translating while receiving
the streaming source inputs, and hence the source sentence is always incomplete
during translating. Different from the full-sentence MT using the conventional
seq-to-seq architecture, SiMT often applies prefix-to-prefix architecture,
which forces each target word to only align with a partial source prefix to
adapt to the incomplete source in streaming inputs. However, the source words
in the front positions are always illusoryly considered more important since
they appear in more prefixes, resulting in position bias, which makes the model
pay more attention on the front source positions in testing. In this paper, we
first analyze the phenomenon of position bias in SiMT, and develop a
Length-Aware Framework to reduce the position bias by bridging the structural
gap between SiMT and full-sentence MT. Specifically, given the streaming
inputs, we first predict the full-sentence length and then fill the future
source position with positional encoding, thereby turning the streaming inputs
into a pseudo full-sentence. The proposed framework can be integrated into most
existing SiMT methods to further improve performance. Experiments on two
representative SiMT methods, including the state-of-the-art adaptive policy,
show that our method successfully reduces the position bias and thereby
achieves better SiMT performance.",0,1,0,0,1,0,0.194456,6.0,0.476531,48
7d9785bc-fb0c-45fa-b504-8dc529e36647,Multi-Agent Reinforcement Learning with Graph Convolutional Neural Networks for optimal Bidding Strategies of Generation Units in Electricity Markets,1,0.017367,0.114822,"Finding optimal bidding strategies for generation units in electricity
markets would result in higher profit. However, it is a challenging problem due
to the system uncertainty which is due to the unknown other generation units'
strategies. Distributed optimization, where each entity or agent decides on its
bid individually, has become state of the art. However, it cannot overcome the
challenges of system uncertainties. Deep reinforcement learning is a promising
approach to learn the optimal strategy in uncertain environments. Nevertheless,
it is not able to integrate the information on the spatial system topology in
the learning process. This paper proposes a distributed learning algorithm
based on deep reinforcement learning (DRL) combined with a graph convolutional
neural network (GCN). In fact, the proposed framework helps the agents to
update their decisions by getting feedback from the environment so that it can
overcome the challenges of the uncertainties. In this proposed algorithm, the
state and connection between nodes are the inputs of the GCN, which can make
agents aware of the structure of the system. This information on the system
topology helps the agents to improve their bidding strategies and increase the
profit. We evaluate the proposed algorithm on the IEEE 30-bus system under
different scenarios. Also, to investigate the generalization ability of the
proposed approach, we test the trained model on IEEE 39-bus system. The results
show that the proposed algorithm has more generalization abilities compare to
the DRL and can result in higher profit when changing the topology of the
system.",0,1,0,0,1,0,0.100174,10.0,0.614202,29
8d192fee-37ea-471c-8e53-fc608e6cfa29,Crowd Source Scene Change Detection and Local Map Update,1,0.0121317,0.0444982,"As scene changes with time map descriptors become outdated, affecting VPS
localization accuracy. In this work, we propose an approach to detect
structural and texture scene changes to be followed by map update. In our
method - map includes 3D points with descriptors generated either via LiDAR or
SFM. Common approaches suffer from shortcomings: 1) Direct comparison of the
two point-clouds for change detection is slow due to the need to build new
point-cloud every time we want to compare; 2) Image based comparison requires
to keep the map images adding substantial storage overhead. To circumvent this
problems, we propose an approach based on point-clouds descriptors comparison:
1) Based on VPS poses select close query and map images pairs, 2) Registration
of query images to map image descriptors, 3) Use segmentation to filter out
dynamic or short term temporal changes, 4) Compare the descriptors between
corresponding segments.",0,0,0,0,0,0,0.245433,12.0,0.760278,40
aba19733-bfa9-453a-81ac-0e49539ff503,Natural Language Proof Checking in Introduction to Proof Classes -- First Experiences with Diproche,5,0.0388861,0.279264,"We present and analyze the employment of the Diproche system, a natural
language proof checker, within a one-semester mathematics beginners lecture
with 228 participants. The system is used to check the students' solution
attempts to proving exercises in Boolean set theory and elementary number
theory and to give them immediate feedback. The benefits of the employment of
the system are assessed via a questionnaire at the end of the semester and via
analyzing the solution attempts of a subgroup of the students. Based on our
results we develop approaches for future improvements.",0,1,0,0,0,0,0.000677613,10.0,0.109397,14
07b18a80-5e9a-4bfc-94a3-2eec5ca48301,Better Intermediates Improve CTC Inference,1,0.00646674,0.028484,"This paper proposes a method for improved CTC inference with searched
intermediates and multi-pass conditioning. The paper first formulates
self-conditioned CTC as a probabilistic model with an intermediate prediction
as a latent representation and provides a tractable conditioning framework. We
then propose two new conditioning methods based on the new formulation: (1)
Searched intermediate conditioning that refines intermediate predictions with
beam-search, (2) Multi-pass conditioning that uses predictions of previous
inference for conditioning the next inference. These new approaches enable
better conditioning than the original self-conditioned CTC during inference and
improve the final performance. Experiments with the LibriSpeech dataset show
relative 3%/12% performance improvement at the maximum in test clean/other sets
compared to the original self-conditioned CTC.",0,0,0,0,0,0,0.128094,8.0,0.550416,31
ab0a9146-3bbc-428a-9cfe-fe3aad73c3ee,Effective Seed-Guided Topic Discovery by Integrating Multiple Types of Contexts,7,0.353821,0.571937,"Instead of mining coherent topics from a given text corpus in a completely
unsupervised manner, seed-guided topic discovery methods leverage user-provided
seed words to extract distinctive and coherent topics so that the mined topics
can better cater to the user's interest. To model the semantic correlation
between words and seeds for discovering topic-indicative terms, existing
seed-guided approaches utilize different types of context signals, such as
document-level word co-occurrences, sliding window-based local contexts, and
generic linguistic knowledge brought by pre-trained language models. In this
work, we analyze and show empirically that each type of context information has
its value and limitation in modeling word semantics under seed guidance, but
combining three types of contexts (i.e., word embeddings learned from local
contexts, pre-trained language model representations obtained from
general-domain training, and topic-indicative sentences retrieved based on seed
information) allows them to complement each other for discovering quality
topics. We propose an iterative framework, SeedTopicMine, which jointly learns
from the three types of contexts and gradually fuses their context signals via
an ensemble ranking process. Under various sets of seeds and on multiple
datasets, SeedTopicMine consistently yields more coherent and accurate topics
than existing seed-guided topic discovery approaches.",1,1,0,0,0,0,0.737911,12.0,0.890212,51
959b0f53-dc81-4b0d-a6a1-104a1d2aac4b,Cross-view and Cross-domain Underwater Localization based on Optical Aerial and Acoustic Underwater Images,9,0.192607,0.528609,"Cross-view image matches have been widely explored on terrestrial image
localization using aerial images from drones or satellites. This study expands
the cross-view image match idea and proposes a cross-domain and cross-view
localization framework. The method identifies the correlation between color
aerial images and underwater acoustic images to improve the localization of
underwater vehicles that travel in partially structured environments such as
harbors and marinas. The approach is validated on a real dataset acquired by an
underwater vehicle in a marina. The results show an improvement in the
localization when compared to the dead reckoning of the vehicle.",1,1,0,0,0,0,0.115827,12.0,0.691319,30
8a529b1d-d012-434c-9c8e-d794cd7d92b4,D-Shape: Demonstration-Shaped Reinforcement Learning via Goal Conditioning,3,0.00416298,0.0952773,"While combining imitation learning (IL) and reinforcement learning (RL) is a
promising way to address poor sample efficiency in autonomous behavior
acquisition, methods that do so typically assume that the requisite behavior
demonstrations are provided by an expert that behaves optimally with respect to
a task reward. If, however, suboptimal demonstrations are provided, a
fundamental challenge appears in that the demonstration-matching objective of
IL conflicts with the return-maximization objective of RL. This paper
introduces D-Shape, a new method for combining IL and RL that uses ideas from
reward shaping and goal-conditioned RL to resolve the above conflict. D-Shape
allows learning from suboptimal demonstrations while retaining the ability to
find the optimal policy with respect to the task reward. We experimentally
validate D-Shape in sparse-reward gridworld domains, showing that it both
improves over RL in terms of sample efficiency and converges consistently to
the optimal policy in the presence of suboptimal demonstrations.",0,1,0,0,0,0,0.00939568,10.0,0.372777,43
213b51bd-61b3-4620-b5a9-eee03d129157,Phylogeny-Inspired Adaptation of Multilingual Models to New Languages,21,0.244506,0.536357,"Large pretrained multilingual models, trained on dozens of languages, have
delivered promising results due to cross-lingual learning capabilities on
variety of language tasks. Further adapting these models to specific languages,
especially ones unseen during pre-training, is an important goal towards
expanding the coverage of language technologies. In this study, we show how we
can use language phylogenetic information to improve cross-lingual transfer
leveraging closely related languages in a structured, linguistically-informed
manner. We perform adapter-based training on languages from diverse language
families (Germanic, Uralic, Tupian, Uto-Aztecan) and evaluate on both syntactic
and semantic tasks, obtaining more than 20% relative performance improvements
over strong commonly used baselines, especially on languages unseen during
pre-training.",1,1,0,0,0,0,0.512219,5.0,0.611824,42
d27a005d-1a26-49bf-ab06-e37798e492bd,PSP-HDRI$+$: A Synthetic Dataset Generator for Pre-Training of Human-Centric Computer Vision Models,14,0.142032,0.733332,"We introduce a new synthetic data generator PSP-HDRI$+$ that proves to be a
superior pre-training alternative to ImageNet and other large-scale synthetic
data counterparts. We demonstrate that pre-training with our synthetic data
will yield a more general model that performs better than alternatives even
when tested on out-of-distribution (OOD) sets. Furthermore, using ablation
studies guided by person keypoint estimation metrics with an off-the-shelf
model architecture, we show how to manipulate our synthetic data generator to
further improve model performance.",0,1,0,1,0,0,0.553124,9.0,0.797138,40
71aa4839-ab88-47ad-9b2f-5fa6596a35de,Exploring Wasserstein Distance across Concept Embeddings for Ontology Matching,2,0.0289185,0.122438,"Measuring the distance between ontological elements is fundamental for
ontology matching. String-based distance metrics are notorious for shallow
syntactic matching. In this exploratory study, we investigate Wasserstein
distance targeting continuous space that can incorporate various types of
information. We use a pre-trained word embeddings system to embed ontology
element labels. We examine the effectiveness of Wasserstein distance for
measuring similarity between ontologies, and discovering and refining matchings
between individual elements. Our experiments with the OAEI conference track and
MSE benchmarks achieved competitive results compared to the leading systems.",0,0,0,0,0,0,0.358996,9.0,0.731139,39
7ec8ff3c-e981-49fb-8ebd-bd329df8fa5b,Efficient Adversarial Training with Robust Early-Bird Tickets,5,0.0122655,0.292621,"Adversarial training is one of the most powerful methods to improve the
robustness of pre-trained language models (PLMs). However, this approach is
typically more expensive than traditional fine-tuning because of the necessity
to generate adversarial examples via gradient descent. Delving into the
optimization process of adversarial training, we find that robust connectivity
patterns emerge in the early training phase (typically $0.15\sim0.3$ epochs),
far before parameters converge. Inspired by this finding, we dig out robust
early-bird tickets (i.e., subnetworks) to develop an efficient adversarial
training method: (1) searching for robust tickets with structured sparsity in
the early stage; (2) fine-tuning robust tickets in the remaining time. To
extract the robust tickets as early as possible, we design a ticket convergence
metric to automatically terminate the searching process. Experiments show that
the proposed efficient adversarial training method can achieve up to $7\times
\sim 13 \times$ training speedups while maintaining comparable or even better
robustness compared to the most competitive state-of-the-art adversarial
training methods.",1,1,0,0,1,0,0.186986,7.0,0.54508,44
f52bd570-902c-4470-bfee-d81805a5c7c7,Social Choice Around the Block: On the Computational Social Choice of Blockchain,13,0.32445,0.621248,"One of the most innovative aspects of blockchain technology consists in the
introduction of an incentive layer to regulate the behavior of distributed
protocols. The designer of a blockchain system faces therefore issues that are
akin to those relevant for the design of economic mechanisms, and faces them in
a computational setting. From this perspective the present paper argues for the
importance of computational social choice in blockchain research. It identifies
a few challenges at the interface of the two fields that illustrate the strong
potential for cross-fertilization between them.",0,0,0,0,0,0,0.156713,18.0,0.812291,70
1da933f4-e005-4840-8825-98678f25d14b,SubeventWriter: Iterative Sub-event Sequence Generation with Coherence Controller,6,0.108373,0.698318,"In this paper, we propose a new task of sub-event generation for an unseen
process to evaluate the understanding of the coherence of sub-event actions and
objects. To solve the problem, we design SubeventWriter, a sub-event sequence
generation framework with a coherence controller. Given an unseen process, the
framework can iteratively construct the sub-event sequence by generating one
sub-event at each iteration. We also design a very effective coherence
controller to decode more coherent sub-events. As our extensive experiments and
analysis indicate, SubeventWriter can generate more reliable and meaningful
sub-event sequences for unseen processes.",1,0,1,0,0,0,0.46425,7.0,0.702728,56
c6f39119-c4dc-4320-8ea5-9518a9218136,SoK: Differential Privacy on Graph-Structured Data,10,0.0652514,0.584724,"In this work, we study the applications of differential privacy (DP) in the
context of graph-structured data. We discuss the formulations of DP applicable
to the publication of graphs and their associated statistics as well as machine
learning on graph-based data, including graph neural networks (GNNs). The
formulation of DP in the context of graph-structured data is difficult, as
individual data points are interconnected (often non-linearly or sparsely).
This connectivity complicates the computation of individual privacy loss in
differentially private learning. The problem is exacerbated by an absence of a
single, well-established formulation of DP in graph settings. This issue
extends to the domain of GNNs, rendering private machine learning on
graph-structured data a challenging task. A lack of prior systematisation work
motivated us to study graph-based learning from a privacy perspective. In this
work, we systematise different formulations of DP on graphs, discuss challenges
and promising applications, including the GNN domain. We compare and separate
works into graph analysis tasks and graph learning tasks with GNNs. Finally, we
conclude our work with a discussion of open questions and potential directions
for further research in this area.",0,0,0,0,0,0,0.0783774,8.0,0.485609,115
04d87ead-1cac-402a-97c4-0d130802f891,Pretrained Domain-Specific Language Model for General Information Retrieval Tasks in the AEC Domain,26,0.145593,0.539876,"As an essential task for the architecture, engineering, and construction
(AEC) industry, information retrieval (IR) from unstructured textual data based
on natural language processing (NLP) is gaining increasing attention. Although
various deep learning (DL) models for IR tasks have been investigated in the
AEC domain, it is still unclear how domain corpora and domain-specific
pretrained DL models can improve performance in various IR tasks. To this end,
this work systematically explores the impacts of domain corpora and various
transfer learning techniques on the performance of DL models for IR tasks and
proposes a pretrained domain-specific language model for the AEC domain. First,
both in-domain and close-domain corpora are developed. Then, two types of
pretrained models, including traditional wording embedding models and
BERT-based models, are pretrained based on various domain corpora and transfer
learning strategies. Finally, several widely used DL models for IR tasks are
further trained and tested based on various configurations and pretrained
models. The result shows that domain corpora have opposite effects on
traditional word embedding models for text classification and named entity
recognition tasks but can further improve the performance of BERT-based models
in all tasks. Meanwhile, BERT-based models dramatically outperform traditional
methods in all IR tasks, with maximum improvements of 5.4% and 10.1% in the F1
score, respectively. This research contributes to the body of knowledge in two
ways: 1) demonstrating the advantages of domain corpora and pretrained DL
models and 2) opening the first domain-specific dataset and pretrained language
model for the AEC domain, to the best of our knowledge. Thus, this work sheds
light on the adoption and application of pretrained models in the AEC domain.",1,1,0,1,0,0,0.135833,9.0,0.607371,55
60fe54ab-90de-4b23-96cb-881fa33ca68c,Mathematical model of printing-imaging channel for blind detection of fake copy detection patterns,6,0.107418,0.470879,"Nowadays, copy detection patterns (CDP) appear as a very promising
anti-counterfeiting technology for physical object protection. However, the
advent of deep learning as a powerful attacking tool has shown that the general
authentication schemes are unable to compete and fail against such attacks. In
this paper, we propose a new mathematical model of printing-imaging channel for
the authentication of CDP together with a new detection scheme based on it. The
results show that even deep learning created copy fakes unknown at the training
stage can be reliably authenticated based on the proposed approach and using
only digital references of CDP during authentication.",0,0,0,0,0,0,0.000754466,14.0,0.371531,13
8a0e08fa-3ee6-4da7-a73a-e0e4c0b6b2fa,Pruning Adversarially Robust Neural Networks without Adversarial Examples,5,0.0110106,0.224179,"Adversarial pruning compresses models while preserving robustness. Current
methods require access to adversarial examples during pruning. This
significantly hampers training efficiency. Moreover, as new adversarial attacks
and training methods develop at a rapid rate, adversarial pruning methods need
to be modified accordingly to keep up. In this work, we propose a novel
framework to prune a previously trained robust neural network while maintaining
adversarial robustness, without further generating adversarial examples. We
leverage concurrent self-distillation and pruning to preserve knowledge in the
original model as well as regularizing the pruned model via the Hilbert-Schmidt
Information Bottleneck. We comprehensively evaluate our proposed framework and
show its superior performance in terms of both adversarial robustness and
efficiency when pruning architectures trained on the MNIST, CIFAR-10, and
CIFAR-100 datasets against five state-of-the-art attacks. Code is available at
https://github.com/neu-spiral/PwoA/.",1,1,0,0,0,0,0.160424,8.0,0.580848,46
0885fd8a-854a-4f90-8869-3621562bcda3,Large Language Models are few(1)-shot Table Reasoners,63,0.791213,0.794004,"Recent literature has shown that large language models (LLMs) are generally
excellent few-shot reasoners to solve text reasoning tasks. However, the
capability of LLMs on table reasoning tasks is yet to be explored. In this
paper, we aim at understanding how well LLMs can perform table-related tasks
with few-shot in-context learning. Specifically, we evaluated LLMs on popular
table QA and fact verification datasets like WikiTableQuestion, FetaQA,
TabFact, and FEVEROUS and found that LLMs are competent at complex reasoning
over table structures, though these models are not pre-trained on any table
corpus. When combined with `chain of thoughts' prompting, LLMs can achieve very
strong performance with only a 1-shot demonstration, even on par with some SoTA
models. We show that LLMs are even more competent at generating comprehensive
long-form answers on FetaQA than tuned T5-large. We further manually studied
the reasoning chains elicited from LLMs and found that these reasoning chains
are highly consistent with the underlying semantic form. We believe that LLMs
can serve as a simple yet generic baseline for future research. The code and
data are released in https://github.com/wenhuchen/TableCoT.",1,1,0,0,0,0,0.951495,5.0,0.899567,48
a59f734a-d686-4865-9de4-099989a5a7ac,Fusing Local Similarities for Retrieval-based 3D Orientation Estimation of Unseen Objects,13,0.240695,0.536357,"In this paper, we tackle the task of estimating the 3D orientation of
previously-unseen objects from monocular images. This task contrasts with the
one considered by most existing deep learning methods which typically assume
that the testing objects have been observed during training. To handle the
unseen objects, we follow a retrieval-based strategy and prevent the network
from learning object-specific features by computing multi-scale local
similarities between the query image and synthetically-generated reference
images. We then introduce an adaptive fusion module that robustly aggregates
the local similarities into a global similarity score of pairwise images.
Furthermore, we speed up the retrieval process by developing a fast retrieval
strategy. Our experiments on the LineMOD, LineMOD-Occluded, and T-LESS datasets
show that our method yields a significantly better generalization to unseen
objects than previous works. Our code and pre-trained models are available at
https://sailor-z.github.io/projects/Unseen_Object_Pose.html.",1,1,1,0,0,0,0.963333,9.0,0.954029,42
428e40b0-fe67-4c7e-81f1-ab27b3500df2,Towards Unified Conversational Recommender Systems via Knowledge-Enhanced Prompt Learning,62,0.513953,0.995101,"Conversational recommender systems (CRS) aim to proactively elicit user
preference and recommend high-quality items through natural language
conversations. Typically, a CRS consists of a recommendation module to predict
preferred items for users and a conversation module to generate appropriate
responses. To develop an effective CRS, it is essential to seamlessly integrate
the two modules. Existing works either design semantic alignment strategies, or
share knowledge resources and representations between the two modules. However,
these approaches still rely on different architectures or techniques to develop
the two modules, making it difficult for effective module integration.
  To address this problem, we propose a unified CRS model named UniCRS based on
knowledge-enhanced prompt learning. Our approach unifies the recommendation and
conversation subtasks into the prompt learning paradigm, and utilizes
knowledge-enhanced prompts based on a fixed pre-trained language model (PLM) to
fulfill both subtasks in a unified approach. In the prompt design, we include
fused knowledge representations, task-specific soft tokens, and the dialogue
context, which can provide sufficient contextual information to adapt the PLM
for the CRS task. Besides, for the recommendation subtask, we also incorporate
the generated response template as an important part of the prompt, to enhance
the information interaction between the two subtasks. Extensive experiments on
two public CRS datasets have demonstrated the effectiveness of our approach.",1,1,0,0,0,0,0.82575,5.0,0.789723,39
aa39f535-2108-4281-9006-47b0bdaa9578,WaveY-Net: Physics-augmented deep learning for high-speed electromagnetic simulation and optimization,16,0.149155,0.844433,"The calculation of electromagnetic field distributions within structured
media is central to the optimization and validation of photonic devices. We
introduce WaveY-Net, a hybrid data- and physics-augmented convolutional neural
network that can predict electromagnetic field distributions with ultra fast
speeds and high accuracy for entire classes of dielectric photonic structures.
This accuracy is achieved by training the neural network to learn only the
magnetic near-field distributions of a system and to use a discrete formalism
of Maxwell's equations in two ways: as physical constraints in the loss
function and as a means to calculate the electric fields from the magnetic
fields. As a model system, we construct a surrogate simulator for periodic
silicon nanostructure arrays and show that the high speed simulator can be
directly and effectively used in the local and global freeform optimization of
metagratings. We anticipate that physics-augmented networks will serve as a
viable Maxwell simulator replacement for many classes of photonic systems,
transforming the way they are designed.",0,1,0,0,0,0,0.797689,6.0,0.809881,67
2587fe7f-947e-4c4c-a0bb-a75809e70324,Leveraging Graph-based Cross-modal Information Fusion for Neural Sign Language Translation,4,0.106176,0.335657,"Sign Language (SL), as the mother tongue of the deaf community, is a special
visual language that most hearing people cannot understand. In recent years,
neural Sign Language Translation (SLT), as a possible way for bridging
communication gap between the deaf and the hearing people, has attracted
widespread academic attention. We found that the current mainstream end-to-end
neural SLT models, which tries to learning language knowledge in a weakly
supervised manner, could not mine enough semantic information under the
condition of low data resources. Therefore, we propose to introduce additional
word-level semantic knowledge of sign language linguistics to assist in
improving current end-to-end neural SLT models. Concretely, we propose a novel
neural SLT model with multi-modal feature fusion based on the dynamic graph, in
which the cross-modal information, i.e. text and video, is first assembled as a
dynamic graph according to their correlation, and then the graph is processed
by a multi-modal graph encoder to generate the multi-modal embeddings for
further usage in the subsequent neural translation models. To the best of our
knowledge, we are the first to introduce graph neural networks, for fusing
multi-modal information, into neural sign language translation models.
Moreover, we conducted experiments on a publicly available popular SLT dataset
RWTH-PHOENIX-Weather-2014T. and the quantitative experiments show that our
method can improve the model.",0,1,0,0,0,0,0.569323,10.0,0.821906,47
d55b6b89-1e51-4374-8b67-b6d8b6e2152a,Code-Switching without Switching: Language Agnostic End-to-End Speech Translation,8,0.379334,0.687652,"We propose a) a Language Agnostic end-to-end Speech Translation model (LAST),
and b) a data augmentation strategy to increase code-switching (CS)
performance. With increasing globalization, multiple languages are increasingly
used interchangeably during fluent speech. Such CS complicates traditional
speech recognition and translation, as we must recognize which language was
spoken first and then apply a language-dependent recognizer and subsequent
translation component to generate the desired target language output. Such a
pipeline introduces latency and errors. In this paper, we eliminate the need
for that, by treating speech recognition and translation as one unified
end-to-end speech translation problem. By training LAST with both input
languages, we decode speech into one target language, regardless of the input
language. LAST delivers comparable recognition and speech translation accuracy
in monolingual usage, while reducing latency and error rate considerably when
CS is observed.",0,1,0,0,0,0,0.943729,7.0,0.921073,29
a2250ee1-4771-4e71-b0bb-f159fc92359c,Reference Resolution and Context Change in Multimodal Situated Dialogue for Exploring Data Visualizations,1,0.00569066,0.0663935,"Reference resolution, which aims to identify entities being referred to by a
speaker, is more complex in real world settings: new referents may be created
by processes the agents engage in and/or be salient only because they belong to
the shared physical setting. Our focus is on resolving references to
visualizations on a large screen display in multimodal dialogue; crucially,
reference resolution is directly involved in the process of creating new
visualizations. We describe our annotations for user references to
visualizations appearing on a large screen via language and hand gesture and
also new entity establishment, which results from executing the user request to
create a new visualization. We also describe our reference resolution pipeline
which relies on an information-state architecture to maintain dialogue context.
We report results on detecting and resolving references, effectiveness of
contextual information on the model, and under-specified requests for creating
visualizations. We also experiment with conventional CRF and deep learning /
transformer models (BiLSTM-CRF and BERT-CRF) for tagging references in user
utterance text. Our results show that transfer learning significantly boost
performance of the deep learning methods, although CRF still out-performs them,
suggesting that conventional methods may generalize better for low resource
data.",0,1,0,1,0,0,0.00511798,14.0,0.508438,47
6c645d57-fa46-44c9-a5f2-13d4af67f7bb,Network Pruning via Feature Shift Minimization,2,0.0134287,0.204435,"Channel pruning is widely used to reduce the complexity of deep network
models. Recent pruning methods usually identify which parts of the network to
discard by proposing a channel importance criterion. However, recent studies
have shown that these criteria do not work well in all conditions. In this
paper, we propose a novel Feature Shift Minimization (FSM) method to compress
CNN models, which evaluates the feature shift by converging the information of
both features and filters. Specifically, we first investigate the compression
efficiency with some prevalent methods in different layer-depths and then
propose the feature shift concept. Then, we introduce an approximation method
to estimate the magnitude of the feature shift, since it is difficult to
compute it directly. Besides, we present a distribution-optimization algorithm
to compensate for the accuracy loss and improve the network compression
efficiency. The proposed method yields state-of-the-art performance on various
benchmark networks and datasets, verified by extensive experiments. Our codes
are available at: https://github.com/lscgx/FSM.",1,1,0,0,1,0,0.466619,9.0,0.769575,52
4d441496-bd1b-4f50-a666-217c90ae5a36,Multi-hop Evidence Retrieval for Cross-document Relation Extraction,6,0.208554,0.711505,"Relation Extraction (RE) has been extended to cross-document scenarios
because many relations are not simply described in a single document. This
inevitably brings the challenge of efficient open-space evidence retrieval to
support the inference of cross-document relations, along with the challenge of
multi-hop reasoning on top of entities and evidence scattered in an open set of
documents. To combat these challenges, we propose MR.COD (Multi-hop evidence
retrieval for Cross-document relation extraction), which is a multi-hop
evidence retrieval method based on evidence path mining and ranking. We explore
multiple variants of retrievers to show evidence retrieval is essential in
cross-document RE. We also propose a contextual dense retriever for this
setting. Experiments on CodRED show that evidence retrieval with MR.COD
effectively acquires crossdocument evidence and boosts end-to-end RE
performance in both closed and open settings.",1,1,1,0,0,0,0.778229,6.0,0.800015,57
21c2ff42-1a41-4063-8fa9-a966f6e40351,Towards Opening the Black Box of Neural Machine Translation: Source and Target Interpretations of the Transformer,28,0.0182408,0.595334,"In Neural Machine Translation (NMT), each token prediction is conditioned on
the source sentence and the target prefix (what has been previously translated
at a decoding step). However, previous work on interpretability in NMT has
mainly focused solely on source sentence tokens' attributions. Therefore, we
lack a full understanding of the influences of every input token (source
sentence and target prefix) in the model predictions. In this work, we propose
an interpretability method that tracks input tokens' attributions for both
contexts. Our method, which can be extended to any encoder-decoder
Transformer-based model, allows us to better comprehend the inner workings of
current NMT models. We apply the proposed method to both bilingual and
multilingual Transformers and present insights into their behaviour.",0,0,0,0,0,0,0.115708,6.0,0.382455,37
bc9d5bf3-6beb-48f2-80d8-1b22328e5906,LEBP -- Language Expectation & Binding Policy: A Two-Stream Framework for Embodied Vision-and-Language Interaction Task Learning Agents,15,0.699943,0.360943,"People always desire an embodied agent that can perform a task by
understanding language instruction. Moreover, they also want to monitor and
expect agents to understand commands the way they expected. But, how to build
such an embodied agent is still unclear. Recently, people can explore this
problem with the Vision-and-Language Interaction benchmark ALFRED, which
requires an agent to perform complicated daily household tasks following
natural language instructions in unseen scenes. In this paper, we propose LEBP
-- Language Expectation and Binding Policy Module to tackle the ALFRED. The
LEBP contains a two-stream process: 1) It first conducts a language expectation
module to generate an expectation describing how to perform tasks by
understanding the language instruction. The expectation consists of a sequence
of sub-steps for the task (e.g., Pick an apple). The expectation allows people
to access and check the understanding results of instructions before the agent
takes actual actions, in case the task might go wrong. 2) Then, it uses the
binding policy module to bind sub-steps in expectation to actual actions to
specific scenarios. Actual actions include navigation and object manipulation.
Experimental results suggest our approach achieves comparable performance to
currently published SOTA methods and can avoid large decay from seen scenarios
to unseen scenarios.",0,1,0,0,1,0,0.981644,5.0,0.955261,26
1bbd2a76-db49-4dac-8fa9-1af1eb26d62f,What Makes Good Contrastive Learning on Small-Scale Wearable-based Tasks?,24,0.0558704,0.418547,"Self-supervised learning establishes a new paradigm of learning
representations with much fewer or even no label annotations. Recently there
has been remarkable progress on large-scale contrastive learning models which
require substantial computing resources, yet such models are not practically
optimal for small-scale tasks. To fill the gap, we aim to study contrastive
learning on the wearable-based activity recognition task. Specifically, we
conduct an in-depth study of contrastive learning from both algorithmic-level
and task-level perspectives. For algorithmic-level analysis, we decompose
contrastive models into several key components and conduct rigorous
experimental evaluations to better understand the efficacy and rationale behind
contrastive learning. More importantly, for task-level analysis, we show that
the wearable-based signals bring unique challenges and opportunities to
existing contrastive models, which cannot be readily solved by existing
algorithms. Our thorough empirical studies suggest important practices and shed
light on future research challenges. In the meantime, this paper presents an
open-source PyTorch library \texttt{CL-HAR}, which can serve as a practical
tool for researchers. The library is highly modularized and easy to use, which
opens up avenues for exploring novel contrastive models quickly in the future.",1,1,0,0,0,0,0.430204,4.0,0.453789,47
844b2036-e401-4b74-a418-542ad79dfb3b,Interpretability for Language Learners Using Example-Based Grammatical Error Correction,22,0.477267,0.986321,"Grammatical Error Correction (GEC) should not focus only on high accuracy of
corrections but also on interpretability for language learning. However,
existing neural-based GEC models mainly aim at improving accuracy, and their
interpretability has not been explored. A promising approach for improving
interpretability is an example-based method, which uses similar retrieved
examples to generate corrections. In addition, examples are beneficial in
language learning, helping learners understand the basis of grammatically
incorrect/correct texts and improve their confidence in writing. Therefore, we
hypothesize that incorporating an example-based method into GEC can improve
interpretability as well as support language learners. In this study, we
introduce an Example-Based GEC (EB-GEC) that presents examples to language
learners as a basis for a correction result. The examples consist of pairs of
correct and incorrect sentences similar to a given input and its predicted
correction. Experiments demonstrate that the examples presented by EB-GEC help
language learners decide to accept or refuse suggestions from the GEC output.
Furthermore, the experiments also show that retrieved examples improve the
accuracy of corrections.",1,1,0,0,0,0,0.454411,8.0,0.736188,58
f33b9c78-41c6-44ec-8427-4c27f20038a0,MOCHA: A Multi-Task Training Approach for Coherent Text Generation from Cognitive Perspective,3,0.0152203,0.0567198,"Teaching neural models to generate narrative coherent texts is a critical
problem. Recent pre-trained language models have achieved promising results,
but there is still a gap between human written texts and machine-generated
outputs. In this work, we propose a novel multi-task training strategy for
coherent text generation grounded on the cognitive theory of writing, which
empowers the model to learn essential subskills needed for writing including
planning and reviewing besides end-to-end generation. We extensively evaluate
our model on three open-ended generation tasks including story generation, news
article writing and argument generation. Experiments show that our model
achieves better results on both few-shot and fully-supervised settings than
strong baselines, and human evaluations confirm that our model can generate
more coherent outputs.",1,1,0,0,0,0,0.184531,7.0,0.542984,46
4190866c-fb45-4978-9f32-65382c2d5527,AdaPrompt: Adaptive Model Training for Prompt-based NLP,35,0.4088,0.598129,"Prompt-based learning, with its capability to tackle zero-shot and few-shot
NLP tasks, has gained much attention in community. The main idea is to bridge
the gap between NLP downstream tasks and language modeling (LM), by mapping
these tasks into natural language prompts, which are then filled by pre-trained
language models (PLMs). However, for prompt learning, there are still two
salient gaps between NLP tasks and pretraining. First, prompt information is
not necessarily sufficiently present during LM pretraining. Second,
task-specific data are not necessarily well represented during pretraining. We
address these two issues by proposing AdaPrompt, adaptively retrieving external
data for continual pretraining of PLMs by making use of both task and prompt
characteristics. In addition, we make use of knowledge in Natural Language
Inference models for deriving adaptive verbalizers. Experimental results on
five NLP benchmarks show that AdaPrompt can improve over standard PLMs in
few-shot settings. In addition, in zero-shot settings, our method outperforms
standard prompt-based methods by up to 26.35\% relative error reduction.",1,1,0,0,0,0,0.938048,5.0,0.882703,55
7ad63ac5-5e2b-4244-8f48-7c9dac4e5401,Equilibrium Aggregation: Encoding Sets via Optimization,5,0.0259102,0.211275,"Processing sets or other unordered, potentially variable-sized inputs in
neural networks is usually handled by aggregating a number of input tensors
into a single representation. While a number of aggregation methods already
exist from simple sum pooling to multi-head attention, they are limited in
their representational power both from theoretical and empirical perspectives.
On the search of a principally more powerful aggregation strategy, we propose
an optimization-based method called Equilibrium Aggregation. We show that many
existing aggregation methods can be recovered as special cases of Equilibrium
Aggregation and that it is provably more efficient in some important cases.
Equilibrium Aggregation can be used as a drop-in replacement in many existing
architectures and applications. We validate its efficiency on three different
tasks: median estimation, class counting, and molecular property prediction. In
all experiments, Equilibrium Aggregation achieves higher performance than the
other aggregation techniques we test.",1,0,0,0,0,1,0.428256,8.0,0.726134,71
d143cc96-7803-43d7-a062-804e22db87c7,Beyond calibration: estimating the grouping loss of modern neural networks,11,0.334729,0.313612,"The ability to ensure that a classifier gives reliable confidence scores is
essential to ensure informed decision-making. To this end, recent work has
focused on miscalibration, i.e., the over or under confidence of model scores.
Yet calibration is not enough: even a perfectly calibrated classifier with the
best possible accuracy can have confidence scores that are far from the true
posterior probabilities. This is due to the grouping loss, created by samples
with the same confidence scores but different true posterior probabilities.
Proper scoring rule theory shows that given the calibration loss, the missing
piece to characterize individual errors is the grouping loss. While there are
many estimators of the calibration loss, none exists for the grouping loss in
standard settings. Here, we propose an estimator to approximate the grouping
loss. We show that modern neural network architectures in vision and NLP
exhibit grouping loss, notably in distribution shifts settings, which
highlights the importance of pre-production validation.",1,0,0,0,0,0,0.669481,12.0,0.874358,39
f98949db-9b76-4140-a0e7-0f432c941afb,Evaluating the Text-to-SQL Capabilities of Large Language Models,66,0.908757,0.999249,"We perform an empirical evaluation of Text-to-SQL capabilities of the Codex
language model. We find that, without any finetuning, Codex is a strong
baseline on the Spider benchmark; we also analyze the failure modes of Codex in
this setting. Furthermore, we demonstrate on the GeoQuery and Scholar
benchmarks that a small number of in-domain examples provided in the prompt
enables Codex to perform better than state-of-the-art models finetuned on such
few-shot examples.",0,1,0,0,0,0,0.988533,4.0,0.97189,23
00f36199-085e-4c83-b27a-dfa0a22f7caa,A Reference Data Model for Process-Related User Interaction Logs,7,0.143666,0.350129,"User interaction (UI) logs are high-resolution event logs that record
low-level activities performed by a user during the execution of a task in an
information system. Each event in a UI log corresponds to a single interaction
between the user and the interface, such as clicking a button or entering a
string into a text field. UI logs are used for purposes like task mining or
robotic process automation (RPA), but each study and tool relies on a different
conceptualization and implementation of the elements and attributes that
constitute user interactions. This lack of standardization makes it difficult
to integrate UI logs from different sources and to combine tools for UI data
collection with downstream analytics or automation solutions. To address this,
we propose a universally applicable reference data model for process-related UI
logs. Based on a review of scientific literature and industry solutions, this
model includes the core attributes of UI logs, but remains flexible with regard
to the scope, level of abstraction, and case notion. We provide an
implementation of the model as an extension to the XES interchange standard for
event logs and demonstrate its practical applicability in a real-life RPA
scenario.",0,1,0,0,0,0,0.0224133,11.0,0.509434,39
2deeed2b-53d8-45aa-830f-572f0a8a4c8d,Vision Transformer Equipped with Neural Resizer on Facial Expression Recognition Task,5,0.0350649,0.52048,"When it comes to wild conditions, Facial Expression Recognition is often
challenged with low-quality data and imbalanced, ambiguous labels. This field
has much benefited from CNN based approaches; however, CNN models have
structural limitation to see the facial regions in distant. As a remedy,
Transformer has been introduced to vision fields with global receptive field,
but requires adjusting input spatial size to the pretrained models to enjoy
their strong inductive bias at hands. We herein raise a question whether using
the deterministic interpolation method is enough to feed low-resolution data to
Transformer. In this work, we propose a novel training framework, Neural
Resizer, to support Transformer by compensating information and downscaling in
a data-driven manner trained with loss function balancing the noisiness and
imbalance. Experiments show our Neural Resizer with F-PDLS loss function
improves the performance with Transformer variants in general and nearly
achieves the state-of-the-art performance.",0,1,0,0,1,0,0.576846,6.0,0.706627,26
614313a7-2f49-478b-a5ef-bb84ba2cbc72,DistSPECTRL: Distributing Specifications in Multi-Agent Reinforcement Learning Systems,3,0.0355324,0.121108,"While notable progress has been made in specifying and learning objectives
for general cyber-physical systems, applying these methods to distributed
multi-agent systems still pose significant challenges. Among these are the need
to (a) craft specification primitives that allow expression and interplay of
both local and global objectives, (b) tame explosion in the state and action
spaces to enable effective learning, and (c) minimize coordination frequency
and the set of engaged participants for global objectives. To address these
challenges, we propose a novel specification framework that allows natural
composition of local and global objectives used to guide training of a
multi-agent system. Our technique enables learning expressive policies that
allow agents to operate in a coordination-free manner for local objectives,
while using a decentralized communication protocol for enforcing global ones.
Experimental results support our claim that sophisticated multi-agent
distributed planning problems can be effectively realized using
specification-guided learning.",1,0,0,0,0,0,0.50476,10.0,0.803776,22
d90b3ae3-acbe-4aaf-b661-ab97604ef184,Learning Smooth Neural Functions via Lipschitz Regularization,59,0.295121,0.879769,"Neural implicit fields have recently emerged as a useful representation for
3D shapes. These fields are commonly represented as neural networks which map
latent descriptors and 3D coordinates to implicit function values. The latent
descriptor of a neural field acts as a deformation handle for the 3D shape it
represents. Thus, smoothness with respect to this descriptor is paramount for
performing shape-editing operations. In this work, we introduce a novel
regularization designed to encourage smooth latent spaces in neural fields by
penalizing the upper bound on the field's Lipschitz constant. Compared with
prior Lipschitz regularized networks, ours is computationally fast, can be
implemented in four lines of code, and requires minimal hyperparameter tuning
for geometric applications. We demonstrate the effectiveness of our approach on
shape interpolation and extrapolation as well as partial shape reconstruction
from 3D point clouds, showing both qualitative and quantitative improvements
over existing state-of-the-art and non-regularized baselines.",0,0,0,0,1,0,0.371774,8.0,0.703067,60
3ee6115d-bef3-427e-8be8-59cb28351e84,Integration of Text and Graph-based Features for Detecting Mental Health Disorders from Voice,1,0.112146,0.167945,"With the availability of voice-enabled devices such as smart phones, mental
health disorders could be detected and treated earlier, particularly
post-pandemic. The current methods involve extracting features directly from
audio signals. In this paper, two methods are used to enrich voice analysis for
depression detection: graph transformation of voice signals, and natural
language processing of the transcript based on representational learning, fused
together to produce final class labels. The results of experiments with the
DAIC-WOZ dataset suggest that integration of text-based voice classification
and learning from low level and graph-based voice signal features can improve
the detection of mental disorders like depression.",0,1,0,0,1,0,0.800046,7.0,0.838084,29
8f1466b1-cf34-4282-b1dd-a92ba71dc91f,RobustLoc: Robust Camera Pose Regression in Challenging Driving Environments,14,0.100623,0.775592,"Camera relocalization has various applications in autonomous driving.
Previous camera pose regression models consider only ideal scenarios where
there is little environmental perturbation. To deal with challenging driving
environments that may have changing seasons, weather, illumination, and the
presence of unstable objects, we propose RobustLoc, which derives its
robustness against perturbations from neural differential equations. Our model
uses a convolutional neural network to extract feature maps from multi-view
images, a robust neural differential equation diffusion block module to diffuse
information interactively, and a branched pose decoder with multi-layer
training to estimate the vehicle poses. Experiments demonstrate that RobustLoc
surpasses current state-of-the-art camera pose regression models and achieves
robust performance in various environments. Our code is released at:
https://github.com/sijieaaa/RobustLoc",1,1,0,0,1,0,0.260828,7.0,0.59914,33
d7dc4cee-a35e-4e50-9404-837288959b8c,Zero-Shot On-the-Fly Event Schema Induction,9,0.185538,0.810376,"What are the events involved in a pandemic outbreak? What steps should be
taken when planning a wedding? The answers to these questions can be found by
collecting many documents on the complex event of interest, extracting relevant
information, and analyzing it. We present a new approach in which large
language models are utilized to generate source documents that allow
predicting, given a high-level event definition, the specific events,
arguments, and relations between them to construct a schema that describes the
complex event in its entirety. Using our model, complete schemas on any topic
can be generated on-the-fly without any manual data collection, i.e., in a
zero-shot manner. Moreover, we develop efficient methods to extract pertinent
information from texts and demonstrate in a series of experiments that these
schemas are considered to be more complete than human-curated ones in the
majority of examined scenarios. Finally, we show that this framework is
comparable in performance with previous supervised schema induction methods
that rely on collecting real texts while being more general and flexible
without the need for a predefined ontology.",0,1,1,0,0,0,0.637117,7.0,0.772021,85
8d9899b1-68ee-437a-a951-a0c923e1efb3,PanoDepth: A Two-Stage Approach for Monocular Omnidirectional Depth Estimation,9,0.10569,0.480173,"Omnidirectional 3D information is essential for a wide range of applications
such as Virtual Reality, Autonomous Driving, Robotics, etc. In this paper, we
propose a novel, model-agnostic, two-stage pipeline for omnidirectional
monocular depth estimation. Our proposed framework PanoDepth takes one 360
image as input, produces one or more synthesized views in the first stage, and
feeds the original image and the synthesized images into the subsequent stereo
matching stage. In the second stage, we propose a differentiable Spherical
Warping Layer to handle omnidirectional stereo geometry efficiently and
effectively. By utilizing the explicit stereo-based geometric constraints in
the stereo matching stage, PanoDepth can generate dense high-quality depth. We
conducted extensive experiments and ablation studies to evaluate PanoDepth with
both the full pipeline as well as the individual modules in each stage. Our
results show that PanoDepth outperforms the state-of-the-art approaches by a
large margin for 360 monocular depth estimation.",0,1,0,0,1,0,0.532426,7.0,0.730912,69
9680b0f1-4221-4296-a74c-3a274f3339b0,Generalized Differentiable RANSAC,9,0.124331,0.871734,"We propose $\nabla$-RANSAC, a generalized differentiable RANSAC that allows
learning the entire randomized robust estimation pipeline. The proposed
approach enables the use of relaxation techniques for estimating the gradients
in the sampling distribution, which are then propagated through a
differentiable solver. The trainable quality function marginalizes over the
scores from all the models estimated within $\nabla$-RANSAC to guide the
network learning accurate and useful inlier probabilities or to train feature
detection and matching networks. Our method directly maximizes the probability
of drawing a good hypothesis, allowing us to learn better sampling
distributions. We test $\nabla$-RANSAC on various real-world scenarios on
fundamental and essential matrix estimation, and 3D point cloud registration,
outdoors and indoors, with handcrafted and learning-based features. It is
superior to the state-of-the-art in terms of accuracy while running at a
similar speed to its less accurate alternatives. The code and trained models
are available at https://github.com/weitong8591/differentiable_ransac.",1,0,0,0,1,0,0.338791,14.0,0.821994,80
8bdfab2f-4c7e-40d2-9d9b-29abbc93fd32,Black-box Prompt Learning for Pre-trained Language Models,48,0.372639,0.447845,"The increasing scale of general-purpose Pre-trained Language Models (PLMs)
necessitates the study of more efficient adaptation across different downstream
tasks. In this paper, we establish a Black-box Discrete Prompt Learning (BDPL)
to resonate with pragmatic interactions between the cloud infrastructure and
edge devices. Particularly, instead of fine-tuning the model in the cloud, we
adapt PLMs by prompt learning, which efficiently optimizes only a few
parameters of the discrete prompts. Moreover, we consider the scenario that we
do not have access to the parameters and gradients of the pre-trained models,
except for its outputs given inputs. This black-box setting secures the cloud
infrastructure from potential attack and misuse to cause a single-point
failure, which is preferable to the white-box counterpart by current
infrastructures. Under this black-box constraint, we apply a variance-reduced
policy gradient algorithm to estimate the gradients of parameters in the
categorical distribution of each discrete prompt. In light of our method, the
user devices can efficiently tune their tasks by querying the PLMs bounded by a
range of API calls. Our experiments on RoBERTa and GPT-3 demonstrate that the
proposed algorithm achieves significant improvement on eight benchmarks in a
cloud-device collaboration manner. Finally, we conduct in-depth case studies to
comprehensively analyze our method in terms of various data sizes, prompt
lengths, training budgets, optimization objectives, prompt transferability, and
explanations of the learned prompts. Our code will be available at
https://github.com/shizhediao/Black-Box-Prompt-Learning.",0,1,0,0,0,0,0.910204,4.0,0.817569,85
28bba39c-b264-4e42-be5f-4c7de6055597,Underspecification in Scene Description-to-Depiction Tasks,25,0.703881,0.908343,"Questions regarding implicitness, ambiguity and underspecification are
crucial for understanding the task validity and ethical concerns of multimodal
image+text systems, yet have received little attention to date. This position
paper maps out a conceptual framework to address this gap, focusing on systems
which generate images depicting scenes from scene descriptions. In doing so, we
account for how texts and images convey meaning differently. We outline a set
of core challenges concerning textual and visual ambiguity, as well as risks
that may be amplified by ambiguous and underspecified elements. We propose and
discuss strategies for addressing these challenges, including generating
visually ambiguous images, and generating a set of diverse images.",0,0,0,0,0,0,0.836694,7.0,0.855009,91
5f43b2aa-b9b1-45d6-a24c-bb8a872301e0,Context based lemmatizer for Polish language,1,0.041657,0.28529,"Lemmatization is the process of grouping together the inflected forms of a
word so they can be analysed as a single item, identified by the word's lemma,
or dictionary form. In computational linguistics, lemmatisation is the
algorithmic process of determining the lemma of a word based on its intended
meaning. Unlike stemming, lemmatisation depends on correctly identifying the
intended part of speech and meaning of a word in a sentence, as well as within
the larger context surrounding that sentence. As a result, developing efficient
lemmatisation algorithm is the complex task. In recent years it can be observed
that deep learning models used for this task outperform other methods including
machine learning algorithms. In this paper the polish lemmatizer based on
Google T5 model is presented. The training was run with different context
lengths. The model achieves the best results for polish language lemmatisation
process.",0,1,0,0,0,0,0.419451,11.0,0.798305,12
485b5810-0b85-4657-a237-4deacd0bbc2e,ViTOL: Vision Transformer for Weakly Supervised Object Localization,17,0.229209,0.735191,"Weakly supervised object localization (WSOL) aims at predicting object
locations in an image using only image-level category labels. Common challenges
that image classification models encounter when localizing objects are, (a)
they tend to look at the most discriminative features in an image that confines
the localization map to a very small region, (b) the localization maps are
class agnostic, and the models highlight objects of multiple classes in the
same image and, (c) the localization performance is affected by background
noise. To alleviate the above challenges we introduce the following simple
changes through our proposed method ViTOL. We leverage the vision-based
transformer for self-attention and introduce a patch-based attention dropout
layer (p-ADL) to increase the coverage of the localization map and a gradient
attention rollout mechanism to generate class-dependent attention maps. We
conduct extensive quantitative, qualitative and ablation experiments on the
ImageNet-1K and CUB datasets. We achieve state-of-the-art MaxBoxAcc-V2
localization scores of 70.47% and 73.17% on the two datasets respectively. Code
is available on https://github.com/Saurav-31/ViTOL",1,1,0,0,1,0,0.864581,8.0,0.885425,34
9fabc5dd-2441-4100-8c47-8d1697cb97c6,SmartBrush: Text and Shape Guided Object Inpainting with Diffusion Model,95,0.90823,0.873921,"Generic image inpainting aims to complete a corrupted image by borrowing
surrounding information, which barely generates novel content. By contrast,
multi-modal inpainting provides more flexible and useful controls on the
inpainted content, \eg, a text prompt can be used to describe an object with
richer attributes, and a mask can be used to constrain the shape of the
inpainted object rather than being only considered as a missing area. We
propose a new diffusion-based model named SmartBrush for completing a missing
region with an object using both text and shape-guidance. While previous work
such as DALLE-2 and Stable Diffusion can do text-guided inapinting they do not
support shape guidance and tend to modify background texture surrounding the
generated object. Our model incorporates both text and shape guidance with
precision control. To preserve the background better, we propose a novel
training and sampling strategy by augmenting the diffusion U-net with
object-mask prediction. Lastly, we introduce a multi-task training strategy by
jointly training inpainting with text-to-image generation to leverage more
training data. We conduct extensive experiments showing that our model
outperforms all baselines in terms of visual quality, mask controllability, and
background preservation.",1,1,0,0,1,0,0.989058,2.0,0.949002,32
411cb6d7-aea1-4d56-81cf-52ccf8c08316,Semi-supervised Object Detection via Virtual Category Learning,5,0.0809557,0.317087,"Due to the costliness of labelled data in real-world applications,
semi-supervised object detectors, underpinned by pseudo labelling, are
appealing. However, handling confusing samples is nontrivial: discarding
valuable confusing samples would compromise the model generalisation while
using them for training would exacerbate the confirmation bias issue caused by
inevitable mislabelling. To solve this problem, this paper proposes to use
confusing samples proactively without label correction. Specifically, a virtual
category (VC) is assigned to each confusing sample such that they can safely
contribute to the model optimisation even without a concrete label. It is
attributed to specifying the embedding distance between the training sample and
the virtual category as the lower bound of the inter-class distance. Moreover,
we also modify the localisation loss to allow high-quality boundaries for
location regression. Extensive experiments demonstrate that the proposed VC
learning significantly surpasses the state-of-the-art, especially with small
amounts of available labels.",1,1,0,0,1,0,0.967106,7.0,0.945512,43
2d70e041-255b-4c8f-ad59-096d10e0c735,DS-GPS : A Deep Statistical Graph Poisson Solver (for faster CFD simulations),1,0.00408945,0.0446911,"This paper proposes a novel Machine Learning-based approach to solve a
Poisson problem with mixed boundary conditions. Leveraging Graph Neural
Networks, we develop a model able to process unstructured grids with the
advantage of enforcing boundary conditions by design. By directly minimizing
the residual of the Poisson equation, the model attempts to learn the physics
of the problem without the need for exact solutions, in contrast to most
previous data-driven processes where the distance with the available solutions
is minimized.",0,1,0,0,0,0,0.124696,6.0,0.395756,23
b1d3e427-a460-42ac-a157-e2275e313a57,An Intelligent Self-driving Truck System For Highway Transportation,8,0.122352,0.658314,"Recently, there have been many advances in autonomous driving society,
attracting a lot of attention from academia and industry. However, existing
works mainly focus on cars, extra development is still required for
self-driving truck algorithms and models. In this paper, we introduce an
intelligent self-driving truck system. Our presented system consists of three
main components, 1) a realistic traffic simulation module for generating
realistic traffic flow in testing scenarios, 2) a high-fidelity truck model
which is designed and evaluated for mimicking real truck response in real-world
deployment, 3) an intelligent planning module with learning-based decision
making algorithm and multi-mode trajectory planner, taking into account the
truck's constraints, road slope changes, and the surrounding traffic flow. We
provide quantitative evaluations for each component individually to demonstrate
the fidelity and performance of each part. We also deploy our proposed system
on a real truck and conduct real world experiments which shows our system's
capacity of mitigating sim-to-real gap. Our code is available at
https://github.com/InceptioResearch/IITS",1,1,0,0,0,0,0.642429,9.0,0.824288,43
7cbdd6d6-8f48-4823-b41d-18f8482b26c8,Option-Aware Adversarial Inverse Reinforcement Learning for Robotic Control,13,0.0977668,0.60274,"Hierarchical Imitation Learning (HIL) has been proposed to recover
highly-complex behaviors in long-horizon tasks from expert demonstrations by
modeling the task hierarchy with the option framework. Existing methods either
overlook the causal relationship between the subtask and its corresponding
policy or cannot learn the policy in an end-to-end fashion, which leads to
suboptimality. In this work, we develop a novel HIL algorithm based on
Adversarial Inverse Reinforcement Learning and adapt it with the
Expectation-Maximization algorithm in order to directly recover a hierarchical
policy from the unannotated demonstrations. Further, we introduce a directed
information term to the objective function to enhance the causality and propose
a Variational Autoencoder framework for learning with our objectives in an
end-to-end fashion. Theoretical justifications and evaluations on challenging
robotic control tasks are provided to show the superiority of our algorithm.
The codes are available at https://github.com/LucasCJYSDL/HierAIRL.",1,0,0,0,0,0,0.301244,13.0,0.797277,41
84e87e5c-9534-45e1-a98a-344a8b4e6b92,CaMEL: Mean Teacher Learning for Image Captioning,23,0.0910202,0.463523,"Describing images in natural language is a fundamental step towards the
automatic modeling of connections between the visual and textual modalities. In
this paper we present CaMEL, a novel Transformer-based architecture for image
captioning. Our proposed approach leverages the interaction of two
interconnected language models that learn from each other during the training
phase. The interplay between the two language models follows a mean teacher
learning paradigm with knowledge distillation. Experimentally, we assess the
effectiveness of the proposed solution on the COCO dataset and in conjunction
with different visual feature extractors. When comparing with existing
proposals, we demonstrate that our model provides state-of-the-art caption
quality with a significantly reduced number of parameters. According to the
CIDEr metric, we obtain a new state of the art on COCO when training without
using external data. The source code and trained models are publicly available
at: https://github.com/aimagelab/camel.",1,1,0,0,1,0,0.667289,6.0,0.747718,63
4ce68478-e79e-4daf-8c24-acdba2a44d3a,A Systematic Evaluation of Response Selection for Open Domain Dialogue,7,0.0284407,0.787324,"Recent progress on neural approaches for language processing has triggered a
resurgence of interest on building intelligent open-domain chatbots. However,
even the state-of-the-art neural chatbots cannot produce satisfying responses
for every turn in a dialog. A practical solution is to generate multiple
response candidates for the same context, and then perform response
ranking/selection to determine which candidate is the best. Previous work in
response selection typically trains response rankers using synthetic data that
is formed from existing dialogs by using a ground truth response as the single
appropriate response and constructing inappropriate responses via random
selection or using adversarial methods. In this work, we curated a dataset
where responses from multiple response generators produced for the same dialog
context are manually annotated as appropriate (positive) and inappropriate
(negative). We argue that such training data better matches the actual use case
examples, enabling the models to learn to rank responses effectively. With this
new dataset, we conduct a systematic evaluation of state-of-the-art methods for
response selection, and demonstrate that both strategies of using multiple
positive candidates and using manually verified hard negative candidates can
bring in significant performance improvement in comparison to using the
adversarial training data, e.g., increase of 3% and 13% in Recall@1 score,
respectively.",0,1,0,1,0,0,0.138311,6.0,0.414303,43
652fa54c-01dc-4731-8102-90049d967fa7,MAViL: Masked Audio-Video Learners,27,0.238777,0.904404,"We present Masked Audio-Video Learners (MAViL) to train audio-visual
representations. Our approach learns with three complementary forms of
self-supervision: (1) reconstruction of masked audio and video input data, (2)
intra- and inter-modal contrastive learning with masking, and (3) self-training
by reconstructing joint audio-video contextualized features learned from the
first two objectives. Pre-training with MAViL not only enables the model to
perform well in audio-visual classification and retrieval tasks but also
improves representations of each modality in isolation, without using
information from the other modality for fine-tuning or inference. Empirically,
MAViL sets a new state-of-the-art on AudioSet (53.1 mAP) and VGGSound (67.1%
accuracy). For the first time, a self-supervised audio-visual model outperforms
ones that use external supervision on these benchmarks.",1,0,0,0,1,0,0.824103,6.0,0.823869,107
83692936-701f-413c-8491-ef4ed38e9af6,Competency Assessment for Autonomous Agents using Deep Generative Models,8,0.165798,0.438298,"For autonomous agents to act as trustworthy partners to human users, they
must be able to reliably communicate their competency for the tasks they are
asked to perform. Towards this objective, we develop probabilistic world models
based on deep generative modelling that allow for the simulation of agent
trajectories and accurate calculation of tasking outcome probabilities. By
combining the strengths of conditional variational autoencoders with recurrent
neural networks, the deep generative world model can probabilistically forecast
trajectories over long horizons to task completion. We show how these
forecasted trajectories can be used to calculate outcome probability
distributions, which enable the precise assessment of agent competency for
specific tasks and initial settings.",0,1,0,0,0,0,0.724114,9.0,0.849275,35
8a10ec12-22a5-46b9-b12a-a48386c8cfa8,The Problem of Semantic Shift in Longitudinal Monitoring of Social Media: A Case Study on Mental Health During the COVID-19 Pandemic,2,0.0325647,0.0454806,"Social media allows researchers to track societal and cultural changes over
time based on language analysis tools. Many of these tools rely on statistical
algorithms which need to be tuned to specific types of language. Recent studies
have shown the absence of appropriate tuning, specifically in the presence of
semantic shift, can hinder robustness of the underlying methods. However,
little is known about the practical effect this sensitivity may have on
downstream longitudinal analyses. We explore this gap in the literature through
a timely case study: understanding shifts in depression during the course of
the COVID-19 pandemic. We find that inclusion of only a small number of
semantically-unstable features can promote significant changes in longitudinal
estimates of our target outcome. At the same time, we demonstrate that a
recently-introduced method for measuring semantic shift may be used to
proactively identify failure points of language-based models and, in turn,
improve predictive generalization.",0,1,0,0,0,0,0.22349,9.0,0.668442,104
64223718-0251-4936-a16d-b8dd0f500d96,Generative Pretraining for Black-Box Optimization,12,0.130248,0.566485,"Many problems in science and engineering involve optimizing an expensive
black-box function over a high-dimensional space. For such black-box
optimization (BBO) problems, we typically assume a small budget for online
function evaluations, but also often have access to a fixed, offline dataset
for pretraining. Prior approaches seek to utilize the offline data to
approximate the function or its inverse but are not sufficiently accurate far
from the data distribution. We propose BONET, a generative framework for
pretraining a novel black-box optimizer using offline datasets. In BONET, we
train an autoregressive model on fixed-length trajectories derived from an
offline dataset. We design a sampling strategy to synthesize trajectories from
offline data using a simple heuristic of rolling out monotonic transitions from
low-fidelity to high-fidelity samples. Empirically, we instantiate BONET using
a causally masked Transformer and evaluate it on Design-Bench, where we rank
the best on average, outperforming state-of-the-art baselines.",1,0,0,0,1,0,0.515292,6.0,0.677981,60
487d6955-37ea-4866-aa74-1b47a37b26a6,PLM-ICD: Automatic ICD Coding with Pretrained Language Models,29,0.758295,0.523438,"Automatically classifying electronic health records (EHRs) into diagnostic
codes has been challenging to the NLP community. State-of-the-art methods
treated this problem as a multilabel classification problem and proposed
various architectures to model this problem. However, these systems did not
leverage the superb performance of pretrained language models, which achieved
superb performance on natural language understanding tasks. Prior work has
shown that pretrained language models underperformed on this task with the
regular finetuning scheme. Therefore, this paper aims at analyzing the causes
of the underperformance and developing a framework for automatic ICD coding
with pretrained language models. We spotted three main issues through the
experiments: 1) large label space, 2) long input sequences, and 3) domain
mismatch between pretraining and fine-tuning. We propose PLMICD, a framework
that tackles the challenges with various strategies. The experimental results
show that our proposed framework can overcome the challenges and achieves
state-of-the-art performance in terms of multiple metrics on the benchmark
MIMIC data. The source code is available at https://github.com/MiuLab/PLM-ICD",1,1,0,0,1,0,0.963012,7.0,0.940518,41
7cfe385e-3718-483a-aba2-bb748644765a,"ESGBERT: Language Model to Help with Classification Tasks Related to Companies Environmental, Social, and Governance Practices",16,0.213766,0.334218,"Environmental, Social, and Governance (ESG) are non-financial factors that
are garnering attention from investors as they increasingly look to apply these
as part of their analysis to identify material risks and growth opportunities.
Some of this attention is also driven by clients who, now more aware than ever,
are demanding for their money to be managed and invested responsibly. As the
interest in ESG grows, so does the need for investors to have access to
consumable ESG information. Since most of it is in text form in reports,
disclosures, press releases, and 10-Q filings, we see a need for sophisticated
NLP techniques for classification tasks for ESG text. We hypothesize that an
ESG domain-specific pre-trained model will help with such and study building of
the same in this paper. We explored doing this by fine-tuning BERTs pre-trained
weights using ESG specific text and then further fine-tuning the model for a
classification task. We were able to achieve accuracy better than the original
BERT and baseline models in environment-specific classification tasks.",0,1,0,0,0,0,0.444761,11.0,0.805467,18
8ec51ac3-17b4-4f91-8427-46d727665f4d,Q-ViT: Fully Differentiable Quantization for Vision Transformer,28,0.0,0.808892,"In this paper, we propose a fully differentiable quantization method for
vision transformer (ViT) named as Q-ViT, in which both of the quantization
scales and bit-widths are learnable parameters. Specifically, based on our
observation that heads in ViT display different quantization robustness, we
leverage head-wise bit-width to squeeze the size of Q-ViT while preserving
performance. In addition, we propose a novel technique named switchable scale
to resolve the convergence problem in the joint training of quantization scales
and bit-widths. In this way, Q-ViT pushes the limits of ViT quantization to
3-bit without heavy performance drop. Moreover, we analyze the quantization
robustness of every architecture component of ViT and show that the Multi-head
Self-Attention (MSA) and the Gaussian Error Linear Units (GELU) are the key
aspects for ViT quantization. This study provides some insights for further
research about ViT quantization. Extensive experiments on different ViT models,
such as DeiT and Swin Transformer show the effectiveness of our quantization
method. In particular, our method outperforms the state-of-the-art uniform
quantization method by 1.5% on DeiT-Tiny.",1,1,0,0,1,0,0.255991,6.0,0.528694,46
ed84bdb8-115c-42fc-8575-e86a37187ac1,Probing Pre-Trained Language Models for Cross-Cultural Differences in Values,53,0.940103,0.997702,"Language embeds information about social, cultural, and political values
people hold. Prior work has explored social and potentially harmful biases
encoded in Pre-Trained Language models (PTLMs). However, there has been no
systematic study investigating how values embedded in these models vary across
cultures. In this paper, we introduce probes to study which values across
cultures are embedded in these models, and whether they align with existing
theories and cross-cultural value surveys. We find that PTLMs capture
differences in values across cultures, but those only weakly align with
established value surveys. We discuss implications of using mis-aligned models
in cross-cultural settings, as well as ways of aligning PTLMs with value
surveys.",0,0,0,0,0,0,0.958727,5.0,0.909962,75
5a29b876-7f3d-4456-b8b4-8b5e32efb5f6,Ensembles for Uncertainty Estimation: Benefits of Prior Functions and Bootstrapping,10,0.0488129,0.656316,"In machine learning, an agent needs to estimate uncertainty to efficiently
explore and adapt and to make effective decisions. A common approach to
uncertainty estimation maintains an ensemble of models. In recent years,
several approaches have been proposed for training ensembles, and conflicting
views prevail with regards to the importance of various ingredients of these
approaches. In this paper, we aim to address the benefits of two ingredients --
prior functions and bootstrapping -- which have come into question. We show
that prior functions can significantly improve an ensemble agent's joint
predictions across inputs and that bootstrapping affords additional benefits if
the signal-to-noise ratio varies across inputs. Our claims are justified by
both theoretical and experimental results.",0,0,0,0,0,0,0.145287,9.0,0.615443,40
64bcf43c-ac0e-4920-89c8-587e294e10e5,Multiview Stereo with Cascaded Epipolar RAFT,23,0.163202,0.877271,"We address multiview stereo (MVS), an important 3D vision task that
reconstructs a 3D model such as a dense point cloud from multiple calibrated
images. We propose CER-MVS (Cascaded Epipolar RAFT Multiview Stereo), a new
approach based on the RAFT (Recurrent All-Pairs Field Transforms) architecture
developed for optical flow. CER-MVS introduces five new changes to RAFT:
epipolar cost volumes, cost volume cascading, multiview fusion of cost volumes,
dynamic supervision, and multiresolution fusion of depth maps. CER-MVS is
significantly different from prior work in multiview stereo. Unlike prior work,
which operates by updating a 3D cost volume, CER-MVS operates by updating a
disparity field. Furthermore, we propose an adaptive thresholding method to
balance the completeness and accuracy of the reconstructed point clouds.
Experiments show that our approach achieves competitive performance on DTU (the
second best among known results) and state-of-the-art performance on the
Tanks-and-Temples benchmark (both the intermediate and advanced set). Code is
available at https://github.com/princeton-vl/CER-MVS",0,1,0,0,1,0,0.608006,6.0,0.720828,44
410df5f5-56fe-4fb2-b799-234750422ee6,Computing Abductive Explanations for Boosted Trees,5,0.170148,0.271404,"Boosted trees is a dominant ML model, exhibiting high accuracy. However,
boosted trees are hardly intelligible, and this is a problem whenever they are
used in safety-critical applications. Indeed, in such a context, rigorous
explanations of the predictions made are expected. Recent work have shown how
subset-minimal abductive explanations can be derived for boosted trees, using
automated reasoning techniques. However, the generation of such well-founded
explanations is intractable in the general case. To improve the scalability of
their generation, we introduce the notion of tree-specific explanation for a
boosted tree. We show that tree-specific explanations are abductive
explanations that can be computed in polynomial time. We also explain how to
derive a subset-minimal abductive explanation from a tree-specific explanation.
Experiments on various datasets show the computational benefits of leveraging
tree-specific explanations for deriving subset-minimal abductive explanations.",0,0,0,0,0,0,0.822643,7.0,0.848349,51
8f11b222-822a-4207-871f-1a409abe1a0f,Informed Multi-context Entity Alignment,8,0.218045,0.556485,"Entity alignment is a crucial step in integrating knowledge graphs (KGs) from
multiple sources. Previous attempts at entity alignment have explored different
KG structures, such as neighborhood-based and path-based contexts, to learn
entity embeddings, but they are limited in capturing the multi-context
features. Moreover, most approaches directly utilize the embedding similarity
to determine entity alignment without considering the global interaction among
entities and relations. In this work, we propose an Informed Multi-context
Entity Alignment (IMEA) model to address these issues. In particular, we
introduce Transformer to flexibly capture the relation, path, and neighborhood
contexts, and design holistic reasoning to estimate alignment probabilities
based on both embedding similarity and the relation/entity functionality. The
alignment evidence obtained from holistic reasoning is further injected back
into the Transformer via the proposed soft label editing to inform embedding
learning. Experimental results on several benchmark datasets demonstrate the
superiority of our IMEA model compared with existing state-of-the-art entity
alignment methods.",0,0,0,0,1,0,0.963685,6.0,0.93153,36
f3c1c6ef-5ade-4870-8928-3b6f535dfb47,Semantic-shape Adaptive Feature Modulation for Semantic Image Synthesis,18,0.137187,0.238035,"Recent years have witnessed substantial progress in semantic image synthesis,
it is still challenging in synthesizing photo-realistic images with rich
details. Most previous methods focus on exploiting the given semantic map,
which just captures an object-level layout for an image. Obviously, a
fine-grained part-level semantic layout will benefit object details generation,
and it can be roughly inferred from an object's shape. In order to exploit the
part-level layouts, we propose a Shape-aware Position Descriptor (SPD) to
describe each pixel's positional feature, where object shape is explicitly
encoded into the SPD feature. Furthermore, a Semantic-shape Adaptive Feature
Modulation (SAFM) block is proposed to combine the given semantic map and our
positional features to produce adaptively modulated features. Extensive
experiments demonstrate that the proposed SPD and SAFM significantly improve
the generation of objects with rich details. Moreover, our method performs
favorably against the SOTA methods in terms of quantitative and qualitative
evaluation. The source code and model are available at
https://github.com/cszy98/SAFM.",0,1,0,0,1,0,0.816471,10.0,0.891847,42
2fb23e8b-104c-4522-a849-71f84ad115ec,Search to Pass Messages for Temporal Knowledge Graph Completion,5,0.0975386,0.60653,"Completing missing facts is a fundamental task for temporal knowledge graphs
(TKGs). Recently, graph neural network (GNN) based methods, which can
simultaneously explore topological and temporal information, have become the
state-of-the-art (SOTA) to complete TKGs. However, these studies are based on
hand-designed architectures and fail to explore the diverse topological and
temporal properties of TKG. To address this issue, we propose to use neural
architecture search (NAS) to design data-specific message passing architecture
for TKG completion. In particular, we develop a generalized framework to
explore topological and temporal information in TKGs. Based on this framework,
we design an expressive search space to fully capture various properties of
different TKGs. Meanwhile, we adopt a search algorithm, which trains a supernet
structure by sampling single path for efficient search with less cost. We
further conduct extensive experiments on three benchmark datasets. The results
show that the searched architectures by our method achieve the SOTA
performances. Besides, the searched models can also implicitly reveal diverse
properties in different TKGs. Our code is released in
https://github.com/striderdu/SPA.",1,1,0,0,1,0,0.776443,7.0,0.827822,52
6ea0fa94-3e1e-4d8c-a0fd-424d6d29dc95,Automatic Language Identification for Celtic Texts,2,0.0221681,0.0827117,"Language identification is an important Natural Language Processing task. It
has been thoroughly researched in the literature. However, some issues are
still open. This work addresses the identification of the related low-resource
languages on the example of the Celtic language family.
  This work's main goals were: (1) to collect the dataset of three Celtic
languages; (2) to prepare a method to identify the languages from the Celtic
family, i.e. to train a successful classification model; (3) to evaluate the
influence of different feature extraction methods, and explore the
applicability of the unsupervised models as a feature extraction technique; (4)
to experiment with the unsupervised feature extraction on a reduced annotated
set.
  We collected a new dataset including Irish, Scottish, Welsh and English
records. We tested supervised models such as SVM and neural networks with
traditional statistical features alongside the output of clustering,
autoencoder, and topic modelling methods. The analysis showed that the
unsupervised features could serve as a valuable extension to the n-gram feature
vectors. It led to an improvement in performance for more entangled classes.
The best model achieved a 98\% F1 score and 97\% MCC. The dense neural network
consistently outperformed the SVM model.
  The low-resource languages are also challenging due to the scarcity of
available annotated training data. This work evaluated the performance of the
classifiers using the unsupervised feature extraction on the reduced labelled
dataset to handle this issue. The results uncovered that the unsupervised
feature vectors are more robust to the labelled set reduction. Therefore, they
proved to help achieve comparable classification performance with much less
labelled data.",0,1,0,1,0,0,0.00505189,11.0,0.373191,27
1edb5b28-336f-4b30-afd4-d8be3f72bb7e,Analyzing Generalization of Vision and Language Navigation to Unseen Outdoor Areas,17,0.524187,0.458816,"Vision and language navigation (VLN) is a challenging visually-grounded
language understanding task. Given a natural language navigation instruction, a
visual agent interacts with a graph-based environment equipped with panorama
images and tries to follow the described route. Most prior work has been
conducted in indoor scenarios where best results were obtained for navigation
on routes that are similar to the training routes, with sharp drops in
performance when testing on unseen environments. We focus on VLN in outdoor
scenarios and find that in contrast to indoor VLN, most of the gain in outdoor
VLN on unseen data is due to features like junction type embedding or heading
delta that are specific to the respective environment graph, while image
information plays a very minor role in generalizing VLN to unseen outdoor
areas. These findings show a bias to specifics of graph representations of
urban environments, demanding that VLN tasks grow in scale and diversity of
geographical environments.",1,1,0,1,0,0,0.919283,7.0,0.901936,34
9035f65e-926b-4df7-8367-ced2eb938e05,Membership Inference Attacks via Adversarial Examples,3,0.0240477,0.132383,"The raise of machine learning and deep learning led to significant
improvement in several domains. This change is supported by both the dramatic
rise in computation power and the collection of large datasets. Such massive
datasets often include personal data which can represent a threat to privacy.
Membership inference attacks are a novel direction of research which aims at
recovering training data used by a learning algorithm. In this paper, we
develop a mean to measure the leakage of training data leveraging a quantity
appearing as a proxy of the total variation of a trained model near its
training samples. We extend our work by providing a novel defense mechanism.
Our contributions are supported by empirical evidence through convincing
numerical experiments.",0,0,0,0,0,0,0.722356,8.0,0.829816,83
abe4edce-d786-43ba-8d97-426a9c2bd2f6,Diffusion-LM Improves Controllable Text Generation,447,0.997075,0.999017,"Controlling the behavior of language models (LMs) without re-training is a
major open problem in natural language generation. While recent works have
demonstrated successes on controlling simple sentence attributes (e.g.,
sentiment), there has been little progress on complex, fine-grained controls
(e.g., syntactic structure). To address this challenge, we develop a new
non-autoregressive language model based on continuous diffusions that we call
Diffusion-LM. Building upon the recent successes of diffusion models in
continuous domains, Diffusion-LM iteratively denoises a sequence of Gaussian
vectors into word vectors, yielding a sequence of intermediate latent
variables. The continuous, hierarchical nature of these intermediate variables
enables a simple gradient-based algorithm to perform complex, controllable
generation tasks. We demonstrate successful control of Diffusion-LM for six
challenging fine-grained control tasks, significantly outperforming prior work.",1,0,0,0,0,0,0.961826,5.0,0.914802,59
99b79646-253c-4f6a-9d50-5ab11e618a55,Detecting Arbitrary Keypoints on Limbs and Skis with Sparse Partly Correct Segmentation Masks,5,0.04758,0.559356,"Analyses based on the body posture are crucial for top-class athletes in many
sports disciplines. If at all, coaches label only the most important keypoints,
since manual annotations are very costly. This paper proposes a method to
detect arbitrary keypoints on the limbs and skis of professional ski jumpers
that requires a few, only partly correct segmentation masks during training.
Our model is based on the Vision Transformer architecture with a special design
for the input tokens to query for the desired keypoints. Since we use
segmentation masks only to generate ground truth labels for the freely
selectable keypoints, partly correct segmentation masks are sufficient for our
training procedure. Hence, there is no need for costly hand-annotated
segmentation masks. We analyze different training techniques for freely
selected and standard keypoints, including pseudo labels, and show in our
experiments that only a few partly correct segmentation masks are sufficient
for learning to detect arbitrary keypoints on limbs and skis.",1,1,0,1,0,0,0.23829,7.0,0.584186,25
935590db-06d5-4312-87ba-279ebabf4bb7,Pushing the Efficiency Limit Using Structured Sparse Convolutions,2,0.03316,0.125902,"Weight pruning is among the most popular approaches for compressing deep
convolutional neural networks. Recent work suggests that in a randomly
initialized deep neural network, there exist sparse subnetworks that achieve
performance comparable to the original network. Unfortunately, finding these
subnetworks involves iterative stages of training and pruning, which can be
computationally expensive. We propose Structured Sparse Convolution (SSC),
which leverages the inherent structure in images to reduce the parameters in
the convolutional filter. This leads to improved efficiency of convolutional
architectures compared to existing methods that perform pruning at
initialization. We show that SSC is a generalization of commonly used layers
(depthwise, groupwise and pointwise convolution) in ``efficient
architectures.'' Extensive experiments on well-known CNN models and datasets
show the effectiveness of the proposed method. Architectures based on SSC
achieve state-of-the-art performance compared to baselines on CIFAR-10,
CIFAR-100, Tiny-ImageNet, and ImageNet classification benchmarks.",1,1,0,0,1,0,0.900533,9.0,0.914101,64
421d373e-68a4-43ee-af5e-37de97080a18,Hyperspectral images classification and Dimensionality Reduction using Homogeneity feature and mutual information,4,0.025838,0.022293,"The Hyperspectral image (HSI) contains several hundred bands of the same
region called the Ground Truth (GT). The bands are taken in juxtaposed
frequencies, but some of them are noisily measured or contain no information.
For the classification, the selection of bands, affects significantly the
results of classification, in fact, using a subset of relevant bands, these
results can be better than those obtained using all bands, from which the need
to reduce the dimensionality of the HSI. In this paper, a categorization of
dimensionality reduction methods, according to the generation process, is
presented. Furthermore, we reproduce an algorithm based on mutual information
(MI) to reduce dimensionality by features selection and we introduce an
algorithm using mutual information and homogeneity. The two schemas are a
filter strategy. Finally, to validate this, we consider the case study AVIRIS
HSI 92AV3C.
  Keywords: Hyperspectrale images; classification; features selection; mutual
information; homogeneity",0,1,0,0,0,0,0.0379314,15.0,0.675857,10
30ec3ec5-0648-4ad3-808e-15cb14179b56,DoCoGen: Domain Counterfactual Generation for Low Resource Domain Adaptation,32,0.100294,0.771033,"Natural language processing (NLP) algorithms have become very successful, but
they still struggle when applied to out-of-distribution examples. In this paper
we propose a controllable generation approach in order to deal with this domain
adaptation (DA) challenge. Given an input text example, our DoCoGen algorithm
generates a domain-counterfactual textual example (D-con) - that is similar to
the original in all aspects, including the task label, but its domain is
changed to a desired one. Importantly, DoCoGen is trained using only unlabeled
examples from multiple domains - no NLP task labels or parallel pairs of
textual examples and their domain-counterfactuals are required. We show that
DoCoGen can generate coherent counterfactuals consisting of multiple sentences.
We use the D-cons generated by DoCoGen to augment a sentiment classifier and a
multi-label intent classifier in 20 and 78 DA setups, respectively, where
source-domain labeled data is scarce. Our model outperforms strong baselines
and improves the accuracy of a state-of-the-art unsupervised DA algorithm.",1,1,0,1,1,0,0.338217,5.0,0.501162,67
6d5d4c71-506b-4a77-9dd3-37efbfc326aa,Dress Well via Fashion Cognitive Learning,2,0.0191053,0.11968,"Fashion compatibility models enable online retailers to easily obtain a large
number of outfit compositions with good quality. However, effective fashion
recommendation demands precise service for each customer with a deeper
cognition of fashion. In this paper, we conduct the first study on fashion
cognitive learning, which is fashion recommendations conditioned on personal
physical information. To this end, we propose a Fashion Cognitive Network (FCN)
to learn the relationships among visual-semantic embedding of outfit
composition and appearance features of individuals. FCN contains two
submodules, namely outfit encoder and Multi-label Graph Neural Network
(ML-GCN). The outfit encoder uses a convolutional layer to encode an outfit
into an outfit embedding. The latter module learns label classifiers via
stacked GCN. We conducted extensive experiments on the newly collected O4U
dataset, and the results provide strong qualitative and quantitative evidence
that our framework outperforms alternative methods.",0,1,1,1,1,0,0.14121,8.0,0.563526,31
5c1234bb-7a93-4794-96f3-4e0cb1a02adb,REGTR: End-to-end Point Cloud Correspondences with Transformers,91,0.829798,0.987995,"Despite recent success in incorporating learning into point cloud
registration, many works focus on learning feature descriptors and continue to
rely on nearest-neighbor feature matching and outlier filtering through RANSAC
to obtain the final set of correspondences for pose estimation. In this work,
we conjecture that attention mechanisms can replace the role of explicit
feature matching and RANSAC, and thus propose an end-to-end framework to
directly predict the final set of correspondences. We use a network
architecture consisting primarily of transformer layers containing self and
cross attentions, and train it to predict the probability each point lies in
the overlapping region and its corresponding position in the other point cloud.
The required rigid transformation can then be estimated directly from the
predicted correspondences without further post-processing. Despite its
simplicity, our approach achieves state-of-the-art performance on 3DMatch and
ModelNet benchmarks. Our source code can be found at
https://github.com/yewzijian/RegTR .",1,1,0,0,1,0,0.970297,7.0,0.949719,64
96dbf247-7367-4a3b-a7c9-ae42d71033e8,Spatiotemporal Costmap Inference for MPC via Deep Inverse Reinforcement Learning,18,0.401627,0.777634,"It can be difficult to autonomously produce driver behavior so that it
appears natural to other traffic participants. Through Inverse Reinforcement
Learning (IRL), we can automate this process by learning the underlying reward
function from human demonstrations. We propose a new IRL algorithm that learns
a goal-conditioned spatiotemporal reward function. The resulting costmap is
used by Model Predictive Controllers (MPCs) to perform a task without any
hand-designing or hand-tuning of the cost function. We evaluate our proposed
Goal-conditioned SpatioTemporal Zeroing Maximum Entropy Deep IRL (GSTZ)-MEDIRL
framework together with MPC in the CARLA simulator for autonomous driving, lane
keeping, and lane changing tasks in a challenging dense traffic highway
scenario. Our proposed methods show higher success rates compared to other
baseline methods including behavior cloning, state-of-the-art RL policies, and
MPC with a learning-based behavior prediction model.",0,1,0,0,0,0,0.805329,9.0,0.875899,38
7318d6cb-1209-4292-bf04-af963c2b9ee1,Annotating Norwegian Language Varieties on Twitter for Part-of-Speech,5,0.279777,0.739761,"Norwegian Twitter data poses an interesting challenge for Natural Language
Processing (NLP) tasks. These texts are difficult for models trained on
standardized text in one of the two Norwegian written forms (Bokm{\aa}l and
Nynorsk), as they contain both the typical variation of social media text, as
well as a large amount of dialectal variety. In this paper we present a novel
Norwegian Twitter dataset annotated with POS-tags. We show that models trained
on Universal Dependency (UD) data perform worse when evaluated against this
dataset, and that models trained on Bokm{\aa}l generally perform better than
those trained on Nynorsk. We also see that performance on dialectal tweets is
comparable to the written standards for some models. Finally we perform a
detailed analysis of the errors that models commonly make on this data.",0,1,0,1,0,0,0.0636562,15.0,0.711267,28
b88a6cdb-3cb3-4929-acb0-1a77dd5b37ea,Large Language Models Can Self-Improve,270,0.810093,1.0,"Large Language Models (LLMs) have achieved excellent performances in various
tasks. However, fine-tuning an LLM requires extensive supervision. Human, on
the other hand, may improve their reasoning abilities by self-thinking without
external inputs. In this work, we demonstrate that an LLM is also capable of
self-improving with only unlabeled datasets. We use a pre-trained LLM to
generate ""high-confidence"" rationale-augmented answers for unlabeled questions
using Chain-of-Thought prompting and self-consistency, and fine-tune the LLM
using those self-generated solutions as target outputs. We show that our
approach improves the general reasoning ability of a 540B-parameter LLM
(74.4%->82.1% on GSM8K, 78.2%->83.0% on DROP, 90.0%->94.4% on OpenBookQA, and
63.4%->67.9% on ANLI-A3) and achieves state-of-the-art-level performance,
without any ground truth label. We conduct ablation studies and show that
fine-tuning on reasoning is critical for self-improvement.",0,0,0,0,1,0,0.893382,4.0,0.799093,54
4b33ca24-9d62-4571-b047-b7576687f225,SAD: A Large-scale Dataset towards Airport Detection in Synthetic Aperture Radar Images,1,0.0400747,0.0147331,"Airports have an important role in both military and civilian domains. The
synthetic aperture radar (SAR) based airport detection has received increasing
attention in recent years. However, due to the high cost of SAR imaging and
annotation process, there is no publicly available SAR dataset for airport
detection. As a result, deep learning methods have not been fully used in
airport detection tasks. To provide a benchmark for airport detection research
in SAR images, this paper introduces a large-scale SAR Airport Dataset (SAD).
In order to adequately reflect the demands of real world applications, it
contains 624 SAR images from Sentinel 1B and covers 104 airfield instances with
different scales, orientations and shapes. The experiments of multiple deep
learning approach on this dataset proves its effectiveness. It developing
state-of-the-art airport area detection algorithms or other relevant tasks.",1,1,1,1,0,0,0.394619,11.0,0.791018,41
18bb3ab3-d9e6-4f55-bf72-6e930ad427da,Interpretable Molecular Graph Generation via Monotonic Constraints,17,0.269005,0.626729,"Designing molecules with specific properties is a long-lasting research
problem and is central to advancing crucial domains such as drug discovery and
material science. Recent advances in deep graph generative models treat
molecule design as graph generation problems which provide new opportunities
toward the breakthrough of this long-lasting problem. Existing models, however,
have many shortcomings, including poor interpretability and controllability
toward desired molecular properties. This paper focuses on new methodologies
for molecule generation with interpretable and controllable deep generative
models, by proposing new monotonically-regularized graph variational
autoencoders. The proposed models learn to represent the molecules with latent
variables and then learn the correspondence between them and molecule
properties parameterized by polynomial functions. To further improve the
intepretability and controllability of molecule generation towards desired
properties, we derive new objectives which further enforce monotonicity of the
relation between some latent variables and target molecule properties such as
toxicity and clogP. Extensive experimental evaluation demonstrates the
superiority of the proposed framework on accuracy, novelty, disentanglement,
and control towards desired molecular properties. The code is open-source at
https://anonymous.4open.science/r/MDVAE-FD2C.",0,0,0,0,0,0,0.610128,9.0,0.814527,52
ab4933fb-9e7b-458e-9932-42434d94bbde,Deep Active Visual Attention for Real-time Robot Motion Generation: Emergence of Tool-body Assimilation and Adaptive Tool-use,4,0.080323,0.170053,"Sufficiently perceiving the environment is a critical factor in robot motion
generation. Although the introduction of deep visual processing models have
contributed in extending this ability, existing methods lack in the ability to
actively modify what to perceive; humans perform internally during visual
cognitive processes. This paper addresses the issue by proposing a novel robot
motion generation model, inspired by a human cognitive structure. The model
incorporates a state-driven active top-down visual attention module, which
acquires attentions that can actively change targets based on task states. We
term such attentions as role-based attentions, since the acquired attention
directed to targets that shared a coherent role throughout the motion. The
model was trained on a robot tool-use task, in which the role-based attentions
perceived the robot grippers and tool as identical end-effectors, during object
picking and object dragging motions respectively. This is analogous to a
biological phenomenon called tool-body assimilation, in which one regards a
handled tool as an extension of one's body. The results suggested an
improvement of flexibility in model's visual perception, which sustained stable
attention and motion even if it was provided with untrained tools or exposed to
experimenter's distractions.",0,0,0,0,0,0,0.133998,14.0,0.74655,33
6492a1b1-fb63-42b5-8b1e-0af186c54570,Mono vs Multilingual BERT: A Case Study in Hindi and Marathi Named Entity Recognition,7,0.0485881,0.608526,"Named entity recognition (NER) is the process of recognising and classifying
important information (entities) in text. Proper nouns, such as a person's
name, an organization's name, or a location's name, are examples of entities.
The NER is one of the important modules in applications like human resources,
customer support, search engines, content classification, and academia. In this
work, we consider NER for low-resource Indian languages like Hindi and Marathi.
The transformer-based models have been widely used for NER tasks. We consider
different variations of BERT like base-BERT, RoBERTa, and AlBERT and benchmark
them on publicly available Hindi and Marathi NER datasets. We provide an
exhaustive comparison of different monolingual and multilingual
transformer-based models and establish simple baselines currently missing in
the literature. We show that the monolingual MahaRoBERTa model performs the
best for Marathi NER whereas the multilingual XLM-RoBERTa performs the best for
Hindi NER. We also perform cross-language evaluation and present mixed
observations.",0,1,0,0,0,0,0.0103914,12.0,0.48575,36
21f04163-605b-47ec-b5b6-c13cb9bce571,Enriching Unsupervised User Embedding via Medical Concepts,1,0.0180868,0.056089,"Clinical notes in Electronic Health Records (EHR) present rich documented
information of patients to inference phenotype for disease diagnosis and study
patient characteristics for cohort selection. Unsupervised user embedding aims
to encode patients into fixed-length vectors without human supervisions.
Medical concepts extracted from the clinical notes contain rich connections
between patients and their clinical categories. However, existing unsupervised
approaches of user embeddings from clinical notes do not explicitly incorporate
medical concepts. In this study, we propose a concept-aware unsupervised user
embedding that jointly leverages text documents and medical concepts from two
clinical corpora, MIMIC-III and Diabetes. We evaluate user embeddings on both
extrinsic and intrinsic tasks, including phenotype classification, in-hospital
mortality prediction, patient retrieval, and patient relatedness. Experiments
on the two clinical corpora show our approach exceeds unsupervised baselines,
and incorporating medical concepts can significantly improve the baseline
performance.",0,1,0,0,0,0,0.3948,8.0,0.712725,61
062f97c9-cbd6-4d6b-af18-c3cb36e32ec8,LST: Ladder Side-Tuning for Parameter and Memory Efficient Transfer Learning,120,0.889395,0.987927,"Fine-tuning large pre-trained models on downstream tasks has been adopted in
a variety of domains recently. However, it is costly to update the entire
parameter set of large pre-trained models. Although recently proposed
parameter-efficient transfer learning (PETL) techniques allow updating a small
subset of parameters (e.g. only using 2% of parameters) inside a pre-trained
backbone network for a new task, they only reduce the training memory
requirement by up to 30%. This is because the gradient computation for the
trainable parameters still requires backpropagation through the large
pre-trained backbone model. To address this, we propose Ladder Side-Tuning
(LST), a new PETL technique that can reduce training memory requirements by
more substantial amounts. Unlike existing parameter-efficient methods that
insert additional parameters inside backbone networks, we train a ladder side
network, a small and separate network that takes intermediate activations as
input via shortcut connections (called ladders) from backbone networks and
makes predictions. LST has significantly lower memory requirements than
previous methods, because it does not require backpropagation through the
backbone network, but instead only through the side network and ladder
connections. We evaluate our method with various models (T5 and CLIP-T5) on
both NLP (GLUE) and vision-and-language (VQA, GQA, NLVR2 , MSCOCO) tasks. LST
saves 69% of the memory costs to fine-tune the whole network, while other
methods only save 26% of that in similar parameter usages (hence, 2.7x more
memory savings). Moreover, LST achieves higher accuracy than Adapter and LoRA
in a low-memory regime. To further show the advantage of this better memory
efficiency, we also apply LST to larger T5 models, attaining better GLUE
performance than full fine-tuning and other PETL methods. The
accuracy-efficiency trade-off also holds on VL tasks.",0,1,0,0,0,0,0.959131,6.0,0.925483,68
7b29085f-d7f9-4cd4-b208-870f1cd9f3b6,MRI-GAN: A Generalized Approach to Detect DeepFakes using Perceptual Image Assessment,4,0.174212,0.0577344,"DeepFakes are synthetic videos generated by swapping a face of an original
image with the face of somebody else. In this paper, we describe our work to
develop general, deep learning-based models to classify DeepFake content. We
propose a novel framework for using Generative Adversarial Network (GAN)-based
models, we call MRI-GAN, that utilizes perceptual differences in images to
detect synthesized videos. We test our MRI-GAN approach and a
plain-frames-based model using the DeepFake Detection Challenge Dataset. Our
plain frames-based-model achieves 91% test accuracy and a model which uses our
MRI-GAN framework with Structural Similarity Index Measurement (SSIM) for the
perceptual differences achieves 74% test accuracy. The results of MRI-GAN are
preliminary and may be improved further by modifying the choice of loss
function, tuning hyper-parameters, or by using a more advanced perceptual
similarity metric.",1,1,0,0,0,0,0.99325,6.0,0.999942,26
825381b4-9166-4171-b5f7-00abb81c6d9c,AUC Maximization for Low-Resource Named Entity Recognition,6,0.0380321,0.34534,"Current work in named entity recognition (NER) uses either cross entropy (CE)
or conditional random fields (CRF) as the objective/loss functions to optimize
the underlying NER model. Both of these traditional objective functions for the
NER problem generally produce adequate performance when the data distribution
is balanced and there are sufficient annotated training examples. But since NER
is inherently an imbalanced tagging problem, the model performance under the
low-resource settings could suffer using these standard objective functions.
Based on recent advances in area under the ROC curve (AUC) maximization, we
propose to optimize the NER model by maximizing the AUC score. We give evidence
that by simply combining two binary-classifiers that maximize the AUC score,
significant performance improvement over traditional loss functions is achieved
under low-resource NER settings. We also conduct extensive experiments to
demonstrate the advantages of our method under the low-resource and
highly-imbalanced data distribution settings. To the best of our knowledge,
this is the first work that brings AUC maximization to the NER setting.
Furthermore, we show that our method is agnostic to different types of NER
embeddings, models and domains. The code to replicate this work will be
provided upon request.",1,1,0,0,0,0,0.149721,10.0,0.657158,48
57e1dc25-4ca5-486e-b807-9601dff37a66,Capabilities for Better ML Engineering,2,0.0074522,0.254747,"In spite of machine learning's rapid growth, its engineering support is
scattered in many forms, and tends to favor certain engineering stages,
stakeholders, and evaluation preferences. We envision a capability-based
framework, which uses fine-grained specifications for ML model behaviors to
unite existing efforts towards better ML engineering. We use concrete scenarios
(model design, debugging, and maintenance) to articulate capabilities' broad
applications across various different dimensions, and their impact on building
safer, more generalizable and more trustworthy models that reflect human needs.
Through preliminary experiments, we show capabilities' potential for reflecting
model generalizability, which can provide guidance for ML engineering process.
We discuss challenges and opportunities for capabilities' integration into ML
engineering.",1,0,0,0,0,1,0.475224,7.0,0.707389,32
35711a57-c9ce-4e25-b96a-fcb35bf05308,On the Identifiability of Nonlinear ICA: Sparsity and Beyond,29,0.66152,0.994915,"Nonlinear independent component analysis (ICA) aims to recover the underlying
independent latent sources from their observable nonlinear mixtures. How to
make the nonlinear ICA model identifiable up to certain trivial indeterminacies
is a long-standing problem in unsupervised learning. Recent breakthroughs
reformulate the standard independence assumption of sources as conditional
independence given some auxiliary variables (e.g., class labels and/or
domain/time indexes) as weak supervision or inductive bias. However, nonlinear
ICA with unconditional priors cannot benefit from such developments. We explore
an alternative path and consider only assumptions on the mixing process, such
as Structural Sparsity. We show that under specific instantiations of such
constraints, the independent latent sources can be identified from their
nonlinear mixtures up to a permutation and a component-wise transformation,
thus achieving nontrivial identifiability of nonlinear ICA without auxiliary
variables. We provide estimation methods and validate the theoretical results
experimentally. The results on image data suggest that our conditions may hold
in a number of practical data generating processes.",1,0,0,0,0,0,0.553841,11.0,0.834203,46
4daa5155-ec34-4299-bef5-3fa59a4d72e8,A Semantic Framework for Neural-Symbolic Computing,1,0.00771804,0.112222,"Two approaches to AI, neural networks and symbolic systems, have been proven
very successful for an array of AI problems. However, neither has been able to
achieve the general reasoning ability required for human-like intelligence. It
has been argued that this is due to inherent weaknesses in each approach.
Luckily, these weaknesses appear to be complementary, with symbolic systems
being adept at the kinds of things neural networks have trouble with and
vice-versa. The field of neural-symbolic AI attempts to exploit this asymmetry
by combining neural networks and symbolic AI into integrated systems. Often
this has been done by encoding symbolic knowledge into neural networks.
Unfortunately, although many different methods for this have been proposed,
there is no common definition of an encoding to compare them. We seek to
rectify this problem by introducing a semantic framework for neural-symbolic
AI, which is then shown to be general enough to account for a large family of
neural-symbolic systems. We provide a number of examples and proofs of the
application of the framework to the neural encoding of various forms of
knowledge representation and neural network. These, at first sight disparate
approaches, are all shown to fall within the framework's formal definition of
what we call semantic encoding for neural-symbolic AI.",0,0,0,0,0,0,0.0172119,17.0,0.666887,57
acae0db4-bd4d-4f55-bae0-2359db8d1721,Sparse Probabilistic Circuits via Pruning and Growing,9,0.160329,0.836845,"Probabilistic circuits (PCs) are a tractable representation of probability
distributions allowing for exact and efficient computation of likelihoods and
marginals. There has been significant recent progress on improving the scale
and expressiveness of PCs. However, PC training performance plateaus as model
size increases. We discover that most capacity in existing large PC structures
is wasted: fully-connected parameter layers are only sparsely used. We propose
two operations: pruning and growing, that exploit the sparsity of PC
structures. Specifically, the pruning operation removes unimportant
sub-networks of the PC for model compression and comes with theoretical
guarantees. The growing operation increases model capacity by increasing the
size of the latent space. By alternatingly applying pruning and growing, we
increase the capacity that is meaningfully used, allowing us to significantly
scale up PC learning. Empirically, our learner achieves state-of-the-art
likelihoods on MNIST-family image datasets and on Penn Tree Bank language data
compared to other PC learners and less tractable deep generative models such as
flow-based models and variational autoencoders (VAEs).",1,1,0,0,1,0,0.293066,8.0,0.666451,67
660f7e16-2cdd-4e29-a7e8-08831cdb4c0e,Towards Uniform Point Distribution in Feature-preserving Point Cloud Filtering,10,0.23853,0.315988,"As a popular representation of 3D data, point cloud may contain noise and
need to be filtered before use. Existing point cloud filtering methods either
cannot preserve sharp features or result in uneven point distribution in the
filtered output. To address this problem, this paper introduces a point cloud
filtering method that considers both point distribution and feature
preservation during filtering. The key idea is to incorporate a repulsion term
with a data term in energy minimization. The repulsion term is responsible for
the point distribution, while the data term is to approximate the noisy
surfaces while preserving the geometric features. This method is capable of
handling models with fine-scale features and sharp features. Extensive
experiments show that our method yields better results with a more uniform
point distribution ($5.8\times10^{-5}$ Chamfer Distance on average) in seconds.",0,1,0,0,1,0,0.646037,8.0,0.80355,38
f49bc2c5-4285-48e1-84f0-c95eb2696fb2,"When did you become so smart, oh wise one?! Sarcasm Explanation in Multi-modal Multi-party Dialogues",16,0.145387,0.602522,"Indirect speech such as sarcasm achieves a constellation of discourse goals
in human communication. While the indirectness of figurative language warrants
speakers to achieve certain pragmatic goals, it is challenging for AI agents to
comprehend such idiosyncrasies of human communication. Though sarcasm
identification has been a well-explored topic in dialogue analysis, for
conversational systems to truly grasp a conversation's innate meaning and
generate appropriate responses, simply detecting sarcasm is not enough; it is
vital to explain its underlying sarcastic connotation to capture its true
essence. In this work, we study the discourse structure of sarcastic
conversations and propose a novel task - Sarcasm Explanation in Dialogue (SED).
Set in a multimodal and code-mixed setting, the task aims to generate natural
language explanations of satirical conversations. To this end, we curate WITS,
a new dataset to support our task. We propose MAF (Modality Aware Fusion), a
multimodal context-aware attention and global information fusion module to
capture multimodality and use it to benchmark WITS. The proposed attention
module surpasses the traditional multimodal fusion baselines and reports the
best performance on almost all metrics. Lastly, we carry out detailed analyses
both quantitatively and qualitatively.",0,0,1,1,0,0,0.0915996,10.0,0.604788,50
2c574df1-2965-4f30-b564-1604dd3aec46,HARP: Autoregressive Latent Video Prediction with High-Fidelity Image Generator,18,0.105158,0.819026,"Video prediction is an important yet challenging problem; burdened with the
tasks of generating future frames and learning environment dynamics. Recently,
autoregressive latent video models have proved to be a powerful video
prediction tool, by separating the video prediction into two sub-problems:
pre-training an image generator model, followed by learning an autoregressive
prediction model in the latent space of the image generator. However,
successfully generating high-fidelity and high-resolution videos has yet to be
seen. In this work, we investigate how to train an autoregressive latent video
prediction model capable of predicting high-fidelity future frames with minimal
modification to existing models, and produce high-resolution (256x256) videos.
Specifically, we scale up prior models by employing a high-fidelity image
generator (VQ-GAN) with a causal transformer model, and introduce additional
techniques of top-k sampling and data augmentation to further improve video
prediction quality. Despite the simplicity, the proposed method achieves
competitive performance to state-of-the-art approaches on standard video
prediction benchmarks with fewer parameters, and enables high-resolution video
prediction on complex and large-scale datasets. Videos are available at
https://sites.google.com/view/harp-videos/home.",0,1,0,0,1,0,0.603711,7.0,0.759037,57
55ca43f7-d2d5-4150-80ef-4e6ad69b31f3,A Web Application for Experimenting and Validating Remote Measurement of Vital Signs,3,0.0960185,0.490844,"With a surge in online medical advising remote monitoring of patient vitals
is required. This can be facilitated with the Remote Photoplethysmography
(rPPG) techniques that compute vital signs from facial videos. It involves
processing video frames to obtain skin pixels, extracting the cardiac data from
it and applying signal processing filters to extract the Blood Volume Pulse
(BVP) signal. Different algorithms are applied to the BVP signal to estimate
the various vital signs. We implemented a web application framework to measure
a person's Heart Rate (HR), Heart Rate Variability (HRV), Oxygen Saturation
(SpO2), Respiration Rate (RR), Blood Pressure (BP), and stress from the face
video. The rPPG technique is highly sensitive to illumination and motion
variation. The web application guides the users to reduce the noise due to
these variations and thereby yield a cleaner BVP signal. The accuracy and
robustness of the framework was validated with the help of volunteers.",0,1,0,0,0,0,0.400901,10.0,0.772177,30
fa0281c7-59ec-4db8-9f0f-827629b828f0,GRiT: A Generative Region-to-text Transformer for Object Understanding,65,0.171819,0.997571,"This paper presents a Generative RegIon-to-Text transformer, GRiT, for object
understanding. The spirit of GRiT is to formulate object understanding as
<region, text> pairs, where region locates objects and text describes objects.
For example, the text in object detection denotes class names while that in
dense captioning refers to descriptive sentences. Specifically, GRiT consists
of a visual encoder to extract image features, a foreground object extractor to
localize objects, and a text decoder to generate open-set object descriptions.
With the same model architecture, GRiT can understand objects via not only
simple nouns, but also rich descriptive sentences including object attributes
or actions. Experimentally, we apply GRiT to object detection and dense
captioning tasks. GRiT achieves 60.4 AP on COCO 2017 test-dev for object
detection and 15.5 mAP on Visual Genome for dense captioning. Code is available
at https://github.com/JialianW/GRiT",1,1,0,0,1,1,0.790552,5.0,0.76747,47
2c363bc1-5a9d-4d41-a9f8-a77a89978b2b,Enhanced Training of Query-Based Object Detection via Selective Query Recollection,15,0.321782,0.555931,"This paper investigates a phenomenon where query-based object detectors
mispredict at the last decoding stage while predicting correctly at an
intermediate stage. We review the training process and attribute the overlooked
phenomenon to two limitations: lack of training emphasis and cascading errors
from decoding sequence. We design and present Selective Query Recollection
(SQR), a simple and effective training strategy for query-based object
detectors. It cumulatively collects intermediate queries as decoding stages go
deeper and selectively forwards the queries to the downstream stages aside from
the sequential structure. Such-wise, SQR places training emphasis on later
stages and allows later stages to work with intermediate queries from earlier
stages directly. SQR can be easily plugged into various query-based object
detectors and significantly enhances their performance while leaving the
inference pipeline unchanged. As a result, we apply SQR on Adamixer, DAB-DETR,
and Deformable-DETR across various settings (backbone, number of queries,
schedule) and consistently brings 1.4-2.8 AP improvement.",1,1,0,0,0,0,0.949217,7.0,0.926079,39
a5e39904-b69d-407e-8ef5-9a33f47ff4bc,Efficient Training of Language Models to Fill in the Middle,97,0.136619,0.719577,"We show that autoregressive language models can learn to infill text after we
apply a straightforward transformation to the dataset, which simply moves a
span of text from the middle of a document to its end. While this data
augmentation has garnered much interest in recent years, we provide extensive
evidence that training models with a large fraction of data transformed in this
way does not harm the original left-to-right generative capability, as measured
by perplexity and sampling evaluations across a wide range of scales. Given the
usefulness, simplicity, and efficiency of training models to fill-in-the-middle
(FIM), we suggest that future autoregressive language models be trained with
FIM by default. To this end, we run a series of ablations on key
hyperparameters, such as the data transformation frequency, the structure of
the transformation, and the method of selecting the infill span. We use these
ablations to prescribe strong default settings and best practices to train FIM
models. We have released our best infilling model trained with best practices
in our API, and release our infilling benchmarks to aid future research.",0,1,0,0,0,0,0.345775,6.0,0.588876,56
2fb66db1-8de3-4f9c-941f-c9c0cdc80785,Near-Term Advances in Quantum Natural Language Processing,5,0.0311055,0.148897,"This paper describes experiments showing that some tasks in natural language
processing (NLP) can already be performed using quantum computers, though so
far only with small datasets.
  We demonstrate various approaches to topic classification. The first uses an
explicit word-based approach, in which word-topic scoring weights are
implemented as fractional rotations of individual qubit, and a new phrase is
classified based on the accumulation of these weights in a scoring qubit using
entangling controlled-NOT gates. This is compared with more scalable quantum
encodings of word embedding vectors, which are used in the computation of
kernel values in a quantum support vector machine: this approach achieved an
average of 62% accuracy on classification tasks involving over 10000 words,
which is the largest such quantum computing experiment to date.
  We describe a quantum probability approach to bigram modeling that can be
applied to sequences of words and formal concepts, investigating a generative
approximation to these distributions using a quantum circuit Born machine, and
an approach to ambiguity resolution in verb-noun composition using single-qubit
rotations for simple nouns and 2-qubit controlled-NOT gates for simple verbs.
  The smaller systems described have been run successfully on physical quantum
computers, and the larger ones have been simulated. We show that statistically
meaningful results can be obtained using real datasets, but this is much more
difficult to predict than with easier artificial language examples used
previously in developing quantum NLP systems.
  Other approaches to quantum NLP are compared, partly with respect to
contemporary issues including informal language, fluency, and truthfulness.",0,0,0,0,0,0,0.111504,9.0,0.583934,70
abacc02d-bf2d-41d3-9359-daaf3ae270a0,Hierarchical Graph Transformer with Adaptive Node Sampling,41,0.395771,0.987943,"The Transformer architecture has achieved remarkable success in a number of
domains including natural language processing and computer vision. However,
when it comes to graph-structured data, transformers have not achieved
competitive performance, especially on large graphs. In this paper, we identify
the main deficiencies of current graph transformers:(1) Existing node sampling
strategies in Graph Transformers are agnostic to the graph characteristics and
the training process. (2) Most sampling strategies only focus on local
neighbors and neglect the long-range dependencies in the graph. We conduct
experimental investigations on synthetic datasets to show that existing
sampling strategies are sub-optimal. To tackle the aforementioned problems, we
formulate the optimization strategies of node sampling in Graph Transformer as
an adversary bandit problem, where the rewards are related to the attention
weights and can vary in the training procedure. Meanwhile, we propose a
hierarchical attention scheme with graph coarsening to capture the long-range
interactions while reducing computational complexity. Finally, we conduct
extensive experiments on real-world datasets to demonstrate the superiority of
our method over existing graph transformers and popular GNNs.",1,0,0,0,1,0,0.760338,6.0,0.791201,61
0037348f-0ded-40e0-be01-3613e5288109,Textual Entailment Recognition with Semantic Features from Empirical Text Representation,1,0.00144058,0.0409291,"Textual entailment recognition is one of the basic natural language
understanding(NLU) tasks. Understanding the meaning of sentences is a
prerequisite before applying any natural language processing(NLP) techniques to
automatically recognize the textual entailment. A text entails a hypothesis if
and only if the true value of the hypothesis follows the text. Classical
approaches generally utilize the feature value of each word from word embedding
to represent the sentences. In this paper, we propose a novel approach to
identifying the textual entailment relationship between text and hypothesis,
thereby introducing a new semantic feature focusing on empirical
threshold-based semantic text representation. We employ an element-wise
Manhattan distance vector-based feature that can identify the semantic
entailment relationship between the text-hypothesis pair. We carried out
several experiments on a benchmark entailment classification(SICK-RTE) dataset.
We train several machine learning(ML) algorithms applying both semantic and
lexical features to classify the text-hypothesis pair as entailment, neutral,
or contradiction. Our empirical sentence representation technique enriches the
semantic information of the texts and hypotheses found to be more efficient
than the classical ones. In the end, our approach significantly outperforms
known methods in understanding the meaning of the sentences for the textual
entailment classification task.",0,1,0,0,1,0,0.00136912,14.0,0.414119,27
a5f92b66-7c8a-456a-ba24-1cc9cd576f2a,Modelling Control Arguments via Cooperation Logic in Unforeseen Scenarios,1,0.0132548,0.100082,"The intent of control argumentation frameworks is to specifically model
strategic scenarios from the perspective of an agent by extending the standard
model of argumentation framework in a way that takes unquantified uncertainty
regarding arguments and attacks into account. They do not, however, adequately
account for coalition formation and interactions among a set of agents in an
uncertain environment. To address this challenge, we propose a formalism of a
multi-agent scenario via cooperation logic and investigate agents' strategies
and actions in a dynamic environment.",0,0,0,0,0,0,0.00045126,19.0,0.509859,14
f6ef226a-6ebe-4095-b5ef-bcdd9299e1c1,Isotropic Representation Can Improve Dense Retrieval,3,0.0873057,0.123073,"The recent advancement in language representation modeling has broadly
affected the design of dense retrieval models. In particular, many of the
high-performing dense retrieval models evaluate representations of query and
document using BERT, and subsequently apply a cosine-similarity based scoring
to determine the relevance. BERT representations, however, are known to follow
an anisotropic distribution of a narrow cone shape and such an anisotropic
distribution can be undesirable for the cosine-similarity based scoring. In
this work, we first show that BERT-based DR also follows an anisotropic
distribution. To cope with the problem, we introduce unsupervised
post-processing methods of Normalizing Flow and whitening, and develop
token-wise method in addition to the sequence-wise method for applying the
post-processing methods to the representations of dense retrieval models. We
show that the proposed methods can effectively enhance the representations to
be isotropic, then we perform experiments with ColBERT and RepBERT to show that
the performance (NDCG at 10) of document re-ranking can be improved by
5.17\%$\sim$8.09\% for ColBERT and 6.88\%$\sim$22.81\% for RepBERT. To examine
the potential of isotropic representation for improving the robustness of DR
models, we investigate out-of-distribution tasks where the test dataset differs
from the training dataset. The results show that isotropic representation can
achieve a generally improved performance. For instance, when training dataset
is MS-MARCO and test dataset is Robust04, isotropy post-processing can improve
the baseline performance by up to 24.98\%. Furthermore, we show that an
isotropic model trained with an out-of-distribution dataset can even outperform
a baseline model trained with the in-distribution dataset.",1,1,0,0,0,0,0.850304,6.0,0.838661,44
80ebb927-64cb-4b63-85ae-c99c93b40e1f,Meta Self-Refinement for Robust Learning with Weak Supervision,6,0.0346948,0.316988,"Training deep neural networks (DNNs) under weak supervision has attracted
increasing research attention as it can significantly reduce the annotation
cost. However, labels from weak supervision can be noisy, and the high capacity
of DNNs enables them to easily overfit the label noise, resulting in poor
generalization. Recent methods leverage self-training to build noise-resistant
models, in which a teacher trained under weak supervision is used to provide
highly confident labels for teaching the students. Nevertheless, the teacher
derived from such frameworks may have fitted a substantial amount of noise and
therefore produce incorrect pseudo-labels with high confidence, leading to
severe error propagation. In this work, we propose Meta Self-Refinement (MSR),
a noise-resistant learning framework, to effectively combat label noise from
weak supervision. Instead of relying on a fixed teacher trained with noisy
labels, we encourage the teacher to refine its pseudo-labels. At each training
step, MSR performs a meta gradient descent on the current mini-batch to
maximize the student performance on a clean validation set. Extensive
experimentation on eight NLP benchmarks demonstrates that MSR is robust against
label noise in all settings and outperforms state-of-the-art methods by up to
11.4% in accuracy and 9.26% in F1 score.",1,1,0,0,1,0,0.538728,7.0,0.73344,50
7e7f6ca0-590a-49a7-89b0-4ac0f85799b1,"This is the way: designing and compiling LEPISZCZE, a comprehensive NLP benchmark for Polish",7,0.0643138,0.328077,"The availability of compute and data to train larger and larger language
models increases the demand for robust methods of benchmarking the true
progress of LM training. Recent years witnessed significant progress in
standardized benchmarking for English. Benchmarks such as GLUE, SuperGLUE, or
KILT have become de facto standard tools to compare large language models.
Following the trend to replicate GLUE for other languages, the KLEJ benchmark
has been released for Polish. In this paper, we evaluate the progress in
benchmarking for low-resourced languages. We note that only a handful of
languages have such comprehensive benchmarks. We also note the gap in the
number of tasks being evaluated by benchmarks for resource-rich English/Chinese
and the rest of the world. In this paper, we introduce LEPISZCZE (the Polish
word for glew, the Middle English predecessor of glue), a new, comprehensive
benchmark for Polish NLP with a large variety of tasks and high-quality
operationalization of the benchmark. We design LEPISZCZE with flexibility in
mind. Including new models, datasets, and tasks is as simple as possible while
still offering data versioning and model tracking. In the first run of the
benchmark, we test 13 experiments (task and dataset pairs) based on the five
most recent LMs for Polish. We use five datasets from the Polish benchmark and
add eight novel datasets. As the paper's main contribution, apart from
LEPISZCZE, we provide insights and experiences learned while creating the
benchmark for Polish as the blueprint to design similar benchmarks for other
low-resourced languages.",1,1,0,1,0,0,0.446646,5.0,0.573181,62
153edd75-c37d-4a58-b83c-a5da58c01e93,A Multibranch Convolutional Neural Network for Hyperspectral Unmixing,10,0.162077,0.632268,"Hyperspectral unmixing remains one of the most challenging tasks in the
analysis of such data. Deep learning has been blooming in the field and proved
to outperform other classic unmixing techniques, and can be effectively
deployed onboard Earth observation satellites equipped with hyperspectral
imagers. In this letter, we follow this research pathway and propose a
multi-branch convolutional neural network that benefits from fusing spectral,
spatial, and spectral-spatial features in the unmixing process. The results of
our experiments, backed up with the ablation study, revealed that our
techniques outperform others from the literature and lead to higher-quality
fractional abundance estimation. Also, we investigated the influence of
reducing the training sets on the capabilities of all algorithms and their
robustness against noise, as capturing large and representative ground-truth
sets is time-consuming and costly in practice, especially in emerging Earth
observation scenarios.",1,0,0,0,0,0,0.189997,7.0,0.547618,22
98f90968-6ed8-4a41-b9e7-c7b718d1791e,Unifying Short and Long-Term Tracking with Graph Hierarchies,17,0.194636,0.775434,"Tracking objects over long videos effectively means solving a spectrum of
problems, from short-term association for un-occluded objects to long-term
association for objects that are occluded and then reappear in the scene.
Methods tackling these two tasks are often disjoint and crafted for specific
scenarios, and top-performing approaches are often a mix of techniques, which
yields engineering-heavy solutions that lack generality. In this work, we
question the need for hybrid approaches and introduce SUSHI, a unified and
scalable multi-object tracker. Our approach processes long clips by splitting
them into a hierarchy of subclips, which enables high scalability. We leverage
graph neural networks to process all levels of the hierarchy, which makes our
model unified across temporal scales and highly general. As a result, we obtain
significant improvements over state-of-the-art on four diverse datasets. Our
code and models are available at bit.ly/sushi-mot.",0,1,0,0,1,0,0.613387,8.0,0.792452,77
d469dd03-7a84-4b36-ae46-0530a82f3499,GreaseLM: Graph REASoning Enhanced Language Models for Question Answering,127,0.745963,0.99987,"Answering complex questions about textual narratives requires reasoning over
both stated context and the world knowledge that underlies it. However,
pretrained language models (LM), the foundation of most modern QA systems, do
not robustly represent latent relationships between concepts, which is
necessary for reasoning. While knowledge graphs (KG) are often used to augment
LMs with structured representations of world knowledge, it remains an open
question how to effectively fuse and reason over the KG representations and the
language context, which provides situational constraints and nuances. In this
work, we propose GreaseLM, a new model that fuses encoded representations from
pretrained LMs and graph neural networks over multiple layers of modality
interaction operations. Information from both modalities propagates to the
other, allowing language context representations to be grounded by structured
world knowledge, and allowing linguistic nuances (e.g., negation, hedging) in
the context to inform the graph representations of knowledge. Our results on
three benchmarks in the commonsense reasoning (i.e., CommonsenseQA, OpenbookQA)
and medical question answering (i.e., MedQA-USMLE) domains demonstrate that
GreaseLM can more reliably answer questions that require reasoning over both
situational constraints and structured knowledge, even outperforming models 8x
larger.",0,0,0,0,1,0,0.899179,5.0,0.844207,49
30aa34b2-34f5-4baa-a417-8128047eaea0,"RoMQA: A Benchmark for Robust, Multi-evidence, Multi-answer Question Answering",13,0.132261,0.698401,"We introduce RoMQA, the first benchmark for robust, multi-evidence,
multi-answer question answering (QA). RoMQA contains clusters of questions that
are derived from related constraints mined from the Wikidata knowledge graph.
RoMQA evaluates robustness of QA models to varying constraints by measuring
worst-case performance within each question cluster. Compared to prior QA
datasets, RoMQA has more human-written questions that require reasoning over
more evidence text and have, on average, many more correct answers. In
addition, human annotators rate RoMQA questions as more natural or likely to be
asked by people. We evaluate state-of-the-art large language models in
zero-shot, few-shot, and fine-tuning settings, and find that RoMQA is
challenging: zero-shot and few-shot models perform similarly to naive
baselines, while supervised retrieval methods perform well below gold evidence
upper bounds. Moreover, existing models are not robust to variations in
question constraints, but can be made more robust by tuning on clusters of
related questions. Our results show that RoMQA is a challenging benchmark for
large language models, and provides a quantifiable test to build more robust QA
methods.",1,1,1,1,0,0,0.907726,9.0,0.917658,24
90d5cbc8-687c-43d5-bc64-0c93ea7a0c73,A Unified Positive-Unlabeled Learning Framework for Document-Level Relation Extraction with Different Levels of Labeling,8,0.195912,0.786775,"Document-level relation extraction (RE) aims to identify relations between
entities across multiple sentences. Most previous methods focused on
document-level RE under full supervision. However, in real-world scenario, it
is expensive and difficult to completely label all relations in a document
because the number of entity pairs in document-level RE grows quadratically
with the number of entities. To solve the common incomplete labeling problem,
we propose a unified positive-unlabeled learning framework - shift and squared
ranking loss positive-unlabeled (SSR-PU) learning. We use positive-unlabeled
(PU) learning on document-level RE for the first time. Considering that labeled
data of a dataset may lead to prior shift of unlabeled data, we introduce a PU
learning under prior shift of training data. Also, using none-class score as an
adaptive threshold, we propose squared ranking loss and prove its Bayesian
consistency with multi-label ranking metrics. Extensive experiments demonstrate
that our method achieves an improvement of about 14 F1 points relative to the
previous baseline with incomplete labeling. In addition, it outperforms
previous state-of-the-art results under both fully supervised and extremely
unlabeled settings as well.",0,1,0,0,1,0,0.686879,5.0,0.707997,45
d64a1ec8-b19b-4f0b-89e1-d00103da33a4,Immersive Text Game and Personality Classification,1,0.0336747,0.0120734,"We designed and built a game called \textit{Immersive Text Game}, which
allows the player to choose a story and a character, and interact with other
characters in the story in an immersive manner of dialogues. The game is based
on several latest models, including text generation language model, information
extraction model, commonsense reasoning model, and psychology evaluation model.
In the past, similar text games usually let players choose from limited actions
instead of answering on their own, and not every time what characters said are
determined by the player. Through the combination of these models and elaborate
game mechanics and modes, the player will find some novel experiences as driven
through the storyline.",0,1,0,0,0,0,0.963151,16.0,0.974048,22
d6dd95e8-f469-48c5-8c65-4fb6bd7b8f94,Convolutional and Residual Networks Provably Contain Lottery Tickets,9,0.203614,0.926726,"The Lottery Ticket Hypothesis continues to have a profound practical impact
on the quest for small scale deep neural networks that solve modern deep
learning tasks at competitive performance. These lottery tickets are identified
by pruning large randomly initialized neural networks with architectures that
are as diverse as their applications. Yet, theoretical insights that attest
their existence have been mostly focused on deep fully-connected feed forward
networks with ReLU activation functions. We prove that also modern
architectures consisting of convolutional and residual layers that can be
equipped with almost arbitrary activation functions can contain lottery tickets
with high probability.",0,0,0,0,0,0,0.852969,5.0,0.808276,47
094a051d-2860-412d-999f-4fa5de010f43,Explain My Surprise: Learning Efficient Long-Term Memory by Predicting Uncertain Outcomes,5,0.0166283,0.214331,"In many sequential tasks, a model needs to remember relevant events from the
distant past to make correct predictions. Unfortunately, a straightforward
application of gradient based training requires intermediate computations to be
stored for every element of a sequence. This requires to store prohibitively
large intermediate data if a sequence consists of thousands or even millions
elements, and as a result, makes learning of very long-term dependencies
infeasible. However, the majority of sequence elements can usually be predicted
by taking into account only temporally local information. On the other hand,
predictions affected by long-term dependencies are sparse and characterized by
high uncertainty given only local information. We propose MemUP, a new training
method that allows to learn long-term dependencies without backpropagating
gradients through the whole sequence at a time. This method can potentially be
applied to any recurrent architecture. LSTM network trained with MemUP performs
better or comparable to baselines while requiring to store less intermediate
data.",1,0,0,0,0,1,0.132736,9.0,0.604614,47
8b0b415f-5fe1-410b-a1a0-beee6a2c0fad,Cognitive Modeling of Semantic Fluency Using Transformers,1,0.0155298,0.0275761,"Can deep language models be explanatory models of human cognition? If so,
what are their limits? In order to explore this question, we propose an
approach called hyperparameter hypothesization that uses predictive
hyperparameter tuning in order to find individuating descriptors of
cognitive-behavioral profiles. We take the first step in this approach by
predicting human performance in the semantic fluency task (SFT), a well-studied
task in cognitive science that has never before been modeled using
transformer-based language models (TLMs). In our task setup, we compare several
approaches to predicting which word an individual performing SFT will utter
next. We report preliminary evidence suggesting that, despite obvious
implementational differences in how people and TLMs learn and use language,
TLMs can be used to identify individual differences in human fluency task
behaviors better than existing computational models, and may offer insights
into human memory retrieval strategies -- cognitive process not typically
considered to be the kinds of things TLMs can model. Finally, we discuss the
implications of this work for cognitive modeling of knowledge representations.",0,0,0,0,1,0,0.22532,11.0,0.729569,45
fc6d0adf-12dd-482d-81d7-9f95e09dc2af,TRoVE: Transforming Road Scene Datasets into Photorealistic Virtual Environments,3,0.042824,0.289401,"High-quality structured data with rich annotations are critical components in
intelligent vehicle systems dealing with road scenes. However, data curation
and annotation require intensive investments and yield low-diversity scenarios.
The recently growing interest in synthetic data raises questions about the
scope of improvement in such systems and the amount of manual work still
required to produce high volumes and variations of simulated data. This work
proposes a synthetic data generation pipeline that utilizes existing datasets,
like nuScenes, to address the difficulties and domain-gaps present in simulated
datasets. We show that using annotations and visual cues from existing
datasets, we can facilitate automated multi-modal data generation, mimicking
real scene properties with high-fidelity, along with mechanisms to diversify
samples in a physically meaningful way. We demonstrate improvements in mIoU
metrics by presenting qualitative and quantitative experiments with real and
synthetic data for semantic segmentation on the Cityscapes and KITTI-STEP
datasets. All relevant code and data is released on github
(https://github.com/shubham1810/trove_toolkit).",1,1,0,0,0,0,0.566425,8.0,0.776383,51
2ab95d94-998c-4fb0-9100-6078682892fe,CrossRE: A Cross-Domain Dataset for Relation Extraction,14,0.141449,0.638642,"Relation Extraction (RE) has attracted increasing attention, but current RE
evaluation is limited to in-domain evaluation setups. Little is known on how
well a RE system fares in challenging, but realistic out-of-distribution
evaluation setups. To address this gap, we propose CrossRE, a new,
freely-available cross-domain benchmark for RE, which comprises six distinct
text domains and includes multi-label annotations. An additional innovation is
that we release meta-data collected during annotation, to include explanations
and flags of difficult instances. We provide an empirical evaluation with a
state-of-the-art model for relation classification. As the meta-data enables us
to shed new light on the state-of-the-art model, we provide a comprehensive
analysis on the impact of difficult cases and find correlations between model
and human annotations. Overall, our empirical investigation highlights the
difficulty of cross-domain RE. We release our dataset, to spur more research in
this direction.",0,1,1,1,0,0,0.362399,9.0,0.732461,34
aa75b6ce-13bd-46a5-a662-abab30cb770b,Domain Adaptation via Bidirectional Cross-Attention Transformer,16,0.0500383,0.492483,"Domain Adaptation (DA) aims to leverage the knowledge learned from a source
domain with ample labeled data to a target domain with unlabeled data only.
Most existing studies on DA contribute to learning domain-invariant feature
representations for both domains by minimizing the domain gap based on
convolution-based neural networks. Recently, vision transformers significantly
improved performance in multiple vision tasks. Built on vision transformers, in
this paper we propose a Bidirectional Cross-Attention Transformer (BCAT) for DA
with the aim to improve the performance. In the proposed BCAT, the attention
mechanism can extract implicit source and target mixup feature representations
to narrow the domain discrepancy. Specifically, in BCAT, we design a
weight-sharing quadruple-branch transformer with a bidirectional
cross-attention mechanism to learn domain-invariant feature representations.
Extensive experiments demonstrate that the proposed BCAT model achieves
superior performance on four benchmark datasets over existing state-of-the-art
DA methods that are based on convolutions or transformers.",0,1,0,0,1,0,0.633575,7.0,0.770645,40
46687199-1bab-42bb-bbc8-eb3ae959c351,"Self-supervised Learning for Human Activity Recognition Using 700,000 Person-days of Wearable Data",18,0.0702471,0.414772,"Advances in deep learning for human activity recognition have been relatively
limited due to the lack of large labelled datasets. In this study, we leverage
self-supervised learning techniques on the UK-Biobank activity tracker
dataset--the largest of its kind to date--containing more than 700,000
person-days of unlabelled wearable sensor data. Our resulting activity
recognition model consistently outperformed strong baselines across seven
benchmark datasets, with an F1 relative improvement of 2.5%-100% (median
18.4%), the largest improvements occurring in the smaller datasets. In contrast
to previous studies, our results generalise across external datasets, devices,
and environments. Our open-source model will help researchers and developers to
build customisable and generalisable activity classifiers with high
performance.",1,1,0,0,1,1,0.133382,9.0,0.605194,68
31b1fd97-1a33-4930-8f61-87c7bf7dedfa,Information Extraction from Scanned Invoice Images using Text Analysis and Layout Features,9,0.170226,0.414656,"While storing invoice content as metadata to avoid paper document processing
may be the future trend, almost all of daily issued invoices are still printed
on paper or generated in digital formats such as PDFs. In this paper, we
introduce the OCRMiner system for information extraction from scanned document
images which is based on text analysis techniques in combination with layout
features to extract indexing metadata of (semi-)structured documents. The
system is designed to process the document in a similar way a human reader
uses, i.e. to employ different layout and text attributes in a coordinated
decision. The system consists of a set of interconnected modules that start
with (possibly erroneous) character-based output from a standard OCR system and
allow to apply different techniques and to expand the extracted knowledge at
each step. Using an open source OCR, the system is able to recover the invoice
data in 90% for English and in 88% for the Czech set.",0,1,0,0,0,0,0.151064,7.0,0.511611,47
52faccd0-8fe0-4570-9b99-fa33526687e8,Hierarchical Average Precision Training for Pertinent Image Retrieval,4,0.107434,0.12649,"Image Retrieval is commonly evaluated with Average Precision (AP) or
Recall@k. Yet, those metrics, are limited to binary labels and do not take into
account errors' severity. This paper introduces a new hierarchical AP training
method for pertinent image retrieval (HAP-PIER). HAPPIER is based on a new H-AP
metric, which leverages a concept hierarchy to refine AP by integrating errors'
importance and better evaluate rankings. To train deep models with H-AP, we
carefully study the problem's structure and design a smooth lower bound
surrogate combined with a clustering loss that ensures consistent ordering.
Extensive experiments on 6 datasets show that HAPPIER significantly outperforms
state-of-the-art methods for hierarchical retrieval, while being on par with
the latest approaches when evaluating fine-grained ranking performances.
Finally, we show that HAPPIER leads to better organization of the embedding
space, and prevents most severe failure cases of non-hierarchical methods. Our
code is publicly available at: https://github.com/elias-ramzi/HAPPIER.",1,0,1,0,1,0,0.771442,12.0,0.898323,59
a06ad7bd-62be-47d7-a47b-c84ac5dc7b0b,Fuse It More Deeply! A Variational Transformer with Layer-Wise Latent Variable Inference for Text Generation,17,0.323138,0.205987,"The past several years have witnessed Variational Auto-Encoder's superiority
in various text generation tasks. However, due to the sequential nature of the
text, auto-regressive decoders tend to ignore latent variables and then reduce
to simple language models, known as the KL vanishing problem, which would
further deteriorate when VAE is combined with Transformer-based structures. To
ameliorate this problem, we propose DELLA, a novel variational Transformer
framework. DELLA learns a series of layer-wise latent variables with each
inferred from those of lower layers and tightly coupled with the hidden states
by low-rank tensor product. In this way, DELLA forces these posterior latent
variables to be fused deeply with the whole computation path and hence
incorporate more information. We theoretically demonstrate that our method can
be regarded as entangling latent variables to avoid posterior information
decrease through layers, enabling DELLA to get higher non-zero KL values even
without any annealing or thresholding tricks. Experiments on four unconditional
and three conditional generation tasks show that DELLA could better alleviate
KL vanishing and improve both quality and diversity compared to several strong
baselines.",1,0,0,0,0,0,0.903244,9.0,0.915424,55
91db3b2d-62a0-41bf-a702-a3270c3e1a2a,Can Model Compression Improve NLP Fairness,18,0.0537591,0.656412,"Model compression techniques are receiving increasing attention; however, the
effect of compression on model fairness is still under explored. This is the
first paper to examine the effect of distillation and pruning on the toxicity
and bias of generative language models. We test Knowledge Distillation and
Pruning methods on the GPT2 model and found a consistent pattern of toxicity
and bias reduction after model distillation; this result can be potentially
interpreted by existing line of research which describes model compression as a
regularization technique; our work not only serves as a reference for safe
deployment of compressed models, but also extends the discussion of
""compression as regularization"" into the setting of neural LMs, and hints at
the possibility of using compression to develop fairer models.",0,0,0,0,0,0,0.465282,5.0,0.584436,35
1d219a0e-90ef-4c5d-923d-05357845afa2,Revisiting Facial Key Point Detection: An Efficient Approach Using Deep Neural Networks,4,0.0521806,0.462075,"Facial landmark detection is a widely researched field of deep learning as
this has a wide range of applications in many fields. These key points are
distinguishing characteristic points on the face, such as the eyes center, the
eye's inner and outer corners, the mouth center, and the nose tip from which
human emotions and intent can be explained. The focus of our work has been
evaluating transfer learning models such as MobileNetV2 and NasNetMobile,
including custom CNN architectures. The objective of the research has been to
develop efficient deep learning models in terms of model size, parameters, and
inference time and to study the effect of augmentation imputation and
fine-tuning on these models. It was found that while augmentation techniques
produced lower RMSE scores than imputation techniques, they did not affect the
inference time. MobileNetV2 architecture produced the lowest RMSE and inference
time. Moreover, our results indicate that manually optimized CNN architectures
performed similarly to Auto Keras tuned architecture. However, manually
optimized architectures yielded better inference time and training curves.",0,1,0,0,0,1,0.252387,10.0,0.715568,19
43fea4b2-9d78-45fc-a1d1-81d2b4efc638,On the Importance of Asymmetry for Siamese Representation Learning,37,0.162712,0.642189,"Many recent self-supervised frameworks for visual representation learning are
based on certain forms of Siamese networks. Such networks are conceptually
symmetric with two parallel encoders, but often practically asymmetric as
numerous mechanisms are devised to break the symmetry. In this work, we conduct
a formal study on the importance of asymmetry by explicitly distinguishing the
two encoders within the network -- one produces source encodings and the other
targets. Our key insight is keeping a relatively lower variance in target than
source generally benefits learning. This is empirically justified by our
results from five case studies covering different variance-oriented designs,
and is aligned with our preliminary theoretical analysis on the baseline.
Moreover, we find the improvements from asymmetric designs generalize well to
longer training schedules, multiple other frameworks and newer backbones.
Finally, the combined effect of several asymmetric designs achieves a
state-of-the-art accuracy on ImageNet linear probing and competitive results on
downstream transfer. We hope our exploration will inspire more research in
exploiting asymmetry for Siamese representation learning.",0,0,0,0,1,0,0.897473,5.0,0.84274,47
307e79dc-a0d0-429d-a706-249754b9cbec,ParGAN: Learning Real Parametrizable Transformations,1,0.00208175,0.0640061,"Current methods for image-to-image translation produce compelling results,
however, the applied transformation is difficult to control, since existing
mechanisms are often limited and non-intuitive. We propose ParGAN, a
generalization of the cycle-consistent GAN framework to learn image
transformations with simple and intuitive controls. The proposed generator
takes as input both an image and a parametrization of the transformation. We
train this network to preserve the content of the input image while ensuring
that the result is consistent with the given parametrization. Our approach does
not require paired data and can learn transformations across several tasks and
datasets. We show how, with disjoint image domains with no annotated
parametrization, our framework can create smooth interpolations as well as
learn multiple transformations simultaneously.",0,1,0,0,0,0,0.19825,11.0,0.716435,33
700555a7-5a8e-4123-9d6d-a8597477de23,mcBERT: Momentum Contrastive Learning with BERT for Zero-Shot Slot Filling,3,0.0799341,0.160136,"Zero-shot slot filling has received considerable attention to cope with the
problem of limited available data for the target domain. One of the important
factors in zero-shot learning is to make the model learn generalized and
reliable representations. For this purpose, we present mcBERT, which stands for
momentum contrastive learning with BERT, to develop a robust zero-shot slot
filling model. mcBERT uses BERT to initialize the two encoders, the query
encoder and key encoder, and is trained by applying momentum contrastive
learning. Our experimental results on the SNIPS benchmark show that mcBERT
substantially outperforms the previous models, recording a new
state-of-the-art. Besides, we also show that each component composing mcBERT
contributes to the performance improvement.",0,1,0,0,1,0,0.792971,8.0,0.855594,16
5cfc2302-dd0d-4db4-a254-919cb1926b40,Efficient Remote Photoplethysmography with Temporal Derivative Modules and Time-Shift Invariant Loss,17,0.14294,0.66878,"We present a lightweight neural model for remote heart rate estimation
focused on the efficient spatio-temporal learning of facial
photoplethysmography (PPG) based on i) modelling of PPG dynamics by
combinations of multiple convolutional derivatives, and ii) increased
flexibility of the model to learn possible offsets between the facial video PPG
and the ground truth. PPG dynamics are modelled by a Temporal Derivative Module
(TDM) constructed by the incremental aggregation of multiple convolutional
derivatives, emulating a Taylor series expansion up to the desired order.
Robustness to ground truth offsets is handled by the introduction of TALOS
(Temporal Adaptive LOcation Shift), a new temporal loss to train learning-based
models. We verify the effectiveness of our model by reporting accuracy and
efficiency metrics on the public PURE and UBFC-rPPG datasets. Compared to
existing models, our approach shows competitive heart rate estimation accuracy
with a much lower number of parameters and lower computational cost.",1,1,0,0,0,0,0.491129,6.0,0.666392,46
0729ed91-f3fd-47a5-a105-5a15e294a761,Interpretation Quality Score for Measuring the Quality of interpretability methods,2,0.0141454,0.210828,"Machine learning (ML) models have been applied to a wide range of natural
language processing (NLP) tasks in recent years. In addition to making accurate
decisions, the necessity of understanding how models make their decisions has
become apparent in many applications. To that end, many interpretability
methods that help explain the decision processes of ML models have been
developed. Yet, there currently exists no widely-accepted metric to evaluate
the quality of explanations generated by these methods. As a result, there
currently is no standard way of measuring to what degree an interpretability
method achieves an intended objective. Moreover, there is no accepted standard
of performance by which we can compare and rank the current existing
interpretability methods. In this paper, we propose a novel metric for
quantifying the quality of explanations generated by interpretability methods.
We compute the metric on three NLP tasks using six interpretability methods and
present our results.",0,0,0,0,0,0,0.460231,8.0,0.738381,36
07e13998-be46-4e6d-84ed-59b8622bffe5,DetMatch: Two Teachers are Better Than One for Joint 2D and 3D Semi-Supervised Object Detection,14,0.175114,0.499327,"While numerous 3D detection works leverage the complementary relationship
between RGB images and point clouds, developments in the broader framework of
semi-supervised object recognition remain uninfluenced by multi-modal fusion.
Current methods develop independent pipelines for 2D and 3D semi-supervised
learning despite the availability of paired image and point cloud frames.
Observing that the distinct characteristics of each sensor cause them to be
biased towards detecting different objects, we propose DetMatch, a flexible
framework for joint semi-supervised learning on 2D and 3D modalities. By
identifying objects detected in both sensors, our pipeline generates a cleaner,
more robust set of pseudo-labels that both demonstrates stronger performance
and stymies single-modality error propagation. Further, we leverage the richer
semantics of RGB images to rectify incorrect 3D class predictions and improve
localization of 3D boxes. Evaluating on the challenging KITTI and Waymo
datasets, we improve upon strong semi-supervised learning methods and observe
higher quality pseudo-labels. Code will be released at
https://github.com/Divadi/DetMatch",0,1,0,0,0,0,0.910172,5.0,0.854026,87
8f6d0c7c-4666-4bbd-ac81-a6480f6af8eb,Evaluating Parameter Efficient Learning for Generation,3,0.00658425,0.100147,"Parameter efficient learning methods (PERMs) have recently gained significant
attention as they provide an efficient way for pre-trained language models
(PLMs) to adapt to a downstream task. However, these conclusions are mostly
drawn from in-domain evaluations over the full training set. In this paper, we
present comparisons between PERMs and finetuning from three new perspectives:
(1) the effect of sample and model size to in-domain evaluations, (2)
generalization to unseen domains and new datasets, and (3) the faithfulness of
generations. Our results show that for in-domain settings (a) there is a cross
point of sample size for which PERMs will perform better than finetuning when
training with fewer samples, and (b) larger PLMs have larger cross points. For
cross-domain and cross-dataset cases, we show that (a) Adapter (Houlsby et al.,
2019) performs the best amongst all the PERMs studied here, and (b) it
outperforms finetuning if the task dataset is below a certain size. We also
compare the faithfulness of generations and show that PERMs can achieve better
faithfulness score than finetuning, especially for small training set, by as
much as 6%. Finally, we apply Adapter to MT-NLG 530b (Smith et al., 2022) and
achieve new state-of-the-art results on Xsum (Narayan et al., 2018) for all
ROUGE scores (ROUGE-1 49.17, ROUGE-2 27.20, ROUGE-L 40.98).",0,1,0,0,1,0,0.572987,4.0,0.557288,54
b7bca2f2-c121-45d0-b4ef-73861e5e8432,Disentangling Identity and Pose for Facial Expression Recognition,10,0.0593559,0.798104,"Facial expression recognition (FER) is a challenging problem because the
expression component is always entangled with other irrelevant factors, such as
identity and head pose. In this work, we propose an identity and pose
disentangled facial expression recognition (IPD-FER) model to learn more
discriminative feature representation. We regard the holistic facial
representation as the combination of identity, pose and expression. These three
components are encoded with different encoders. For identity encoder, a well
pre-trained face recognition model is utilized and fixed during training, which
alleviates the restriction on specific expression training data in previous
works and makes the disentanglement practicable on in-the-wild datasets. At the
same time, the pose and expression encoder are optimized with corresponding
labels. Combining identity and pose feature, a neutral face of input individual
should be generated by the decoder. When expression feature is added, the input
image should be reconstructed. By comparing the difference between synthesized
neutral and expressional images of the same individual, the expression
component is further disentangled from identity and pose. Experimental results
verify the effectiveness of our method on both lab-controlled and in-the-wild
databases and we achieve state-of-the-art recognition performance.",0,1,0,0,1,0,0.137403,9.0,0.608746,49
c230d3b9-21ea-48d3-9ef6-9e790ad9f28b,HarmoF0: Logarithmic Scale Dilated Convolution For Pitch Estimation,11,0.224136,0.328853,"Sounds, especially music, contain various harmonic components scattered in
the frequency dimension. It is difficult for normal convolutional neural
networks to observe these overtones. This paper introduces a multiple rates
dilated causal convolution (MRDC-Conv) method to capture the harmonic structure
in logarithmic scale spectrograms efficiently. The harmonic is helpful for
pitch estimation, which is important for many sound processing applications. We
propose HarmoF0, a fully convolutional network, to evaluate the MRDC-Conv and
other dilated convolutions in pitch estimation. The results show that this
model outperforms the DeepF0, yields state-of-the-art performance in three
datasets, and simultaneously reduces more than 90% parameters. We also find
that it has stronger noise resistance and fewer octave errors. The code and
pre-trained model are available at https://github.com/WX-Wei/HarmoF0.",1,1,0,0,1,0,0.649843,12.0,0.869896,20
45f880e0-ac02-465d-a055-4a8acf199727,Deep Learning-Based Perceptual Stimulus Encoder for Bionic Vision,11,0.0149442,0.19968,"Retinal implants have the potential to treat incurable blindness, yet the
quality of the artificial vision they produce is still rudimentary. An
outstanding challenge is identifying electrode activation patterns that lead to
intelligible visual percepts (phosphenes). Here we propose a PSE based on CNN
that is trained in an end-to-end fashion to predict the electrode activation
patterns required to produce a desired visual percept. We demonstrate the
effectiveness of the encoder on MNIST using a psychophysically validated
phosphene model tailored to individual retinal implant users. The present work
constitutes an essential first step towards improving the quality of the
artificial vision provided by retinal implants.",0,1,0,0,0,0,0.0262029,6.0,0.126987,11
bc37fe7e-5e42-4276-a94a-33b6047e857b,Are Current Task-oriented Dialogue Systems Able to Satisfy Impolite Users?,3,0.129212,0.383945,"Task-oriented dialogue (TOD) systems have assisted users on many tasks,
including ticket booking and service inquiries. While existing TOD systems have
shown promising performance in serving customer needs, these systems mostly
assume that users would interact with the dialogue agent politely. This
assumption is unrealistic as impatient or frustrated customers may also
interact with TOD systems impolitely. This paper aims to address this research
gap by investigating impolite users' effects on TOD systems. Specifically, we
constructed an impolite dialogue corpus and conducted extensive experiments to
evaluate the state-of-the-art TOD systems on our impolite dialogue corpus. Our
experimental results show that existing TOD systems are unable to handle
impolite user utterances. We also present a data augmentation method to improve
TOD performance in impolite dialogues. Nevertheless, handling impolite
dialogues remains a very challenging research task. We hope by releasing the
impolite dialogue corpus and establishing the benchmark evaluations, more
researchers are encouraged to investigate this new challenging research task.",0,0,1,1,0,0,0.916558,7.0,0.900039,56
539047f9-7fda-44b1-aba0-735b71b9afe4,Mediators: Conversational Agents Explaining NLP Model Behavior,14,0.165688,0.651645,"The human-centric explainable artificial intelligence (HCXAI) community has
raised the need for framing the explanation process as a conversation between
human and machine. In this position paper, we establish desiderata for
Mediators, text-based conversational agents which are capable of explaining the
behavior of neural models interactively using natural language. From the
perspective of natural language processing (NLP) research, we engineer a
blueprint of such a Mediator for the task of sentiment analysis and assess how
far along current research is on the path towards dialogue-based explanations.",0,0,0,0,0,0,0.50119,5.0,0.605496,109
62dbdfaf-6b06-4660-8ae4-048a8c27673b,Exploring Document-Level Literary Machine Translation with Parallel Paragraphs from World Literature,21,0.225516,0.83929,"Literary translation is a culturally significant task, but it is bottlenecked
by the small number of qualified literary translators relative to the many
untranslated works published around the world. Machine translation (MT) holds
potential to complement the work of human translators by improving both
training procedures and their overall efficiency. Literary translation is less
constrained than more traditional MT settings since translators must balance
meaning equivalence, readability, and critical interpretability in the target
language. This property, along with the complex discourse-level context present
in literary texts, also makes literary MT more challenging to computationally
model and evaluate. To explore this task, we collect a dataset (Par3) of
non-English language novels in the public domain, each aligned at the paragraph
level to both human and automatic English translations. Using Par3, we discover
that expert literary translators prefer reference human translations over
machine-translated paragraphs at a rate of 84%, while state-of-the-art
automatic MT metrics do not correlate with those preferences. The experts note
that MT outputs contain not only mistranslations, but also discourse-disrupting
errors and stylistic inconsistencies. To address these problems, we train a
post-editing model whose output is preferred over normal MT output at a rate of
69% by experts. We publicly release Par3 at
https://github.com/katherinethai/par3/ to spur future research into literary
MT.",1,1,0,1,0,0,0.146901,9.0,0.616773,50
19dd9f56-ab0e-440a-ad45-34504d108cfb,Atari-5: Distilling the Arcade Learning Environment down to Five Games,6,0.0543082,0.791756,"The Arcade Learning Environment (ALE) has become an essential benchmark for
assessing the performance of reinforcement learning algorithms. However, the
computational cost of generating results on the entire 57-game dataset limits
ALE's use and makes the reproducibility of many results infeasible. We propose
a novel solution to this problem in the form of a principled methodology for
selecting small but representative subsets of environments within a benchmark
suite. We applied our method to identify a subset of five ALE games, called
Atari-5, which produces 57-game median score estimates within 10% of their true
values. Extending the subset to 10-games recovers 80% of the variance for
log-scores for all games within the 57-game set. We show this level of
compression is possible due to a high degree of correlation between many of the
games in ALE.",1,1,0,0,0,0,0.505264,10.0,0.80392,33
e4a36043-d014-4888-83ce-a586ea124476,Uncertainty Inspired Underwater Image Enhancement,47,0.850669,0.994254,"A main challenge faced in the deep learning-based Underwater Image
Enhancement (UIE) is that the ground truth high-quality image is unavailable.
Most of the existing methods first generate approximate reference maps and then
train an enhancement network with certainty. This kind of method fails to
handle the ambiguity of the reference map. In this paper, we resolve UIE into
distribution estimation and consensus process. We present a novel probabilistic
network to learn the enhancement distribution of degraded underwater images.
Specifically, we combine conditional variational autoencoder with adaptive
instance normalization to construct the enhancement distribution. After that,
we adopt a consensus process to predict a deterministic result based on a set
of samples from the distribution. By learning the enhancement distribution, our
method can cope with the bias introduced in the reference map labeling to some
extent. Additionally, the consensus process is useful to capture a robust and
stable result. We examined the proposed method on two widely used real-world
underwater image enhancement datasets. Experimental results demonstrate that
our approach enables sampling possible enhancement predictions. Meanwhile, the
consensus estimate yields competitive performance compared with
state-of-the-art UIE methods. Code available at
https://github.com/zhenqifu/PUIE-Net.",1,1,0,0,0,0,0.894185,8.0,0.899968,60
b6c276ac-f669-4b34-82be-eb577508773b,Beyond the Return: Off-policy Function Estimation under User-specified Error-measuring Distributions,6,0.0754096,0.749518,"Off-policy evaluation often refers to two related tasks: estimating the
expected return of a policy and estimating its value function (or other
functions of interest, such as density ratios). While recent works on
marginalized importance sampling (MIS) show that the former can enjoy provable
guarantees under realizable function approximation, the latter is only known to
be feasible under much stronger assumptions such as prohibitively expressive
discriminators. In this work, we provide guarantees for off-policy function
estimation under only realizability, by imposing proper regularization on the
MIS objectives. Compared to commonly used regularization in MIS, our
regularizer is much more flexible and can account for an arbitrary
user-specified distribution, under which the learned function will be close to
the groundtruth. We provide exact characterization of the optimal dual solution
that needs to be realized by the discriminator class, which determines the
data-coverage assumption in the case of value-function learning. As another
surprising observation, the regularizer can be altered to relax the
data-coverage requirement, and completely eliminate it in the ideal case with
strong side information.",0,0,0,0,0,0,0.61463,8.0,0.792875,42
30e5b2d5-5a77-422f-b2b6-9cf90f71e134,Quark: Controllable Text Generation with Reinforced Unlearning,123,0.441677,0.882168,"Large-scale language models often learn behaviors that are misaligned with
user expectations. Generated text may contain offensive or toxic language,
contain significant repetition, or be of a different sentiment than desired by
the user. We consider the task of unlearning these misalignments by fine-tuning
the language model on signals of what not to do. We introduce Quantized Reward
Konditioning (Quark), an algorithm for optimizing a reward function that
quantifies an (un)wanted property, while not straying too far from the original
model. Quark alternates between (i) collecting samples with the current
language model, (ii) sorting them into quantiles based on reward, with each
quantile identified by a reward token prepended to the language model's input,
and (iii) using a standard language modeling loss on samples from each quantile
conditioned on its reward token, while remaining nearby the original language
model via a KL-divergence penalty. By conditioning on a high-reward token at
generation time, the model generates text that exhibits less of the unwanted
property. For unlearning toxicity, negative sentiment, and repetition, our
experiments show that Quark outperforms both strong baselines and
state-of-the-art reinforcement learning methods like PPO (Schulman et al.
2017), while relying only on standard language modeling primitives.",1,0,1,0,0,0,0.414708,6.0,0.627713,99
f94c6083-65ab-44ec-a6ec-065a9e7fd520,Adversarial Contrastive Learning for Evidence-aware Fake News Detection with Graph Neural Networks,10,0.162208,0.72153,"The prevalence and perniciousness of fake news have been a critical issue on
the Internet, which stimulates the development of automatic fake news detection
in turn. In this paper, we focus on evidence-based fake news detection, where
several evidences are utilized to probe the veracity of news (i.e., a claim).
Most previous methods first employ sequential models to embed the semantic
information and then capture the claim-evidence interaction based on attention
mechanisms. Despite their effectiveness, they still suffer from three
weaknesses. Firstly, sequential models fail to integrate the relevant
information that is scattered far apart in evidences. Secondly, they
underestimate much redundant information in evidences may be useless or
harmful. Thirdly, insufficient data utilization limits the separability and
reliability of representations captured by the model. To solve these problems,
we propose a unified Graph-based sEmantic structure mining framework with
ConTRAstive Learning, namely GETRAL in short. Specifically, we first model
claims and evidences as graph-structured data to capture the long-distance
semantic dependency. Consequently, we reduce information redundancy by
performing graph structure learning. Then the fine-grained semantic
representations are fed into the claim-evidence interaction module for
predictions. Finally, an adversarial contrastive learning module is applied to
make full use of data and strengthen representation learning. Comprehensive
experiments have demonstrated the superiority of GETRAL over the
state-of-the-arts and validated the efficacy of semantic mining with graph
structure and contrastive learning.",1,0,0,0,1,0,0.759844,7.0,0.820823,85
047c9884-bdc5-4357-802b-6b163b6e352e,PeerDA: Data Augmentation via Modeling Peer Relation for Span Identification Tasks,6,0.124728,0.254682,"Span identification aims at identifying specific text spans from text input
and classifying them into pre-defined categories. Different from previous works
that merely leverage the Subordinate (SUB) relation (i.e. if a span is an
instance of a certain category) to train models, this paper for the first time
explores the Peer (PR) relation, which indicates that two spans are instances
of the same category and share similar features. Specifically, a novel Peer
Data Augmentation (PeerDA) approach is proposed which employs span pairs with
the PR relation as the augmentation data for training. PeerDA has two unique
advantages: (1) There are a large number of PR span pairs for augmenting the
training data. (2) The augmented data can prevent the trained model from
over-fitting the superficial span-category mapping by pushing the model to
leverage the span semantics. Experimental results on ten datasets over four
diverse tasks across seven domains demonstrate the effectiveness of PeerDA.
Notably, PeerDA achieves state-of-the-art results on six of them.",1,1,1,0,1,0,0.710948,5.0,0.72132,97
abf13567-9500-417d-be71-d4a5f214e15c,KnowGL: Knowledge Generation and Linking from Text,9,0.153023,0.635611,"We propose KnowGL, a tool that allows converting text into structured
relational data represented as a set of ABox assertions compliant with the TBox
of a given Knowledge Graph (KG), such as Wikidata. We address this problem as a
sequence generation task by leveraging pre-trained sequence-to-sequence
language models, e.g. BART. Given a sentence, we fine-tune such models to
detect pairs of entity mentions and jointly generate a set of facts consisting
of the full set of semantic annotations for a KG, such as entity labels, entity
types, and their relationships. To showcase the capabilities of our tool, we
build a web application consisting of a set of UI widgets that help users to
navigate through the semantic data extracted from a given input text. We make
the KnowGL model available at https://huggingface.co/ibm/knowgl-large.",0,1,0,0,0,0,0.883227,3.0,0.718293,25
5626ad9f-7098-4a63-b094-9636c71388c1,Distilling Inter-Class Distance for Semantic Segmentation,13,0.410587,0.671495,"Knowledge distillation is widely adopted in semantic segmentation to reduce
the computation cost.The previous knowledge distillation methods for semantic
segmentation focus on pixel-wise feature alignment and intra-class feature
variation distillation, neglecting to transfer the knowledge of the inter-class
distance in the feature space, which is important for semantic segmentation. To
address this issue, we propose an Inter-class Distance Distillation (IDD)
method to transfer the inter-class distance in the feature space from the
teacher network to the student network. Furthermore, semantic segmentation is a
position-dependent task,thus we exploit a position information distillation
module to help the student network encode more position information. Extensive
experiments on three popular datasets: Cityscapes, Pascal VOC and ADE20K show
that our method is helpful to improve the accuracy of semantic segmentation
models and achieves the state-of-the-art performance. E.g. it boosts the
benchmark model(""PSPNet+ResNet18"") by 7.50% in accuracy on the Cityscapes
dataset.",0,1,0,0,1,1,0.991276,9.0,0.994107,32
344727f5-9170-4ba8-b4f2-c83bf8ab3f6a,"RARR: Researching and Revising What Language Models Say, Using Language Models",115,0.863112,0.986403,"Language models (LMs) now excel at many tasks such as few-shot learning,
question answering, reasoning, and dialog. However, they sometimes generate
unsupported or misleading content. A user cannot easily determine whether their
outputs are trustworthy or not, because most LMs do not have any built-in
mechanism for attribution to external evidence. To enable attribution while
still preserving all the powerful advantages of recent generation models, we
propose RARR (Retrofit Attribution using Research and Revision), a system that
1) automatically finds attribution for the output of any text generation model
and 2) post-edits the output to fix unsupported content while preserving the
original output as much as possible. When applied to the output of several
state-of-the-art LMs on a diverse set of generation tasks, we find that RARR
significantly improves attribution while otherwise preserving the original
input to a much greater degree than previously explored edit models.
Furthermore, the implementation of RARR requires only a handful of training
examples, a large language model, and standard web search.",0,1,0,0,0,1,0.83524,5.0,0.796032,117
10375c61-7096-4100-9449-b52a7ddd90f8,Transfer Dynamics in Emergent Evolutionary Curricula,5,0.0339143,0.194582,"PINSKY is a system for open-ended learning through neuroevolution in
game-based domains. It builds on the Paired Open-Ended Trailblazer (POET)
system, which originally explored learning and environment generation for
bipedal walkers, and adapts it to games in the General Video Game AI (GVGAI)
system. Previous work showed that by co-evolving levels and neural network
policies, levels could be found for which successful policies could not be
created via optimization alone. Studied in the realm of Artificial Life as a
potentially open-ended alternative to gradient-based fitness, minimal criteria
(MC)-based selection helps foster diversity in evolutionary populations. The
main question addressed by this paper is how the open-ended learning actually
works, focusing in particular on the role of transfer of policies from one
evolutionary branch (""species"") to another. We analyze the dynamics of the
system through creating phylogenetic trees, analyzing evolutionary trajectories
of policies, and temporally breaking down transfers according to species type.
Furthermore, we analyze the impact of the minimal criterion on generated level
diversity and inter-species transfer. The most insightful finding is that
inter-species transfer, while rare, is crucial to the system's success.",0,0,0,0,0,0,0.269595,9.0,0.692522,60
c9090333-146a-410c-bef3-4e2b171b14f6,Sky Computing: Accelerating Geo-distributed Computing in Federated Learning,1,0.0424671,0.0582988,"Federated learning is proposed by Google to safeguard data privacy through
training models locally on users' devices. However, with deep learning models
growing in size to achieve better results, it becomes increasingly difficult to
accommodate the whole model on one single device. Thus, model parallelism is
then used to divide the model weights among several devices. With this logic,
the approach currently used evenly allocates weights among devices. However, in
reality, a computation bottleneck may occur resulting from variant computing
power of different users' devices. To address this problem, load balancing is
needed to allocate the model weights based on the computational capability of
the device. In this paper, we proposed Sky Computing, a load-balanced model
parallelism framework to adaptively allocate the weights to devices. Sky
Computing outperforms the baseline method by 55% in training time when training
160-layer BERT with 64 nodes. The source code can be found at
https://github.com/hpcaitech/SkyComputing.",1,1,0,0,1,0,0.938595,7.0,0.916671,23
d592b983-4c25-4cef-a6db-9c9a1f55752f,CONSISTENT: Open-Ended Question Generation From News Articles,6,0.0297056,0.398173,"Recent work on question generation has largely focused on factoid questions
such as who, what, where, when about basic facts. Generating open-ended why,
how, what, etc. questions that require long-form answers have proven more
difficult. To facilitate the generation of open-ended questions, we propose
CONSISTENT, a new end-to-end system for generating open-ended questions that
are answerable from and faithful to the input text. Using news articles as a
trustworthy foundation for experimentation, we demonstrate our model's strength
over several baselines using both automatic and human=based evaluations. We
contribute an evaluation dataset of expert-generated open-ended questions.We
discuss potential downstream applications for news media organizations.",1,1,0,1,0,0,0.247637,7.0,0.590524,45
4297a8f8-5e25-4754-b0d2-eee1ceea4620,MentSum: A Resource for Exploring Summarization of Mental Health Online Posts,6,0.184059,0.49556,"Mental health remains a significant challenge of public health worldwide.
With increasing popularity of online platforms, many use the platforms to share
their mental health conditions, express their feelings, and seek help from the
community and counselors. Some of these platforms, such as Reachout, are
dedicated forums where the users register to seek help. Others such as Reddit
provide subreddits where the users publicly but anonymously post their mental
health distress. Although posts are of varying length, it is beneficial to
provide a short, but informative summary for fast processing by the counselors.
To facilitate research in summarization of mental health online posts, we
introduce Mental Health Summarization dataset, MentSum, containing over 24k
carefully selected user posts from Reddit, along with their short user-written
summary (called TLDR) in English from 43 mental health subreddits. This
domain-specific dataset could be of interest not only for generating short
summaries on Reddit, but also for generating summaries of posts on the
dedicated mental health forums such as Reachout. We further evaluate both
extractive and abstractive state-of-the-art summarization baselines in terms of
Rouge scores, and finally conduct an in-depth human evaluation study of both
user-written and system-generated summaries, highlighting challenges in this
research.",0,1,0,1,0,0,0.596933,10.0,0.829477,64
3d7d8a2f-93d4-42a0-b7e8-6e8a0f9383bd,A Compact Pretraining Approach for Neural Language Models,1,0.0,0.0230007,"Domain adaptation for large neural language models (NLMs) is coupled with
massive amounts of unstructured data in the pretraining phase. In this study,
however, we show that pretrained NLMs learn in-domain information more
effectively and faster from a compact subset of the data that focuses on the
key information in the domain. We construct these compact subsets from the
unstructured data using a combination of abstractive summaries and extractive
keywords. In particular, we rely on BART to generate abstractive summaries, and
KeyBERT to extract keywords from these summaries (or the original unstructured
text directly). We evaluate our approach using six different settings: three
datasets combined with two distinct NLMs. Our results reveal that the
task-specific classifiers trained on top of NLMs pretrained using our method
outperform methods based on traditional pretraining, i.e., random masking on
the entire data, as well as methods without pretraining. Further, we show that
our strategy reduces pretraining time by up to five times compared to vanilla
pretraining. The code for all of our experiments is publicly available at
https://github.com/shahriargolchin/compact-pretraining.",1,1,0,0,0,0,0.840772,10.0,0.899892,24
644b1032-b809-43d8-995f-a9c559f742b4,Are GAN-based Morphs Threatening Face Recognition?,29,0.23796,0.716655,"Morphing attacks are a threat to biometric systems where the biometric
reference in an identity document can be altered. This form of attack presents
an important issue in applications relying on identity documents such as border
security or access control. Research in generation of face morphs and their
detection is developing rapidly, however very few datasets with morphing
attacks and open-source detection toolkits are publicly available. This paper
bridges this gap by providing two datasets and the corresponding code for four
types of morphing attacks: two that rely on facial landmarks based on OpenCV
and FaceMorpher, and two that use StyleGAN 2 to generate synthetic morphs. We
also conduct extensive experiments to assess the vulnerability of four
state-of-the-art face recognition systems, including FaceNet, VGG-Face,
ArcFace, and ISV. Surprisingly, the experiments demonstrate that, although
visually more appealing, morphs based on StyleGAN 2 do not pose a significant
threat to the state to face recognition systems, as these morphs were
outmatched by the simple morphs that are based facial landmarks.",1,1,0,1,0,0,0.246031,9.0,0.680683,31
fabf32c6-4521-4bf4-8b5f-fb7323ba63a8,Implicit and Efficient Point Cloud Completion for 3D Single Object Tracking,6,0.147962,0.278834,"The point cloud based 3D single object tracking has drawn increasing
attention. Although many breakthroughs have been achieved, we also reveal two
severe issues. By extensive analysis, we find the prediction manner of current
approaches is non-robust, i.e., exposing a misalignment gap between prediction
score and actually localization accuracy. Another issue is the sparse point
returns will damage the feature matching procedure of the SOT task. Based on
these insights, we introduce two novel modules, i.e., Adaptive Refine
Prediction (ARP) and Target Knowledge Transfer (TKT), to tackle them,
respectively. To this end, we first design a strong pipeline to extract
discriminative features and conduct the matching with the attention mechanism.
Then, ARP module is proposed to tackle the misalignment issue by aggregating
all predicted candidates with valuable clues. Finally, TKT module is designed
to effectively overcome incomplete point cloud due to sparse and occlusion
issues. We call our overall framework PCET. By conducting extensive experiments
on the KITTI and Waymo Open Dataset, our model achieves state-of-the-art
performance while maintaining a lower computational cost.",0,1,0,0,1,0,0.930564,5.0,0.87433,43
1759771a-821d-407a-83ef-d2ae8a0432ca,MorisienMT: A Dataset for Mauritian Creole Machine Translation,6,0.132774,0.811124,"In this paper, we describe MorisienMT, a dataset for benchmarking machine
translation quality of Mauritian Creole. Mauritian Creole (Morisien) is the
lingua franca of the Republic of Mauritius and is a French-based creole
language. MorisienMT consists of a parallel corpus between English and
Morisien, French and Morisien and a monolingual corpus for Morisien. We first
give an overview of Morisien and then describe the steps taken to create the
corpora and, from it, the training and evaluation splits. Thereafter, we
establish a variety of baseline models using the created parallel corpora as
well as large French--English corpora for transfer learning. We release our
datasets publicly for research purposes and hope that this spurs research for
Morisien machine translation.",0,1,1,1,0,0,0.210123,11.0,0.722375,20
f398e952-23c2-43aa-90d4-4f02b9e88c3d,Learning Disentangled Textual Representations via Statistical Measures of Similarity,16,0.1416,0.67503,"When working with textual data, a natural application of disentangled
representations is fair classification where the goal is to make predictions
without being biased (or influenced) by sensitive attributes that may be
present in the data (e.g., age, gender or race). Dominant approaches to
disentangle a sensitive attribute from textual representations rely on learning
simultaneously a penalization term that involves either an adversarial loss
(e.g., a discriminator) or an information measure (e.g., mutual information).
However, these methods require the training of a deep neural network with
several parameter updates for each update of the representation model. As a
matter of fact, the resulting nested optimization loop is both time consuming,
adding complexity to the optimization dynamic, and requires a fine
hyperparameter selection (e.g., learning rates, architecture). In this work, we
introduce a family of regularizers for learning disentangled representations
that do not require training. These regularizers are based on statistical
measures of similarity between the conditional probability distributions with
respect to the sensitive attributes. Our novel regularizers do not require
additional training, are faster and do not involve additional tuning while
achieving better results both when combined with pretrained and randomly
initialized text encoders.",0,0,0,0,0,0,0.318834,7.0,0.633331,88
4d4b63f1-02b3-4243-9305-941704d7f940,On Rate-Distortion Theory in Capacity-Limited Cognition & Reinforcement Learning,2,0.141274,0.303438,"Throughout the cognitive-science literature, there is widespread agreement
that decision-making agents operating in the real world do so under limited
information-processing capabilities and without access to unbounded cognitive
or computational resources. Prior work has drawn inspiration from this fact and
leveraged an information-theoretic model of such behaviors or policies as
communication channels operating under a bounded rate constraint. Meanwhile, a
parallel line of work also capitalizes on the same principles from
rate-distortion theory to formalize capacity-limited decision making through
the notion of a learning target, which facilitates Bayesian regret bounds for
provably-efficient learning algorithms. In this paper, we aim to elucidate this
latter perspective by presenting a brief survey of these information-theoretic
models of capacity-limited decision making in biological and artificial agents.",0,0,0,0,0,0,0.606257,12.0,0.860017,137
8a6eab74-3c3b-4945-aedd-66a06e4bdeaa,KPE: Keypoint Pose Encoding for Transformer-based Image Generation,5,0.0508237,0.101623,"Transformers have recently been shown to generate high quality images from
text input. However, the existing method of pose conditioning using skeleton
image tokens is computationally inefficient and generate low quality images.
Therefore we propose a new method; Keypoint Pose Encoding (KPE); KPE is 10
times more memory efficient and over 73% faster at generating high quality
images from text input conditioned on the pose. The pose constraint improves
the image quality and reduces errors on body extremities such as arms and legs.
The additional benefits include invariance to changes in the target image
domain and image resolution, making it easily scalable to higher resolution
images. We demonstrate the versatility of KPE by generating photorealistic
multiperson images derived from the DeepFashion dataset. We also introduce a
evaluation method People Count Error (PCE) that is effective in detecting error
in generated human images.",0,1,0,0,0,0,0.965391,6.0,0.933931,43
88eb2437-41a8-426e-840f-7d31d07b9e8e,All you need is feedback: Communication with block attention feedback codes,19,0.0866664,0.398667,"Deep learning based channel code designs have recently gained interest as an
alternative to conventional coding algorithms, particularly for channels for
which existing codes do not provide effective solutions. Communication over a
feedback channel is one such problem, for which promising results have recently
been obtained by employing various deep learning architectures. In this paper,
we introduce a novel learning-aided code design for feedback channels, called
generalized block attention feedback (GBAF) codes, which i) employs a modular
architecture that can be implemented using different neural network
architectures; ii) provides order-of-magnitude improvements in the probability
of error compared to existing designs; and iii) can transmit at desired code
rates.",0,1,0,0,1,0,0.108089,11.0,0.656583,36
2dfffd70-6b8e-479f-972f-50df7e469088,Device-friendly Guava fruit and leaf disease detection using deep learning,3,0.0812885,0.314037,"This work presents a deep learning-based plant disease diagnostic system
using images of fruits and leaves. Five state-of-the-art convolutional neural
networks (CNN) have been employed for implementing the system. Hitherto model
accuracy has been the focus for such applications and model optimization has
not been accounted for the model to be applicable to end-user devices. Two
model quantization techniques such as float16 and dynamic range quantization
have been applied to the five state-of-the-art CNN architectures. The study
shows that the quantized GoogleNet model achieved the size of 0.143 MB with an
accuracy of 97%, which is the best candidate model considering the size
criterion. The EfficientNet model achieved the size of 4.2MB with an accuracy
of 99%, which is the best model considering the performance criterion. The
source codes are available at
https://github.com/CompostieAI/Guava-disease-detection.",1,1,0,0,0,0,0.299834,7.0,0.62271,24
cb0236bb-897d-4dec-b2ff-90740427d0f2,Weight Predictor Network with Feature Selection for Small Sample Tabular Biomedical Data,5,0.132886,0.456113,"Tabular biomedical data is often high-dimensional but with a very small
number of samples. Although recent work showed that well-regularised simple
neural networks could outperform more sophisticated architectures on tabular
data, they are still prone to overfitting on tiny datasets with many
potentially irrelevant features. To combat these issues, we propose Weight
Predictor Network with Feature Selection (WPFS) for learning neural networks
from high-dimensional and small sample data by reducing the number of learnable
parameters and simultaneously performing feature selection. In addition to the
classification network, WPFS uses two small auxiliary networks that together
output the weights of the first layer of the classification model. We evaluate
on nine real-world biomedical datasets and demonstrate that WPFS outperforms
other standard as well as more recent methods typically applied to tabular
data. Furthermore, we investigate the proposed feature selection mechanism and
show that it improves performance while providing useful insights into the
learning task.",1,1,0,0,1,0,0.718054,13.0,0.894343,48
c4d232e5-48d2-4f67-98d7-3d8b3dbc770b,Iterative Patch Selection for High-Resolution Image Recognition,5,0.0386948,0.666843,"High-resolution images are prevalent in various applications, such as
autonomous driving and computer-aided diagnosis. However, training neural
networks on such images is computationally challenging and easily leads to
out-of-memory errors even on modern GPUs. We propose a simple method, Iterative
Patch Selection (IPS), which decouples the memory usage from the input size and
thus enables the processing of arbitrarily large images under tight hardware
constraints. IPS achieves this by selecting only the most salient patches,
which are then aggregated into a global representation for image recognition.
For both patch selection and aggregation, a cross-attention based transformer
is introduced, which exhibits a close connection to Multiple Instance Learning.
Our method demonstrates strong performance and has wide applicability across
different domains, training regimes and image sizes while using minimal
accelerator memory. For example, we are able to finetune our model on
whole-slide images consisting of up to 250k patches (>16 gigapixels) with only
5 GB of GPU VRAM at a batch size of 16.",1,1,0,0,0,0,0.439507,8.0,0.7305,46
2f5a297a-e46f-4b88-b60d-c1a5c2f941a3,Universal Domain Adaptive Object Detector,11,0.122208,0.539209,"Universal domain adaptive object detection (UniDAOD)is more challenging than
domain adaptive object detection (DAOD) since the label space of the source
domain may not be the same as that of the target and the scale of objects in
the universal scenarios can vary dramatically (i.e, category shift and scale
shift). To this end, we propose US-DAF, namely Universal Scale-Aware Domain
Adaptive Faster RCNN with Multi-Label Learning, to reduce the negative transfer
effect during training while maximizing transferability as well as
discriminability in both domains under a variety of scales. Specifically, our
method is implemented by two modules: 1) We facilitate the feature alignment of
common classes and suppress the interference of private classes by designing a
Filter Mechanism module to overcome the negative transfer caused by category
shift. 2) We fill the blank of scale-aware adaptation in object detection by
introducing a new Multi-Label Scale-Aware Adapter to perform individual
alignment between the corresponding scale for two domains. Experiments show
that US-DAF achieves state-of-the-art results on three scenarios (i.e,
Open-Set, Partial-Set, and Closed-Set) and yields 7.1% and 5.9% relative
improvement on benchmark datasets Clipart1k and Watercolor in particular.",0,1,1,0,1,0,0.855164,10.0,0.90492,40
8c2d3778-a9b6-4349-a5ed-58a31147488f,Semantic-aligned Fusion Transformer for One-shot Object Detection,18,0.0951119,0.488785,"One-shot object detection aims at detecting novel objects according to merely
one given instance. With extreme data scarcity, current approaches explore
various feature fusions to obtain directly transferable meta-knowledge. Yet,
their performances are often unsatisfactory. In this paper, we attribute this
to inappropriate correlation methods that misalign query-support semantics by
overlooking spatial structures and scale variances. Upon analysis, we leverage
the attention mechanism and propose a simple but effective architecture named
Semantic-aligned Fusion Transformer (SaFT) to resolve these issues.
Specifically, we equip SaFT with a vertical fusion module (VFM) for cross-scale
semantic enhancement and a horizontal fusion module (HFM) for cross-sample
feature fusion. Together, they broaden the vision for each feature point from
the support to a whole augmented feature pyramid from the query, facilitating
semantic-aligned associations. Extensive experiments on multiple benchmarks
demonstrate the superiority of our framework. Without fine-tuning on novel
classes, it brings significant performance gains to one-stage baselines,
lifting state-of-the-art results to a higher level.",0,1,0,0,1,0,0.686472,6.0,0.756478,67
4902089f-584f-4b3d-8c73-21b2a751fc72,Unified Question Generation with Continual Lifelong Learning,6,0.0297056,0.11822,"Question Generation (QG), as a challenging Natural Language Processing task,
aims at generating questions based on given answers and context. Existing QG
methods mainly focus on building or training models for specific QG datasets.
These works are subject to two major limitations: (1) They are dedicated to
specific QG formats (e.g., answer-extraction or multi-choice QG), therefore, if
we want to address a new format of QG, a re-design of the QG model is required.
(2) Optimal performance is only achieved on the dataset they were just trained
on. As a result, we have to train and keep various QG models for different QG
datasets, which is resource-intensive and ungeneralizable.
  To solve the problems, we propose a model named Unified-QG based on lifelong
learning techniques, which can continually learn QG tasks across different
datasets and formats. Specifically, we first build a format-convert encoding to
transform different kinds of QG formats into a unified representation. Then, a
method named \emph{STRIDER} (\emph{S}imilari\emph{T}y \emph{R}egular\emph{I}zed
\emph{D}ifficult \emph{E}xample \emph{R}eplay) is built to alleviate
catastrophic forgetting in continual QG learning. Extensive experiments were
conducted on $8$ QG datasets across $4$ QG formats (answer-extraction,
answer-abstraction, multi-choice, and boolean QG) to demonstrate the
effectiveness of our approach. Experimental results demonstrate that our
Unified-QG can effectively and continually adapt to QG tasks when datasets and
formats vary. In addition, we verify the ability of a single trained Unified-QG
model in improving $8$ Question Answering (QA) systems' performance through
generating synthetic QA data.",0,1,0,0,0,0,0.382232,7.0,0.665715,95
edefef80-da95-4cb6-bca6-8d702b46cdde,ILDAE: Instance-Level Difficulty Analysis of Evaluation Data,15,0.162763,0.252541,"Knowledge of questions' difficulty level helps a teacher in several ways,
such as estimating students' potential quickly by asking carefully selected
questions and improving quality of examination by modifying trivial and hard
questions. Can we extract such benefits of instance difficulty in NLP? To this
end, we conduct Instance-Level Difficulty Analysis of Evaluation data (ILDAE)
in a large-scale setup of 23 datasets and demonstrate its five novel
applications: 1) conducting efficient-yet-accurate evaluations with fewer
instances saving computational cost and time, 2) improving quality of existing
evaluation datasets by repairing erroneous and trivial instances, 3) selecting
the best model based on application requirements, 4) analyzing dataset
characteristics for guiding future data creation, 5) estimating Out-of-Domain
performance reliably. Comprehensive experiments for these applications result
in several interesting findings, such as evaluation using just 5% instances
(selected via ILDAE) achieves as high as 0.93 Kendall correlation with
evaluation using complete dataset and computing weighted accuracy using
difficulty scores leads to 5.2% higher correlation with Out-of-Domain
performance. We release the difficulty scores and hope our analyses and
findings will bring more attention to this important yet understudied field of
leveraging instance difficulty in evaluations.",0,0,0,0,0,0,0.805358,6.0,0.813863,62
7843c206-f8c9-45f4-bcd8-94c8615ab980,Distill the Image to Nowhere: Inversion Knowledge Distillation for Multimodal Machine Translation,8,0.126505,0.468744,"Past works on multimodal machine translation (MMT) elevate bilingual setup by
incorporating additional aligned vision information. However, an image-must
requirement of the multimodal dataset largely hinders MMT's development --
namely that it demands an aligned form of [image, source text, target text].
This limitation is generally troublesome during the inference phase especially
when the aligned image is not provided as in the normal NMT setup. Thus, in
this work, we introduce IKD-MMT, a novel MMT framework to support the
image-free inference phase via an inversion knowledge distillation scheme. In
particular, a multimodal feature generator is executed with a knowledge
distillation module, which directly generates the multimodal feature from
(only) source texts as the input. While there have been a few prior works
entertaining the possibility to support image-free inference for machine
translation, their performances have yet to rival the image-must translation.
In our experiments, we identify our method as the first image-free approach to
comprehensively rival or even surpass (almost) all image-must frameworks, and
achieved the state-of-the-art result on the often-used Multi30k benchmark. Our
code and data are available at: https://github.com/pengr/IKD-mmt/tree/master..",1,1,1,0,1,0,0.259377,11.0,0.744315,48
b5752d33-2d6c-41a4-bd0c-4089df54fe8b,Differentially Private Decoding in Large Language Models,17,0.113926,0.898162,"Recent large-scale natural language processing (NLP) systems use a
pre-trained Large Language Model (LLM) on massive and diverse corpora as a
headstart. In practice, the pre-trained model is adapted to a wide array of
tasks via fine-tuning on task-specific datasets. LLMs, while effective, have
been shown to memorize instances of training data thereby potentially revealing
private information processed during pre-training. The potential leakage might
further propagate to the downstream tasks for which LLMs are fine-tuned. On the
other hand, privacy-preserving algorithms usually involve retraining from
scratch, which is prohibitively expensive for LLMs. In this work, we propose a
simple, easy to interpret, and computationally lightweight perturbation
mechanism to be applied to an already trained model at the decoding stage. Our
perturbation mechanism is model-agnostic and can be used in conjunction with
any LLM. We provide theoretical analysis showing that the proposed mechanism is
differentially private, and experimental results showing a privacy-utility
trade-off.",0,1,0,0,0,0,0.542682,7.0,0.73502,30
98d6c247-3d9a-480e-b4e5-02c2c6edbe0e,Hyperdecoders: Instance-specific decoders for multi-task NLP,12,0.0900912,0.465664,"We investigate input-conditioned hypernetworks for multi-tasking in NLP,
generating parameter-efficient adaptations for a decoder using a hypernetwork
conditioned on the output of an encoder. This approach produces a unique
decoder adaptation for every input instance, allowing the network a larger
degree of flexibility than prior work that only produces one decoder adaptation
per task. We apply our method to sequence classification tasks, extractive QA,
and summarisation and find that it surpasses previous parameter efficient
fine-tuning methods and often outperforms fully finetuning the underlying
model. An analysis of the embeddings used by our hypernetwork shows that they
are sensitive to output label and type, suggesting that our approach better
maps from encoder representations to output labels. Our code is publicly
available at https://github.com/allenai/hyperdecoders.",1,1,0,0,0,0,0.837406,6.0,0.831246,84
09654c07-794b-44d7-b576-ba3c3f6ef34c,Data Augmentation by Selecting Mixed Classes Considering Distance Between Classes,2,0.0057569,0.0982838,"Data augmentation is an essential technique for improving recognition
accuracy in object recognition using deep learning. Methods that generate mixed
data from multiple data sets, such as mixup, can acquire new diversity that is
not included in the training data, and thus contribute significantly to
accuracy improvement. However, since the data selected for mixing are randomly
sampled throughout the training process, there are cases where appropriate
classes or data are not selected. In this study, we propose a data augmentation
method that calculates the distance between classes based on class
probabilities and can select data from suitable classes to be mixed in the
training process. Mixture data is dynamically adjusted according to the
training trend of each class to facilitate training. The proposed method is
applied in combination with conventional methods for generating mixed data.
Evaluation experiments show that the proposed method improves recognition
performance on general and long-tailed image recognition datasets.",0,1,0,0,0,1,0.499565,7.0,0.717542,23
b3cc0139-6269-4755-8fbd-15f6f0eb9bca,Neural Contourlet Network for Monocular 360 Depth Estimation,6,0.0833547,0.550373,"For a monocular 360 image, depth estimation is a challenging because the
distortion increases along the latitude. To perceive the distortion, existing
methods devote to designing a deep and complex network architecture. In this
paper, we provide a new perspective that constructs an interpretable and sparse
representation for a 360 image. Considering the importance of the geometric
structure in depth estimation, we utilize the contourlet transform to capture
an explicit geometric cue in the spectral domain and integrate it with an
implicit cue in the spatial domain. Specifically, we propose a neural
contourlet network consisting of a convolutional neural network and a
contourlet transform branch. In the encoder stage, we design a spatial-spectral
fusion module to effectively fuse two types of cues. Contrary to the encoder,
we employ the inverse contourlet transform with learned low-pass subbands and
band-pass directional subbands to compose the depth in the decoder. Experiments
on the three popular panoramic image datasets demonstrate that the proposed
approach outperforms the state-of-the-art schemes with faster convergence. Code
is available at
https://github.com/zhijieshen-bjtu/Neural-Contourlet-Network-for-MODE.",1,1,0,0,1,0,0.361002,7.0,0.655325,44
dfd1f2eb-7166-4676-8668-5ace01a3b18b,RNNPose: Recurrent 6-DoF Object Pose Refinement with Robust Correspondence Field Estimation and Pose Optimization,40,0.280371,0.870943,"6-DoF object pose estimation from a monocular image is challenging, and a
post-refinement procedure is generally needed for high-precision estimation. In
this paper, we propose a framework based on a recurrent neural network (RNN)
for object pose refinement, which is robust to erroneous initial poses and
occlusions. During the recurrent iterations, object pose refinement is
formulated as a non-linear least squares problem based on the estimated
correspondence field (between a rendered image and the observed image). The
problem is then solved by a differentiable Levenberg-Marquardt (LM) algorithm
enabling end-to-end training. The correspondence field estimation and pose
refinement are conducted alternatively in each iteration to recover the object
poses. Furthermore, to improve the robustness to occlusion, we introduce a
consistency-check mechanism based on the learned descriptors of the 3D model
and observed 2D images, which downweights the unreliable correspondences during
pose optimization. Extensive experiments on LINEMOD, Occlusion-LINEMOD, and
YCB-Video datasets validate the effectiveness of our method and demonstrate
state-of-the-art performance.",1,1,0,0,1,0,0.633617,7.0,0.770662,63
0e6c1376-b792-42cd-8c79-a68315aea38b,Recovering Patient Journeys: A Corpus of Biomedical Entities and Relations on Twitter (BEAR),3,0.0647616,0.272249,"Text mining and information extraction for the medical domain has focused on
scientific text generated by researchers. However, their direct access to
individual patient experiences or patient-doctor interactions can be limited.
Information provided on social media, e.g., by patients and their relatives,
complements the knowledge in scientific text. It reflects the patient's journey
and their subjective perspective on the process of developing symptoms, being
diagnosed and offered a treatment, being cured or learning to live with a
medical condition. The value of this type of data is therefore twofold:
Firstly, it offers direct access to people's perspectives. Secondly, it might
cover information that is not available elsewhere, including self-treatment or
self-diagnoses. Named entity recognition and relation extraction are methods to
structure information that is available in unstructured text. However, existing
medical social media corpora focused on a comparably small set of entities and
relations and particular domains, rather than putting the patient into the
center of analyses. With this paper we contribute a corpus with a rich set of
annotation layers following the motivation to uncover and model patients'
journeys and experiences in more detail. We label 14 entity classes (incl.
environmental factors, diagnostics, biochemical processes, patients'
quality-of-life descriptions, pathogens, medical conditions, and treatments)
and 20 relation classes (e.g., prevents, influences, interactions, causes) most
of which have not been considered before for social media data. The publicly
available dataset consists of 2,100 tweets with approx. 6,000 entity and 3,000
relation annotations. In a corpus analysis we find that over 80 % of documents
contain relevant entities. Over 50 % of tweets express relations which we
consider essential for uncovering patients' narratives about their journeys.",0,1,1,1,0,0,0.206516,10.0,0.692662,50
39bf4838-e9eb-497a-987c-a7226201ca18,Compositional Law Parsing with Latent Random Functions,4,0.152218,0.603469,"Human cognition has compositionality. We understand a scene by decomposing
the scene into different concepts (e.g., shape and position of an object) and
learning the respective laws of these concepts, which may be either natural
(e.g., laws of motion) or man-made (e.g., laws of a game). The automatic
parsing of these laws indicates the model's ability to understand the scene,
which makes law parsing play a central role in many visual tasks. This paper
proposes a deep latent variable model for Compositional LAw Parsing (CLAP),
which achieves the human-like compositionality ability through an
encoding-decoding architecture to represent concepts in the scene as latent
variables. CLAP employs concept-specific latent random functions instantiated
with Neural Processes to capture the law of concepts. Our experimental results
demonstrate that CLAP outperforms the baseline methods in multiple visual tasks
such as intuitive physics, abstract visual reasoning, and scene representation.
The law manipulation experiments illustrate CLAP's interpretability by
modifying specific latent random functions on samples. For example, CLAP learns
the laws of position-changing and appearance constancy from the moving balls in
a scene, making it possible to exchange laws between samples or compose
existing laws into novel laws.",1,0,0,0,0,0,0.863583,8.0,0.884965,59
b3bb9e92-9917-4a14-b0c4-2b1e5efd5ece,An Energy-aware and Fault-tolerant Deep Reinforcement Learning based approach for Multi-agent Patrolling Problems,1,0.00872153,0.139771,"Autonomous vehicles are suited for continuous area patrolling problems.
However, finding an optimal patrolling strategy can be challenging for many
reasons. Firstly, patrolling environments are often complex and can include
unknown environmental factors, such as wind or landscape. Secondly, autonomous
vehicles can have failures or hardware constraints, such as limited battery
life. Importantly, patrolling large areas often requires multiple agents that
need to collectively coordinate their actions. In this work, we consider these
limitations and propose an approach based on model-free, deep multi-agent
reinforcement learning. In this approach, the agents are trained to patrol an
environment with various unknown dynamics and factors. They can automatically
recharge themselves to support continuous collective patrolling. A distributed
homogeneous multi-agent architecture is proposed, where all patrolling agents
execute identical policies locally based on their local observations and shared
location information. This architecture provides a patrolling system that can
tolerate agent failures and allow supplementary agents to be added to replace
failed agents or to increase the overall patrol performance. The solution is
validated through simulation experiments from multiple perspectives, including
the overall patrol performance, the efficiency of battery recharging
strategies, the overall fault tolerance, and the ability to cooperate with
supplementary agents.",0,1,0,0,0,0,0.000491743,18.0,0.487403,55
fcdc90ee-8ead-4be3-80a3-9af1c51ff65d,Darwinian Model Upgrades: Model Evolving with Selective Compatibility,3,0.00488428,0.150483,"The traditional model upgrading paradigm for retrieval requires recomputing
all gallery embeddings before deploying the new model (dubbed as
""backfilling""), which is quite expensive and time-consuming considering
billions of instances in industrial applications. BCT presents the first step
towards backward-compatible model upgrades to get rid of backfilling. It is
workable but leaves the new model in a dilemma between new feature
discriminativeness and new-to-old compatibility due to the undifferentiated
compatibility constraints. In this work, we propose Darwinian Model Upgrades
(DMU), which disentangle the inheritance and variation in the model evolving
with selective backward compatibility and forward adaptation, respectively. The
old-to-new heritable knowledge is measured by old feature discriminativeness,
and the gallery features, especially those of poor quality, are evolved in a
lightweight manner to become more adaptive in the new latent space. We
demonstrate the superiority of DMU through comprehensive experiments on
large-scale landmark retrieval and face recognition benchmarks. DMU effectively
alleviates the new-to-new degradation and improves new-to-old compatibility,
rendering a more proper model upgrading paradigm in large-scale retrieval
systems.",0,1,0,0,0,0,0.105689,5.0,0.239727,24
224498e4-2097-452a-8b2b-ac34e25215a2,Semi-Supervised Knowledge-Grounded Pre-training for Task-Oriented Dialog Systems,14,0.267643,0.885175,"Recent advances in neural approaches greatly improve task-oriented dialogue
(TOD) systems which assist users to accomplish their goals. However, such
systems rely on costly manually labeled dialogs which are not available in
practical scenarios. In this paper, we present our models for Track 2 of the
SereTOD 2022 challenge, which is the first challenge of building
semi-supervised and reinforced TOD systems on a large-scale real-world Chinese
TOD dataset MobileCS. We build a knowledge-grounded dialog model to formulate
dialog history and local KB as input and predict the system response. And we
perform semi-supervised pre-training both on the labeled and unlabeled data.
Our system achieves the first place both in the automatic evaluation and human
interaction, especially with higher BLEU (+7.64) and Success (+13.6\%) than the
second place.",1,1,0,0,1,0,0.396015,7.0,0.672256,23
4f4e01d1-1e9e-4566-a269-92b19fdc2d22,TencentPretrain: A Scalable and Flexible Toolkit for Pre-training Models of Different Modalities,12,0.267468,0.397541,"Recently, the success of pre-training in text domain has been fully extended
to vision, audio, and cross-modal scenarios. The proposed pre-training models
of different modalities are showing a rising trend of homogeneity in their
model structures, which brings the opportunity to implement different
pre-training models within a uniform framework. In this paper, we present
TencentPretrain, a toolkit supporting pre-training models of different
modalities. The core feature of TencentPretrain is the modular design. The
toolkit uniformly divides pre-training models into 5 components: embedding,
encoder, target embedding, decoder, and target. As almost all of common modules
are provided in each component, users can choose the desired modules from
different components to build a complete pre-training model. The modular design
enables users to efficiently reproduce existing pre-training models or build
brand-new one. We test the toolkit on text, vision, and audio benchmarks and
show that it can match the performance of the original implementations.",1,1,0,0,0,0,0.954764,7.0,0.931519,48
4c3193a2-ae0e-48f9-b192-218c06a90e6e,"Robustness in deep learning: The good (width), the bad (depth), and the ugly (initialization)",12,0.0397858,0.452609,"We study the average robustness notion in deep neural networks in (selected)
wide and narrow, deep and shallow, as well as lazy and non-lazy training
settings. We prove that in the under-parameterized setting, width has a
negative effect while it improves robustness in the over-parameterized setting.
The effect of depth closely depends on the initialization and the training
mode. In particular, when initialized with LeCun initialization, depth helps
robustness with the lazy training regime. In contrast, when initialized with
Neural Tangent Kernel (NTK) and He-initialization, depth hurts the robustness.
Moreover, under the non-lazy training regime, we demonstrate how the width of a
two-layer ReLU network benefits robustness. Our theoretical developments
improve the results by [Huang et al. NeurIPS21; Wu et al. NeurIPS21] and are
consistent with [Bubeck and Sellke NeurIPS21; Bubeck et al. COLT21].",1,0,0,0,0,0,0.203085,7.0,0.558256,63
e23a56b2-0e7a-40dd-95f8-e44dc209712b,Mixture-Net: Low-Rank Deep Image Prior Inspired by Mixture Models for Spectral Image Recovery,5,0.059751,0.387464,"This paper proposes a non-data-driven deep neural network for spectral image
recovery problems such as denoising, single hyperspectral image
super-resolution, and compressive spectral imaging reconstruction. Unlike
previous methods, the proposed approach, dubbed Mixture-Net, implicitly learns
the prior information through the network. Mixture-Net consists of a deep
generative model whose layers are inspired by the linear and non-linear
low-rank mixture models, where the recovered image is composed of a weighted
sum between the linear and non-linear decomposition. Mixture-Net also provides
a low-rank decomposition interpreted as the spectral image abundances and
endmembers, helpful in achieving remote sensing tasks without running
additional routines. The experiments show the MixtureNet effectiveness
outperforming state-of-the-art methods in recovery quality with the advantage
of architecture interpretability.",0,1,0,0,1,0,0.189012,8.0,0.603443,56
f71a861d-f652-4f33-b3db-aa66a5a5a408,Generalizing to the Future: Mitigating Entity Bias in Fake News Detection,38,0.727194,0.722496,"The wide dissemination of fake news is increasingly threatening both
individuals and society. Fake news detection aims to train a model on the past
news and detect fake news of the future. Though great efforts have been made,
existing fake news detection methods overlooked the unintended entity bias in
the real-world data, which seriously influences models' generalization ability
to future data. For example, 97\% of news pieces in 2010-2017 containing the
entity `Donald Trump' are real in our data, but the percentage falls down to
merely 33\% in 2018. This would lead the model trained on the former set to
hardly generalize to the latter, as it tends to predict news pieces about
`Donald Trump' as real for lower training loss. In this paper, we propose an
entity debiasing framework (\textbf{ENDEF}) which generalizes fake news
detection models to the future data by mitigating entity bias from a
cause-effect perspective. Based on the causal graph among entities, news
contents, and news veracity, we separately model the contribution of each cause
(entities and contents) during training. In the inference stage, we remove the
direct effect of the entities to mitigate entity bias. Extensive offline
experiments on the English and Chinese datasets demonstrate that the proposed
framework can largely improve the performance of base fake news detectors, and
online tests verify its superiority in practice. To the best of our knowledge,
this is the first work to explicitly improve the generalization ability of fake
news detection models to the future data. The code has been released at
https://github.com/ICTMCG/ENDEF-SIGIR2022.",1,1,0,0,1,0,0.909563,6.0,0.877887,44
dbb9fe99-e5a3-43ed-b2d7-b0272e80f0ef,PolyLoss: A Polynomial Expansion Perspective of Classification Loss Functions,91,0.857382,0.922778,"Cross-entropy loss and focal loss are the most common choices when training
deep neural networks for classification problems. Generally speaking, however,
a good loss function can take on much more flexible forms, and should be
tailored for different tasks and datasets. Motivated by how functions can be
approximated via Taylor expansion, we propose a simple framework, named
PolyLoss, to view and design loss functions as a linear combination of
polynomial functions. Our PolyLoss allows the importance of different
polynomial bases to be easily adjusted depending on the targeting tasks and
datasets, while naturally subsuming the aforementioned cross-entropy loss and
focal loss as special cases. Extensive experimental results show that the
optimal choice within the PolyLoss is indeed dependent on the task and dataset.
Simply by introducing one extra hyperparameter and adding one line of code, our
Poly-1 formulation outperforms the cross-entropy loss and focal loss on 2D
image classification, instance segmentation, object detection, and 3D object
detection tasks, sometimes by a large margin.",1,0,1,0,1,1,0.898875,7.0,0.888532,41
66c0eee0-1755-464d-a84d-f231dc08f257,Dataless Knowledge Fusion by Merging Weights of Language Models,55,0.615862,0.991309,"Fine-tuning pre-trained language models has become the prevalent paradigm for
building downstream NLP models. Oftentimes fine-tuned models are readily
available but their training data is not, due to data privacy or intellectual
property concerns. This creates a barrier to fusing knowledge across individual
models to yield a better single model. In this paper, we study the problem of
merging individual models built on different training data sets to obtain a
single model that performs well both across all data set domains and can
generalize on out-of-domain data. We propose a dataless knowledge fusion method
that merges models in their parameter space, guided by weights that minimize
prediction differences between the merged model and the individual models. Over
a battery of evaluation settings, we show that the proposed method
significantly outperforms baselines such as Fisher-weighted averaging or model
ensembling. Further, we find that our method is a promising alternative to
multi-task learning that can preserve or sometimes improve over the individual
models without access to the training data. Finally, model merging is more
efficient than training a multi-task model, thus making it applicable to a
wider set of scenarios.",1,1,0,0,0,0,0.698034,8.0,0.821344,58
3a115f40-130d-4b7a-84b5-fb3f6b83f168,Neural-Symbolic Entangled Framework for Complex Query Answering,18,0.669833,0.835968,"Answering complex queries over knowledge graphs (KG) is an important yet
challenging task because of the KG incompleteness issue and cascading errors
during reasoning. Recent query embedding (QE) approaches to embed the entities
and relations in a KG and the first-order logic (FOL) queries into a low
dimensional space, answering queries by dense similarity search. However,
previous works mainly concentrate on the target answers, ignoring intermediate
entities' usefulness, which is essential for relieving the cascading error
problem in logical query answering. In addition, these methods are usually
designed with their own geometric or distributional embeddings to handle
logical operators like union, intersection, and negation, with the sacrifice of
the accuracy of the basic operator - projection, and they could not absorb
other embedding methods to their models. In this work, we propose a Neural and
Symbolic Entangled framework (ENeSy) for complex query answering, which enables
the neural and symbolic reasoning to enhance each other to alleviate the
cascading error and KG incompleteness. The projection operator in ENeSy could
be any embedding method with the capability of link prediction, and the other
FOL operators are handled without parameters. With both neural and symbolic
reasoning results contained, ENeSy answers queries in ensembles. ENeSy achieves
the SOTA performance on several benchmarks, especially in the setting of the
training model only with the link prediction task.",0,0,0,0,1,0,0.980554,11.0,0.978343,31
cf654840-8e8c-40d9-81c4-43d4d145cb4e,Less is More: Learning to Refine Dialogue History for Personalized Dialogue Generation,32,0.384411,0.896592,"Personalized dialogue systems explore the problem of generating responses
that are consistent with the user's personality, which has raised much
attention in recent years. Existing personalized dialogue systems have tried to
extract user profiles from dialogue history to guide personalized response
generation. Since the dialogue history is usually long and noisy, most existing
methods truncate the dialogue history to model the user's personality. Such
methods can generate some personalized responses, but a large part of dialogue
history is wasted, leading to sub-optimal performance of personalized response
generation. In this work, we propose to refine the user dialogue history on a
large scale, based on which we can handle more dialogue history and obtain more
abundant and accurate persona information. Specifically, we design an MSP model
which consists of three personal information refiners and a personalized
response generator. With these multi-level refiners, we can sparsely extract
the most valuable information (tokens) from the dialogue history and leverage
other similar users' data to enhance personalization. Experimental results on
two real-world datasets demonstrate the superiority of our model in generating
more informative and personalized responses.",0,1,0,0,0,0,0.534013,9.0,0.791205,47
2a3abecc-9139-4579-b6f5-012332b8f2bc,Consistency driven Sequential Transformers Attention Model for Partially Observable Scenes,12,0.0403706,0.177354,"Most hard attention models initially observe a complete scene to locate and
sense informative glimpses, and predict class-label of a scene based on
glimpses. However, in many applications (e.g., aerial imaging), observing an
entire scene is not always feasible due to the limited time and resources
available for acquisition. In this paper, we develop a Sequential Transformers
Attention Model (STAM) that only partially observes a complete image and
predicts informative glimpse locations solely based on past glimpses. We design
our agent using DeiT-distilled and train it with a one-step actor-critic
algorithm. Furthermore, to improve classification performance, we introduce a
novel training objective, which enforces consistency between the class
distribution predicted by a teacher model from a complete image and the class
distribution predicted by our agent using glimpses. When the agent senses only
4% of the total image area, the inclusion of the proposed consistency loss in
our training objective yields 3% and 8% higher accuracy on ImageNet and fMoW
datasets, respectively. Moreover, our agent outperforms previous
state-of-the-art by observing nearly 27% and 42% fewer pixels in glimpses on
ImageNet and fMoW.",1,1,0,0,1,0,0.552314,6.0,0.695331,55
887ea035-1900-4479-88d0-f1d9969adb76,The first neural machine translation system for the Erzya language,1,0.00248002,0.0962424,"We present the first neural machine translation system for translation
between the endangered Erzya language and Russian and the dataset collected by
us to train and evaluate it. The BLEU scores are 17 and 19 for translation to
Erzya and Russian respectively, and more than half of the translations are
rated as acceptable by native speakers. We also adapt our model to translate
between Erzya and 10 other languages, but without additional parallel data, the
quality on these directions remains low. We release the translation models
along with the collected text corpus, a new language identification model, and
a multilingual sentence encoder adapted for the Erzya language. These resources
will be available at https://github.com/slone-nlp/myv-nmt.",1,1,1,1,0,0,0.0885246,6.0,0.335346,30
b3cf5538-f7b1-41b0-b876-d7f75fc231cd,Vision Transformers: State of the Art and Research Challenges,10,0.0488886,0.20978,"Transformers have achieved great success in natural language processing. Due
to the powerful capability of self-attention mechanism in transformers,
researchers develop the vision transformers for a variety of computer vision
tasks, such as image recognition, object detection, image segmentation, pose
estimation, and 3D reconstruction. This paper presents a comprehensive overview
of the literature on different architecture designs and training tricks
(including self-supervised learning) for vision transformers. Our goal is to
provide a systematic review with the open research opportunities.",0,0,0,0,0,0,0.956387,3.0,0.844121,57
03af8efb-b327-4be7-b4ab-db2fe6c2924f,Hyperspherical Consistency Regularization,20,0.230127,0.868667,"Recent advances in contrastive learning have enlightened diverse applications
across various semi-supervised fields. Jointly training supervised learning and
unsupervised learning with a shared feature encoder becomes a common scheme.
Though it benefits from taking advantage of both feature-dependent information
from self-supervised learning and label-dependent information from supervised
learning, this scheme remains suffering from bias of the classifier. In this
work, we systematically explore the relationship between self-supervised
learning and supervised learning, and study how self-supervised learning helps
robust data-efficient deep learning. We propose hyperspherical consistency
regularization (HCR), a simple yet effective plug-and-play method, to
regularize the classifier using feature-dependent information and thus avoid
bias from labels. Specifically, HCR first projects logits from the classifier
and feature projections from the projection head on the respective hypersphere,
then it enforces data points on hyperspheres to have similar structures by
minimizing binary cross entropy of pairwise distances' similarity metrics.
Extensive experiments on semi-supervised and weakly-supervised learning
demonstrate the effectiveness of our method, by showing superior performance
with HCR.",0,0,0,0,0,0,0.88419,8.0,0.894841,95
14da4dae-7160-443e-9432-9ff41410f632,NeuralPassthrough: Learned Real-Time View Synthesis for VR,14,0.102994,0.257087,"Virtual reality (VR) headsets provide an immersive, stereoscopic visual
experience, but at the cost of blocking users from directly observing their
physical environment. Passthrough techniques are intended to address this
limitation by leveraging outward-facing cameras to reconstruct the images that
would otherwise be seen by the user without the headset. This is inherently a
real-time view synthesis challenge, since passthrough cameras cannot be
physically co-located with the eyes. Existing passthrough techniques suffer
from distracting reconstruction artifacts, largely due to the lack of accurate
depth information (especially for near-field and disoccluded objects), and also
exhibit limited image quality (e.g., being low resolution and monochromatic).
In this paper, we propose the first learned passthrough method and assess its
performance using a custom VR headset that contains a stereo pair of RGB
cameras. Through both simulations and experiments, we demonstrate that our
learned passthrough method delivers superior image quality compared to
state-of-the-art methods, while meeting strict VR requirements for real-time,
perspective-correct stereoscopic view synthesis over a wide field of view for
desktop-connected headsets.",1,1,0,0,1,0,0.558077,7.0,0.74114,42
61e581df-e00a-4d55-a44f-1c2aa5b4e46c,Measuring Inconsistency in Declarative Process Specifications,6,0.0977817,0.342388,"We address the problem of measuring inconsistency in declarative process
specifications, with an emphasis on linear temporal logic on fixed traces
(LTLff). As we will show, existing inconsistency measures for classical logic
cannot provide a meaningful assessment of inconsistency in LTL in general, as
they cannot adequately handle the temporal operators. We therefore propose a
novel paraconsistent semantics as a framework for inconsistency measurement. We
then present two new inconsistency measures based on these semantics and show
that they satisfy important desirable properties. We show how these measures
can be applied to declarative process models and investigate the computational
complexity of the introduced approach.",0,0,0,0,0,0,0.00176075,22.0,0.63861,26
2920a34d-0687-4f79-b37b-3b2548fb0aec,Privacy-Friendly Peer-to-Peer Energy Trading: A Game Theoretical Approach,5,0.235408,0.362125,"In this paper, we propose a decentralized, privacy-friendly energy trading
platform (PFET) based on game theoretical approach - specifically Stackelberg
competition. Unlike existing trading schemes, PFET provides a competitive
market in which prices and demands are determined based on competition, and
computations are performed in a decentralized manner which does not rely on
trusted third parties. It uses homomorphic encryption cryptosystem to encrypt
sensitive information of buyers and sellers such as sellers$'$ prices and
buyers$'$ demands. Buyers calculate total demand on particular seller using an
encrypted data and sensitive buyer profile data is hidden from sellers. Hence,
privacy of both sellers and buyers is preserved. Through privacy analysis and
performance evaluation, we show that PFET preserves users$'$ privacy in an
efficient manner.",0,1,0,0,0,0,0.602892,9.0,0.812336,19
4c4f99e1-3f29-44b3-b9d8-b0a9de659ccd,What do we Really Know about State of the Art NER?,11,0.0659075,0.824037,"Named Entity Recognition (NER) is a well researched NLP task and is widely
used in real world NLP scenarios. NER research typically focuses on the
creation of new ways of training NER, with relatively less emphasis on
resources and evaluation. Further, state of the art (SOTA) NER models, trained
on standard datasets, typically report only a single performance measure
(F-score) and we don't really know how well they do for different entity types
and genres of text, or how robust are they to new, unseen entities. In this
paper, we perform a broad evaluation of NER using a popular dataset, that takes
into consideration various text genres and sources constituting the dataset at
hand. Additionally, we generate six new adversarial test sets through small
perturbations in the original test set, replacing select entities while
retaining the context. We also train and test our models on randomly generated
train/dev/test splits followed by an experiment where the models are trained on
a select set of genres but tested genres not seen in training. These
comprehensive evaluation strategies were performed using three SOTA NER models.
Based on our results, we recommend some useful reporting practices for NER
researchers, that could help in providing a better understanding of a SOTA
model's performance in future.",0,1,0,0,0,1,0.373935,4.0,0.40798,44
57ff2b0c-e8db-42bb-a6d2-2aeb7db8f94c,Open Vocabulary Semantic Segmentation with Patch Aligned Contrastive Learning,41,0.091832,0.858513,"We introduce Patch Aligned Contrastive Learning (PACL), a modified
compatibility function for CLIP's contrastive loss, intending to train an
alignment between the patch tokens of the vision encoder and the CLS token of
the text encoder. With such an alignment, a model can identify regions of an
image corresponding to a given text input, and therefore transfer seamlessly to
the task of open vocabulary semantic segmentation without requiring any
segmentation annotations during training. Using pre-trained CLIP encoders with
PACL, we are able to set the state-of-the-art on the task of open vocabulary
zero-shot segmentation on 4 different segmentation benchmarks: Pascal VOC,
Pascal Context, COCO Stuff and ADE20K. Furthermore, we show that PACL is also
applicable to image-level predictions and when used with a CLIP backbone,
provides a general improvement in zero-shot classification accuracy compared to
CLIP, across a suite of 12 image classification datasets.",0,1,0,0,1,0,0.650563,5.0,0.688143,64
b414ab38-87f8-4d06-bc75-47edf42c7509,MMKGR: Multi-hop Multi-modal Knowledge Graph Reasoning,11,0.269119,0.551736,"Multi-modal knowledge graphs (MKGs) include not only the relation triplets,
but also related multi-modal auxiliary data (i.e., texts and images), which
enhance the diversity of knowledge. However, the natural incompleteness has
significantly hindered the applications of MKGs. To tackle the problem,
existing studies employ the embedding-based reasoning models to infer the
missing knowledge after fusing the multi-modal features. However, the reasoning
performance of these methods is limited due to the following problems: (1)
ineffective fusion of multi-modal auxiliary features; (2) lack of complex
reasoning ability as well as inability to conduct the multi-hop reasoning which
is able to infer more missing knowledge. To overcome these problems, we propose
a novel model entitled MMKGR (Multi-hop Multi-modal Knowledge Graph Reasoning).
Specifically, the model contains the following two components: (1) a unified
gate-attention network which is designed to generate effective multi-modal
complementary features through sufficient attention interaction and noise
reduction; (2) a complementary feature-aware reinforcement learning method
which is proposed to predict missing elements by performing the multi-hop
reasoning process, based on the features obtained in component (1). The
experimental results demonstrate that MMKGR outperforms the state-of-the-art
approaches in the MKG reasoning task.",0,0,0,0,1,0,0.591417,9.0,0.808855,80
23dd3364-8c88-4608-9460-fad0ae90fa89,How Well Do Self-Supervised Methods Perform in Cross-Domain Few-Shot Learning?,3,0.00945542,0.0646932,"Cross-domain few-shot learning (CDFSL) remains a largely unsolved problem in
the area of computer vision, while self-supervised learning presents a
promising solution. Both learning methods attempt to alleviate the dependency
of deep networks on the requirement of large-scale labeled data. Although
self-supervised methods have recently advanced dramatically, their utility on
CDFSL is relatively unexplored. In this paper, we investigate the role of
self-supervised representation learning in the context of CDFSL via a thorough
evaluation of existing methods. It comes as a surprise that even with shallow
architectures or small training datasets, self-supervised methods can perform
favorably compared to the existing SOTA methods. Nevertheless, no single
self-supervised approach dominates all datasets indicating that existing
self-supervised methods are not universally applicable. In addition, we find
that representations extracted from self-supervised methods exhibit stronger
robustness than the supervised method. Intriguingly, whether self-supervised
representations perform well on the source domain has little correlation with
their applicability on the target domain. As part of our study, we conduct an
objective measurement of the performance for six kinds of representative
classifiers. The results suggest Prototypical Classifier as the standard
evaluation recipe for CDFSL.",0,1,0,0,0,0,0.834152,6.0,0.829417,45
996ba743-87e1-4c68-85de-e5c343e94cd9,RA-Depth: Resolution Adaptive Self-Supervised Monocular Depth Estimation,27,0.157666,0.906333,"Existing self-supervised monocular depth estimation methods can get rid of
expensive annotations and achieve promising results. However, these methods
suffer from severe performance degradation when directly adopting a model
trained on a fixed resolution to evaluate at other different resolutions. In
this paper, we propose a resolution adaptive self-supervised monocular depth
estimation method (RA-Depth) by learning the scale invariance of the scene
depth. Specifically, we propose a simple yet efficient data augmentation method
to generate images with arbitrary scales for the same scene. Then, we develop a
dual high-resolution network that uses the multi-path encoder and decoder with
dense interactions to aggregate multi-scale features for accurate depth
inference. Finally, to explicitly learn the scale invariance of the scene
depth, we formulate a cross-scale depth consistency loss on depth predictions
with different scales. Extensive experiments on the KITTI, Make3D and NYU-V2
datasets demonstrate that RA-Depth not only achieves state-of-the-art
performance, but also exhibits a good ability of resolution adaptation.",1,1,1,0,1,0,0.523843,9.0,0.788018,49
874e6997-09b7-495a-b813-17db1344cca6,Occlusion-Aware Cost Constructor for Light Field Depth Estimation,43,0.6119,0.991032,"Matching cost construction is a key step in light field (LF) depth
estimation, but was rarely studied in the deep learning era. Recent deep
learning-based LF depth estimation methods construct matching cost by
sequentially shifting each sub-aperture image (SAI) with a series of predefined
offsets, which is complex and time-consuming. In this paper, we propose a
simple and fast cost constructor to construct matching cost for LF depth
estimation. Our cost constructor is composed by a series of convolutions with
specifically designed dilation rates. By applying our cost constructor to SAI
arrays, pixels under predefined disparities can be integrated and matching cost
can be constructed without using any shifting operation. More importantly, the
proposed cost constructor is occlusion-aware and can handle occlusions by
dynamically modulating pixels from different views. Based on the proposed cost
constructor, we develop a deep network for LF depth estimation. Our network
ranks first on the commonly used 4D LF benchmark in terms of the mean square
error (MSE), and achieves a faster running time than other state-of-the-art
methods.",0,1,0,0,1,0,0.593536,8.0,0.785686,42
fd25600f-8c7f-44ff-a678-b3bcef6a6238,Learning to Imitate Object Interactions from Internet Videos,14,0.0413555,0.693131,"We study the problem of imitating object interactions from Internet videos.
This requires understanding the hand-object interactions in 4D, spatially in 3D
and over time, which is challenging due to mutual hand-object occlusions. In
this paper we make two main contributions: (1) a novel reconstruction technique
RHOV (Reconstructing Hands and Objects from Videos), which reconstructs 4D
trajectories of both the hand and the object using 2D image cues and temporal
smoothness constraints; (2) a system for imitating object interactions in a
physics simulator with reinforcement learning. We apply our reconstruction
technique to 100 challenging Internet videos. We further show that we can
successfully imitate a range of different object interactions in a physics
simulator. Our object-centric approach is not limited to human-like
end-effectors and can learn to imitate object interactions using different
embodiments, like a robotic arm with a parallel jaw gripper.",0,1,0,0,0,0,0.149447,7.0,0.509941,66
1f643a27-dd81-49f1-a5da-602f9eb4bd66,PSFormer: Point Transformer for 3D Salient Object Detection,1,0.00732024,0.0202627,"We propose PSFormer, an effective point transformer model for 3D salient
object detection. PSFormer is an encoder-decoder network that takes full
advantage of transformers to model the contextual information in both
multi-scale point- and scene-wise manners. In the encoder, we develop a Point
Context Transformer (PCT) module to capture region contextual features at the
point level; PCT contains two different transformers to excavate the
relationship among points. In the decoder, we develop a Scene Context
Transformer (SCT) module to learn context representations at the scene level;
SCT contains both Upsampling-and-Transformer blocks and Multi-context
Aggregation units to integrate the global semantic and multi-level features
from the encoder into the global scene context. Experiments show clear
improvements of PSFormer over its competitors and validate that PSFormer is
more robust to challenging cases such as small objects, multiple objects, and
objects with complex structures.",0,1,0,0,1,0,0.899941,6.0,0.870723,15
83b6fd79-60e9-4f75-b797-4b2bad4c96c0,Unsupervised Face Morphing Attack Detection via Self-paced Anomaly Detection,7,0.404072,0.688597,"The supervised-learning-based morphing attack detection (MAD) solutions
achieve outstanding success in dealing with attacks from known morphing
techniques and known data sources. However, given variations in the morphing
attacks, the performance of supervised MAD solutions drops significantly due to
the insufficient diversity and quantity of the existing MAD datasets. To
address this concern, we propose a completely unsupervised MAD solution via
self-paced anomaly detection (SPL-MAD) by leveraging the existing large-scale
face recognition (FR) datasets and the unsupervised nature of convolutional
autoencoders. Using general FR datasets that might contain unintentionally and
unlabeled manipulated samples to train an autoencoder can lead to a diverse
reconstruction behavior of attack and bona fide samples. We analyze this
behavior empirically to provide a solid theoretical ground for designing our
unsupervised MAD solution. This also results in proposing to integrate our
adapted modified self-paced learning paradigm to enhance the reconstruction
error separability between the bona fide and attack samples in a completely
unsupervised manner. Our experimental results on a diverse set of MAD
evaluation datasets show that the proposed unsupervised SPL-MAD solution
outperforms the overall performance of a wide range of supervised MAD solutions
and provides higher generalizability on unknown attacks.",1,1,0,0,0,0,0.935243,8.0,0.924683,55
551a5080-71e6-4e10-8dd8-6628e76e558b,Object Goal Navigation Based on Semantics and RGB Ego View,1,0.0156557,0.0419559,"This paper presents an architecture and methodology to empower a service
robot to navigate an indoor environment with semantic decision making, given
RGB ego view. This method leverages the knowledge of robot's actuation
capability and that of scenes, objects and their relations -- represented in a
semantic form. The robot navigates based on GeoSem map - a relational
combination of geometric and semantic map. The goal given to the robot is to
find an object in a unknown environment with no navigational map and only
egocentric RGB camera perception. The approach is tested both on a simulation
environment and real life indoor settings. The presented approach was found to
outperform human users in gamified evaluations with respect to average
completion time.",0,1,0,0,0,0,0.168414,10.0,0.670003,13
bfdd5493-0076-4bb6-a787-e4f081ba6a39,Characterizing the Efficiency vs. Accuracy Trade-off for Long-Context NLP Models,5,0.117798,0.35213,"With many real-world applications of Natural Language Processing (NLP)
comprising of long texts, there has been a rise in NLP benchmarks that measure
the accuracy of models that can handle longer input sequences. However, these
benchmarks do not consider the trade-offs between accuracy, speed, and power
consumption as input sizes or model sizes are varied. In this work, we perform
a systematic study of this accuracy vs. efficiency trade-off on two widely used
long-sequence models - Longformer-Encoder-Decoder (LED) and Big Bird - during
fine-tuning and inference on four datasets from the SCROLLS benchmark. To study
how this trade-off differs across hyperparameter settings, we compare the
models across four sequence lengths (1024, 2048, 3072, 4096) and two model
sizes (base and large) under a fixed resource budget. We find that LED
consistently achieves better accuracy at lower energy costs than Big Bird. For
summarization, we find that increasing model size is more energy efficient than
increasing sequence length for higher accuracy. However, this comes at the cost
of a large drop in inference speed. For question answering, we find that
smaller models are both more efficient and more accurate due to the larger
training batch sizes possible under a fixed resource budget.",1,1,0,0,0,0,0.97273,5.0,0.934407,23
235c99bd-4b8f-4aed-9841-6cccfa57cf9d,Large Language Models are Pretty Good Zero-Shot Video Game Bug Detectors,5,0.294035,0.786445,"Video game testing requires game-specific knowledge as well as common sense
reasoning about the events in the game. While AI-driven agents can satisfy the
first requirement, it is not yet possible to meet the second requirement
automatically. Therefore, video game testing often still relies on manual
testing, and human testers are required to play the game thoroughly to detect
bugs. As a result, it is challenging to fully automate game testing. In this
study, we explore the possibility of leveraging the zero-shot capabilities of
large language models for video game bug detection. By formulating the bug
detection problem as a question-answering task, we show that large language
models can identify which event is buggy in a sequence of textual descriptions
of events from a game. To this end, we introduce the GameBugDescriptions
benchmark dataset, which consists of 167 buggy gameplay videos and a total of
334 question-answer pairs across 8 games. We extensively evaluate the
performance of six models across the OPT and InstructGPT large language model
families on our benchmark dataset. Our results show promising results for
employing language models to detect video game bugs. With the proper prompting
technique, we could achieve an accuracy of 70.66%, and on some video games, up
to 78.94%. Our code, evaluation data and the benchmark can be found on
https://asgaardlab.github.io/LLMxBugs",0,1,0,1,0,0,0.940052,4.0,0.856317,54
1e3e426f-1a9d-45a3-b2df-a8ee3f8d62a0,FLAG: Flow-based 3D Avatar Generation from Sparse Observations,35,0.308601,0.894228,"To represent people in mixed reality applications for collaboration and
communication, we need to generate realistic and faithful avatar poses.
However, the signal streams that can be applied for this task from head-mounted
devices (HMDs) are typically limited to head pose and hand pose estimates.
While these signals are valuable, they are an incomplete representation of the
human body, making it challenging to generate a faithful full-body avatar. We
address this challenge by developing a flow-based generative model of the 3D
human body from sparse observations, wherein we learn not only a conditional
distribution of 3D human pose, but also a probabilistic mapping from
observations to the latent space from which we can generate a plausible pose
along with uncertainty estimates for the joints. We show that our approach is
not only a strong predictive model, but can also act as an efficient pose prior
in different optimization settings where a good initial latent code plays a
major role.",0,1,0,0,0,0,0.826667,5.0,0.790326,56
9895d788-326e-450a-95ce-6bd2d798217a,Unified Fourier-based Kernel and Nonlinearity Design for Equivariant Networks on Homogeneous Spaces,13,0.268472,0.57126,"We introduce a unified framework for group equivariant networks on
homogeneous spaces derived from a Fourier perspective. We consider
tensor-valued feature fields, before and after a convolutional layer. We
present a unified derivation of kernels via the Fourier domain by leveraging
the sparsity of Fourier coefficients of the lifted feature fields. The sparsity
emerges when the stabilizer subgroup of the homogeneous space is a compact Lie
group. We further introduce a nonlinear activation, via an elementwise
nonlinearity on the regular representation after lifting and projecting back to
the field through an equivariant convolution. We show that other methods
treating features as the Fourier coefficients in the stabilizer subgroup are
special cases of our activation. Experiments on $SO(3)$ and $SE(3)$ show
state-of-the-art performance in spherical vector field regression, point cloud
classification, and molecular completion.",0,0,0,0,1,0,0.869669,7.0,0.871768,54
bad79eee-e631-44ca-85ab-152864f75af0,On Text-based Personality Computing: Challenges and Future Directions,3,0.159961,0.273428,"Text-based personality computing (TPC) has gained many research interests in
NLP. In this paper, we describe 15 challenges that we consider deserving the
attention of the research community. These challenges are organized by the
following topics: personality taxonomies, measurement quality, datasets,
performance evaluation, modelling choices, as well as ethics and fairness. When
addressing each challenge, not only do we combine perspectives from both NLP
and social sciences, but also offer concrete suggestions. We hope to inspire
more valid and reliable TPC research.",0,0,0,0,0,0,0.263821,11.0,0.746119,151
ce71dc41-ea60-412e-b33c-4e300ea6b7d8,Semi-Supervised Formality Style Transfer with Consistency Training,12,0.130388,0.750027,"Formality style transfer (FST) is a task that involves paraphrasing an
informal sentence into a formal one without altering its meaning. To address
the data-scarcity problem of existing parallel datasets, previous studies tend
to adopt a cycle-reconstruction scheme to utilize additional unlabeled data,
where the FST model mainly benefits from target-side unlabeled sentences. In
this work, we propose a simple yet effective semi-supervised framework to
better utilize source-side unlabeled sentences based on consistency training.
Specifically, our approach augments pseudo-parallel data obtained from a
source-side informal sentence by enforcing the model to generate similar
outputs for its perturbed version. Moreover, we empirically examined the
effects of various data perturbation methods and propose effective data
filtering strategies to improve our framework. Experimental results on the
GYAFC benchmark demonstrate that our approach can achieve state-of-the-art
results, even with less than 40% of the parallel data.",1,1,0,0,1,0,0.768217,6.0,0.795056,38
10b76c5f-33f4-458b-917e-49870b725704,SaiNet: Stereo aware inpainting behind objects with generative networks,1,0.0463467,0.0739639,"In this work, we present an end-to-end network for stereo-consistent image
inpainting with the objective of inpainting large missing regions behind
objects. The proposed model consists of an edge-guided UNet-like network using
Partial Convolutions. We enforce multi-view stereo consistency by introducing a
disparity loss. More importantly, we develop a training scheme where the model
is learned from realistic stereo masks representing object occlusions, instead
of the more common random masks. The technique is trained in a supervised way.
Our evaluation shows competitive results compared to previous state-of-the-art
techniques.",0,1,0,0,1,0,0.698283,14.0,0.89796,38
6acb851e-228a-4f6f-bd83-0329e78f29e6,On the expressive power of message-passing neural networks as global feature map transformers,5,0.0883166,0.376225,"We investigate the power of message-passing neural networks (MPNNs) in their
capacity to transform the numerical features stored in the nodes of their input
graphs. Our focus is on global expressive power, uniformly over all input
graphs, or over graphs of bounded degree with features from a bounded domain.
Accordingly, we introduce the notion of a global feature map transformer
(GFMT). As a yardstick for expressiveness, we use a basic language for GFMTs,
which we call MPLang. Every MPNN can be expressed in MPLang, and our results
clarify to which extent the converse inclusion holds. We consider exact versus
approximate expressiveness; the use of arbitrary activation functions; and the
case where only the ReLU activation function is allowed.",0,0,0,0,0,0,0.711486,7.0,0.801157,24
4a74699b-27e9-48ee-9479-6677369efae7,DA Wand: Distortion-Aware Selection using Neural Mesh Parameterization,2,0.0525868,0.109653,"We present a neural technique for learning to select a local sub-region
around a point which can be used for mesh parameterization. The motivation for
our framework is driven by interactive workflows used for decaling, texturing,
or painting on surfaces. Our key idea is to incorporate segmentation
probabilities as weights of a classical parameterization method, implemented as
a novel differentiable parameterization layer within a neural network
framework. We train a segmentation network to select 3D regions that are
parameterized into 2D and penalized by the resulting distortion, giving rise to
segmentations which are distortion-aware. Following training, a user can use
our system to interactively select a point on the mesh and obtain a large,
meaningful region around the selection which induces a low-distortion
parameterization. Our code and project page are currently available.",1,1,0,0,0,0,0.0170548,21.0,0.729897,82
a8fb3985-5f49-4312-a414-dd9ac3415967,Pay Attention to Your Tone: Introducing a New Dataset for Polite Language Rewrite,2,0.0325068,0.205569,"We introduce \textsc{PoliteRewrite} -- a dataset for polite language rewrite
which is a novel sentence rewrite task. Compared with previous text style
transfer tasks that can be mostly addressed by slight token- or phrase-level
edits, polite language rewrite requires deep understanding and extensive
sentence-level edits over an offensive and impolite sentence to deliver the
same message euphemistically and politely, which is more challenging -- not
only for NLP models but also for human annotators to rewrite with effort. To
alleviate the human effort for efficient annotation, we first propose a novel
annotation paradigm by a collaboration of human annotators and GPT-3.5 to
annotate \textsc{PoliteRewrite}. The released dataset has 10K polite sentence
rewrites annotated collaboratively by GPT-3.5 and human, which can be used as
gold standard for training, validation and test; and 100K high-quality polite
sentence rewrites by GPT-3.5 without human review. We wish this work (The
dataset (10K+100K) will be released soon) could contribute to the research on
more challenging sentence rewrite, and provoke more thought in future on
resource annotation paradigm with the help of the large-scaled pretrained
models.",0,1,1,1,0,0,0.907591,7.0,0.894043,9
2edb301a-fe85-4e62-950a-4c775dd21fa8,Personalized Game Difficulty Prediction Using Factorization Machines,2,0.0312273,0.228748,"The accurate and personalized estimation of task difficulty provides many
opportunities for optimizing user experience. However, user diversity makes
such difficulty estimation hard, in that empirical measurements from some user
sample do not necessarily generalize to others. In this paper, we contribute a
new approach for personalized difficulty estimation of game levels, borrowing
methods from content recommendation. Using factorization machines (FM) on a
large dataset from a commercial puzzle game, we are able to predict difficulty
as the number of attempts a player requires to pass future game levels, based
on observed attempt counts from earlier levels and levels played by others. In
addition to performance and scalability, FMs offer the benefit that the learned
latent variable model can be used to study the characteristics of both players
and game levels that contribute to difficulty. We compare the approach to a
simple non-personalized baseline and a personalized prediction using Random
Forests. Our results suggest that FMs are a promising tool enabling game
designers to both optimize player experience and learn more about their players
and the game.",0,1,0,1,0,0,0.0082174,11.0,0.417562,57
145cba63-22a0-4597-9afd-e8f7e24e9b9c,Defending Black-box Skeleton-based Human Activity Classifiers,6,0.130484,0.62166,"Skeletal motions have been heavily replied upon for human activity
recognition (HAR). Recently, a universal vulnerability of skeleton-based HAR
has been identified across a variety of classifiers and data, calling for
mitigation. To this end, we propose the first black-box defense method for
skeleton-based HAR to our best knowledge. Our method is featured by full
Bayesian treatments of the clean data, the adversaries and the classifier,
leading to (1) a new Bayesian Energy-based formulation of robust discriminative
classifiers, (2) a new adversary sampling scheme based on natural motion
manifolds, and (3) a new post-train Bayesian strategy for black-box defense. We
name our framework Bayesian Energy-based Adversarial Training or BEAT. BEAT is
straightforward but elegant, which turns vulnerable black-box classifiers into
robust ones without sacrificing accuracy. It demonstrates surprising and
universal effectiveness across a wide range of skeletal HAR classifiers and
datasets, under various attacks. Code is available at
https://github.com/realcrane/RobustActionRecogniser.",0,0,0,0,0,0,0.812657,6.0,0.81771,80
99ed037a-826b-465e-b721-2efea138021f,SuperCon: Supervised Contrastive Learning for Imbalanced Skin Lesion Classification,3,0.0697215,0.292919,"Convolutional neural networks (CNNs) have achieved great success in skin
lesion classification. A balanced dataset is required to train a good model.
However, due to the appearance of different skin lesions in practice, severe or
even deadliest skin lesion types (e.g., melanoma) naturally have quite small
amount represented in a dataset. In that, classification performance
degradation occurs widely, it is significantly important to have CNNs that work
well on class imbalanced skin lesion image dataset. In this paper, we propose
SuperCon, a two-stage training strategy to overcome the class imbalance problem
on skin lesion classification. It contains two stages: (i) representation
training that tries to learn a feature representation that closely aligned
among intra-classes and distantly apart from inter-classes, and (ii) classifier
fine-tuning that aims to learn a classifier that correctly predict the label
based on the learnt representations. In the experimental evaluation, extensive
comparisons have been made among our approach and other existing approaches on
skin lesion benchmark datasets. The results show that our two-stage training
strategy effectively addresses the class imbalance classification problem, and
significantly improves existing works in terms of F1-score and AUC score,
resulting in state-of-the-art performance.",1,1,0,0,1,0,0.769214,8.0,0.84666,37
85eef442-94dd-4418-8bde-1d7b52ee173d,CitySpec: An Intelligent Assistant System for Requirement Specification in Smart Cities,5,0.0279029,0.473595,"An increasing number of monitoring systems have been developed in smart
cities to ensure that real-time operations of a city satisfy safety and
performance requirements. However, many existing city requirements are written
in English with missing, inaccurate, or ambiguous information. There is a high
demand for assisting city policy makers in converting human-specified
requirements to machine-understandable formal specifications for monitoring
systems. To tackle this limitation, we build CitySpec, the first intelligent
assistant system for requirement specification in smart cities. To create
CitySpec, we first collect over 1,500 real-world city requirements across
different domains from over 100 cities and extract city-specific knowledge to
generate a dataset of city vocabulary with 3,061 words. We also build a
translation model and enhance it through requirement synthesis and develop a
novel online learning framework with validation under uncertainty. The
evaluation results on real-world city requirements show that CitySpec increases
the sentence-level accuracy of requirement specification from 59.02% to 86.64%,
and has strong adaptability to a new city and a new domain (e.g., F1 score for
requirements in Seattle increases from 77.6% to 93.75% with online learning).",0,1,0,1,0,0,0.383235,9.0,0.740375,18
4ff88da4-03b8-413e-884c-ffc44e69a82c,Decoupling Makes Weakly Supervised Local Feature Better,31,0.320163,0.90894,"Weakly supervised learning can help local feature methods to overcome the
obstacle of acquiring a large-scale dataset with densely labeled
correspondences. However, since weak supervision cannot distinguish the losses
caused by the detection and description steps, directly conducting weakly
supervised learning within a joint describe-then-detect pipeline suffers
limited performance. In this paper, we propose a decoupled describe-then-detect
pipeline tailored for weakly supervised local feature learning. Within our
pipeline, the detection step is decoupled from the description step and
postponed until discriminative and robust descriptors are learned. In addition,
we introduce a line-to-window search strategy to explicitly use the camera pose
information for better descriptor learning. Extensive experiments show that our
method, namely PoSFeat (Camera Pose Supervised Feature), outperforms previous
fully and weakly supervised methods and achieves state-of-the-art performance
on a wide range of downstream tasks.",1,0,0,0,0,0,0.669208,8.0,0.811444,60
e752f19e-ff90-4aa1-aaa8-4c8985a63d32,Optimal Activation Functions for the Random Features Regression Model,2,0.0,0.161977,"The asymptotic mean squared test error and sensitivity of the Random Features
Regression model (RFR) have been recently studied. We build on this work and
identify in closed-form the family of Activation Functions (AFs) that minimize
a combination of the test error and sensitivity of the RFR under different
notions of functional parsimony. We find scenarios under which the optimal AFs
are linear, saturated linear functions, or expressible in terms of Hermite
polynomials. Finally, we show how using optimal AFs impacts well-established
properties of the RFR model, such as its double descent curve, and the
dependency of its optimal regularization parameter on the observation noise
level.",1,0,0,0,0,0,0.407855,8.0,0.718038,80
4dd15a0b-d38a-4855-8d8d-9ae9e8885659,EnvEdit: Environment Editing for Vision-and-Language Navigation,50,0.377016,0.72194,"In Vision-and-Language Navigation (VLN), an agent needs to navigate through
the environment based on natural language instructions. Due to limited
available data for agent training and finite diversity in navigation
environments, it is challenging for the agent to generalize to new, unseen
environments. To address this problem, we propose EnvEdit, a data augmentation
method that creates new environments by editing existing environments, which
are used to train a more generalizable agent. Our augmented environments can
differ from the seen environments in three diverse aspects: style, object
appearance, and object classes. Training on these edit-augmented environments
prevents the agent from overfitting to existing environments and helps
generalize better to new, unseen environments. Empirically, on both the
Room-to-Room and the multi-lingual Room-Across-Room datasets, we show that our
proposed EnvEdit method gets significant improvements in all metrics on both
pre-trained and non-pre-trained VLN agents, and achieves the new
state-of-the-art on the test leaderboard. We further ensemble the VLN agents
augmented on different edited environments and show that these edit methods are
complementary. Code and data are available at
https://github.com/jialuli-luka/EnvEdit",1,1,0,1,1,0,0.537455,7.0,0.73293,81
36d2aeb0-87bd-4345-bd51-8556f0f1cf86,Dense Learning based Semi-Supervised Object Detection,35,0.540066,0.864572,"Semi-supervised object detection (SSOD) aims to facilitate the training and
deployment of object detectors with the help of a large amount of unlabeled
data. Though various self-training based and consistency-regularization based
SSOD methods have been proposed, most of them are anchor-based detectors,
ignoring the fact that in many real-world applications anchor-free detectors
are more demanded. In this paper, we intend to bridge this gap and propose a
DenSe Learning (DSL) based anchor-free SSOD algorithm. Specifically, we achieve
this goal by introducing several novel techniques, including an Adaptive
Filtering strategy for assigning multi-level and accurate dense pixel-wise
pseudo-labels, an Aggregated Teacher for producing stable and precise
pseudo-labels, and an uncertainty-consistency-regularization term among scales
and shuffled patches for improving the generalization capability of the
detector. Extensive experiments are conducted on MS-COCO and PASCAL-VOC, and
the results show that our proposed DSL method records new state-of-the-art SSOD
performance, surpassing existing methods by a large margin. Codes can be found
at \textcolor{blue}{https://github.com/chenbinghui1/DSL}.",1,1,0,0,1,0,0.922369,7.0,0.904132,52
dd6d5197-b3ae-4c3d-b46c-0fb78bd83592,PalGAN: Image Colorization with Palette Generative Adversarial Networks,13,0.0361106,0.595831,"Multimodal ambiguity and color bleeding remain challenging in colorization.
To tackle these problems, we propose a new GAN-based colorization approach
PalGAN, integrated with palette estimation and chromatic attention. To
circumvent the multimodality issue, we present a new colorization formulation
that estimates a probabilistic palette from the input gray image first, then
conducts color assignment conditioned on the palette through a generative
model. Further, we handle color bleeding with chromatic attention. It studies
color affinities by considering both semantic and intensity correlation. In
extensive experiments, PalGAN outperforms state-of-the-arts in quantitative
evaluation and visual comparison, delivering notable diverse, contrastive, and
edge-preserving appearances. With the palette design, our method enables color
transfer between images even with irrelevant contexts.",1,1,0,0,1,0,0.0755181,10.0,0.584618,58
cbad9711-4396-48c4-8e45-d2676f0d2eba,Adversarially-Aware Robust Object Detector,14,0.0602321,0.641784,"Object detection, as a fundamental computer vision task, has achieved a
remarkable progress with the emergence of deep neural networks. Nevertheless,
few works explore the adversarial robustness of object detectors to resist
adversarial attacks for practical applications in various real-world scenarios.
Detectors have been greatly challenged by unnoticeable perturbation, with sharp
performance drop on clean images and extremely poor performance on adversarial
images. In this work, we empirically explore the model training for adversarial
robustness in object detection, which greatly attributes to the conflict
between learning clean images and adversarial images. To mitigate this issue,
we propose a Robust Detector (RobustDet) based on adversarially-aware
convolution to disentangle gradients for model learning on clean and
adversarial images. RobustDet also employs the Adversarial Image Discriminator
(AID) and Consistent Features with Reconstruction (CFR) to ensure a reliable
robustness. Extensive experiments on PASCAL VOC and MS-COCO demonstrate that
our model effectively disentangles gradients and significantly enhances the
detection robustness with maintaining the detection ability on clean images.",1,1,0,0,1,0,0.54403,10.0,0.814891,38
4d53e077-f887-4400-8add-6c576c963278,ALTO: A Large-Scale Dataset for UAV Visual Place Recognition and Localization,9,0.0382027,0.812782,"We present the ALTO dataset, a vision-focused dataset for the development and
benchmarking of Visual Place Recognition and Localization methods for Unmanned
Aerial Vehicles. The dataset is composed of two long (approximately 150km and
260km) trajectories flown by a helicopter over Ohio and Pennsylvania, and it
includes high precision GPS-INS ground truth location data, high precision
accelerometer readings, laser altimeter readings, and RGB downward facing
camera imagery. In addition, we provide reference imagery over the flight
paths, which makes this dataset suitable for VPR benchmarking and other tasks
common in Localization, such as image registration and visual odometry. To the
author's knowledge, this is the largest real-world aerial-vehicle dataset of
this kind. Our dataset is available at https://github.com/MetaSLAM/ALTO.",1,1,1,1,0,0,0.000497266,19.0,0.51497,5
c6dd35e2-f7be-47f4-b41d-7e01a36b1f3d,Half Wavelet Attention on M-Net+ for Low-Light Image Enhancement,35,0.649534,0.895265,"Low-Light Image Enhancement is a computer vision task which intensifies the
dark images to appropriate brightness. It can also be seen as an ill-posed
problem in image restoration domain. With the success of deep neural networks,
the convolutional neural networks surpass the traditional algorithm-based
methods and become the mainstream in the computer vision area. To advance the
performance of enhancement algorithms, we propose an image enhancement network
(HWMNet) based on an improved hierarchical model: M-Net+. Specifically, we use
a half wavelet attention block on M-Net+ to enrich the features from wavelet
domain. Furthermore, our HWMNet has competitive performance results on two
image enhancement datasets in terms of quantitative metrics and visual quality.
The source code and pretrained model are available at
https://github.com/FanChiMao/HWMNet.",1,1,0,0,1,0,0.8586,6.0,0.843591,41
1dccaafe-f61c-4412-9663-c9f700adfa53,mGPT: Few-Shot Learners Go Multilingual,87,0.45172,0.847959,"Recent studies report that autoregressive language models can successfully
solve many NLP tasks via zero- and few-shot learning paradigms, which opens up
new possibilities for using the pre-trained language models. This paper
introduces two autoregressive GPT-like models with 1.3 billion and 13 billion
parameters trained on 60 languages from 25 language families using Wikipedia
and Colossal Clean Crawled Corpus. We reproduce the GPT-3 architecture using
GPT-2 sources and the sparse attention mechanism; Deepspeed and Megatron
frameworks allow us to parallelize the training and inference steps
effectively. The resulting models show performance on par with the recently
released XGLM models by Facebook, covering more languages and enhancing NLP
possibilities for low resource languages of CIS countries and Russian small
nations. We detail the motivation for the choices of the architecture design,
thoroughly describe the data preparation pipeline, and train five small
versions of the model to choose the most optimal multilingual tokenization
strategy. We measure the model perplexity in all covered languages and evaluate
it on the wide spectre of multilingual tasks, including classification,
generative, sequence labeling and knowledge probing. The models were evaluated
with the zero-shot and few-shot methods. Furthermore, we compared the
classification tasks with the state-of-the-art multilingual model XGLM. source
code and the mGPT XL model are publicly released.",0,1,0,0,0,0,0.773643,4.0,0.696603,115
00820106-30b4-4df3-8fab-8de49650486c,Incorporating Explicit Knowledge in Pre-trained Language Models for Passage Re-ranking,16,0.0542346,0.554433,"Passage re-ranking is to obtain a permutation over the candidate passage set
from retrieval stage. Re-rankers have been boomed by Pre-trained Language
Models (PLMs) due to their overwhelming advantages in natural language
understanding. However, existing PLM based re-rankers may easily suffer from
vocabulary mismatch and lack of domain specific knowledge. To alleviate these
problems, explicit knowledge contained in knowledge graph is carefully
introduced in our work. Specifically, we employ the existing knowledge graph
which is incomplete and noisy, and first apply it in passage re-ranking task.
To leverage a reliable knowledge, we propose a novel knowledge graph
distillation method and obtain a knowledge meta graph as the bridge between
query and passage. To align both kinds of embedding in the latent space, we
employ PLM as text encoder and graph neural network over knowledge meta graph
as knowledge encoder. Besides, a novel knowledge injector is designed for the
dynamic interaction between text and knowledge encoder. Experimental results
demonstrate the effectiveness of our method especially in queries requiring
in-depth domain knowledge.",1,1,0,0,0,0,0.40783,6.0,0.624037,53
170b1fb3-0b1d-406f-ab19-a7b46f0e856b,Rethinking Implicit Neural Representations for Vision Learners,5,0.0222409,0.252537,"Implicit Neural Representations (INRs) are powerful to parameterize
continuous signals in computer vision. However, almost all INRs methods are
limited to low-level tasks, e.g., image/video compression, super-resolution,
and image generation. The questions on how to explore INRs to high-level tasks
and deep networks are still under-explored. Existing INRs methods suffer from
two problems: 1) narrow theoretical definitions of INRs are inapplicable to
high-level tasks; 2) lack of representation capabilities to deep networks.
Motivated by the above facts, we reformulate the definitions of INRs from a
novel perspective and propose an innovative Implicit Neural Representation
Network (INRN), which is the first study of INRs to tackle both low-level and
high-level tasks. Specifically, we present three key designs for basic blocks
in INRN along with two different stacking ways and corresponding loss
functions. Extensive experiments with analysis on both low-level tasks (image
fitting) and high-level vision tasks (image classification, object detection,
instance segmentation) demonstrate the effectiveness of the proposed method.",0,0,0,0,0,0,0.720557,6.0,0.772246,26
ee601675-0aa5-44aa-9eb6-bf838d67a8ce,TextGrad: Advancing Robustness Evaluation in NLP by Gradient-Driven Optimization,9,0.0445343,0.591278,"Robustness evaluation against adversarial examples has become increasingly
important to unveil the trustworthiness of the prevailing deep models in
natural language processing (NLP). However, in contrast to the computer vision
domain where the first-order projected gradient descent (PGD) is used as the
benchmark approach to generate adversarial examples for robustness evaluation,
there lacks a principled first-order gradient-based robustness evaluation
framework in NLP. The emerging optimization challenges lie in 1) the discrete
nature of textual inputs together with the strong coupling between the
perturbation location and the actual content, and 2) the additional constraint
that the perturbed text should be fluent and achieve a low perplexity under a
language model. These challenges make the development of PGD-like NLP attacks
difficult. To bridge the gap, we propose TextGrad, a new attack generator using
gradient-driven optimization, supporting high-accuracy and high-quality
assessment of adversarial robustness in NLP. Specifically, we address the
aforementioned challenges in a unified optimization framework. And we develop
an effective convex relaxation method to co-optimize the continuously-relaxed
site selection and perturbation variables and leverage an effective sampling
method to establish an accurate mapping from the continuous optimization
variables to the discrete textual perturbations. Moreover, as a first-order
attack generation method, TextGrad can be baked into adversarial training to
further improve the robustness of NLP models. Extensive experiments are
provided to demonstrate the effectiveness of TextGrad not only in attack
generation for robustness evaluation but also in adversarial defense.",1,1,0,0,1,0,0.616703,8.0,0.79358,64
70f5806a-35af-427f-b03b-fe4168a4ff77,Egocentric Human-Object Interaction Detection Exploiting Synthetic Data,13,0.332866,0.762107,"We consider the problem of detecting Egocentric HumanObject Interactions
(EHOIs) in industrial contexts. Since collecting and labeling large amounts of
real images is challenging, we propose a pipeline and a tool to generate
photo-realistic synthetic First Person Vision (FPV) images automatically
labeled for EHOI detection in a specific industrial scenario. To tackle the
problem of EHOI detection, we propose a method that detects the hands, the
objects in the scene, and determines which objects are currently involved in an
interaction. We compare the performance of our method with a set of
state-of-the-art baselines. Results show that using a synthetic dataset
improves the performance of an EHOI detection system, especially when few real
data are available. To encourage research on this topic, we publicly release
the proposed dataset at the following url:
https://iplab.dmi.unict.it/EHOI_SYNTH/.",1,1,0,1,0,0,0.593744,9.0,0.809562,35
07b7568d-be8b-4eb6-9175-ac9c5a7b7618,The Shared Task on Gender Rewriting,1,0.019977,0.134897,"In this paper, we present the results and findings of the Shared Task on
Gender Rewriting, which was organized as part of the Seventh Arabic Natural
Language Processing Workshop. The task of gender rewriting refers to generating
alternatives of a given sentence to match different target user gender contexts
(e.g., female speaker with a male listener, a male speaker with a male
listener, etc.). This requires changing the grammatical gender (masculine or
feminine) of certain words referring to the users. In this task, we focus on
Arabic, a gender-marking morphologically rich language. A total of five teams
from four countries participated in the shared task.",1,1,1,1,0,0,0.788069,7.0,0.832826,37
9381c5ce-d97d-40f4-801c-c25647908de3,LightX3ECG: A Lightweight and eXplainable Deep Learning System for 3-lead Electrocardiogram Classification,15,0.38556,0.964725,"Cardiovascular diseases (CVDs) are a group of heart and blood vessel
disorders that is one of the most serious dangers to human health, and the
number of such patients is still growing. Early and accurate detection plays a
key role in successful treatment and intervention. Electrocardiogram (ECG) is
the gold standard for identifying a variety of cardiovascular abnormalities. In
clinical practices and most of the current research, standard 12-lead ECG is
mainly used. However, using a lower number of leads can make ECG more prevalent
as it can be conveniently recorded by portable or wearable devices. In this
research, we develop a novel deep learning system to accurately identify
multiple cardiovascular abnormalities by using only three ECG leads.",1,1,0,0,1,0,0.499561,9.0,0.780309,73
ae797824-6625-4716-9719-dcb94b68acb0,Simple and Effective Knowledge-Driven Query Expansion for QA-Based Product Attribute Extraction,10,0.140728,0.338282,"A key challenge in attribute value extraction (AVE) from e-commerce sites is
how to handle a large number of attributes for diverse products. Although this
challenge is partially addressed by a question answering (QA) approach which
finds a value in product data for a given query (attribute), it does not work
effectively for rare and ambiguous queries. We thus propose simple
knowledge-driven query expansion based on possible answers (values) of a query
(attribute) for QA-based AVE. We retrieve values of a query (attribute) from
the training data to expand the query. We train a model with two tricks,
knowledge dropout and knowledge token mixing, which mimic the imperfection of
the value knowledge in testing. Experimental results on our cleaned version of
AliExpress dataset show that our method improves the performance of AVE (+6.08
macro F1), especially for rare and ambiguous attributes (+7.82 and +6.86 macro
F1, respectively).",1,1,0,0,0,0,0.310998,7.0,0.62901,24
db974a13-9201-4bff-ac73-d528828ea891,Improving Rare Word Recognition with LM-aware MWER Training,11,0.086362,0.807174,"Language models (LMs) significantly improve the recognition accuracy of
end-to-end (E2E) models on words rarely seen during training, when used in
either the shallow fusion or the rescoring setups. In this work, we introduce
LMs in the learning of hybrid autoregressive transducer (HAT) models in the
discriminative training framework, to mitigate the training versus inference
gap regarding the use of LMs. For the shallow fusion setup, we use LMs during
both hypotheses generation and loss computation, and the LM-aware MWER-trained
model achieves 10\% relative improvement over the model trained with standard
MWER on voice search test sets containing rare words. For the rescoring setup,
we learn a small neural module to generate per-token fusion weights in a
data-dependent manner. This model achieves the same rescoring WER as regular
MWER-trained model, but without the need for sweeping fusion weights.",0,1,0,0,0,0,0.132941,9.0,0.604799,35
959d5829-3f25-4882-b99b-4ede307abe27,Indoor Localization for Personalized Ambient Assisted Living of Multiple Users in Multi-Floor Smart Environments,22,0.148757,0.916186,"This paper presents a multifunctional interdisciplinary framework that makes
four scientific contributions towards the development of personalized ambient
assisted living, with a specific focus to address the different and dynamic
needs of the diverse aging population in the future of smart living
environments. First, it presents a probabilistic reasoning-based mathematical
approach to model all possible forms of user interactions for any activity
arising from the user diversity of multiple users in such environments. Second,
it presents a system that uses this approach with a machine learning method to
model individual user profiles and user-specific user interactions for
detecting the dynamic indoor location of each specific user. Third, to address
the need to develop highly accurate indoor localization systems for increased
trust, reliance, and seamless user acceptance, the framework introduces a novel
methodology where two boosting approaches Gradient Boosting and the AdaBoost
algorithm are integrated and used on a decision tree-based learning model to
perform indoor localization. Fourth, the framework introduces two novel
functionalities to provide semantic context to indoor localization in terms of
detecting each user's floor-specific location as well as tracking whether a
specific user was located inside or outside a given spatial region in a
multi-floor-based indoor setting. These novel functionalities of the proposed
framework were tested on a dataset of localization-related Big Data collected
from 18 different users who navigated in 3 buildings consisting of 5 floors and
254 indoor spatial regions. The results show that this approach of indoor
localization for personalized AAL that models each specific user always
achieves higher accuracy as compared to the traditional approach of modeling an
average user.",0,1,0,0,0,0,0.0501086,7.0,0.346085,114
4bde831c-553f-4277-8dd6-d8c180282e75,Flexible Sampling for Long-tailed Skin Lesion Classification,5,0.0866045,0.359225,"Most of the medical tasks naturally exhibit a long-tailed distribution due to
the complex patient-level conditions and the existence of rare diseases.
Existing long-tailed learning methods usually treat each class equally to
re-balance the long-tailed distribution. However, considering that some
challenging classes may present diverse intra-class distributions, re-balancing
all classes equally may lead to a significant performance drop. To address
this, in this paper, we propose a curriculum learning-based framework called
Flexible Sampling for the long-tailed skin lesion classification task.
Specifically, we initially sample a subset of training data as anchor points
based on the individual class prototypes. Then, these anchor points are used to
pre-train an inference model to evaluate the per-class learning difficulty.
Finally, we use a curriculum sampling module to dynamically query new samples
from the rest training samples with the learning difficulty-aware sampling
probability. We evaluated our model against several state-of-the-art methods on
the ISIC dataset. The results with two long-tailed settings have demonstrated
the superiority of our proposed training strategy, which achieves a new
benchmark for long-tailed skin lesion classification.",0,1,0,0,1,0,0.937869,7.0,0.916068,22
8ab1eea7-5604-4936-a295-b88a3686e5bb,Comparing Feature Importance and Rule Extraction for Interpretability on Text Data,1,0.00998175,0.119221,"Complex machine learning algorithms are used more and more often in critical
tasks involving text data, leading to the development of interpretability
methods. Among local methods, two families have emerged: those computing
importance scores for each feature and those extracting simple logical rules.
In this paper we show that using different methods can lead to unexpectedly
different explanations, even when applied to simple models for which we would
expect qualitative coincidence. To quantify this effect, we propose a new
approach to compare explanations produced by different methods.",1,0,0,0,0,0,0.609888,9.0,0.814454,17
cb27a751-3100-4538-b06a-24c25866a60e,"On the sensitivity of pose estimation neural networks: rotation parameterizations, Lipschitz constants, and provable bounds",1,0.00312391,0.0543873,"In this paper, we approach the task of determining sensitivity bounds for
pose estimation neural networks. This task is particularly challenging as it
requires characterizing the sensitivity of 3D rotations. We develop a
sensitivity measure that describes the maximum rotational change in a network's
output with respect to a Euclidean change in its input. We show that this
measure is a type of Lipschitz constant, and that it is bounded by the product
of a network's Euclidean Lipschitz constant and an intrinsic property of a
rotation parameterization which we call the ""distance ratio constant"". We
derive the distance ratio constant for several rotation parameterizations, and
then discuss why the structure of most of these parameterizations makes it
difficult to construct a pose estimation network with provable sensitivity
bounds. However, we show that sensitivity bounds can be computed for networks
which parameterize rotation using unconstrained exponential coordinates. We
then construct and train such a network and compute sensitivity bounds for it.",1,0,0,0,0,0,0.258586,9.0,0.687101,20
5fac2e2d-0c81-4db1-a1b8-aeb3d43abcd0,Optimizing Camera Placements for Overlapped Coverage with 3D Camera Projections,2,0.118319,0.0716476,"This paper proposes a method to compute camera 6Dof poses to achieve a user
defined coverage. The camera placement problem is modeled as a combinatorial
optimization where given the maximum number of cameras, a camera set is
selected from a larger pool of possible camera poses. We propose to minimize
the squared error between the desired and the achieved coverage, and formulate
the non-linear cost function as a mixed integer linear programming problem. A
camera lens model is utilized to project the cameras view on a 3D voxel map to
compute a coverage score which makes the optimization problem in real
environments tractable. Experimental results in two real retail store
environments demonstrate the better performance of the proposed formulation in
terms of coverage and overlap for triangulation compared to existing methods.",0,1,0,0,0,0,0.00938171,21.0,0.701251,37
21a6cf2a-3583-4d39-9fd6-60af3b826126,Pair-Based Joint Encoding with Relational Graph Convolutional Networks for Emotion-Cause Pair Extraction,6,0.483915,0.868504,"Emotion-cause pair extraction (ECPE) aims to extract emotion clauses and
corresponding cause clauses, which have recently received growing attention.
Previous methods sequentially encode features with a specified order. They
first encode the emotion and cause features for clause extraction and then
combine them for pair extraction. This lead to an imbalance in inter-task
feature interaction where features extracted later have no direct contact with
the former. To address this issue, we propose a novel Pair-Based Joint Encoding
(PBJE) network, which generates pairs and clauses features simultaneously in a
joint feature encoding manner to model the causal relationship in clauses. PBJE
can balance the information flow among emotion clauses, cause clauses and
pairs. From a multi-relational perspective, we construct a heterogeneous
undirected graph and apply the Relational Graph Convolutional Network (RGCN) to
capture the various relationship between clauses and the relationship between
pairs and clauses. Experimental results show that PBJE achieves
state-of-the-art performance on the Chinese benchmark corpus.",1,1,0,0,1,0,0.981112,5.0,0.953827,39
3bc519cf-2ef9-4969-9c17-eba31097cac1,Semantic Similarity Computing Model Based on Multi Model Fine-Grained Nonlinear Fusion,19,0.0570696,0.764845,"Natural language processing (NLP) task has achieved excellent performance in
many fields, including semantic understanding, automatic summarization, image
recognition and so on. However, most of the neural network models for NLP
extract the text in a fine-grained way, which is not conducive to grasp the
meaning of the text from a global perspective. To alleviate the problem, the
combination of the traditional statistical method and deep learning model as
well as a novel model based on multi model nonlinear fusion are proposed in
this paper. The model uses the Jaccard coefficient based on part of speech,
Term Frequency-Inverse Document Frequency (TF-IDF) and word2vec-CNN algorithm
to measure the similarity of sentences respectively. According to the
calculation accuracy of each model, the normalized weight coefficient is
obtained and the calculation results are compared. The weighted vector is input
into the fully connected neural network to give the final classification
results. As a result, the statistical sentence similarity evaluation algorithm
reduces the granularity of feature extraction, so it can grasp the sentence
features globally. Experimental results show that the matching of sentence
similarity calculation method based on multi model nonlinear fusion is 84%, and
the F1 value of the model is 75%.",0,0,0,0,0,0,0.000932499,12.0,0.284449,48
2412262b-a9ca-4d47-a543-d441309dffd2,PseudoClick: Interactive Image Segmentation with Click Imitation,33,0.29936,0.77027,"The goal of click-based interactive image segmentation is to obtain precise
object segmentation masks with limited user interaction, i.e., by a minimal
number of user clicks. Existing methods require users to provide all the
clicks: by first inspecting the segmentation mask and then providing points on
mislabeled regions, iteratively. We ask the question: can our model directly
predict where to click, so as to further reduce the user interaction cost? To
this end, we propose {\PseudoClick}, a generic framework that enables existing
segmentation networks to propose candidate next clicks. These automatically
generated clicks, termed pseudo clicks in this work, serve as an imitation of
human clicks to refine the segmentation mask.",0,1,0,0,1,1,0.666934,9.0,0.831705,57
8e4e01e6-7c3d-43b6-8b76-acca6d67d383,Revisiting Checkpoint Averaging for Neural Machine Translation,9,0.0945414,0.364203,"Checkpoint averaging is a simple and effective method to boost the
performance of converged neural machine translation models. The calculation is
cheap to perform and the fact that the translation improvement almost comes for
free, makes it widely adopted in neural machine translation research. Despite
the popularity, the method itself simply takes the mean of the model parameters
from several checkpoints, the selection of which is mostly based on empirical
recipes without many justifications. In this work, we revisit the concept of
checkpoint averaging and consider several extensions. Specifically, we
experiment with ideas such as using different checkpoint selection strategies,
calculating weighted average instead of simple mean, making use of gradient
information and fine-tuning the interpolation weights on development data. Our
results confirm the necessity of applying checkpoint averaging for optimal
performance, but also suggest that the landscape between the converged
checkpoints is rather flat and not much further improvement compared to simple
averaging is to be obtained.",0,1,0,0,0,0,0.254069,7.0,0.594771,39
96e4bf7e-6e41-4455-8747-9d5e8026c4b0,SEMI-FND: Stacked Ensemble Based Multimodal Inference For Faster Fake News Detection,18,0.0354197,0.541593,"Fake News Detection (FND) is an essential field in natural language
processing that aims to identify and check the truthfulness of major claims in
a news article to decide the news veracity. FND finds its uses in preventing
social, political and national damage caused due to misrepresentation of facts
which may harm a certain section of society. Further, with the explosive rise
in fake news dissemination over social media, including images and text, it has
become imperative to identify fake news faster and more accurately. To solve
this problem, this work investigates a novel multimodal stacked ensemble-based
approach (SEMIFND) to fake news detection. Focus is also kept on ensuring
faster performance with fewer parameters. Moreover, to improve multimodal
performance, a deep unimodal analysis is done on the image modality to identify
NasNet Mobile as the most appropriate model for the task. For text, an ensemble
of BERT and ELECTRA is used. The approach was evaluated on two datasets:
Twitter MediaEval and Weibo Corpus. The suggested framework offered accuracies
of 85.80% and 86.83% on the Twitter and Weibo datasets respectively. These
reported metrics are found to be superior when compared to similar recent
works. Further, we also report a reduction in the number of parameters used in
training when compared to recent relevant works. SEMI-FND offers an overall
parameter reduction of at least 20% with unimodal parametric reduction on text
being 60%. Therefore, based on the investigations presented, it is concluded
that the application of a stacked ensembling significantly improves FND over
other approaches while also improving speed.",0,1,0,0,1,0,0.0468788,8.0,0.419285,52
9fff456f-6ef7-4fe4-b504-a48a40d0b381,Reducing the Vision and Language Bias for Temporal Sentence Grounding,32,0.361204,0.869615,"Temporal sentence grounding (TSG) is an important yet challenging task in
multimedia information retrieval. Although previous TSG methods have achieved
decent performance, they tend to capture the selection biases of frequently
appeared video-query pairs in the dataset rather than present robust multimodal
reasoning abilities, especially for the rarely appeared pairs. In this paper,
we study the above issue of selection biases and accordingly propose a
Debiasing-TSG (D-TSG) model to filter and remove the negative biases in both
vision and language modalities for enhancing the model generalization ability.
Specifically, we propose to alleviate the issue from two perspectives: 1)
Feature distillation. We built a multi-modal debiasing branch to firstly
capture the vision and language biases, and then apply a bias identification
module to explicitly recognize the true negative biases and remove them from
the benign multi-modal representations. 2) Contrastive sample generation. We
construct two types of negative samples to enforce the model to accurately
learn the aligned multi-modal semantics and make complete semantic reasoning.
We apply the proposed model to both commonly and rarely appeared TSG cases, and
demonstrate its effectiveness by achieving the state-of-the-art performance on
three benchmark datasets (ActivityNet Caption, TACoS, and Charades-STA).",0,1,0,0,1,0,0.842512,6.0,0.834148,51
8353c382-2611-417d-9977-59718ec56441,SAIBench: Benchmarking AI for Science,4,0.0,0.0204902,"Scientific research communities are embracing AI-based solutions to target
tractable scientific tasks and improve research workflows. However, the
development and evaluation of such solutions are scattered across multiple
disciplines. We formalize the problem of scientific AI benchmarking, and
propose a system called SAIBench in the hope of unifying the efforts and
enabling low-friction on-boarding of new disciplines. The system approaches
this goal with SAIL, a domain-specific language to decouple research problems,
AI models, ranking criteria, and software/hardware configuration into reusable
modules. We show that this approach is flexible and can adapt to problems, AI
models, and evaluation methods defined in different perspectives. The project
homepage is https://www.computercouncil.org/SAIBench",0,0,0,0,0,1,0.0282975,7.0,0.262843,37
6fbe035f-f920-48ee-a27e-a27233f9a110,DrugEHRQA: A Question Answering Dataset on Structured and Unstructured Electronic Health Records For Medicine Related Queries,7,0.107996,0.235378,"This paper develops the first question answering dataset (DrugEHRQA)
containing question-answer pairs from both structured tables and unstructured
notes from a publicly available Electronic Health Record (EHR). EHRs contain
patient records, stored in structured tables and unstructured clinical notes.
The information in structured and unstructured EHRs is not strictly disjoint:
information may be duplicated, contradictory, or provide additional context
between these sources. Our dataset has medication-related queries, containing
over 70,000 question-answer pairs. To provide a baseline model and help analyze
the dataset, we have used a simple model (MultimodalEHRQA) which uses the
predictions of a modality selection network to choose between EHR tables and
clinical notes to answer the questions. This is used to direct the questions to
the table-based or text-based state-of-the-art QA model. In order to address
the problem arising from complex, nested queries, this is the first time
Relation-Aware Schema Encoding and Linking for Text-to-SQL Parsers (RAT-SQL)
has been used to test the structure of query templates in EHR data. Our goal is
to provide a benchmark dataset for multi-modal QA systems, and to open up new
avenues of research in improving question answering over EHR structured data by
using context from unstructured clinical data.",0,1,1,1,0,0,0.529414,5.0,0.62158,30
ebcc82f1-c747-49c4-a2f8-3a24fa2ad85b,GTFLAT: Game Theory Based Add-On For Empowering Federated Learning Aggregation Techniques,1,0.00486186,0.0545831,"GTFLAT, as a game theory-based add-on, addresses an important research
question: How can a federated learning algorithm achieve better performance and
training efficiency by setting more effective adaptive weights for averaging in
the model aggregation phase? The main objectives for the ideal method of
answering the question are: (1) empowering federated learning algorithms to
reach better performance in fewer communication rounds, notably in the face of
heterogeneous scenarios, and last but not least, (2) being easy to use
alongside the state-of-the-art federated learning algorithms as a new module.
To this end, GTFLAT models the averaging task as a strategic game among active
users. Then it proposes a systematic solution based on the population game and
evolutionary dynamics to find the equilibrium. In contrast with existing
approaches that impose the weights on the participants, GTFLAT concludes a
self-enforcement agreement among clients in a way that none of them is
motivated to deviate from it individually. The results reveal that, on average,
using GTFLAT increases the top-1 test accuracy by 1.38%, while it needs 21.06%
fewer communication rounds to reach the accuracy.",0,1,0,0,0,0,0.365962,7.0,0.657789,38
20095cc6-89a4-4c41-b506-418a66e4eb41,Data-Driven Mitigation of Adversarial Text Perturbation,7,0.0379862,0.418859,"Social networks have become an indispensable part of our lives, with billions
of people producing ever-increasing amounts of text. At such scales, content
policies and their enforcement become paramount. To automate moderation,
questionable content is detected by Natural Language Processing (NLP)
classifiers. However, high-performance classifiers are hampered by misspellings
and adversarial text perturbations. In this paper, we classify intentional and
unintentional adversarial text perturbation into ten types and propose a
deobfuscation pipeline to make NLP models robust to such perturbations. We
propose Continuous Word2Vec (CW2V), our data-driven method to learn word
embeddings that ensures that perturbations of words have embeddings similar to
those of the original words. We show that CW2V embeddings are generally more
robust to text perturbations than embeddings based on character ngrams. Our
robust classification pipeline combines deobfuscation and classification, using
proposed defense methods and word embeddings to classify whether Facebook posts
are requesting engagement such as likes. Our pipeline results in engagement
bait classification that goes from 0.70 to 0.67 AUC with adversarial text
perturbation, while character ngram-based word embedding methods result in
downstream classification that goes from 0.76 to 0.64.",0,1,0,0,0,0,0.334627,7.0,0.641803,20
2b7a7d55-71ff-47be-85da-0f4aabdc1df1,Simple Open-Vocabulary Object Detection with Vision Transformers,197,0.844463,0.999984,"Combining simple architectures with large-scale pre-training has led to
massive improvements in image classification. For object detection,
pre-training and scaling approaches are less well established, especially in
the long-tailed and open-vocabulary setting, where training data is relatively
scarce. In this paper, we propose a strong recipe for transferring image-text
models to open-vocabulary object detection. We use a standard Vision
Transformer architecture with minimal modifications, contrastive image-text
pre-training, and end-to-end detection fine-tuning. Our analysis of the scaling
properties of this setup shows that increasing image-level pre-training and
model size yield consistent improvements on the downstream detection task. We
provide the adaptation strategies and regularizations needed to attain very
strong performance on zero-shot text-conditioned and one-shot image-conditioned
object detection. Code and models are available on GitHub.",1,1,0,0,0,0,0.954261,4.0,0.879262,48
a65d371d-e97c-40df-82f6-df7401b7bd85,Omnifont Persian OCR System Using Primitives,6,0.0970669,0.715982,"In this paper, we introduce a model-based omnifont Persian OCR system. The
system uses a set of 8 primitive elements as structural features for
recognition. First, the scanned document is preprocessed. After normalizing the
preprocessed image, text rows and sub-words are separated and then thinned.
After recognition of dots in sub-words, strokes are extracted and primitive
elements of each sub-word are recognized using the strokes. Finally, the
primitives are compared with a predefined set of character identification
vectors in order to identify sub-word characters. The separation and
recognition steps of the system are concurrent, eliminating unavoidable errors
of independent separation of letters. The system has been tested on documents
with 14 standard Persian fonts in 6 sizes. The achieved precision is 97.06%.",0,1,0,0,0,0,1.51558e-06,25.0,0.399634,7
5c7a6dde-7cee-42b1-8880-da338fe7e494,A Frequency-aware Software Cache for Large Recommendation System Embeddings,1,0.00280861,0.0550609,"Deep learning recommendation models (DLRMs) have been widely applied in
Internet companies. The embedding tables of DLRMs are too large to fit on GPU
memory entirely. We propose a GPU-based software cache approaches to
dynamically manage the embedding table in the CPU and GPU memory space by
leveraging the id's frequency statistics of the target dataset. Our proposed
software cache is efficient in training entire DLRMs on GPU in a synchronized
update manner. It is also scaled to multiple GPUs in combination with the
widely used hybrid parallel training approaches. Evaluating our prototype
system shows that we can keep only 1.5% of the embedding parameters in the GPU
to obtain a decent end-to-end training speed.",1,1,0,0,0,0,0.423521,3.0,0.264736,10
5dfc4f70-9ab6-4bfc-ae92-e7ba568ec48c,Content Addressable Memory Without Catastrophic Forgetting by Heteroassociation with a Fixed Scaffold,11,0.163046,0.379098,"Content-addressable memory (CAM) networks, so-called because stored items can
be recalled by partial or corrupted versions of the items, exhibit near-perfect
recall of a small number of information-dense patterns below capacity and a
'memory cliff' beyond, such that inserting a single additional pattern results
in catastrophic loss of all stored patterns. We propose a novel CAM
architecture, Memory Scaffold with Heteroassociation (MESH), that factorizes
the problems of internal attractor dynamics and association with external
content to generate a CAM continuum without a memory cliff: Small numbers of
patterns are stored with complete information recovery matching standard CAMs,
while inserting more patterns still results in partial recall of every pattern,
with a graceful trade-off between pattern number and pattern richness.
Motivated by the architecture of the Entorhinal-Hippocampal memory circuit in
the brain, MESH is a tripartite architecture with pairwise interactions that
uses a predetermined set of internally stabilized states together with
heteroassociation between the internal states and arbitrary external patterns.
We show analytically and experimentally that for any number of stored patterns,
MESH nearly saturates the total information bound (given by the number of
synapses) for CAM networks, outperforming all existing CAM models.",1,0,0,0,1,0,0.00116276,26.0,0.678238,49
14f1c326-12fa-460f-a695-8ab582f068d0,DFA: Dynamic Feature Aggregation for Efficient Video Object Detection,7,0.0379545,0.622218,"Video object detection is a fundamental yet challenging task in computer
vision. One practical solution is to take advantage of temporal information
from the video and apply feature aggregation to enhance the object features in
each frame. Though effective, those existing methods always suffer from low
inference speeds because they use a fixed number of frames for feature
aggregation regardless of the input frame. Therefore, this paper aims to
improve the inference speed of the current feature aggregation-based video
object detectors while maintaining their performance. To achieve this goal, we
propose a vanilla dynamic aggregation module that adaptively selects the frames
for feature enhancement. Then, we extend the vanilla dynamic aggregation module
to a more effective and reconfigurable deformable version. Finally, we
introduce inplace distillation loss to improve the representations of objects
aggregated with fewer frames. Extensive experimental results validate the
effectiveness and efficiency of our proposed methods: On the ImageNet VID
benchmark, integrated with our proposed methods, FGFA and SELSA can improve the
inference speed by 31% and 76% respectively while getting comparable
performance on accuracy.",0,1,0,0,0,0,0.235904,7.0,0.582535,55
17af170b-e16c-48a1-93aa-01a6848a7ca8,Multi-Scale Representation Learning on Proteins,66,0.333461,0.52746,"Proteins are fundamental biological entities mediating key roles in cellular
function and disease. This paper introduces a multi-scale graph construction of
a protein -- HoloProt -- connecting surface to structure and sequence. The
surface captures coarser details of the protein, while sequence as primary
component and structure -- comprising secondary and tertiary components --
capture finer details. Our graph encoder then learns a multi-scale
representation by allowing each level to integrate the encoding from level(s)
below with the graph at that level. We test the learned representation on
different tasks, (i.) ligand binding affinity (regression), and (ii.) protein
function prediction (classification). On the regression task, contrary to
previous methods, our model performs consistently and reliably across different
dataset splits, outperforming all baselines on most splits. On the
classification task, it achieves a performance close to the top-performing
model while using 10x fewer parameters. To improve the memory efficiency of our
construction, we segment the multiplex protein surface manifold into molecular
superpixels and substitute the surface with these superpixels at little to no
performance loss.",0,1,0,0,0,0,0.282529,11.0,0.753456,69
319f4eaf-20ff-4519-b65f-df954757e76a,Addressing the Challenges of Cross-Lingual Hate Speech Detection,3,0.204818,0.0877459,"The goal of hate speech detection is to filter negative online content aiming
at certain groups of people. Due to the easy accessibility of social media
platforms it is crucial to protect everyone which requires building hate speech
detection systems for a wide range of languages. However, the available labeled
hate speech datasets are limited making it problematic to build systems for
many languages. In this paper we focus on cross-lingual transfer learning to
support hate speech detection in low-resource languages. We leverage
cross-lingual word embeddings to train our neural network systems on the source
language and apply it to the target language, which lacks labeled examples, and
show that good performance can be achieved. We then incorporate unlabeled
target language data for further model improvements by bootstrapping labels
using an ensemble of different model architectures. Furthermore, we investigate
the issue of label imbalance of hate speech datasets, since the high ratio of
non-hate examples compared to hate examples often leads to low model
performance. We test simple data undersampling and oversampling techniques and
show their effectiveness.",0,1,0,0,0,0,0.964683,9.0,0.955283,36
082d3fac-442c-4868-9fc3-0ae7ed4212bf,RuArg-2022: Argument Mining Evaluation,8,0.157353,0.425209,"Argumentation analysis is a field of computational linguistics that studies
methods for extracting arguments from texts and the relationships between them,
as well as building argumentation structure of texts. This paper is a report of
the organizers on the first competition of argumentation analysis systems
dealing with Russian language texts within the framework of the Dialogue
conference. During the competition, the participants were offered two tasks:
stance detection and argument classification. A corpus containing 9,550
sentences (comments on social media posts) on three topics related to the
COVID-19 pandemic (vaccination, quarantine, and wearing masks) was prepared,
annotated, and used for training and testing. The system that won the first
place in both tasks used the NLI (Natural Language Inference) variant of the
BERT architecture, automatic translation into English to apply a specialized
BERT model, retrained on Twitter posts discussing COVID-19, as well as
additional masking of target entities. This system showed the following
results: for the stance detection task an F1-score of 0.6968, for the argument
classification task an F1-score of 0.7404. We hope that the prepared dataset
and baselines will help to foster further research on argument mining for the
Russian language.",0,1,0,1,0,0,0.352055,5.0,0.511147,48
9dfa3dff-882f-4a41-af22-79fb0cda62f7,Biological Robots: Perspectives on an Emerging Interdisciplinary Field,9,0.114827,0.70404,"Advances in science and engineering often reveal the limitations of classical
approaches initially used to understand, predict, and control phenomena. With
progress, conceptual categories must often be re-evaluated to better track
recently discovered invariants across disciplines. It is essential to refine
frameworks and resolve conflicting boundaries between disciplines such that
they better facilitate, not restrict, experimental approaches and capabilities.
In this essay, we discuss issues at the intersection of developmental biology,
computer science, and robotics. In the context of biological robots, we explore
changes across concepts and previously distinct fields that are driven by
recent advances in materials, information, and life sciences. Herein, each
author provides their own perspective on the subject, framed by their own
disciplinary training. We argue that as with computation, certain aspects of
developmental biology and robotics are not tied to specific materials; rather,
the consilience of these fields can help to shed light on issues of multi-scale
control, self-assembly, and relationships between form and function. We hope
new fields can emerge as boundaries arising from technological limitations are
overcome, furthering practical applications from regenerative medicine to
useful synthetic living machines.",0,0,0,0,0,0,0.0829268,8.0,0.492967,121
3ab25628-30f3-4bac-98dd-f44836cf2e1a,Eliciting and Understanding Cross-Task Skills with Task-Level Mixture-of-Experts,4,0.347164,0.150036,"Recent works suggest that transformer models are capable of multi-tasking on
diverse NLP tasks and adapting to new tasks efficiently. However, the potential
of these multi-task models may be limited as they use the same set of
parameters for all tasks. In contrast, humans tackle tasks in a more flexible
way, by making proper presumptions on what skills and knowledge are relevant
and executing only the necessary computations. Inspired by this, we propose to
use task-level mixture-of-expert models, which has a collection of transformer
layers (i.e., experts) and a router component that chooses from these experts
dynamically and flexibly. We find that these models help improve the average
performance gain (ARG) metric by 2.6% when adapting to unseen tasks in the
few-shot setting and by 5.6% in the zero-shot generalization setting. Further,
we show that the learned routing decisions partly rediscover human
categorization of NLP tasks -- certain experts are strongly associated with
extractive tasks, some with classification tasks, and some with tasks requiring
world knowledge.",1,0,0,0,0,0,0.984205,7.0,0.973314,166
abebe689-a055-49fe-a298-9930f60ba44f,CAKE: A Scalable Commonsense-Aware Framework For Multi-View Knowledge Graph Completion,25,0.451533,0.784557,"Knowledge graphs store a large number of factual triples while they are still
incomplete, inevitably. The previous knowledge graph completion (KGC) models
predict missing links between entities merely relying on fact-view data,
ignoring the valuable commonsense knowledge. The previous knowledge graph
embedding (KGE) techniques suffer from invalid negative sampling and the
uncertainty of fact-view link prediction, limiting KGC's performance. To
address the above challenges, we propose a novel and scalable Commonsense-Aware
Knowledge Embedding (CAKE) framework to automatically extract commonsense from
factual triples with entity concepts. The generated commonsense augments
effective self-supervision to facilitate both high-quality negative sampling
(NS) and joint commonsense and fact-view link prediction. Experimental results
on the KGC task demonstrate that assembling our framework could enhance the
performance of the original KGE models, and the proposed commonsense-aware NS
module is superior to other NS techniques. Besides, our proposed framework
could be easily adaptive to various KGE models and explain the predicted
results.",1,0,0,0,0,0,0.922304,8.0,0.916075,39
2704a77f-cd75-418b-9534-849ac59abdc1,Selective Residual M-Net for Real Image Denoising,12,0.0419388,0.36953,"Image restoration is a low-level vision task which is to restore degraded
images to noise-free images. With the success of deep neural networks, the
convolutional neural networks surpass the traditional restoration methods and
become the mainstream in the computer vision area. To advance the performanceof
denoising algorithms, we propose a blind real image denoising network (SRMNet)
by employing a hierarchical architecture improved from U-Net. Specifically, we
use a selective kernel with residual block on the hierarchical structure called
M-Net to enrich the multi-scale semantic information. Furthermore, our SRMNet
has competitive performance results on two synthetic and two real-world noisy
datasets in terms of quantitative metrics and visual quality. The source code
and pretrained model are available at
https://github.com/TentativeGitHub/SRMNet.",1,1,0,0,1,0,0.183747,8.0,0.599521,57
356994ab-e8ba-4e5f-8425-499d65b595de,Equivariant Self-Supervision for Musical Tempo Estimation,5,0.146046,0.905989,"Self-supervised methods have emerged as a promising avenue for representation
learning in the recent years since they alleviate the need for labeled
datasets, which are scarce and expensive to acquire. Contrastive methods are a
popular choice for self-supervision in the audio domain, and typically provide
a learning signal by forcing the model to be invariant to some transformations
of the input. These methods, however, require measures such as negative
sampling or some form of regularisation to be taken to prevent the model from
collapsing on trivial solutions. In this work, instead of invariance, we
propose to use equivariance as a self-supervision signal to learn audio tempo
representations from unlabelled data. We derive a simple loss function that
prevents the network from collapsing on a trivial solution during training,
without requiring any form of regularisation or negative sampling. Our
experiments show that it is possible to learn meaningful representations for
tempo estimation by solely relying on equivariant self-supervision, achieving
performance comparable with supervised methods on several benchmarks. As an
added benefit, our method only requires moderate compute resources and
therefore remains accessible to a wide research community.",0,0,0,0,0,0,0.872068,5.0,0.82229,52
920e2aff-984b-4708-b506-3fedd5ad223a,Question rewriting? Assessing its importance for conversational question answering,16,0.374472,0.311409,"In conversational question answering, systems must correctly interpret the
interconnected interactions and generate knowledgeable answers, which may
require the retrieval of relevant information from a background repository.
Recent approaches to this problem leverage neural language models, although
different alternatives can be considered in terms of modules for (a)
representing user questions in context, (b) retrieving the relevant background
information, and (c) generating the answer. This work presents a conversational
question answering system designed specifically for the Search-Oriented
Conversational AI (SCAI) shared task, and reports on a detailed analysis of its
question rewriting module. In particular, we considered different variations of
the question rewriting module to evaluate the influence on the subsequent
components, and performed a careful analysis of the results obtained with the
best system configuration. Our system achieved the best performance in the
shared task and our analysis emphasizes the importance of the conversation
context representation for the overall system performance.",0,1,0,0,1,0,0.983241,5.0,0.959762,13
c5cc417f-6a84-45cc-b366-3e58bd6f2c32,Deep Generalized Unfolding Networks for Image Restoration,104,0.318615,0.998185,"Deep neural networks (DNN) have achieved great success in image restoration.
However, most DNN methods are designed as a black box, lacking transparency and
interpretability. Although some methods are proposed to combine traditional
optimization algorithms with DNN, they usually demand pre-defined degradation
processes or handcrafted assumptions, making it difficult to deal with complex
and real-world applications. In this paper, we propose a Deep Generalized
Unfolding Network (DGUNet) for image restoration. Concretely, without loss of
interpretability, we integrate a gradient estimation strategy into the gradient
descent step of the Proximal Gradient Descent (PGD) algorithm, driving it to
deal with complex and real-world image degradation. In addition, we design
inter-stage information pathways across proximal mapping in different PGD
iterations to rectify the intrinsic information loss in most deep unfolding
networks (DUN) through a multi-scale and spatial-adaptive way. By integrating
the flexible gradient descent and informative proximal mapping, we unfold the
iterative PGD algorithm into a trainable DNN. Extensive experiments on various
image restoration tasks demonstrate the superiority of our method in terms of
state-of-the-art performance, interpretability, and generalizability. The
source code is available at
https://github.com/MC-E/Deep-Generalized-Unfolding-Networks-for-Image-Restoration.",1,1,0,0,1,1,0.314033,10.0,0.741485,101
77ce9502-4cd2-42c0-be8f-ab7898584846,Jury Learning: Integrating Dissenting Voices into Machine Learning Models,99,0.0654538,0.80373,"Whose labels should a machine learning (ML) algorithm learn to emulate? For
ML tasks ranging from online comment toxicity to misinformation detection to
medical diagnosis, different groups in society may have irreconcilable
disagreements about ground truth labels. Supervised ML today resolves these
label disagreements implicitly using majority vote, which overrides minority
groups' labels. We introduce jury learning, a supervised ML approach that
resolves these disagreements explicitly through the metaphor of a jury:
defining which people or groups, in what proportion, determine the classifier's
prediction. For example, a jury learning model for online toxicity might
centrally feature women and Black jurors, who are commonly targets of online
harassment. To enable jury learning, we contribute a deep learning architecture
that models every annotator in a dataset, samples from annotators' models to
populate the jury, then runs inference to classify. Our architecture enables
juries that dynamically adapt their composition, explore counterfactuals, and
visualize dissent.",0,0,1,0,0,0,0.0191189,7.0,0.20616,97
cd82e0ff-23fb-4936-9618-0d3abb2907aa,Visual-tactile Fusion for Transparent Object Grasping in Complex Backgrounds,2,0.0680999,0.438341,"The accurate detection and grasping of transparent objects are challenging
but of significance to robots. Here, a visual-tactile fusion framework for
transparent object grasping under complex backgrounds and variant light
conditions is proposed, including the grasping position detection, tactile
calibration, and visual-tactile fusion based classification. First, a
multi-scene synthetic grasping dataset generation method with a Gaussian
distribution based data annotation is proposed. Besides, a novel grasping
network named TGCNN is proposed for grasping position detection, showing good
results in both synthetic and real scenes. In tactile calibration, inspired by
human grasping, a fully convolutional network based tactile feature extraction
method and a central location based adaptive grasping strategy are designed,
improving the success rate by 36.7% compared to direct grasping. Furthermore, a
visual-tactile fusion method is proposed for transparent objects
classification, which improves the classification accuracy by 34%. The proposed
framework synergizes the advantages of vision and touch, and greatly improves
the grasping efficiency of transparent objects.",0,1,0,1,0,0,0.0664934,10.0,0.571412,67
7efab64b-17c8-4be3-894c-83b9896a3df2,On the focusing of thermal images,26,0.263365,0.426022,"In this paper we present a new thermographic image database suitable for the
analysis of automatic focus measures. This database consists of 8 different
sets of scenes, where each scene contains one image for 96 different focus
positions. Using this database we evaluate the usefulness of six focus measures
with the goal to determine the optimal focus position. Experimental results
reveal that an accurate automatic detection of optimal focus position is
possible, even with a low computational burden. We also present an acquisition
tool able to help the acquisition of thermal images. To the best of our
knowledge, this is the first study about automatic focus of thermal images.",0,1,1,1,0,0,0.0685561,11.0,0.613251,14
d4e3d63e-a340-42b2-9b8b-a3ceceb05a11,"When a sentence does not introduce a discourse entity, Transformer-based models still sometimes refer to it",22,0.273183,0.5956,"Understanding longer narratives or participating in conversations requires
tracking of discourse entities that have been mentioned. Indefinite noun
phrases (NPs), such as 'a dog', frequently introduce discourse entities but
this behavior is modulated by sentential operators such as negation. For
example, 'a dog' in 'Arthur doesn't own a dog' does not introduce a discourse
entity due to the presence of negation. In this work, we adapt the
psycholinguistic assessment of language models paradigm to higher-level
linguistic phenomena and introduce an English evaluation suite that targets the
knowledge of the interactions between sentential operators and indefinite NPs.
We use this evaluation suite for a fine-grained investigation of the entity
tracking abilities of the Transformer-based models GPT-2 and GPT-3. We find
that while the models are to a certain extent sensitive to the interactions we
investigate, they are all challenged by the presence of multiple NPs and their
behavior is not systematic, which suggests that even models at the scale of
GPT-3 do not fully acquire basic entity tracking abilities.",1,0,0,0,0,0,0.728932,6.0,0.776177,42
454a3b4a-eeb3-4577-8762-4e2200001e9e,Overview of the WANLP 2022 Shared Task on Propaganda Detection in Arabic,37,0.620823,0.531621,"Propaganda is the expression of an opinion or an action by an individual or a
group deliberately designed to influence the opinions or the actions of other
individuals or groups with reference to predetermined ends, which is achieved
by means of well-defined rhetorical and psychological devices. Propaganda
techniques are commonly used in social media to manipulate or to mislead users.
Thus, there has been a lot of recent research on automatic detection of
propaganda techniques in text as well as in memes. However, so far the focus
has been primarily on English. With the aim to bridge this language gap, we ran
a shared task on detecting propaganda techniques in Arabic tweets as part of
the WANLP 2022 workshop, which included two subtasks. Subtask~1 asks to
identify the set of propaganda techniques used in a tweet, which is a
multilabel classification problem, while Subtask~2 asks to detect the
propaganda techniques used in a tweet together with the exact span(s) of text
in which each propaganda technique appears. The task attracted 63 team
registrations, and eventually 14 and 3 teams made submissions for subtask 1 and
2, respectively. Finally, 11 teams submitted system description papers.",0,1,0,0,0,0,0.523037,5.0,0.617977,64
83008a05-3ca4-4c63-94b7-384d5bc47094,Why is Winoground Hard? Investigating Failures in Visuolinguistic Compositionality,30,0.0896726,0.903804,"Recent visuolinguistic pre-trained models show promising progress on various
end tasks such as image retrieval and video captioning. Yet, they fail
miserably on the recently proposed Winoground dataset, which challenges models
to match paired images and English captions, with items constructed to overlap
lexically but differ in meaning (e.g., ""there is a mug in some grass"" vs.
""there is some grass in a mug""). By annotating the dataset using new
fine-grained tags, we show that solving the Winoground task requires not just
compositional language understanding, but a host of other abilities like
commonsense reasoning or locating small, out-of-focus objects in low-resolution
images. In this paper, we identify the dataset's main challenges through a
suite of experiments on related tasks (probing task, image retrieval task),
data augmentation, and manual inspection of the dataset. Our analysis suggests
that a main challenge in visuolinguistic models may lie in fusing visual and
textual representations, rather than in compositional language understanding.
We release our annotation and code at
https://github.com/ajd12342/why-winoground-hard .",1,0,0,0,0,0,0.302426,5.0,0.473864,48
ef579802-ee26-429a-a4e3-dd88d51461cc,Toward Robust Spiking Neural Network Against Adversarial Perturbation,6,0.0625324,0.408406,"As spiking neural networks (SNNs) are deployed increasingly in real-world
efficiency critical applications, the security concerns in SNNs attract more
attention. Currently, researchers have already demonstrated an SNN can be
attacked with adversarial examples. How to build a robust SNN becomes an urgent
issue. Recently, many studies apply certified training in artificial neural
networks (ANNs), which can improve the robustness of an NN model promisely.
However, existing certifications cannot transfer to SNNs directly because of
the distinct neuron behavior and input formats for SNNs. In this work, we first
design S-IBP and S-CROWN that tackle the non-linear functions in SNNs' neuron
modeling. Then, we formalize the boundaries for both digital and spike inputs.
Finally, we demonstrate the efficiency of our proposed robust training method
in different datasets and model architectures. Based on our experiment, we can
achieve a maximum $37.7\%$ attack error reduction with $3.7\%$ original
accuracy loss. To the best of our knowledge, this is the first analysis on
robust training of SNNs.",0,0,1,0,0,0,0.451971,6.0,0.647018,43
33d20390-66f7-4265-a385-25cb04f31527,RestoreFormer: High-Quality Blind Face Restoration from Undegraded Key-Value Pairs,65,0.643821,0.855099,"Blind face restoration is to recover a high-quality face image from unknown
degradations. As face image contains abundant contextual information, we
propose a method, RestoreFormer, which explores fully-spatial attentions to
model contextual information and surpasses existing works that use local
operators. RestoreFormer has several benefits compared to prior arts. First,
unlike the conventional multi-head self-attention in previous Vision
Transformers (ViTs), RestoreFormer incorporates a multi-head cross-attention
layer to learn fully-spatial interactions between corrupted queries and
high-quality key-value pairs. Second, the key-value pairs in ResotreFormer are
sampled from a reconstruction-oriented high-quality dictionary, whose elements
are rich in high-quality facial features specifically aimed for face
reconstruction, leading to superior restoration results. Third, RestoreFormer
outperforms advanced state-of-the-art methods on one synthetic dataset and
three real-world datasets, as well as produces images with better visual
quality.",1,1,0,1,1,0,0.930248,7.0,0.909992,48
182b66c6-e1ac-4ea4-ad2f-a2a5fcf74658,JAMES: Normalizing Job Titles with Multi-Aspect Graph Embeddings and Reasoning,1,0.0325735,0.272866,"In online job marketplaces, it is important to establish a well-defined job
title taxonomy for various downstream tasks (e.g., job recommendation, users'
career analysis, and turnover prediction). Job Title Normalization (JTN) is
such a cleaning step to classify user-created non-standard job titles into
normalized ones. However, solving the JTN problem is non-trivial with
challenges: (1) semantic similarity of different job titles, (2) non-normalized
user-created job titles, and (3) large-scale and long-tailed job titles in
real-world applications. To this end, we propose a novel solution, named JAMES,
that constructs three unique embeddings (i.e., graph, contextual, and
syntactic) of a target job title to effectively capture its various traits. We
further propose a multi-aspect co-attention mechanism to attentively combine
these embeddings, and employ neural logical reasoning representations to
collaboratively estimate similarities between messy job titles and normalized
job titles in a reasoning space. To evaluate JAMES, we conduct comprehensive
experiments against ten competing models on a large-scale real-world dataset
with over 350,000 job titles. Our experimental results show that JAMES
significantly outperforms the best baseline by 10.06% in Precision@10 and by
17.52% in NDCG@10, respectively.",1,1,0,0,1,0,0.672442,7.0,0.78577,36
43d228de-caba-4b5d-8d57-5e03b4413e88,A Low-Cost Lane-Following Algorithm for Cyber-Physical Robots,1,0.0259705,0.0606273,"Duckiebots are low-cost mobile robots that are widely used in the fields of
research and education. Although there are existing self-driving algorithms for
the Duckietown platform, they are either too complex or perform too poorly to
navigate a multi-lane track. Moreover, it is essential to give memory and
computational resources to a Duckiebot so it can perform additional tasks such
as out-of-distribution input detection. In order to satisfy these constraints,
we built a low-cost autonomous driving algorithm capable of driving on a
two-lane track. The algorithm uses traditional computer vision techniques to
identify the central lane on the track and obtain the relevant steering angle.
The steering is then controlled by a PID controller that smoothens the movement
of the Duckiebot. The performance of the algorithm was compared to that of the
NeurIPS 2018 AI Driving Olympics (AIDO) finalists, and it outperformed all but
one finalists. The two main contributions of our algorithm are its low
computational requirements and very quick set-up, with ongoing efforts to make
it more reliable.",0,1,0,0,0,0,0.0989268,10.0,0.612882,9
c585244e-26b7-4e7b-a1f7-9e0942604b1e,Contrastive Language-Image Pre-Training with Knowledge Graphs,19,0.155102,0.584548,"Recent years have witnessed the fast development of large-scale pre-training
frameworks that can extract multi-modal representations in a unified form and
achieve promising performances when transferred to downstream tasks.
Nevertheless, existing approaches mainly focus on pre-training with simple
image-text pairs, while neglecting the semantic connections between concepts
from different modalities. In this paper, we propose a knowledge-based
pre-training framework, dubbed Knowledge-CLIP, which injects semantic
information into the widely used CLIP model. Through introducing
knowledge-based objectives in the pre-training process and utilizing different
types of knowledge graphs as training data, our model can semantically align
the representations in vision and language with higher quality, and enhance the
reasoning ability across scenarios and modalities. Extensive experiments on
various vision-language downstream tasks demonstrate the effectiveness of
Knowledge-CLIP compared with the original CLIP and competitive baselines.",0,1,0,0,0,0,0.713846,7.0,0.802098,82
a8cacc4b-960d-408d-8b9c-f71284b75266,Semantic Novelty Detection and Characterization in Factual Text Involving Named Entities,1,0.00668567,0.0613487,"Much of the existing work on text novelty detection has been studied at the
topic level, i.e., identifying whether the topic of a document or a sentence is
novel or not. Little work has been done at the fine-grained semantic level (or
contextual level). For example, given that we know Elon Musk is the CEO of a
technology company, the sentence ""Elon Musk acted in the sitcom The Big Bang
Theory"" is novel and surprising because normally a CEO would not be an actor.
Existing topic-based novelty detection methods work poorly on this problem
because they do not perform semantic reasoning involving relations between
named entities in the text and their background knowledge. This paper proposes
an effective model (called PAT-SND) to solve the problem, which can also
characterize the novelty. An annotated dataset is also created. Evaluation
shows that PAT-SND outperforms 10 baselines by large margins.",0,0,1,1,1,0,0.0302329,11.0,0.537004,104
08e5935d-feee-460a-a610-5961b9c15115,Mono-surrogate vs Multi-surrogate in Multi-objective Bayesian Optimisation,3,0.073055,0.115205,"Bayesian optimisation (BO) has been widely used to solve problems with
expensive function evaluations. In multi-objective optimisation problems, BO
aims to find a set of approximated Pareto optimal solutions. There are
typically two ways to build surrogates in multi-objective BO: One surrogate by
aggregating objective functions (by using a scalarising function, also called
mono-surrogate approach) and multiple surrogates (for each objective function,
also called multi-surrogate approach). In both approaches, an acquisition
function (AF) is used to guide the search process. Mono-surrogate has the
advantage that only one model is used, however, the approach has two major
limitations. Firstly, the fitness landscape of the scalarising function and the
objective functions may not be similar. Secondly, the approach assumes that the
scalarising function distribution is Gaussian, and thus a closed-form
expression of the AF can be used. In this work, we overcome these limitations
by building a surrogate model for each objective function and show that the
scalarising function distribution is not Gaussian. We approximate the
distribution using Generalised extreme value distribution. The results and
comparison with existing approaches on standard benchmark and real-world
optimisation problems show the potential of the multi-surrogate approach.",0,1,0,0,0,0,0.153844,14.0,0.757222,30
8ee27c4e-a807-488f-be45-9564a64bc04b,Unsupervised detection of ash dieback disease (Hymenoscyphus fraxineus) using diffusion-based hyperspectral image clustering,4,0.130537,0.293011,"Ash dieback (Hymenoscyphus fraxineus) is an introduced fungal disease that is
causing the widespread death of ash trees across Europe. Remote sensing
hyperspectral images encode rich structure that has been exploited for the
detection of dieback disease in ash trees using supervised machine learning
techniques. However, to understand the state of forest health at
landscape-scale, accurate unsupervised approaches are needed. This article
investigates the use of the unsupervised Diffusion and VCA-Assisted Image
Segmentation (D-VIS) clustering algorithm for the detection of ash dieback
disease in a forest site near Cambridge, United Kingdom. The unsupervised
clustering presented in this work has high overlap with the supervised
classification of previous work on this scene (overall accuracy = 71%). Thus,
unsupervised learning may be used for the remote detection of ash dieback
disease without the need for expert labeling.",0,1,0,0,0,0,0.207403,11.0,0.72104,21
133654ac-2463-4740-84cd-0fc280455540,Modern Machine-Learning Predictive Models for Diagnosing Infectious Diseases,10,0.368581,0.449469,"Controlling infectious diseases is a major health priority because they can
spread and infect humans, thus evolving into epidemics or pandemics. Therefore,
early detection of infectious diseases is a significant need, and many
researchers have developed models to diagnose them in the early stages. This
paper reviewed research articles for recent machine-learning (ML) algorithms
applied to infectious disease diagnosis. We searched the Web of Science,
ScienceDirect, PubMed, Springer, and IEEE databases from 2015 to 2022,
identified the pros and cons of the reviewed ML models, and discussed the
possible recommendations to advance the studies in this field. We found that
most of the articles used small datasets, and few of them used real-time data.
Our results demonstrated that a suitable ML technique depends on the nature of
the dataset and the desired goal.",0,1,0,0,0,0,0.769903,6.0,0.795886,54
6010196d-b335-489e-89de-31a62eb66bb1,Dungeons and Dragons as a Dialog Challenge for Artificial Intelligence,25,0.201252,0.961656,"AI researchers have posited Dungeons and Dragons (D&D) as a challenge problem
to test systems on various language-related capabilities. In this paper, we
frame D&D specifically as a dialogue system challenge, where the tasks are to
both generate the next conversational turn in the game and predict the state of
the game given the dialogue history. We create a gameplay dataset consisting of
nearly 900 games, with a total of 7,000 players, 800,000 dialogue turns,
500,000 dice rolls, and 58 million words. We automatically annotate the data
with partial state information about the game play. We train a large language
model (LM) to generate the next game turn, conditioning it on different
information. The LM can respond as a particular character or as the player who
runs the game--i.e., the Dungeon Master (DM). It is trained to produce dialogue
that is either in-character (roleplaying in the fictional world) or
out-of-character (discussing rules or strategy). We perform a human evaluation
to determine what factors make the generated output plausible and interesting.
We further perform an automatic evaluation to determine how well the model can
predict the game state given the history and examine how well tracking the game
state improves its ability to produce plausible conversational output.",0,1,0,1,0,0,0.127509,7.0,0.485488,47
3772b3d3-de32-496a-a930-5159d496217c,CNSNet: A Cleanness-Navigated-Shadow Network for Shadow Removal,6,0.240463,0.520938,"The key to shadow removal is recovering the contents of the shadow regions
with the guidance of the non-shadow regions. Due to the inadequate long-range
modeling, the CNN-based approaches cannot thoroughly investigate the
information from the non-shadow regions. To solve this problem, we propose a
novel cleanness-navigated-shadow network (CNSNet), with a shadow-oriented
adaptive normalization (SOAN) module and a shadow-aware aggregation with
transformer (SAAT) module based on the shadow mask. Under the guidance of the
shadow mask, the SOAN module formulates the statistics from the non-shadow
region and adaptively applies them to the shadow region for region-wise
restoration. The SAAT module utilizes the shadow mask to precisely guide the
restoration of each shadowed pixel by considering the highly relevant pixels
from the shadow-free regions for global pixel-wise restoration. Extensive
experiments on three benchmark datasets (ISTD, ISTD+, and SRD) show that our
method achieves superior de-shadowing performance.",0,1,0,0,1,0,0.940727,7.0,0.918469,54
fbfa6ab6-569e-4db2-ac01-50f6bdcd4a00,RepMix: Representation Mixing for Robust Attribution of Synthesized Images,22,0.0705943,0.779171,"Rapid advances in Generative Adversarial Networks (GANs) raise new challenges
for image attribution; detecting whether an image is synthetic and, if so,
determining which GAN architecture created it. Uniquely, we present a solution
to this task capable of 1) matching images invariant to their semantic content;
2) robust to benign transformations (changes in quality, resolution, shape,
etc.) commonly encountered as images are re-shared online. In order to
formalize our research, a challenging benchmark, Attribution88, is collected
for robust and practical image attribution. We then propose RepMix, our GAN
fingerprinting technique based on representation mixing and a novel loss. We
validate its capability of tracing the provenance of GAN-generated images
invariant to the semantic content of the image and also robust to
perturbations. We show our approach improves significantly from existing GAN
fingerprinting works on both semantic generalization and robustness. Data and
code are available at https://github.com/TuBui/image_attribution.",1,1,0,1,0,0,0.221537,8.0,0.62575,65
746c8f10-3bfd-4bb1-a4f6-b885c60be706,Distributed Control using Reinforcement Learning with Temporal-Logic-Based Reward Shaping,2,0.0593398,0.18831,"We present a computational framework for synthesis of distributed control
strategies for a heterogeneous team of robots in a partially observable
environment. The goal is to cooperatively satisfy specifications given as
Truncated Linear Temporal Logic (TLTL) formulas. Our approach formulates the
synthesis problem as a stochastic game and employs a policy graph method to
find a control strategy with memory for each agent. We construct the stochastic
game on the product between the team transition system and a finite state
automaton (FSA) that tracks the satisfaction of the TLTL formula. We use the
quantitative semantics of TLTL as the reward of the game, and further reshape
it using the FSA to guide and accelerate the learning process. Simulation
results demonstrate the efficacy of the proposed solution under demanding task
specifications and the effectiveness of reward shaping in significantly
accelerating the speed of learning.",0,0,0,0,0,0,0.912756,11.0,0.934743,20
69640d9f-25c8-49ab-a01b-c88e912e8191,Gaussian Multi-head Attention for Simultaneous Machine Translation,16,0.138074,0.607292,"Simultaneous machine translation (SiMT) outputs translation while receiving
the streaming source inputs, and hence needs a policy to determine where to
start translating. The alignment between target and source words often implies
the most informative source word for each target word, and hence provides the
unified control over translation quality and latency, but unfortunately the
existing SiMT methods do not explicitly model the alignment to perform the
control. In this paper, we propose Gaussian Multi-head Attention (GMA) to
develop a new SiMT policy by modeling alignment and translation in a unified
manner. For SiMT policy, GMA models the aligned source position of each target
word, and accordingly waits until its aligned position to start translating. To
integrate the learning of alignment into the translation model, a Gaussian
distribution centered on predicted aligned position is introduced as an
alignment-related prior, which cooperates with translation-related soft
attention to determine the final attention. Experiments on En-Vi and De-En
tasks show that our method outperforms strong baselines on the trade-off
between translation and latency.",1,0,0,0,0,0,0.212511,6.0,0.493146,43
41269266-3e04-4e89-9134-12d7b9411edf,Socio-cognitive Optimization of Time-delay Control Problems using Evolutionary Metaheuristics,4,0.0781782,0.261135,"Metaheuristics are universal optimization algorithms which should be used for
solving difficult problems, unsolvable by classic approaches. In this paper we
aim at constructing novel socio-cognitive metaheuristic based on castes, and
apply several versions of this algorithm to optimization of time-delay system
model. Besides giving the background and the details of the proposed algorithms
we apply them to optimization of selected variants of the problem and discuss
the results.",0,0,0,0,0,0,0.00849269,42.0,0.848245,32
a8cb1d09-e993-4af1-8243-b9e044bc66b3,Formal Mathematics Statement Curriculum Learning,68,0.743172,0.999974,"We explore the use of expert iteration in the context of language modeling
applied to formal mathematics. We show that at same compute budget, expert
iteration, by which we mean proof search interleaved with learning,
dramatically outperforms proof search only. We also observe that when applied
to a collection of formal statements of sufficiently varied difficulty, expert
iteration is capable of finding and solving a curriculum of increasingly
difficult problems, without the need for associated ground-truth proofs.
Finally, by applying this expert iteration to a manually curated set of problem
statements, we achieve state-of-the-art on the miniF2F benchmark, automatically
solving multiple challenging problems drawn from high school olympiads.",0,0,0,0,1,0,0.836403,5.0,0.796816,46
4ea0000c-8d3a-4f1f-9d6e-2adf654784c6,Pointillism: Accurate 3D bounding box estimation with multi-radars,44,0.696553,0.933622,"Autonomous perception requires high-quality environment sensing in the form
of 3D bounding boxes of dynamic objects. The primary sensors used in automotive
systems are light-based cameras and LiDARs. However, they are known to fail in
adverse weather conditions. Radars can potentially solve this problem as they
are barely affected by adverse weather conditions. However, specular
reflections of wireless signals cause poor performance of radar point clouds.
We introduce Pointillism, a system that combines data from multiple spatially
separated radars with an optimal separation to mitigate these problems. We
introduce a novel concept of Cross Potential Point Clouds, which uses the
spatial diversity induced by multiple radars and solves the problem of noise
and sparsity in radar point clouds. Furthermore, we present the design of
RP-net, a novel deep learning architecture, designed explicitly for radar's
sparse data distribution, to enable accurate 3D bounding box estimation. The
spatial techniques designed and proposed in this paper are fundamental to
radars point cloud distribution and would benefit other radar sensing
applications.",1,1,0,0,0,0,0.60032,7.0,0.757716,64
72d9a907-47ef-46d4-8927-5ccff925b264,Low Resource Style Transfer via Domain Adaptive Meta Learning,6,0.111883,0.164484,"Text style transfer (TST) without parallel data has achieved some practical
success. However, most of the existing unsupervised text style transfer methods
suffer from (i) requiring massive amounts of non-parallel data to guide
transferring different text styles. (ii) colossal performance degradation when
fine-tuning the model in new domains. In this work, we propose DAML-ATM (Domain
Adaptive Meta-Learning with Adversarial Transfer Model), which consists of two
parts: DAML and ATM. DAML is a domain adaptive meta-learning approach to learn
general knowledge in multiple heterogeneous source domains, capable of adapting
to new unseen domains with a small amount of data. Moreover, we propose a new
unsupervised TST approach Adversarial Transfer Model (ATM), composed of a
sequence-to-sequence pre-trained language model and uses adversarial style
training for better content preservation and style transfer. Results on
multi-domain datasets demonstrate that our approach generalizes well on unseen
low-resource domains, achieving state-of-the-art results against ten strong
baselines.",0,1,0,0,1,0,0.59114,7.0,0.754134,62
43547cae-131d-4b33-beba-f5ea758e9312,"Control Globally, Understand Locally: A Global-to-Local Hierarchical Graph Network for Emotional Support Conversation",45,0.538726,0.99951,"Emotional support conversation aims at reducing the emotional distress of the
help-seeker, which is a new and challenging task. It requires the system to
explore the cause of help-seeker's emotional distress and understand their
psychological intention to provide supportive responses. However, existing
methods mainly focus on the sequential contextual information, ignoring the
hierarchical relationships with the global cause and local psychological
intention behind conversations, thus leads to a weak ability of emotional
support. In this paper, we propose a Global-to-Local Hierarchical Graph Network
to capture the multi-source information (global cause, local intentions and
dialog history) and model hierarchical relationships between them, which
consists of a multi-source encoder, a hierarchical graph reasoner, and a
global-guide decoder. Furthermore, a novel training objective is designed to
monitor semantic information of the global cause. Experimental results on the
emotional support conversation dataset, ESConv, confirm that the proposed GLHG
has achieved the state-of-the-art performance on the automatic and human
evaluations. The code will be released in here
\footnote{\small{~https://github.com/pengwei-iie/GLHG}}.",1,1,1,0,1,0,0.677796,6.0,0.752508,35
d2c31b52-e0f4-4dc0-84a1-42b477d83d7a,Teaching language models to support answers with verified quotes,160,0.984632,1.0,"Recent large language models often answer factual questions correctly. But
users can't trust any given claim a model makes without fact-checking, because
language models can hallucinate convincing nonsense. In this work we use
reinforcement learning from human preferences (RLHP) to train ""open-book"" QA
models that generate answers whilst also citing specific evidence for their
claims, which aids in the appraisal of correctness. Supporting evidence is
drawn from multiple documents found via a search engine, or from a single
user-provided document. Our 280 billion parameter model, GopherCite, is able to
produce answers with high quality supporting evidence and abstain from
answering when unsure. We measure the performance of GopherCite by conducting
human evaluation of answers to questions in a subset of the NaturalQuestions
and ELI5 datasets. The model's response is found to be high-quality 80\% of the
time on this Natural Questions subset, and 67\% of the time on the ELI5 subset.
Abstaining from the third of questions for which it is most unsure improves
performance to 90\% and 80\% respectively, approaching human baselines.
However, analysis on the adversarial TruthfulQA dataset shows why citation is
only one part of an overall strategy for safety and trustworthiness: not all
claims supported by evidence are true.",0,0,0,0,0,0,0.97742,5.0,0.944623,72
3b4a7c63-ea68-4c1e-b23f-6088fee1824e,Low-resource Neural Machine Translation with Cross-modal Alignment,6,0.24201,0.279801,"How to achieve neural machine translation with limited parallel data?
Existing techniques often rely on large-scale monolingual corpora, which is
impractical for some low-resource languages. In this paper, we turn to connect
several low-resource languages to a particular high-resource one by additional
visual modality. Specifically, we propose a cross-modal contrastive learning
method to learn a shared space for all languages, where both a coarse-grained
sentence-level objective and a fine-grained token-level one are introduced.
Experimental results and further analysis show that our method can effectively
learn the cross-modal and cross-lingual alignment with a small amount of
image-text pairs and achieves significant improvements over the text-only
baseline under both zero-shot and few-shot scenarios.",0,1,0,0,0,0,0.900271,8.0,0.903222,57
6904dc81-193d-49e0-8d8c-11bf22372a82,MotionSC: Data Set and Network for Real-Time Semantic Mapping in Dynamic Environments,15,0.113484,0.484447,"This work addresses a gap in semantic scene completion (SSC) data by creating
a novel outdoor data set with accurate and complete dynamic scenes. Our data
set is formed from randomly sampled views of the world at each time step, which
supervises generalizability to complete scenes without occlusions or traces. We
create SSC baselines from state-of-the-art open source networks and construct a
benchmark real-time dense local semantic mapping algorithm, MotionSC, by
leveraging recent 3D deep learning architectures to enhance SSC with temporal
information. Our network shows that the proposed data set can quantify and
supervise accurate scene completion in the presence of dynamic objects, which
can lead to the development of improved dynamic mapping algorithms. All
software is available at https://github.com/UMich-CURLY/3DMapping.",1,1,1,1,0,0,0.265228,8.0,0.651692,62
0e0c6c70-c6b0-48ac-aca3-709c8f14e927,Efficient universal shuffle attack for visual object tracking,26,0.134021,0.622087,"Recently, adversarial attacks have been applied in visual object tracking to
deceive deep trackers by injecting imperceptible perturbations into video
frames. However, previous work only generates the video-specific perturbations,
which restricts its application scenarios. In addition, existing attacks are
difficult to implement in reality due to the real-time of tracking and the
re-initialization mechanism. To address these issues, we propose an offline
universal adversarial attack called Efficient Universal Shuffle Attack. It
takes only one perturbation to cause the tracker malfunction on all videos. To
improve the computational efficiency and attack performance, we propose a
greedy gradient strategy and a triple loss to efficiently capture and attack
model-specific feature representations through the gradients. Experimental
results show that EUSA can significantly reduce the performance of
state-of-the-art trackers on OTB2015 and VOT2018.",0,1,0,0,0,0,0.453155,8.0,0.735713,18
910a0c8e-b3f0-4292-80a4-fa5cb3e494a1,Overview of the HASOC Subtrack at FIRE 2022: Offensive Language Identification in Marathi,12,0.258406,0.897191,"The widespread of offensive content online has become a reason for great
concern in recent years, motivating researchers to develop robust systems
capable of identifying such content automatically. With the goal of carrying
out a fair evaluation of these systems, several international competitions have
been organized, providing the community with important benchmark data and
evaluation methods for various languages. Organized since 2019, the HASOC (Hate
Speech and Offensive Content Identification) shared task is one of these
initiatives. In its fourth iteration, HASOC 2022 included three subtracks for
English, Hindi, and Marathi. In this paper, we report the results of the HASOC
2022 Marathi subtrack which provided participants with a dataset containing
data from Twitter manually annotated using the popular OLID taxonomy. The
Marathi track featured three additional subtracks, each corresponding to one
level of the taxonomy: Task A - offensive content identification (offensive vs.
non-offensive); Task B - categorization of offensive types (targeted vs.
untargeted), and Task C - offensive target identification (individual vs. group
vs. others). Overall, 59 runs were submitted by 10 teams. The best systems
obtained an F1 of 0.9745 for Subtrack 3A, an F1 of 0.9207 for Subtrack 3B, and
F1 of 0.9607 for Subtrack 3C. The best performing algorithms were a mixture of
traditional and deep learning approaches.",0,1,0,1,0,0,0.480666,7.0,0.70968,66
d1a30245-3de5-4fe6-8bac-6233882131c7,Fine-grained Category Discovery under Coarse-grained supervision with Hierarchical Weighted Self-contrastive Learning,10,0.111666,0.798809,"Novel category discovery aims at adapting models trained on known categories
to novel categories. Previous works only focus on the scenario where known and
novel categories are of the same granularity. In this paper, we investigate a
new practical scenario called Fine-grained Category Discovery under
Coarse-grained supervision (FCDC). FCDC aims at discovering fine-grained
categories with only coarse-grained labeled data, which can adapt models to
categories of different granularity from known ones and reduce significant
labeling cost. It is also a challenging task since supervised training on
coarse-grained categories tends to focus on inter-class distance (distance
between coarse-grained classes) but ignore intra-class distance (distance
between fine-grained sub-classes) which is essential for separating
fine-grained categories. Considering most current methods cannot transfer
knowledge from coarse-grained level to fine-grained level, we propose a
hierarchical weighted self-contrastive network by building a novel weighted
self-contrastive module and combining it with supervised learning in a
hierarchical manner. Extensive experiments on public datasets show both
effectiveness and efficiency of our model over compared methods. Code and data
are available at https://github.com/Lackel/Hierarchical_Weighted_SCL.",1,1,1,0,0,0,0.860938,6.0,0.845006,56
e5219e19-e64a-41fe-8fa2-8f0b29894cb4,Lighting (In)consistency of Paint by Text,17,0.889989,0.389544,"Whereas generative adversarial networks are capable of synthesizing highly
realistic images of faces, cats, landscapes, or almost any other single
category, paint-by-text synthesis engines can -- from a single text prompt --
synthesize realistic images of seemingly endless categories with arbitrary
configurations and combinations. This powerful technology poses new challenges
to the photo-forensic community. Motivated by the fact that paint by text is
not based on explicit geometric or physical models, and the human visual
system's general insensitivity to lighting inconsistencies, we provide an
initial exploration of the lighting consistency of DALL-E-2 synthesized images
to determine if physics-based forensic analyses will prove fruitful in
detecting this new breed of synthetic media.",0,0,0,0,0,0,0.904382,24.0,0.968494,22
ca15bd4c-b404-4cf1-a08c-dff6be94e0c4,Saliency-based Multiple Region of Interest Detection from a Single 360° image,1,0.0693794,0.249849,"360{\deg} images are informative -- it contains omnidirectional visual
information around the camera. However, the areas that cover a 360{\deg} image
is much larger than the human's field of view, therefore important information
in different view directions is easily overlooked. To tackle this issue, we
propose a method for predicting the optimal set of Region of Interest (RoI)
from a single 360{\deg} image using the visual saliency as a clue. To deal with
the scarce, strongly biased training data of existing single 360{\deg} image
saliency prediction dataset, we also propose a data augmentation method based
on the spherical random data rotation. From the predicted saliency map and
redundant candidate regions, we obtain the optimal set of RoIs considering both
the saliency within a region and the Interaction-Over-Union (IoU) between
regions. We conduct the subjective evaluation to show that the proposed method
can select regions that properly summarize the input 360{\deg} image.",0,1,0,0,0,0,0.707677,11.0,0.872499,34
574a22bf-52d9-49f1-950e-3c90a88e958e,Diagnostics for Deep Neural Networks with Automated Copy/Paste Attacks,9,0.0317409,0.57359,"This paper considers the problem of helping humans exercise scalable
oversight over deep neural networks (DNNs). Adversarial examples can be useful
by helping to reveal weaknesses in DNNs, but they can be difficult to interpret
or draw actionable conclusions from. Some previous works have proposed using
human-interpretable adversarial attacks including copy/paste attacks in which
one natural image pasted into another causes an unexpected misclassification.
We build on these with two contributions. First, we introduce Search for
Natural Adversarial Features Using Embeddings (SNAFUE) which offers a fully
automated method for finding copy/paste attacks. Second, we use SNAFUE to red
team an ImageNet classifier. We reproduce copy/paste attacks from previous
works and find hundreds of other easily-describable vulnerabilities, all
without a human in the loop. Code is available at
https://github.com/thestephencasper/snafue",1,1,0,0,0,0,0.154098,8.0,0.575363,73
1d7f5bdd-0944-438b-ae93-af5fd2bad0b3,Deep Neural Networks as Complex Networks,3,0.0107731,0.348481,"Deep Neural Networks are, from a physical perspective, graphs whose `links`
and `vertices` iteratively process data and solve tasks sub-optimally. We use
Complex Network Theory (CNT) to represents Deep Neural Networks (DNNs) as
directed weighted graphs: within this framework, we introduce metrics to study
DNNs as dynamical systems, with a granularity that spans from weights to
layers, including neurons. CNT discriminates networks that differ in the number
of parameters and neurons, the type of hidden layers and activations, and the
objective task. We further show that our metrics discriminate low vs. high
performing networks. CNT is a comprehensive method to reason about DNNs and a
complementary approach to explain a model's behavior that is physically
grounded to networks theory and goes beyond the well-studied input-output
relation.",0,0,0,0,0,0,0.0196837,11.0,0.497502,16
1248984d-4a1a-43b2-a7fc-86b2d730adec,Video-based Cross-modal Auxiliary Network for Multimodal Sentiment Analysis,6,0.0899742,0.365504,"Multimodal sentiment analysis has a wide range of applications due to its
information complementarity in multimodal interactions. Previous works focus
more on investigating efficient joint representations, but they rarely consider
the insufficient unimodal features extraction and data redundancy of multimodal
fusion. In this paper, a Video-based Cross-modal Auxiliary Network (VCAN) is
proposed, which is comprised of an audio features map module and a cross-modal
selection module. The first module is designed to substantially increase
feature diversity in audio feature extraction, aiming to improve classification
accuracy by providing more comprehensive acoustic representations. To empower
the model to handle redundant visual features, the second module is addressed
to efficiently filter the redundant visual frames during integrating
audiovisual data. Moreover, a classifier group consisting of several image
classification networks is introduced to predict sentiment polarities and
emotion categories. Extensive experimental results on RAVDESS, CMU-MOSI, and
CMU-MOSEI benchmarks indicate that VCAN is significantly superior to the
state-of-the-art methods for improving the classification accuracy of
multimodal sentiment analysis.",0,1,0,0,1,0,0.148644,8.0,0.570469,58
04f5d57b-d871-4d2c-a471-06abc30adaa8,Counterfactual harm,16,0.168806,0.679612,"To act safely and ethically in the real world, agents must be able to reason
about harm and avoid harmful actions. However, to date there is no statistical
method for measuring harm and factoring it into algorithmic decisions. In this
paper we propose the first formal definition of harm and benefit using causal
models. We show that any factual definition of harm must violate basic
intuitions in certain scenarios, and show that standard machine learning
algorithms that cannot perform counterfactual reasoning are guaranteed to
pursue harmful policies following distributional shifts. We use our definition
of harm to devise a framework for harm-averse decision making using
counterfactual objective functions. We demonstrate this framework on the
problem of identifying optimal drug doses using a dose-response model learned
from randomized control trial data. We find that the standard method of
selecting doses using treatment effects results in unnecessarily harmful doses,
while our counterfactual approach allows us to identify doses that are
significantly less harmful without sacrificing efficacy.",0,0,1,0,0,0,0.288293,9.0,0.701335,128
